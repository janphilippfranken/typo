INFO 01-12 15:54:18 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mixtral-8x7B-v0.1', tokenizer='mistralai/Mixtral-8x7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1', load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
INFO 01-12 15:54:38 llm_engine.py:275] # GPU blocks: 21894, # CPU blocks: 4096
INFO 01-12 15:54:40 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-12 15:54:40 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=1841541)[0m INFO 01-12 15:54:40 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=1841541)[0m INFO 01-12 15:54:40 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 01-12 15:55:14 model_runner.py:547] Graph capturing finished in 35 secs.
[36m(RayWorkerVllm pid=1841541)[0m INFO 01-12 15:55:14 model_runner.py:547] Graph capturing finished in 35 secs.
NAME
    label.py

SYNOPSIS
    label.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     np
       NumPy =====

     List
       A generic version of list.

     Optional
       Optional type.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     re
       Support for regular expressions (RE).

     EVALUATION_PROMPTS

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     VLLMInferenceModel
       Wrapper for running inference with VLLM.

     load_from_disk
       Loads a dataset that was previously saved using [`~Dataset.save_to_disk`] from a dataset directory, or from a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.

     Dataset
       A Dataset backed by an Arrow table.

     remove_final_answer
       Remove final assistant answer which is our inference target.

     build_eval_prompt_mcq

     build_eval_prompt_log_probs
       Format a list of prompts and answers into a list of dialogues for mistral/llama base models.

     run_eval_log_probs
       Run log probs eval.

     main

VALUES
    VALUE is one of the following:

     BOS_TOKEN

     EOS_TOKEN

     B_INST

     E_INST

     B_SYS

     E_SYS
