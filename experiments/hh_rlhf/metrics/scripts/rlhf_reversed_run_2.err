Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:27,  1.55s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:02<00:24,  1.46s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:04<00:22,  1.43s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:05<00:21,  1.41s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:07<00:19,  1.40s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:08<00:18,  1.40s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:09<00:16,  1.40s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:11<00:15,  1.39s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:12<00:13,  1.39s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:14<00:12,  1.38s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:15<00:11,  1.39s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:16<00:09,  1.38s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:18<00:08,  1.38s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:19<00:06,  1.37s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:20<00:05,  1.38s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:22<00:04,  1.38s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:23<00:02,  1.37s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:25<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:26<00:00,  1.34s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:26<00:00,  1.38s/it]
  0%|          | 0/500 [00:00<?, ?it/s]/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  0%|          | 1/500 [00:26<3:38:59, 26.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  0%|          | 2/500 [00:51<3:32:32, 25.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  1%|          | 3/500 [01:16<3:30:25, 25.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  1%|          | 4/500 [01:42<3:32:00, 25.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  1%|          | 5/500 [02:07<3:29:03, 25.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  1%|          | 6/500 [02:32<3:28:00, 25.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  1%|▏         | 7/500 [02:56<3:24:04, 24.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  2%|▏         | 8/500 [03:21<3:23:43, 24.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  2%|▏         | 9/500 [03:47<3:25:32, 25.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  2%|▏         | 10/500 [04:12<3:26:05, 25.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  2%|▏         | 11/500 [04:37<3:24:07, 25.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  2%|▏         | 12/500 [05:01<3:22:50, 24.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  3%|▎         | 13/500 [05:26<3:21:23, 24.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  3%|▎         | 14/500 [05:51<3:21:32, 24.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  3%|▎         | 15/500 [06:17<3:23:15, 25.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  3%|▎         | 16/500 [06:40<3:17:55, 24.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  3%|▎         | 17/500 [07:06<3:21:33, 25.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  4%|▎         | 18/500 [07:31<3:22:11, 25.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  4%|▍         | 19/500 [07:56<3:21:02, 25.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
  4%|▍         | 20/500 [08:23<3:23:54, 25.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
