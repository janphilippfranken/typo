model_type: huggingface
name: mixtral_7b_peft_dpo_cai_data_synthetic

model_config:
  pretrained_model_name_or_path: "/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf_synthetic/merged/"
  model_cache_dir: "/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf_synthetic/merged/"
  tokenizer_cache_dir: "/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf_synthetic/merged/"
  load_in_4bit: true
  device_map: cuda
  torch_dtype: float16
  attn_implementation: null

completion_config:
  temperature: 0.7
  top_p: 0.9
  max_new_tokens: 20
  num_return_sequences: 2