hydra:
  run:
    dir: outputs

model_type: huggingface
name: mistral_7b_instruct

model_config:
  pretrained_model_name_or_path: "mistralai/Mixtral-8x7B-Instruct-v0.1"
  load_in_4bit: true
  device_map: "cuda:0"
  torch_dtype: "float16"
  model_cache_dir: "/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-Instruct-v0.1"
  tokenizer_cache_dir: "/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-Instruct-v0.1"
  attn_implementation: "flash_attention_2"

completion_config:
  do_sample: false
  temperature: 0.1
  top_p: 0.5
  max_new_tokens: 100
  num_return_sequences: 1