Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.58s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.80s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it]
  0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 208.77 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 203.82 examples/s]
  1%|          | 1/100 [02:08<3:31:53, 128.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1677.05 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1627.02 examples/s]
  2%|▏         | 2/100 [03:51<3:05:06, 113.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2616.21 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2543.24 examples/s]
  3%|▎         | 3/100 [05:54<3:10:54, 118.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2638.42 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2564.70 examples/s]
  4%|▍         | 4/100 [07:51<3:07:57, 117.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2477.00 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2412.88 examples/s]
  5%|▌         | 5/100 [10:00<3:12:18, 121.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2506.91 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2439.54 examples/s]
  6%|▌         | 6/100 [12:05<3:12:41, 122.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2295.73 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2240.91 examples/s]
  7%|▋         | 7/100 [14:46<3:29:41, 135.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2442.24 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2380.02 examples/s]
  8%|▊         | 8/100 [16:45<3:19:31, 130.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2544.16 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2475.25 examples/s]
  9%|▉         | 9/100 [18:51<3:15:22, 128.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2205.44 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2156.57 examples/s]
 10%|█         | 10/100 [20:46<3:06:41, 124.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2097.05 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2019.89 examples/s]
 11%|█         | 11/100 [23:00<3:09:03, 127.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (0/1 shards): 100%|██████████| 10/10 [00:00<00:00, 94.69 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 94.69 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 93.09 examples/s]
 12%|█▏        | 12/100 [24:54<3:00:41, 123.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2173.10 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2123.38 examples/s]
 13%|█▎        | 13/100 [26:57<2:58:42, 123.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2268.17 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2216.86 examples/s]
 14%|█▍        | 14/100 [29:00<2:56:33, 123.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2315.89 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2263.52 examples/s]
 15%|█▌        | 15/100 [31:00<2:53:02, 122.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2112.68 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2063.42 examples/s]
 16%|█▌        | 16/100 [32:57<2:49:01, 120.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2463.33 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2365.79 examples/s]
 17%|█▋        | 17/100 [35:02<2:48:41, 121.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2465.50 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2401.41 examples/s]
 18%|█▊        | 18/100 [37:13<2:50:14, 124.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2299.00 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2244.62 examples/s]
 19%|█▉        | 19/100 [39:26<2:51:48, 127.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2112.25 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2064.23 examples/s]
 20%|██        | 20/100 [41:45<2:54:14, 130.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2466.08 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2406.51 examples/s]
 21%|██        | 21/100 [44:18<3:00:49, 137.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2308.62 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2248.23 examples/s]
 22%|██▏       | 22/100 [46:26<2:55:00, 134.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2356.22 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2264.01 examples/s]
 23%|██▎       | 23/100 [48:36<2:50:59, 133.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1748.72 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1699.54 examples/s]
 24%|██▍       | 24/100 [50:36<2:43:46, 129.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1993.02 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1923.11 examples/s]
 25%|██▌       | 25/100 [52:37<2:38:36, 126.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1875.05 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1839.45 examples/s]
 26%|██▌       | 26/100 [54:30<2:31:23, 122.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1874.38 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1819.58 examples/s]
 27%|██▋       | 27/100 [56:25<2:26:24, 120.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2106.63 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2063.42 examples/s]
 28%|██▊       | 28/100 [58:34<2:27:26, 122.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1966.94 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1925.32 examples/s]
 29%|██▉       | 29/100 [1:00:43<2:27:41, 124.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2164.58 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2116.52 examples/s]
 30%|███       | 30/100 [1:03:36<2:42:28, 139.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
 30%|███       | 30/100 [1:05:45<2:33:26, 131.52s/it]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/main.py", line 228, in main
    torch.stack(
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument tensors in method wrapper_CUDA_cat)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
