
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:10<00:10, 10.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.26s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.53s/it]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:50<00:50, 50.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 33.91s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [01:12<00:00, 36.40s/it]

  0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 174.49 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 170.91 examples/s]

  1%|          | 1/100 [01:31<2:31:39, 91.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 3101.38 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2991.66 examples/s]

  2%|▏         | 2/100 [03:23<2:49:24, 103.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 643.95 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 624.36 examples/s]

  3%|▎         | 3/100 [06:23<3:43:42, 138.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2926.33 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2828.07 examples/s]

  4%|▍         | 4/100 [09:25<4:08:48, 155.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 338.32 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 324.67 examples/s]

  5%|▌         | 5/100 [12:43<4:30:30, 170.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 278.44 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 267.78 examples/s]

  6%|▌         | 6/100 [15:40<4:30:54, 172.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 277.96 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 269.27 examples/s]

  7%|▋         | 7/100 [19:00<4:41:54, 181.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 262.18 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 252.86 examples/s]

  8%|▊         | 8/100 [22:48<5:01:21, 196.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 243.12 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 235.87 examples/s]

  9%|▉         | 9/100 [26:28<5:09:12, 203.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 269.56 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 260.30 examples/s]

 10%|█         | 10/100 [30:31<5:23:56, 215.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 308.67 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 295.47 examples/s]

 11%|█         | 11/100 [34:14<5:23:39, 218.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 311.93 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 298.35 examples/s]

 12%|█▏        | 12/100 [38:40<5:40:59, 232.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 243.16 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 229.44 examples/s]

 13%|█▎        | 13/100 [42:23<5:33:05, 229.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 302.39 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 290.14 examples/s]

 14%|█▍        | 14/100 [46:11<5:28:31, 229.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 254.38 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 247.36 examples/s]

 15%|█▌        | 15/100 [49:23<5:09:04, 218.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 218.20 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 212.40 examples/s]

 16%|█▌        | 16/100 [53:20<5:13:19, 223.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 267.15 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 258.97 examples/s]

 17%|█▋        | 17/100 [58:04<5:34:26, 241.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 262.47 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 253.25 examples/s]

 18%|█▊        | 18/100 [1:02:53<5:49:51, 256.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 283.11 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 273.12 examples/s]

 19%|█▉        | 19/100 [1:07:34<5:55:40, 263.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 339.75 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 325.98 examples/s]

 20%|██        | 20/100 [1:11:42<5:45:02, 258.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 414.40 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 387.94 examples/s]

 21%|██        | 21/100 [1:15:29<5:28:07, 249.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 236.71 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 229.49 examples/s]

 22%|██▏       | 22/100 [1:19:29<5:20:36, 246.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 354.86 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 341.17 examples/s]

 23%|██▎       | 23/100 [1:23:56<5:24:20, 252.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 301.29 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 289.15 examples/s]

 24%|██▍       | 24/100 [1:28:17<5:23:07, 255.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 292.65 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 282.67 examples/s]

 25%|██▌       | 25/100 [1:32:16<5:13:03, 250.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 272.62 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 263.25 examples/s]

 26%|██▌       | 26/100 [1:35:55<4:57:03, 240.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 287.25 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 276.72 examples/s]

 27%|██▋       | 27/100 [1:40:26<5:04:15, 250.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 231.85 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 224.58 examples/s]

 28%|██▊       | 28/100 [1:45:24<5:17:11, 264.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 218.88 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 210.94 examples/s]

 29%|██▉       | 29/100 [1:49:42<5:10:34, 262.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 296.01 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 284.30 examples/s]

 30%|███       | 30/100 [1:53:54<5:02:36, 259.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 259.71 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 250.01 examples/s]

 31%|███       | 31/100 [1:58:14<4:58:27, 259.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 270.43 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 258.36 examples/s]

 32%|███▏      | 32/100 [2:02:21<4:49:52, 255.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 296.29 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 286.59 examples/s]

 33%|███▎      | 33/100 [2:06:06<4:35:21, 246.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 287.96 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 276.96 examples/s]

 34%|███▍      | 34/100 [2:10:41<4:40:34, 255.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 252.31 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 243.27 examples/s]

 35%|███▌      | 35/100 [2:15:10<4:40:55, 259.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 235.51 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 228.06 examples/s]

 36%|███▌      | 36/100 [2:19:03<4:28:04, 251.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 293.28 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 283.42 examples/s]

 37%|███▋      | 37/100 [2:23:13<4:23:35, 251.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 265.57 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 257.14 examples/s]

 38%|███▊      | 38/100 [2:28:04<4:31:37, 262.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 253.01 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 240.74 examples/s]

 39%|███▉      | 39/100 [2:32:19<4:24:46, 260.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 217.31 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 211.43 examples/s]

 40%|████      | 40/100 [2:36:10<4:11:39, 251.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 262.70 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 254.55 examples/s]

 41%|████      | 41/100 [2:40:11<4:04:24, 248.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 281.83 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 272.43 examples/s]

 42%|████▏     | 42/100 [2:44:41<4:06:17, 254.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 258.87 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 251.21 examples/s]

 43%|████▎     | 43/100 [2:48:51<4:00:46, 253.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 208.40 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 201.73 examples/s]

 44%|████▍     | 44/100 [2:53:10<3:58:16, 255.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 288.15 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 277.86 examples/s]

 45%|████▌     | 45/100 [2:57:40<3:57:59, 259.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 290.58 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 278.66 examples/s]

 46%|████▌     | 46/100 [3:02:06<3:55:20, 261.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 240.70 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 233.54 examples/s]

 47%|████▋     | 47/100 [3:06:09<3:45:58, 255.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 280.26 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 269.25 examples/s]

 48%|████▊     | 48/100 [3:10:20<3:40:28, 254.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 241.34 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 233.77 examples/s]

 49%|████▉     | 49/100 [3:14:03<3:28:14, 244.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 260.25 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 251.98 examples/s]

 50%|█████     | 50/100 [3:18:23<3:28:03, 249.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 260.53 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 244.89 examples/s]

 51%|█████     | 51/100 [3:22:21<3:20:55, 246.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 273.23 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 103.68 examples/s]

 52%|█████▏    | 52/100 [3:26:24<3:16:01, 245.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 267.67 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 142.83 examples/s]

 53%|█████▎    | 53/100 [3:29:42<3:01:00, 231.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 253.38 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 135.14 examples/s]

 54%|█████▍    | 54/100 [3:33:49<3:00:45, 235.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 255.75 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 247.26 examples/s]

 55%|█████▌    | 55/100 [3:37:42<2:56:16, 235.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 172.05 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 163.57 examples/s]

 56%|█████▌    | 56/100 [3:41:19<2:48:17, 229.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 125.67 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 119.35 examples/s]

 57%|█████▋    | 57/100 [3:44:55<2:41:36, 225.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 106.71 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 104.20 examples/s]

 58%|█████▊    | 58/100 [3:49:29<2:48:01, 240.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 142.87 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 139.67 examples/s]

 59%|█████▉    | 59/100 [3:54:02<2:50:49, 250.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 223.04 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 212.39 examples/s]

 60%|██████    | 60/100 [3:58:47<2:53:39, 260.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 286.23 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 274.98 examples/s]

 61%|██████    | 61/100 [4:03:09<2:49:41, 261.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 243.95 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 235.63 examples/s]

 62%|██████▏   | 62/100 [4:07:35<2:46:12, 262.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 223.45 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 217.29 examples/s]

 63%|██████▎   | 63/100 [4:11:39<2:38:26, 256.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 112.57 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 109.83 examples/s]

 64%|██████▍   | 64/100 [4:16:11<2:36:52, 261.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 272.06 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 264.21 examples/s]

 65%|██████▌   | 65/100 [4:20:19<2:30:03, 257.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 116.42 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 114.10 examples/s]

 66%|██████▌   | 66/100 [4:23:50<2:17:59, 243.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 109.37 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 107.06 examples/s]

 67%|██████▋   | 67/100 [4:28:00<2:15:03, 245.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 112.75 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 110.38 examples/s]

 68%|██████▊   | 68/100 [4:32:16<2:12:35, 248.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 141.18 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 133.74 examples/s]

 69%|██████▉   | 69/100 [4:36:18<2:07:22, 246.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 120.27 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 118.54 examples/s]

 70%|███████   | 70/100 [4:40:31<2:04:12, 248.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 125.44 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 123.32 examples/s]

 71%|███████   | 71/100 [4:45:32<2:07:42, 264.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 143.99 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 140.61 examples/s]

 72%|███████▏  | 72/100 [4:49:37<2:00:37, 258.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 268.02 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 257.02 examples/s]

 73%|███████▎  | 73/100 [4:53:49<1:55:27, 256.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 228.35 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 217.78 examples/s]

 74%|███████▍  | 74/100 [4:57:52<1:49:29, 252.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 134.54 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 132.35 examples/s]

 75%|███████▌  | 75/100 [5:02:44<1:50:07, 264.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 113.94 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 111.54 examples/s]

 76%|███████▌  | 76/100 [5:07:25<1:47:47, 269.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 253.55 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 242.13 examples/s]

 77%|███████▋  | 77/100 [5:11:30<1:40:24, 261.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 228.28 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 220.95 examples/s]

 78%|███████▊  | 78/100 [5:15:43<1:35:04, 259.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 173.53 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 167.29 examples/s]

 79%|███████▉  | 79/100 [5:20:13<1:31:52, 262.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 204.62 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 197.00 examples/s]

 80%|████████  | 80/100 [5:24:02<1:24:08, 252.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 124.73 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 122.61 examples/s]

 81%|████████  | 81/100 [5:28:52<1:23:28, 263.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 238.06 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 228.06 examples/s]

 82%|████████▏ | 82/100 [5:32:51<1:16:56, 256.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 128.26 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 126.22 examples/s]

 83%|████████▎ | 83/100 [5:36:56<1:11:39, 252.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 130.55 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 127.98 examples/s]

 84%|████████▍ | 84/100 [5:41:31<1:09:13, 259.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 136.64 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 133.30 examples/s]

 85%|████████▌ | 85/100 [5:45:26<1:03:00, 252.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 224.02 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 217.66 examples/s]

 86%|████████▌ | 86/100 [5:49:16<57:17, 245.51s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 134.00 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 131.98 examples/s]

 87%|████████▋ | 87/100 [5:53:44<54:40, 252.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 220.70 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 214.20 examples/s]

 88%|████████▊ | 88/100 [5:58:06<51:03, 255.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 225.02 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 215.09 examples/s]

 89%|████████▉ | 89/100 [6:02:06<45:55, 250.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 115.59 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 113.57 examples/s]

 90%|█████████ | 90/100 [6:06:18<41:51, 251.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.



Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 137.19 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 134.89 examples/s]

 91%|█████████ | 91/100 [6:12:05<41:57, 279.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 192.42 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 187.46 examples/s]

 92%|█████████▏| 92/100 [6:16:20<36:19, 272.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 113.82 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 110.44 examples/s]

 93%|█████████▎| 93/100 [6:20:14<30:25, 260.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 149.08 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 146.40 examples/s]

 94%|█████████▍| 94/100 [6:24:02<25:05, 250.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 204.35 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 198.75 examples/s]

 95%|█████████▌| 95/100 [6:28:12<20:54, 250.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 248.91 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 240.50 examples/s]

 96%|█████████▌| 96/100 [6:32:11<16:29, 247.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 229.78 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 219.32 examples/s]

 97%|█████████▋| 97/100 [6:36:38<12:39, 253.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 137.90 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 135.49 examples/s]

 98%|█████████▊| 98/100 [6:41:15<08:40, 260.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 212.72 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 205.70 examples/s]

 99%|█████████▉| 99/100 [6:45:18<04:15, 255.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.


Saving the dataset (0/1 shards):   0%|          | 0/10 [00:00<?, ? examples/s][A

Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 140.40 examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 137.79 examples/s]

100%|██████████| 100/100 [6:48:55<00:00, 243.75s/it]
100%|██████████| 100/100 [6:48:55<00:00, 245.36s/it]
ERROR: Cannot find key: sampler.run_id=1
Usage: sampler.py <group|command|value>
  available groups:      os | time | copy | random | logging | fire | hydra |
                         torch | pd | List | Tuple | Optional | Callable |
                         Dict | np | SEED_PRINCIPLES
  available commands:    tqdm | DictConfig | DataParallel | load_dataset |
                         Dataset | remove_final_answer | format_eval_prompt |
                         split_conversation_hh_rlhf | build_generation_prompt |
                         initialize_constitutions | format_responses |
                         get_eval_examples | extend_batches |
                         build_eval_prompt | run_generate | run_eval |
                         GPT4Agent | AsyncAzureChatLLM | HFInferenceModel | main
  available values:      BOS_TOKEN | EOS_TOKEN | B_INST | E_INST | B_SYS | E_SYS

For detailed information on this command, run:
  sampler.py --help
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/sampler.py", line 16, in <module>
    from helpers import *
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/helpers.py", line 18, in <module>
    from prompts import SEED_PRINCIPLES
  File "/sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/prompts.py", line 69
    3. Strictly adhere to the response format, especially for the 'Revised List of Preferences:' section."""",
                                                                                                            ^
SyntaxError: unterminated string literal (detected at line 69)
