Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:28,  1.61s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:25,  1.53s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:04<00:24,  1.50s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:06<00:22,  1.49s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:07<00:20,  1.48s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:08<00:19,  1.48s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:10<00:17,  1.47s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:11<00:16,  1.47s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:13<00:14,  1.47s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:14<00:13,  1.46s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:16<00:11,  1.46s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:17<00:10,  1.46s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:19<00:08,  1.46s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:20<00:07,  1.45s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:22<00:05,  1.46s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:23<00:04,  1.46s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:25<00:02,  1.47s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:26<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:27<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:27<00:00,  1.46s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.32s/it]
  0%|          | 0/200 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 189.46 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 184.43 examples/s]
  0%|          | 1/200 [02:12<7:18:18, 132.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 227.85 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 220.49 examples/s]
  1%|          | 2/200 [04:03<6:35:27, 119.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 214.24 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 204.33 examples/s]
  2%|▏         | 3/200 [05:42<6:02:40, 110.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 224.75 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 215.37 examples/s]
  2%|▏         | 4/200 [07:44<6:15:25, 114.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 231.73 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 224.40 examples/s]
  2%|▎         | 5/200 [10:05<6:44:20, 124.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 237.54 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 230.60 examples/s]
  3%|▎         | 6/200 [12:08<6:40:01, 123.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 235.74 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 228.10 examples/s]
  4%|▎         | 7/200 [14:00<6:25:55, 119.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 237.87 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 229.54 examples/s]
  4%|▍         | 8/200 [16:16<6:40:13, 125.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 223.43 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 217.16 examples/s]
  4%|▍         | 9/200 [18:34<6:51:35, 129.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 225.91 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 219.39 examples/s]
  5%|▌         | 10/200 [21:03<7:08:16, 135.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
