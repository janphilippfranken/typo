wandb_version: 1

model:
  desc: null
  value:
    model_type: huggingface
    name: mistral_7b_base
    model_config:
      pretrained_model_name_or_path: mistralai/Mistral-7B-v0.1
      cache_dir: /scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1
    tokenizer_config:
      pretrained_model_name_or_path: mistralai/Mistral-7B-v0.1
      cache_dir: /scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1
      model_max_length: 918
data:
  desc: null
  value:
    hh_rlhf_cai: ../label/synthetic-harmless-base
wandb:
  desc: null
  value:
    project: scai
    log_model: checkpoint
    name: hh_rlhf_cai_16_bit_mistral
validation_split_size:
  desc: null
  value: 0.025
dpo_beta:
  desc: null
  value: 0.1
max_prompt_length:
  desc: null
  value: 612
training_args:
  desc: null
  value:
    output_dir: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/16bit
    evaluation_strategy: steps
    eval_steps: 500
    per_device_train_batch_size: 4
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 4
    logging_steps: 1
    logging_strategy: steps
    lr_scheduler_type: linear
    lr_scheduler_warmup_ratio: 0.1
    learning_rate: 5.0e-07
    max_steps: 5000
    save_strategy: steps
    save_steps: 250
    save_total_limit: 20
    seed: 42
    report_to: wandb
    bf16: true
    remove_unused_columns: false
_wandb:
  desc: null
  value:
    python_version: 3.10.13
    cli_version: 0.16.0
    framework: huggingface
    huggingface_version: 4.36.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1706554908.246497
    t:
      1:
      - 1
      - 5
      - 11
      - 49
      - 50
      - 51
      - 53
      - 55
      - 71
      - 84
      - 98
      - 105
      2:
      - 1
      - 5
      - 11
      - 49
      - 50
      - 51
      - 53
      - 55
      - 71
      - 84
      - 98
      - 105
      3:
      - 13
      - 16
      - 23
      4: 3.10.13
      5: 0.16.0
      6: 4.36.2
      8:
      - 5
      13: linux-x86_64
