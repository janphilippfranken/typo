[2024-01-31 23:51:10,908][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_helpful_chosen
[2024-01-31 23:51:10,908][root][INFO] - Wandb name: sft_helpful_chosen
[2024-01-31 23:51:10,908][root][INFO] - Max seq length: 2048
[2024-01-31 23:51:10,908][root][INFO] - Devices: 4
[2024-01-31 23:51:10,908][root][INFO] - Data: {'type': 'human', 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'helpful-base', 'cache_dir': '/scr/jphilipp/scai/datasets/hh-rlhf'}, 'split': 'train'}
[2024-01-31 23:51:10,931][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_helpful_chosen
[2024-01-31 23:51:10,931][root][INFO] - Wandb name: sft_helpful_chosen
[2024-01-31 23:51:10,931][root][INFO] - Max seq length: 2048
[2024-01-31 23:51:10,931][root][INFO] - Devices: 4
[2024-01-31 23:51:10,931][root][INFO] - Data: {'type': 'human', 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'helpful-base', 'cache_dir': '/scr/jphilipp/scai/datasets/hh-rlhf'}, 'split': 'train'}
[2024-01-31 23:51:10,971][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_helpful_chosen
[2024-01-31 23:51:10,971][root][INFO] - Wandb name: sft_helpful_chosen
[2024-01-31 23:51:10,971][root][INFO] - Max seq length: 2048
[2024-01-31 23:51:10,971][root][INFO] - Devices: 4
[2024-01-31 23:51:10,971][root][INFO] - Data: {'type': 'human', 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'helpful-base', 'cache_dir': '/scr/jphilipp/scai/datasets/hh-rlhf'}, 'split': 'train'}
[2024-01-31 23:51:10,974][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_helpful_chosen
[2024-01-31 23:51:10,974][root][INFO] - Wandb name: sft_helpful_chosen
[2024-01-31 23:51:10,974][root][INFO] - Max seq length: 2048
[2024-01-31 23:51:10,974][root][INFO] - Devices: 4
[2024-01-31 23:51:10,974][root][INFO] - Data: {'type': 'human', 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'helpful-base', 'cache_dir': '/scr/jphilipp/scai/datasets/hh-rlhf'}, 'split': 'train'}
[2024-01-31 23:51:23,651] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-31 23:51:23,722] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-31 23:51:23,727] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-31 23:51:23,761] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-31 23:51:23,795] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-31 23:51:23,857] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-31 23:51:23,863] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-31 23:51:23,902] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-31 23:51:23,902] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-01-31 23:51:28,636][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: Hi, I want to learn to play horseshoes. Can you teach me?

Assistant: I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 horseshoes.

Human: Okay. What else is needed to play, and what are the rules?

Assistant:
[2024-01-31 23:51:28,637][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: How do yo dig a trench?

Assistant: Digging a trench is hard work.  You’ll need to find a place that’s relatively level and won’t be muddy or icy, and then you have to start scooping out the dirt.  You’ll want to pile it to one side as you go, to keep the sides straight.  You want to dig a little deeper than the width of the pipe you’re burying, so you can use a hand auger to smooth the corners.  You can use a pick to stab the dirt to break up the mud or hard ground.  You can also put some wood in the hole before you put the pipe in, that’ll help you scoop out the dirt.

Human: I didn't say anything about laying down pipes but still your advice on trench digging was useful. Thank you.

Assistant:
[2024-01-31 23:51:28,637][root][INFO] - A horseshoe is usually made out of metal and is about 3 to 3.5 inches long and around 1 inch thick. The horseshoe should also have a 2 inch by 3 inch flat at the bottom where the rubber meets the metal. We also need two stakes and six horseshoes.
[2024-01-31 23:51:28,637][root][INFO] - Not a problem.  I hope you didn’t mind that I guessed you were doing some sort of home project.
[2024-01-31 23:51:28,849][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: Hi, I want to learn to play horseshoes. Can you teach me?

Assistant: I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 horseshoes.

Human: Okay. What else is needed to play, and what are the rules?

Assistant:
[2024-01-31 23:51:28,850][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: How do yo dig a trench?

Assistant: Digging a trench is hard work.  You’ll need to find a place that’s relatively level and won’t be muddy or icy, and then you have to start scooping out the dirt.  You’ll want to pile it to one side as you go, to keep the sides straight.  You want to dig a little deeper than the width of the pipe you’re burying, so you can use a hand auger to smooth the corners.  You can use a pick to stab the dirt to break up the mud or hard ground.  You can also put some wood in the hole before you put the pipe in, that’ll help you scoop out the dirt.

Human: I didn't say anything about laying down pipes but still your advice on trench digging was useful. Thank you.

Assistant:
[2024-01-31 23:51:28,850][root][INFO] - A horseshoe is usually made out of metal and is about 3 to 3.5 inches long and around 1 inch thick. The horseshoe should also have a 2 inch by 3 inch flat at the bottom where the rubber meets the metal. We also need two stakes and six horseshoes.
[2024-01-31 23:51:28,850][root][INFO] - Not a problem.  I hope you didn’t mind that I guessed you were doing some sort of home project.
[2024-01-31 23:51:29,100][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: Hi, I want to learn to play horseshoes. Can you teach me?

Assistant: I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 horseshoes.

Human: Okay. What else is needed to play, and what are the rules?

Assistant:
[2024-01-31 23:51:29,101][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: How do yo dig a trench?

Assistant: Digging a trench is hard work.  You’ll need to find a place that’s relatively level and won’t be muddy or icy, and then you have to start scooping out the dirt.  You’ll want to pile it to one side as you go, to keep the sides straight.  You want to dig a little deeper than the width of the pipe you’re burying, so you can use a hand auger to smooth the corners.  You can use a pick to stab the dirt to break up the mud or hard ground.  You can also put some wood in the hole before you put the pipe in, that’ll help you scoop out the dirt.

Human: I didn't say anything about laying down pipes but still your advice on trench digging was useful. Thank you.

Assistant:
[2024-01-31 23:51:29,101][root][INFO] - A horseshoe is usually made out of metal and is about 3 to 3.5 inches long and around 1 inch thick. The horseshoe should also have a 2 inch by 3 inch flat at the bottom where the rubber meets the metal. We also need two stakes and six horseshoes.
[2024-01-31 23:51:29,101][root][INFO] - Not a problem.  I hope you didn’t mind that I guessed you were doing some sort of home project.
[2024-01-31 23:51:30,431][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: Hi, I want to learn to play horseshoes. Can you teach me?

Assistant: I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 horseshoes.

Human: Okay. What else is needed to play, and what are the rules?

Assistant:
[2024-01-31 23:51:30,431][root][INFO] - System: Write a response for the assistant that completes the human request.

Human: How do yo dig a trench?

Assistant: Digging a trench is hard work.  You’ll need to find a place that’s relatively level and won’t be muddy or icy, and then you have to start scooping out the dirt.  You’ll want to pile it to one side as you go, to keep the sides straight.  You want to dig a little deeper than the width of the pipe you’re burying, so you can use a hand auger to smooth the corners.  You can use a pick to stab the dirt to break up the mud or hard ground.  You can also put some wood in the hole before you put the pipe in, that’ll help you scoop out the dirt.

Human: I didn't say anything about laying down pipes but still your advice on trench digging was useful. Thank you.

Assistant:
[2024-01-31 23:51:30,431][root][INFO] - A horseshoe is usually made out of metal and is about 3 to 3.5 inches long and around 1 inch thick. The horseshoe should also have a 2 inch by 3 inch flat at the bottom where the rubber meets the metal. We also need two stakes and six horseshoes.
[2024-01-31 23:51:30,431][root][INFO] - Not a problem.  I hope you didn’t mind that I guessed you were doing some sort of home project.
[2024-01-31 23:52:09,397][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 41643
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 2192
    })
})
[2024-01-31 23:52:09,405][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-01-31 23:52:09,637][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 41643
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 2192
    })
})
[2024-01-31 23:52:10,258][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 41643
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 2192
    })
})
[2024-01-31 23:52:11,613][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 41643
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 2192
    })
})
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-01-31 23:52:21,330] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'transformers.cache_utils.DynamicCache'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.
{'loss': 1.7805, 'learning_rate': 7.692307692307694e-07, 'epoch': 0.02}
{'loss': 1.6635, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.03}
{'loss': 1.5853, 'learning_rate': 2.307692307692308e-06, 'epoch': 0.05}
{'loss': 1.57, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.06}
{'loss': 1.5426, 'learning_rate': 3.846153846153847e-06, 'epoch': 0.08}
{'loss': 1.5462, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.09}
{'loss': 1.558, 'learning_rate': 5.384615384615385e-06, 'epoch': 0.11}
{'loss': 1.584, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.12}
{'loss': 1.5684, 'learning_rate': 6.923076923076923e-06, 'epoch': 0.14}
{'loss': 1.5478, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.15}
{'loss': 1.5792, 'learning_rate': 8.461538461538462e-06, 'epoch': 0.17}
{'loss': 1.5522, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.18}
{'loss': 1.5441, 'learning_rate': 1e-05, 'epoch': 0.2}
{'loss': 1.5532, 'learning_rate': 1.076923076923077e-05, 'epoch': 0.22}
{'loss': 1.5686, 'learning_rate': 1.1538461538461538e-05, 'epoch': 0.23}
{'loss': 1.5786, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.25}
{'loss': 1.5657, 'learning_rate': 1.3076923076923078e-05, 'epoch': 0.26}
{'loss': 1.5788, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.28}
{'loss': 1.5773, 'learning_rate': 1.4615384615384615e-05, 'epoch': 0.29}
{'loss': 1.6053, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.31}
{'loss': 1.6148, 'learning_rate': 1.6153846153846154e-05, 'epoch': 0.32}
{'loss': 1.598, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.34}
{'loss': 1.5722, 'learning_rate': 1.7692307692307694e-05, 'epoch': 0.35}
{'loss': 1.6442, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.37}
{'loss': 1.6155, 'learning_rate': 1.923076923076923e-05, 'epoch': 0.38}
{'loss': 1.6076, 'learning_rate': 2e-05, 'epoch': 0.4}
{'loss': 1.617, 'learning_rate': 1.999909877856721e-05, 'epoch': 0.41}
{'loss': 1.6239, 'learning_rate': 1.9996395276708856e-05, 'epoch': 0.43}
{'loss': 1.554, 'learning_rate': 1.9991889981715696e-05, 'epoch': 0.45}
{'loss': 1.6278, 'learning_rate': 1.9985583705641418e-05, 'epoch': 0.46}
{'loss': 1.6151, 'learning_rate': 1.9977477585156252e-05, 'epoch': 0.48}
{'loss': 1.576, 'learning_rate': 1.9967573081342103e-05, 'epoch': 0.49}
{'loss': 1.6014, 'learning_rate': 1.9955871979429188e-05, 'epoch': 0.51}
{'loss': 1.5633, 'learning_rate': 1.9942376388474282e-05, 'epoch': 0.52}
{'loss': 1.5961, 'learning_rate': 1.992708874098054e-05, 'epoch': 0.54}
{'loss': 1.559, 'learning_rate': 1.9910011792459086e-05, 'epoch': 0.55}
{'loss': 1.5675, 'learning_rate': 1.989114862093232e-05, 'epoch': 0.57}
{'loss': 1.6153, 'learning_rate': 1.9870502626379127e-05, 'epoch': 0.58}
{'loss': 1.58, 'learning_rate': 1.9848077530122083e-05, 'epoch': 0.6}
{'loss': 1.5278, 'learning_rate': 1.9823877374156647e-05, 'epoch': 0.61}
{'loss': 1.5794, 'learning_rate': 1.979790652042268e-05, 'epoch': 0.63}
{'loss': 1.531, 'learning_rate': 1.977016965001817e-05, 'epoch': 0.65}
{'loss': 1.5338, 'learning_rate': 1.9740671762355548e-05, 'epoch': 0.66}
{'loss': 1.5317, 'learning_rate': 1.9709418174260523e-05, 'epoch': 0.68}
{'loss': 1.5083, 'learning_rate': 1.9676414519013782e-05, 'epoch': 0.69}
{'loss': 1.5117, 'learning_rate': 1.9641666745335626e-05, 'epoch': 0.71}
{'loss': 1.4988, 'learning_rate': 1.9605181116313725e-05, 'epoch': 0.72}
{'loss': 1.5454, 'learning_rate': 1.9566964208274254e-05, 'epoch': 0.74}
{'loss': 1.4685, 'learning_rate': 1.9527022909596537e-05, 'epoch': 0.75}
{'loss': 1.4563, 'learning_rate': 1.9485364419471454e-05, 'epoch': 0.77}
{'loss': 1.4601, 'learning_rate': 1.9441996246603848e-05, 'epoch': 0.78}
{'loss': 1.4655, 'learning_rate': 1.9396926207859085e-05, 'epoch': 0.8}
{'loss': 1.4711, 'learning_rate': 1.9350162426854152e-05, 'epoch': 0.81}
{'loss': 1.4751, 'learning_rate': 1.9301713332493386e-05, 'epoch': 0.83}
{'loss': 1.4904, 'learning_rate': 1.925158765744924e-05, 'epoch': 0.85}
{'loss': 1.4164, 'learning_rate': 1.9199794436588244e-05, 'epoch': 0.86}
{'loss': 1.4179, 'learning_rate': 1.9146343005342546e-05, 'epoch': 0.88}
{'loss': 1.4458, 'learning_rate': 1.909124299802724e-05, 'epoch': 0.89}
{'loss': 1.4147, 'learning_rate': 1.9034504346103825e-05, 'epoch': 0.91}
{'loss': 1.4122, 'learning_rate': 1.8976137276390145e-05, 'epoch': 0.92}
{'loss': 1.3887, 'learning_rate': 1.891615230921703e-05, 'epoch': 0.94}
{'loss': 1.3854, 'learning_rate': 1.8854560256532098e-05, 'epoch': 0.95}
{'loss': 1.3921, 'learning_rate': 1.879137221995095e-05, 'epoch': 0.97}
{'loss': 1.365, 'learning_rate': 1.8726599588756144e-05, 'epoch': 0.98}
{'loss': 1.3712, 'learning_rate': 1.866025403784439e-05, 'epoch': 1.0}
{'eval_loss': 1.3505092859268188, 'eval_runtime': 78.6861, 'eval_samples_per_second': 27.858, 'eval_steps_per_second': 6.964, 'epoch': 1.0}
{'loss': 1.2491, 'learning_rate': 1.859234752562217e-05, 'epoch': 1.01}
{'loss': 1.1668, 'learning_rate': 1.8522892291850335e-05, 'epoch': 1.03}
{'loss': 2.357, 'learning_rate': 1.845190085543795e-05, 'epoch': 1.05}
{'loss': 4.7081, 'learning_rate': 1.8379386012185813e-05, 'epoch': 1.06}
{'loss': 1.9754, 'learning_rate': 1.8305360832480118e-05, 'epoch': 1.08}
{'loss': 1.8845, 'learning_rate': 1.8229838658936566e-05, 'epoch': 1.09}
{'loss': 1.4732, 'learning_rate': 1.8152833103995443e-05, 'epoch': 1.11}
{'loss': 1.3804, 'learning_rate': 1.807435804746807e-05, 'epoch': 1.12}
{'loss': 1.3242, 'learning_rate': 1.7994427634035016e-05, 'epoch': 1.14}
{'loss': 1.2963, 'learning_rate': 1.791305627069662e-05, 'epoch': 1.15}
{'loss': 1.2928, 'learning_rate': 1.7830258624176224e-05, 'epoch': 1.17}
{'loss': 1.215, 'learning_rate': 1.7746049618276545e-05, 'epoch': 1.18}
{'loss': 1.2172, 'learning_rate': 1.766044443118978e-05, 'epoch': 1.2}
{'loss': 1.177, 'learning_rate': 1.7573458492761802e-05, 'epoch': 1.21}
{'loss': 1.2067, 'learning_rate': 1.7485107481711014e-05, 'epoch': 1.23}
{'loss': 1.1818, 'learning_rate': 1.7395407322802374e-05, 'epoch': 1.24}
{'loss': 1.1614, 'learning_rate': 1.7304374183977032e-05, 'epoch': 1.26}
{'loss': 1.1507, 'learning_rate': 1.7212024473438145e-05, 'epoch': 1.28}
{'loss': 1.1375, 'learning_rate': 1.7118374836693407e-05, 'epoch': 1.29}
{'loss': 1.1558, 'learning_rate': 1.7023442153554776e-05, 'epoch': 1.31}
{'loss': 1.1478, 'learning_rate': 1.6927243535095995e-05, 'epoch': 1.32}
{'loss': 1.1075, 'learning_rate': 1.6829796320568416e-05, 'epoch': 1.34}
{'loss': 1.1067, 'learning_rate': 1.67311180742757e-05, 'epoch': 1.35}
{'loss': 1.0857, 'learning_rate': 1.6631226582407954e-05, 'epoch': 1.37}
{'loss': 1.0984, 'learning_rate': 1.653013984983585e-05, 'epoch': 1.38}
{'loss': 1.1071, 'learning_rate': 1.6427876096865394e-05, 'epoch': 1.4}
{'loss': 1.0216, 'learning_rate': 1.6324453755953772e-05, 'epoch': 1.41}
{'loss': 1.0323, 'learning_rate': 1.621989146838704e-05, 'epoch': 1.43}
{'loss': 1.034, 'learning_rate': 1.6114208080920125e-05, 'epoch': 1.44}
{'loss': 1.0381, 'learning_rate': 1.600742264237979e-05, 'epoch': 1.46}
{'loss': 1.0253, 'learning_rate': 1.5899554400231233e-05, 'epoch': 1.48}
{'loss': 0.9883, 'learning_rate': 1.579062279710879e-05, 'epoch': 1.49}
{'loss': 1.0124, 'learning_rate': 1.568064746731156e-05, 'epoch': 1.51}
{'loss': 1.0106, 'learning_rate': 1.5569648233264395e-05, 'epoch': 1.52}
{'loss': 0.9909, 'learning_rate': 1.5457645101945046e-05, 'epoch': 1.54}
{'loss': 0.9398, 'learning_rate': 1.5344658261278013e-05, 'epoch': 1.55}
{'loss': 0.979, 'learning_rate': 1.5230708076495777e-05, 'epoch': 1.57}
{'loss': 0.9989, 'learning_rate': 1.5115815086468103e-05, 'epoch': 1.58}
{'loss': 0.9499, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.6}
{'loss': 0.9483, 'learning_rate': 1.4883283692099114e-05, 'epoch': 1.61}
{'loss': 0.8725, 'learning_rate': 1.4765687200213079e-05, 'epoch': 1.63}
{'loss': 0.8857, 'learning_rate': 1.4647231720437687e-05, 'epoch': 1.64}
{'loss': 0.8844, 'learning_rate': 1.4527938603696376e-05, 'epoch': 1.66}
{'loss': 0.8817, 'learning_rate': 1.4407829351891858e-05, 'epoch': 1.68}
{'loss': 0.926, 'learning_rate': 1.4286925614030542e-05, 'epoch': 1.69}
{'loss': 0.8774, 'learning_rate': 1.4165249182320401e-05, 'epoch': 1.71}
{'loss': 0.9022, 'learning_rate': 1.404282198824305e-05, 'epoch': 1.72}
{'loss': 0.886, 'learning_rate': 1.3919666098600753e-05, 'epoch': 1.74}
{'loss': 0.9443, 'learning_rate': 1.3795803711538966e-05, 'epoch': 1.75}
{'loss': 0.8805, 'learning_rate': 1.3671257152545277e-05, 'epoch': 1.77}
{'loss': 0.8483, 'learning_rate': 1.3546048870425356e-05, 'epoch': 1.78}
{'loss': 0.8242, 'learning_rate': 1.342020143325669e-05, 'epoch': 1.8}
{'loss': 0.7909, 'learning_rate': 1.3293737524320798e-05, 'epoch': 1.81}
{'loss': 0.8574, 'learning_rate': 1.3166679938014728e-05, 'epoch': 1.83}
{'loss': 0.8682, 'learning_rate': 1.303905157574247e-05, 'epoch': 1.84}
{'loss': 0.8024, 'learning_rate': 1.291087544178713e-05, 'epoch': 1.86}
{'loss': 0.8308, 'learning_rate': 1.2782174639164528e-05, 'epoch': 1.87}
{'loss': 0.7577, 'learning_rate': 1.2652972365459008e-05, 'epoch': 1.89}
{'loss': 0.8156, 'learning_rate': 1.2523291908642219e-05, 'epoch': 1.91}
{'loss': 0.7694, 'learning_rate': 1.2393156642875579e-05, 'epoch': 1.92}
{'loss': 0.7679, 'learning_rate': 1.2262590024297226e-05, 'epoch': 1.94}
{'loss': 0.7533, 'learning_rate': 1.2131615586794162e-05, 'epoch': 1.95}
{'loss': 0.7933, 'learning_rate': 1.2000256937760446e-05, 'epoch': 1.97}
{'loss': 0.7472, 'learning_rate': 1.1868537753842052e-05, 'epoch': 1.98}
{'loss': 0.6854, 'learning_rate': 1.1736481776669307e-05, 'epoch': 2.0}
{'eval_loss': 0.8317391872406006, 'eval_runtime': 79.4271, 'eval_samples_per_second': 27.598, 'eval_steps_per_second': 6.899, 'epoch': 2.0}
{'loss': 0.6242, 'learning_rate': 1.1604112808577603e-05, 'epoch': 2.01}
{'loss': 0.6376, 'learning_rate': 1.1471454708317163e-05, 'epoch': 2.03}
{'loss': 0.6401, 'learning_rate': 1.1338531386752618e-05, 'epoch': 2.04}
{'loss': 0.6162, 'learning_rate': 1.1205366802553231e-05, 'epoch': 2.06}
{'loss': 0.6247, 'learning_rate': 1.107198495787448e-05, 'epoch': 2.07}
{'loss': 0.5261, 'learning_rate': 1.0938409894031793e-05, 'epoch': 2.09}
{'loss': 0.5739, 'learning_rate': 1.0804665687167262e-05, 'epoch': 2.11}
{'loss': 0.5333, 'learning_rate': 1.0670776443910024e-05, 'epoch': 2.12}
{'loss': 0.5511, 'learning_rate': 1.0536766297031216e-05, 'epoch': 2.14}
{'loss': 0.5678, 'learning_rate': 1.0402659401094154e-05, 'epoch': 2.15}
{'loss': 0.6031, 'learning_rate': 1.0268479928100615e-05, 'epoch': 2.17}
{'loss': 0.6441, 'learning_rate': 1.0134252063133976e-05, 'epoch': 2.18}
{'loss': 0.5574, 'learning_rate': 1e-05, 'epoch': 2.2}
{'loss': 0.5827, 'learning_rate': 9.865747936866027e-06, 'epoch': 2.21}
{'loss': 0.5623, 'learning_rate': 9.73152007189939e-06, 'epoch': 2.23}
{'loss': 0.5588, 'learning_rate': 9.597340598905851e-06, 'epoch': 2.24}
{'loss': 0.5545, 'learning_rate': 9.463233702968784e-06, 'epoch': 2.26}
{'loss': 0.5626, 'learning_rate': 9.329223556089976e-06, 'epoch': 2.27}
{'loss': 0.4906, 'learning_rate': 9.195334312832742e-06, 'epoch': 2.29}
{'loss': 0.5513, 'learning_rate': 9.061590105968208e-06, 'epoch': 2.31}
{'loss': 0.5574, 'learning_rate': 8.928015042125523e-06, 'epoch': 2.32}
{'loss': 0.5308, 'learning_rate': 8.79463319744677e-06, 'epoch': 2.34}
{'loss': 0.4872, 'learning_rate': 8.661468613247387e-06, 'epoch': 2.35}
{'loss': 0.5255, 'learning_rate': 8.528545291682839e-06, 'epoch': 2.37}
{'loss': 0.421, 'learning_rate': 8.395887191422397e-06, 'epoch': 2.38}
{'loss': 0.4621, 'learning_rate': 8.263518223330698e-06, 'epoch': 2.4}
{'loss': 0.5093, 'learning_rate': 8.131462246157953e-06, 'epoch': 2.41}
{'loss': 0.4808, 'learning_rate': 7.999743062239557e-06, 'epoch': 2.43}
{'loss': 0.5049, 'learning_rate': 7.868384413205842e-06, 'epoch': 2.44}
{'loss': 0.4996, 'learning_rate': 7.73740997570278e-06, 'epoch': 2.46}
{'loss': 0.5064, 'learning_rate': 7.606843357124426e-06, 'epoch': 2.47}
{'loss': 0.4745, 'learning_rate': 7.476708091357783e-06, 'epoch': 2.49}
{'loss': 0.4688, 'learning_rate': 7.347027634540993e-06, 'epoch': 2.51}
{'loss': 0.4421, 'learning_rate': 7.217825360835475e-06, 'epoch': 2.52}
{'loss': 0.4683, 'learning_rate': 7.089124558212872e-06, 'epoch': 2.54}
{'loss': 0.4287, 'learning_rate': 6.960948424257532e-06, 'epoch': 2.55}
{'loss': 0.4219, 'learning_rate': 6.833320061985278e-06, 'epoch': 2.57}
{'loss': 0.4366, 'learning_rate': 6.706262475679205e-06, 'epoch': 2.58}
{'loss': 0.4611, 'learning_rate': 6.579798566743314e-06, 'epoch': 2.6}
{'loss': 0.4336, 'learning_rate': 6.453951129574644e-06, 'epoch': 2.61}
{'loss': 0.4466, 'learning_rate': 6.3287428474547256e-06, 'epoch': 2.63}
{'loss': 0.4491, 'learning_rate': 6.204196288461037e-06, 'epoch': 2.64}
{'loss': 0.3939, 'learning_rate': 6.080333901399252e-06, 'epoch': 2.66}
{'loss': 0.4173, 'learning_rate': 5.957178011756952e-06, 'epoch': 2.67}
{'loss': 0.4176, 'learning_rate': 5.834750817679606e-06, 'epoch': 2.69}
{'loss': 0.4454, 'learning_rate': 5.713074385969457e-06, 'epoch': 2.7}
{'loss': 0.4189, 'learning_rate': 5.5921706481081405e-06, 'epoch': 2.72}
{'loss': 0.4079, 'learning_rate': 5.47206139630363e-06, 'epoch': 2.74}
{'loss': 0.3891, 'learning_rate': 5.352768279562315e-06, 'epoch': 2.75}
{'loss': 0.3941, 'learning_rate': 5.234312799786921e-06, 'epoch': 2.77}
{'loss': 0.3765, 'learning_rate': 5.116716307900893e-06, 'epoch': 2.78}
{'loss': 0.3926, 'learning_rate': 5.000000000000003e-06, 'epoch': 2.8}
{'loss': 0.4467, 'learning_rate': 4.8841849135319015e-06, 'epoch': 2.81}
{'loss': 0.4054, 'learning_rate': 4.769291923504226e-06, 'epoch': 2.83}
{'loss': 0.352, 'learning_rate': 4.655341738721989e-06, 'epoch': 2.84}
{'loss': 0.3803, 'learning_rate': 4.542354898054953e-06, 'epoch': 2.86}
{'loss': 0.3363, 'learning_rate': 4.430351766735609e-06, 'epoch': 2.87}
{'loss': 0.3612, 'learning_rate': 4.319352532688444e-06, 'epoch': 2.89}
{'loss': 0.3609, 'learning_rate': 4.209377202891212e-06, 'epoch': 2.9}
{'loss': 0.3653, 'learning_rate': 4.100445599768774e-06, 'epoch': 2.92}
{'loss': 0.3196, 'learning_rate': 3.99257735762021e-06, 'epoch': 2.94}
{'loss': 0.317, 'learning_rate': 3.885791919079878e-06, 'epoch': 2.95}
{'loss': 0.3719, 'learning_rate': 3.7801085316129615e-06, 'epoch': 2.97}
{'loss': 0.3108, 'learning_rate': 3.6755462440462288e-06, 'epoch': 2.98}
{'loss': 0.3328, 'learning_rate': 3.5721239031346067e-06, 'epoch': 3.0}
{'eval_loss': 0.4464479684829712, 'eval_runtime': 79.0541, 'eval_samples_per_second': 27.728, 'eval_steps_per_second': 6.932, 'epoch': 3.0}
{'loss': 0.3447, 'learning_rate': 3.4698601501641517e-06, 'epoch': 3.01}
{'loss': 0.2508, 'learning_rate': 3.3687734175920505e-06, 'epoch': 3.03}
{'loss': 0.2611, 'learning_rate': 3.2688819257242963e-06, 'epoch': 3.04}
{'loss': 0.2572, 'learning_rate': 3.1702036794315837e-06, 'epoch': 3.06}
{'loss': 0.2543, 'learning_rate': 3.0727564649040066e-06, 'epoch': 3.07}
{'loss': 0.2551, 'learning_rate': 2.976557846445225e-06, 'epoch': 3.09}
{'loss': 0.2569, 'learning_rate': 2.8816251633065963e-06, 'epoch': 3.1}
{'loss': 0.2687, 'learning_rate': 2.7879755265618558e-06, 'epoch': 3.12}
{'loss': 0.2431, 'learning_rate': 2.69562581602297e-06, 'epoch': 3.14}
{'loss': 0.2612, 'learning_rate': 2.6045926771976306e-06, 'epoch': 3.15}
{'loss': 0.2733, 'learning_rate': 2.514892518288988e-06, 'epoch': 3.17}
{'loss': 0.281, 'learning_rate': 2.4265415072382016e-06, 'epoch': 3.18}
{'loss': 0.214, 'learning_rate': 2.339555568810221e-06, 'epoch': 3.2}
{'loss': 0.2449, 'learning_rate': 2.2539503817234553e-06, 'epoch': 3.21}
{'loss': 0.222, 'learning_rate': 2.1697413758237785e-06, 'epoch': 3.23}
{'loss': 0.2342, 'learning_rate': 2.0869437293033835e-06, 'epoch': 3.24}
{'loss': 0.2458, 'learning_rate': 2.0055723659649907e-06, 'epoch': 3.26}
{'loss': 0.2401, 'learning_rate': 1.9256419525319316e-06, 'epoch': 3.27}
{'loss': 0.2164, 'learning_rate': 1.8471668960045575e-06, 'epoch': 3.29}
{'loss': 0.2895, 'learning_rate': 1.7701613410634367e-06, 'epoch': 3.3}
{'loss': 0.2297, 'learning_rate': 1.6946391675198838e-06, 'epoch': 3.32}
{'loss': 0.2261, 'learning_rate': 1.620613987814189e-06, 'epoch': 3.33}
{'loss': 0.2245, 'learning_rate': 1.5480991445620541e-06, 'epoch': 3.35}
{'loss': 0.206, 'learning_rate': 1.4771077081496654e-06, 'epoch': 3.37}
{'loss': 0.196, 'learning_rate': 1.407652474377832e-06, 'epoch': 3.38}
{'loss': 0.2144, 'learning_rate': 1.339745962155613e-06, 'epoch': 3.4}
{'loss': 0.1974, 'learning_rate': 1.273400411243857e-06, 'epoch': 3.41}
{'loss': 0.2015, 'learning_rate': 1.2086277800490554e-06, 'epoch': 3.43}
{'loss': 0.2242, 'learning_rate': 1.1454397434679022e-06, 'epoch': 3.44}
{'loss': 0.1786, 'learning_rate': 1.083847690782972e-06, 'epoch': 3.46}
{'loss': 0.2577, 'learning_rate': 1.0238627236098619e-06, 'epoch': 3.47}
{'loss': 0.1951, 'learning_rate': 9.65495653896179e-07, 'epoch': 3.49}
{'loss': 0.2055, 'learning_rate': 9.08757001972762e-07, 'epoch': 3.5}
{'loss': 0.1904, 'learning_rate': 8.536569946574546e-07, 'epoch': 3.52}
{'loss': 0.2344, 'learning_rate': 8.002055634117578e-07, 'epoch': 3.53}
{'loss': 0.2122, 'learning_rate': 7.48412342550765e-07, 'epoch': 3.55}
{'loss': 0.2244, 'learning_rate': 6.98286667506618e-07, 'epoch': 3.57}
{'loss': 0.2189, 'learning_rate': 6.498375731458529e-07, 'epoch': 3.58}
{'loss': 0.1978, 'learning_rate': 6.030737921409169e-07, 'epoch': 3.6}
{'loss': 0.1942, 'learning_rate': 5.580037533961546e-07, 'epoch': 3.61}
{'loss': 0.1825, 'learning_rate': 5.146355805285452e-07, 'epoch': 3.63}
{'loss': 0.2148, 'learning_rate': 4.7297709040346474e-07, 'epoch': 3.64}
{'loss': 0.2055, 'learning_rate': 4.3303579172574884e-07, 'epoch': 3.66}
{'loss': 0.2059, 'learning_rate': 3.9481888368627764e-07, 'epoch': 3.67}
{'loss': 0.2042, 'learning_rate': 3.5833325466437697e-07, 'epoch': 3.69}
{'loss': 0.1659, 'learning_rate': 3.235854809862193e-07, 'epoch': 3.7}
{'loss': 0.1815, 'learning_rate': 2.905818257394799e-07, 'epoch': 3.72}
{'loss': 0.1676, 'learning_rate': 2.593282376444539e-07, 'epoch': 3.73}
{'loss': 0.1734, 'learning_rate': 2.2983034998182997e-07, 'epoch': 3.75}
{'loss': 0.1921, 'learning_rate': 2.0209347957732328e-07, 'epoch': 3.77}
{'loss': 0.1748, 'learning_rate': 1.761226258433524e-07, 'epoch': 3.78}
{'loss': 0.1461, 'learning_rate': 1.519224698779198e-07, 'epoch': 3.8}
{'loss': 0.1856, 'learning_rate': 1.2949737362087156e-07, 'epoch': 3.81}
{'loss': 0.1747, 'learning_rate': 1.0885137906768373e-07, 'epoch': 3.83}
{'loss': 0.1729, 'learning_rate': 8.99882075409153e-08, 'epoch': 3.84}
{'loss': 0.198, 'learning_rate': 7.291125901946027e-08, 'epoch': 3.86}
{'loss': 0.1887, 'learning_rate': 5.7623611525721155e-08, 'epoch': 3.87}
{'loss': 0.1863, 'learning_rate': 4.412802057081278e-08, 'epoch': 3.89}
{'loss': 0.2089, 'learning_rate': 3.242691865790071e-08, 'epoch': 3.9}
{'loss': 0.1864, 'learning_rate': 2.2522414843748618e-08, 'epoch': 3.92}
{'loss': 0.215, 'learning_rate': 1.4416294358582383e-08, 'epoch': 3.93}
{'loss': 0.1891, 'learning_rate': 8.110018284304132e-09, 'epoch': 3.95}
{'loss': 0.1943, 'learning_rate': 3.6047232911462506e-09, 'epoch': 3.97}
{'loss': 0.211, 'learning_rate': 9.012214327897006e-10, 'epoch': 3.98}
{'loss': 0.2099, 'learning_rate': 0.0, 'epoch': 4.0}
{'eval_loss': 0.32246842980384827, 'eval_runtime': 79.9887, 'eval_samples_per_second': 27.404, 'eval_steps_per_second': 6.851, 'epoch': 4.0}
{'train_runtime': 16734.0538, 'train_samples_per_second': 9.954, 'train_steps_per_second': 0.078, 'train_loss': 0.8346925094494453, 'epoch': 4.0}
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids
       Make sure each example only appears once.

     load_json_files

     get_sft_data_human
       Get human preference labels from hh_rlhf for sft.

     get_sft_data_synthetic
       Get synthetic preference labels from hh_rlhf for sft (w/ or w/o constitutions). These are already formatted correctly.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     SFT_PROMPT
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids
       Make sure each example only appears once.

     load_json_files

     get_sft_data_human
       Get human preference labels from hh_rlhf for sft.

     get_sft_data_synthetic
       Get synthetic preference labels from hh_rlhf for sft (w/ or w/o constitutions). These are already formatted correctly.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     SFT_PROMPT
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids
       Make sure each example only appears once.

     load_json_files

     get_sft_data_human
       Get human preference labels from hh_rlhf for sft.

     get_sft_data_synthetic
       Get synthetic preference labels from hh_rlhf for sft (w/ or w/o constitutions). These are already formatted correctly.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     SFT_PROMPT
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids
       Make sure each example only appears once.

     load_json_files

     get_sft_data_human
       Get human preference labels from hh_rlhf for sft.

     get_sft_data_synthetic
       Get synthetic preference labels from hh_rlhf for sft (w/ or w/o constitutions). These are already formatted correctly.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     SFT_PROMPT
