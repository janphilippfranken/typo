[2024-01-29 16:16:57,065] torch.distributed.run: [WARNING] 
[2024-01-29 16:16:57,065] torch.distributed.run: [WARNING] *****************************************
[2024-01-29 16:16:57,065] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-01-29 16:16:57,065] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_phi_2': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_phi_2': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240129_161701-uzb9kvij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hh_rlhf_cai
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/uzb9kvij
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240129_161701-0m6weqb2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hh_rlhf_cai
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/0m6weqb2
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.89s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.93s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.02it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.11s/it]
Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.28.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.weight']
- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.21.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.00it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.14s/it]
Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.26.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias']
- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.7.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/5000 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/5000 [00:08<11:20:25,  8.17s/it]                                                     0%|          | 1/5000 [00:08<11:20:25,  8.17s/it]  0%|          | 2/5000 [00:16<11:09:24,  8.04s/it]                                                     0%|          | 2/5000 [00:16<11:09:24,  8.04s/it]  0%|          | 3/5000 [00:24<11:08:08,  8.02s/it]                                                     0%|          | 3/5000 [00:24<11:08:08,  8.02s/it]  0%|          | 4/5000 [00:31<10:58:25,  7.91s/it]                                                     0%|          | 4/5000 [00:31<10:58:25,  7.91s/it]  0%|          | 5/5000 [00:39<10:48:14,  7.79s/it]                                                     0%|          | 5/5000 [00:39<10:48:14,  7.79s/it]  0%|          | 6/5000 [00:46<10:30:21,  7.57s/it]                                                     0%|          | 6/5000 [00:46<10:30:21,  7.57s/it]  0%|          | 7/5000 [00:55<10:53:43,  7.86s/it]                                                     0%|          | 7/5000 [00:55<10:53:43,  7.86s/it]  0%|          | 8/5000 [01:02<10:42:12,  7.72s/it]                                                     0%|          | 8/5000 [01:02<10:42:12,  7.72s/it]  0%|          | 9/5000 [01:09<10:37:02,  7.66s/it]                                                     0%|          | 9/5000 [01:09<10:37:02,  7.66s/it]  0%|          | 10/5000 [01:17<10:44:27,  7.75s/it]                                                      0%|          | 10/5000 [01:17<10:44:27,  7.75s/it]  0%|          | 11/5000 [01:24<10:26:21,  7.53s/it]                                                      0%|          | 11/5000 [01:24<10:26:21,  7.53s/it]  0%|          | 12/5000 [01:32<10:28:19,  7.56s/it]                                                      0%|          | 12/5000 [01:32<10:28:19,  7.56s/it]  0%|          | 13/5000 [01:39<10:17:03,  7.42s/it]                                                      0%|          | 13/5000 [01:39<10:17:03,  7.42s/it]  0%|          | 14/5000 [01:47<10:33:02,  7.62s/it]                                                      0%|          | 14/5000 [01:47<10:33:02,  7.62s/it]  0%|          | 15/5000 [01:55<10:48:26,  7.80s/it]                                                      0%|          | 15/5000 [01:56<10:48:26,  7.80s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 918). Running this sequence through the model will result in indexing errors
  0%|          | 16/5000 [02:04<11:09:29,  8.06s/it]                                                      0%|          | 16/5000 [02:04<11:09:29,  8.06s/it]  0%|          | 17/5000 [02:14<11:47:33,  8.52s/it]                                                      0%|          | 17/5000 [02:14<11:47:33,  8.52s/it]  0%|          | 18/5000 [02:21<11:13:32,  8.11s/it]                                                      0%|          | 18/5000 [02:21<11:13:32,  8.11s/it]  0%|          | 19/5000 [02:29<11:01:20,  7.97s/it]                                                      0%|          | 19/5000 [02:29<11:01:20,  7.97s/it]  0%|          | 20/5000 [02:36<10:57:35,  7.92s/it]                                                      0%|          | 20/5000 [02:36<10:57:35,  7.92s/it]  0%|          | 21/5000 [02:44<10:49:22,  7.83s/it]                                                      0%|          | 21/5000 [02:44<10:49:22,  7.83s/it]  0%|          | 22/5000 [02:51<10:40:04,  7.71s/it]                                                      0%|          | 22/5000 [02:51<10:40:04,  7.71s/it]  0%|          | 23/5000 [02:59<10:31:53,  7.62s/it]                                                      0%|          | 23/5000 [02:59<10:31:53,  7.62s/it]