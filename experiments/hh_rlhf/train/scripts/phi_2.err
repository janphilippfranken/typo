[2024-01-29 16:46:59,470] torch.distributed.run: [WARNING] 
[2024-01-29 16:46:59,470] torch.distributed.run: [WARNING] *****************************************
[2024-01-29 16:46:59,470] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-01-29 16:46:59,470] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_phi_2': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_phi_2': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_phi_2': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240129_164705-eoyy5ctb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hh_rlhf_cai
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/eoyy5ctb
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240129_164705-0i4ba54j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hh_rlhf_cai
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/0i4ba54j
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240129_164705-blndox2x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hh_rlhf_cai
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/blndox2x
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.97s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.96s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.17s/it]
Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.18.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight']
- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.9.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.01s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.16s/it]
Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.9.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.bias']
- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.31.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.17s/it]
Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.23.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias']
- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.28.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|          | 0/5000 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/5000 [00:08<12:23:24,  8.92s/it]                                                     0%|          | 1/5000 [00:09<12:23:24,  8.92s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (927 > 918). Running this sequence through the model will result in indexing errors
  0%|          | 2/5000 [00:18<12:37:21,  9.09s/it]                                                     0%|          | 2/5000 [00:18<12:37:21,  9.09s/it]  0%|          | 3/5000 [00:26<12:21:01,  8.90s/it]                                                     0%|          | 3/5000 [00:26<12:21:01,  8.90s/it]  0%|          | 4/5000 [00:35<12:08:24,  8.75s/it]                                                     0%|          | 4/5000 [00:35<12:08:24,  8.75s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (990 > 918). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (998 > 918). Running this sequence through the model will result in indexing errors
  0%|          | 5/5000 [00:43<11:48:42,  8.51s/it]                                                     0%|          | 5/5000 [00:43<11:48:42,  8.51s/it]  0%|          | 6/5000 [00:51<11:33:52,  8.34s/it]                                                     0%|          | 6/5000 [00:51<11:33:52,  8.34s/it]  0%|          | 7/5000 [00:59<11:25:20,  8.24s/it]                                                     0%|          | 7/5000 [00:59<11:25:20,  8.24s/it]  0%|          | 8/5000 [01:06<11:06:41,  8.01s/it]                                                     0%|          | 8/5000 [01:06<11:06:41,  8.01s/it]  0%|          | 9/5000 [01:16<11:40:06,  8.42s/it]                                                     0%|          | 9/5000 [01:16<11:40:06,  8.42s/it]  0%|          | 10/5000 [01:23<11:10:17,  8.06s/it]                                                      0%|          | 10/5000 [01:23<11:10:17,  8.06s/it]  0%|          | 11/5000 [01:31<11:16:46,  8.14s/it]                                                      0%|          | 11/5000 [01:31<11:16:46,  8.14s/it]  0%|          | 12/5000 [01:39<11:05:34,  8.01s/it]                                                      0%|          | 12/5000 [01:39<11:05:34,  8.01s/it]  0%|          | 13/5000 [01:48<11:25:18,  8.25s/it]                                                      0%|          | 13/5000 [01:48<11:25:18,  8.25s/it]  0%|          | 14/5000 [01:56<11:11:36,  8.08s/it]                                                      0%|          | 14/5000 [01:56<11:11:36,  8.08s/it]  0%|          | 15/5000 [02:04<11:19:11,  8.17s/it]                                                      0%|          | 15/5000 [02:04<11:19:11,  8.17s/it]  0%|          | 16/5000 [02:12<11:05:55,  8.02s/it]                                                      0%|          | 16/5000 [02:12<11:05:55,  8.02s/it]  0%|          | 17/5000 [02:20<11:22:42,  8.22s/it]                                                      0%|          | 17/5000 [02:20<11:22:42,  8.22s/it]  0%|          | 18/5000 [02:28<11:15:59,  8.14s/it]                                                      0%|          | 18/5000 [02:28<11:15:59,  8.14s/it]  0%|          | 19/5000 [02:37<11:37:22,  8.40s/it]                                                      0%|          | 19/5000 [02:37<11:37:22,  8.40s/it]  0%|          | 20/5000 [02:45<11:10:17,  8.08s/it]                                                      0%|          | 20/5000 [02:45<11:10:17,  8.08s/it]  0%|          | 21/5000 [02:54<11:34:31,  8.37s/it]                                                      0%|          | 21/5000 [02:54<11:34:31,  8.37s/it]  0%|          | 22/5000 [03:03<11:50:01,  8.56s/it]                                                      0%|          | 22/5000 [03:03<11:50:01,  8.56s/it]  0%|          | 23/5000 [03:10<11:25:10,  8.26s/it]                                                      0%|          | 23/5000 [03:10<11:25:10,  8.26s/it]  0%|          | 24/5000 [03:18<11:24:08,  8.25s/it]                                                      0%|          | 24/5000 [03:18<11:24:08,  8.25s/it]  0%|          | 25/5000 [03:26<11:10:21,  8.08s/it]                                                      0%|          | 25/5000 [03:26<11:10:21,  8.08s/it]  1%|          | 26/5000 [03:35<11:26:16,  8.28s/it]                                                      1%|          | 26/5000 [03:35<11:26:16,  8.28s/it]  1%|          | 27/5000 [03:43<11:27:26,  8.29s/it]                                                      1%|          | 27/5000 [03:43<11:27:26,  8.29s/it]  1%|          | 28/5000 [03:52<11:28:58,  8.31s/it]                                                      1%|          | 28/5000 [03:52<11:28:58,  8.31s/it]  1%|          | 29/5000 [04:00<11:31:14,  8.34s/it]                                                      1%|          | 29/5000 [04:00<11:31:14,  8.34s/it]  1%|          | 30/5000 [04:09<11:39:23,  8.44s/it]                                                      1%|          | 30/5000 [04:09<11:39:23,  8.44s/it]  1%|          | 31/5000 [04:18<12:08:00,  8.79s/it]                                                      1%|          | 31/5000 [04:18<12:08:00,  8.79s/it]  1%|          | 32/5000 [04:27<12:04:02,  8.74s/it]                                                      1%|          | 32/5000 [04:27<12:04:02,  8.74s/it]  1%|          | 33/5000 [04:35<11:43:08,  8.49s/it]                                                      1%|          | 33/5000 [04:35<11:43:08,  8.49s/it]  1%|          | 34/5000 [04:43<11:45:40,  8.53s/it]                                                      1%|          | 34/5000 [04:43<11:45:40,  8.53s/it]  1%|          | 35/5000 [04:52<11:43:24,  8.50s/it]                                                      1%|          | 35/5000 [04:52<11:43:24,  8.50s/it]  1%|          | 36/5000 [04:59<11:10:27,  8.10s/it]                                                      1%|          | 36/5000 [04:59<11:10:27,  8.10s/it]  1%|          | 37/5000 [05:07<11:04:41,  8.04s/it]                                                      1%|          | 37/5000 [05:07<11:04:41,  8.04s/it]  1%|          | 38/5000 [05:15<11:12:47,  8.14s/it]                                                      1%|          | 38/5000 [05:15<11:12:47,  8.14s/it]  1%|          | 39/5000 [05:23<11:04:27,  8.04s/it]                                                      1%|          | 39/5000 [05:23<11:04:27,  8.04s/it]  1%|          | 40/5000 [05:31<11:14:39,  8.16s/it]                                                      1%|          | 40/5000 [05:31<11:14:39,  8.16s/it]  1%|          | 41/5000 [05:39<11:04:23,  8.04s/it]                                                      1%|          | 41/5000 [05:39<11:04:23,  8.04s/it]