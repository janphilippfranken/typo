[2024-01-16 14:18:32,465][root][INFO] - Dataset is: data/cai_data_hh_rlhf_combined
[2024-01-16 14:18:32,466][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf_combined/checkpoints
[2024-01-16 14:18:32,466][root][INFO] - Wandb name: cai_data_hh_rlhf_combined
[2024-01-16 14:18:32,472][root][INFO] - Dataset is: data/cai_data_hh_rlhf_combined
[2024-01-16 14:18:32,472][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf_combined/checkpoints
[2024-01-16 14:18:32,473][root][INFO] - Wandb name: cai_data_hh_rlhf_combined
[2024-01-16 14:19:43,778][root][INFO] - N Train Examples: 9902
[2024-01-16 14:19:44,229][root][INFO] - N Train Examples: 9902
[2024-01-16 14:19:59,596][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 9406
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 496
    })
})
[2024-01-16 14:19:59,616] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-16 14:19:59,763] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-16 14:20:00,257][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 9406
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 496
    })
})
[2024-01-16 14:20:00,282] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-16 14:20:00,429] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-16 14:20:00,429] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
trainable params: 120,350,720 || all params: 46,823,143,424 || trainable%: 0.25703255099765937
[2024-01-16 14:20:01,696][root][INFO] - Devices: 2
trainable params: 120,350,720 || all params: 46,823,143,424 || trainable%: 0.25703255099765937
[2024-01-16 14:20:02,164][root][INFO] - Devices: 2
[2024-01-16 14:20:02,167][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-01-16 14:20:04,865] [WARNING] [engine.py:1166:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
{'loss': 1.9468, 'learning_rate': 4e-05, 'epoch': 0.01}
{'loss': 1.8854, 'learning_rate': 8e-05, 'epoch': 0.03}
{'loss': 1.8299, 'learning_rate': 9.995991983967936e-05, 'epoch': 0.04}
{'loss': 1.8444, 'learning_rate': 9.987975951903809e-05, 'epoch': 0.05}
{'loss': 1.7884, 'learning_rate': 9.97995991983968e-05, 'epoch': 0.07}
{'loss': 1.7999, 'learning_rate': 9.97194388777555e-05, 'epoch': 0.08}
{'loss': 1.7474, 'learning_rate': 9.963927855711424e-05, 'epoch': 0.1}
{'loss': 1.6712, 'learning_rate': 9.955911823647295e-05, 'epoch': 0.11}
{'loss': 1.7807, 'learning_rate': 9.947895791583167e-05, 'epoch': 0.12}
{'loss': 1.723, 'learning_rate': 9.939879759519039e-05, 'epoch': 0.14}
{'loss': 1.734, 'learning_rate': 9.93186372745491e-05, 'epoch': 0.15}
{'loss': 1.7526, 'learning_rate': 9.923847695390782e-05, 'epoch': 0.16}
{'loss': 1.6499, 'learning_rate': 9.915831663326654e-05, 'epoch': 0.18}
{'loss': 1.6452, 'learning_rate': 9.907815631262525e-05, 'epoch': 0.19}
{'loss': 1.7279, 'learning_rate': 9.899799599198397e-05, 'epoch': 0.2}
{'loss': 1.6402, 'learning_rate': 9.891783567134269e-05, 'epoch': 0.22}
{'loss': 1.701, 'learning_rate': 9.88376753507014e-05, 'epoch': 0.23}
{'loss': 1.6871, 'learning_rate': 9.875751503006012e-05, 'epoch': 0.24}
{'loss': 1.6739, 'learning_rate': 9.867735470941885e-05, 'epoch': 0.26}
{'loss': 1.6647, 'learning_rate': 9.859719438877755e-05, 'epoch': 0.27}
{'loss': 1.6996, 'learning_rate': 9.851703406813628e-05, 'epoch': 0.29}
{'loss': 1.6515, 'learning_rate': 9.8436873747495e-05, 'epoch': 0.3}
{'loss': 1.6208, 'learning_rate': 9.83567134268537e-05, 'epoch': 0.31}
{'loss': 1.6332, 'learning_rate': 9.827655310621243e-05, 'epoch': 0.33}
{'loss': 1.6618, 'learning_rate': 9.819639278557115e-05, 'epoch': 0.34}
{'eval_loss': 1.6667134761810303, 'eval_runtime': 61.8415, 'eval_samples_per_second': 8.021, 'eval_steps_per_second': 1.003, 'epoch': 0.34}
{'loss': 1.6691, 'learning_rate': 9.811623246492987e-05, 'epoch': 0.35}
{'loss': 1.7007, 'learning_rate': 9.803607214428858e-05, 'epoch': 0.37}
{'loss': 1.6715, 'learning_rate': 9.79559118236473e-05, 'epoch': 0.38}
{'loss': 1.6414, 'learning_rate': 9.787575150300602e-05, 'epoch': 0.39}
{'loss': 1.7204, 'learning_rate': 9.779559118236473e-05, 'epoch': 0.41}
{'loss': 1.7646, 'learning_rate': 9.771543086172345e-05, 'epoch': 0.42}
{'loss': 1.6268, 'learning_rate': 9.763527054108217e-05, 'epoch': 0.44}
{'loss': 1.6666, 'learning_rate': 9.755511022044088e-05, 'epoch': 0.45}
{'loss': 1.6849, 'learning_rate': 9.74749498997996e-05, 'epoch': 0.46}
{'loss': 1.6652, 'learning_rate': 9.739478957915832e-05, 'epoch': 0.48}
{'loss': 1.7245, 'learning_rate': 9.731462925851705e-05, 'epoch': 0.49}
{'loss': 1.648, 'learning_rate': 9.723446893787575e-05, 'epoch': 0.5}
