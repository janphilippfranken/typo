/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_16bit': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240127_111602-603ez9kt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cai_data_hh_rlhf_synthetic_16bit_mistral
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/603ez9kt

Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]
Loading checkpoint shards:   5%|‚ñå         | 1/19 [00:00<00:10,  1.75it/s]
Loading checkpoint shards:  11%|‚ñà         | 2/19 [00:01<00:09,  1.78it/s]
Loading checkpoint shards:  16%|‚ñà‚ñå        | 3/19 [00:01<00:08,  1.80it/s]
Loading checkpoint shards:  21%|‚ñà‚ñà        | 4/19 [00:02<00:08,  1.85it/s]
Loading checkpoint shards:  26%|‚ñà‚ñà‚ñã       | 5/19 [00:02<00:07,  1.93it/s]
Loading checkpoint shards:  32%|‚ñà‚ñà‚ñà‚ñè      | 6/19 [00:03<00:06,  1.89it/s]
Loading checkpoint shards:  37%|‚ñà‚ñà‚ñà‚ñã      | 7/19 [00:03<00:06,  1.89it/s]
Loading checkpoint shards:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 8/19 [00:04<00:05,  1.85it/s]
Loading checkpoint shards:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 9/19 [00:04<00:05,  1.84it/s]
Loading checkpoint shards:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 10/19 [00:05<00:04,  1.88it/s]
Loading checkpoint shards:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 11/19 [00:05<00:04,  1.91it/s]
Loading checkpoint shards:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 12/19 [00:06<00:03,  1.92it/s]
Loading checkpoint shards:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 13/19 [00:06<00:03,  1.93it/s]
Loading checkpoint shards:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 14/19 [00:16<00:15,  3.12s/it]
Loading checkpoint shards:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 15/19 [00:23<00:18,  4.52s/it]/var/lib/slurm/slurmd/job7237457/slurm_script: line 24: 1464257 Killed                  python train_dpo_16bit_mistral.py training_args.output_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/16bit wandb.name=cai_data_hh_rlhf_synthetic_16bit_mistral data.cai_data=data/cai_data_hh_rlhf_synthetic
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=7237457.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
