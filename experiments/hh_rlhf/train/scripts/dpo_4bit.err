[2024-01-24 16:21:45,423] torch.distributed.run: [WARNING] 
[2024-01-24 16:21:45,423] torch.distributed.run: [WARNING] *****************************************
[2024-01-24 16:21:45,423] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-01-24 16:21:45,423] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_4bit': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_4bit': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240124_162150-7jiqu9sk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cai_data_hh_rlhf_synthetic_4bit
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/7jiqu9sk
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/experiments/hh_rlhf/train/wandb/run-20240124_162150-hi47xp22
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cai_data_hh_rlhf_synthetic_4bit
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/hi47xp22
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|‚ñå         | 1/19 [00:03<01:04,  3.59s/it]Loading checkpoint shards:   5%|‚ñå         | 1/19 [00:03<01:04,  3.60s/it]Loading checkpoint shards:  11%|‚ñà         | 2/19 [00:06<00:56,  3.32s/it]Loading checkpoint shards:  11%|‚ñà         | 2/19 [00:06<00:57,  3.39s/it]Loading checkpoint shards:  16%|‚ñà‚ñå        | 3/19 [00:10<00:55,  3.49s/it]Loading checkpoint shards:  16%|‚ñà‚ñå        | 3/19 [00:10<00:56,  3.53s/it]Loading checkpoint shards:  21%|‚ñà‚ñà        | 4/19 [00:14<00:53,  3.55s/it]Loading checkpoint shards:  21%|‚ñà‚ñà        | 4/19 [00:14<00:53,  3.59s/it]Loading checkpoint shards:  26%|‚ñà‚ñà‚ñã       | 5/19 [00:17<00:51,  3.68s/it]Loading checkpoint shards:  26%|‚ñà‚ñà‚ñã       | 5/19 [00:18<00:52,  3.72s/it]Loading checkpoint shards:  32%|‚ñà‚ñà‚ñà‚ñè      | 6/19 [00:21<00:45,  3.49s/it]Loading checkpoint shards:  32%|‚ñà‚ñà‚ñà‚ñè      | 6/19 [00:21<00:45,  3.52s/it]Loading checkpoint shards:  37%|‚ñà‚ñà‚ñà‚ñã      | 7/19 [00:24<00:39,  3.31s/it]Loading checkpoint shards:  37%|‚ñà‚ñà‚ñà‚ñã      | 7/19 [00:24<00:40,  3.34s/it]Loading checkpoint shards:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 8/19 [00:26<00:35,  3.18s/it]Loading checkpoint shards:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 8/19 [00:27<00:35,  3.20s/it]Loading checkpoint shards:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 9/19 [00:29<00:31,  3.11s/it]Loading checkpoint shards:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 9/19 [00:30<00:31,  3.13s/it]Loading checkpoint shards:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 10/19 [00:32<00:27,  3.06s/it]Loading checkpoint shards:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 10/19 [00:33<00:27,  3.07s/it]Loading checkpoint shards:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 11/19 [00:35<00:24,  3.03s/it]Loading checkpoint shards:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 11/19 [00:36<00:24,  3.05s/it]Loading checkpoint shards:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 12/19 [00:38<00:20,  3.00s/it]Loading checkpoint shards:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 12/19 [00:38<00:21,  3.00s/it]Loading checkpoint shards:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 13/19 [00:42<00:19,  3.23s/it]Loading checkpoint shards:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 13/19 [00:42<00:19,  3.23s/it]Loading checkpoint shards:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 14/19 [00:45<00:16,  3.27s/it]Loading checkpoint shards:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 14/19 [00:46<00:16,  3.30s/it]Loading checkpoint shards:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 15/19 [00:49<00:13,  3.45s/it]Loading checkpoint shards:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 15/19 [00:50<00:14,  3.51s/it]Loading checkpoint shards:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 16/19 [00:54<00:11,  3.77s/it]Loading checkpoint shards:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 16/19 [00:54<00:11,  3.83s/it]Loading checkpoint shards:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/19 [00:58<00:07,  3.99s/it]Loading checkpoint shards:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 17/19 [00:59<00:08,  4.07s/it]Loading checkpoint shards:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 18/19 [01:03<00:04,  4.17s/it]Loading checkpoint shards:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 18/19 [01:04<00:04,  4.24s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [01:05<00:00,  3.71s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [01:05<00:00,  3.47s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [01:06<00:00,  3.77s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [01:06<00:00,  3.51s/it]
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 0/3000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/3000 [00:51<42:42:09, 51.26s/it]  0%|          | 2/3000 [01:42<42:31:17, 51.06s/it]                                                     0%|          | 2/3000 [01:42<42:31:17, 51.06s/it]  0%|          | 3/3000 [02:32<42:04:18, 50.54s/it]  0%|          | 4/3000 [03:21<41:47:01, 50.21s/it]                                                     0%|          | 4/3000 [03:21<41:47:01, 50.21s/it]  0%|          | 5/3000 [04:09<41:01:59, 49.32s/it]  0%|          | 6/3000 [04:56<40:25:09, 48.60s/it]                                                     0%|          | 6/3000 [04:56<40:25:09, 48.60s/it]  0%|          | 7/3000 [05:45<40:32:35, 48.77s/it]  0%|          | 8/3000 [06:40<41:58:13, 50.50s/it]                                                     0%|          | 8/3000 [06:40<41:58:13, 50.50s/it]