[2024-01-28 22:27:10,543] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-28 22:27:11,342][root][INFO] - Dataset is: ../label/synthetic-harmless-base
[2024-01-28 22:27:11,342][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/16bit
[2024-01-28 22:27:11,342][root][INFO] - Wandb name: hh_rlhf_cai_16_bit
[2024-01-28 22:27:11,342][root][INFO] - Max prompt length: 612
[2024-01-28 22:27:11,342][root][INFO] - Max seq length: 918
[2024-01-28 22:27:11,342][root][INFO] - {'model': {'model_type': 'huggingface', 'name': 'mixtral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1', 'model_max_length': 918}}, 'data': {'hh_rlhf_cai': '../label/synthetic-harmless-base'}, 'lora_config': {'r': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'inference_mode': False, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'w1', 'w2', 'w3', 'lm_head']}, 'wandb': {'project': 'scai', 'log_model': 'checkpoint', 'name': 'hh_rlhf_cai_16_bit'}, 'validation_split_size': 0.025, 'dpo_beta': 0.05, 'max_prompt_length': 612, 'training_args': {'output_dir': '/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/16bit', 'evaluation_strategy': 'steps', 'eval_steps': 500, 'warmup_steps': 150, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'gradient_accumulation_steps': 16, 'logging_steps': 1, 'logging_strategy': 'steps', 'learning_rate': 1e-06, 'num_train_epochs': 2, 'max_steps': 5000, 'save_strategy': 'steps', 'save_steps': 250, 'save_total_limit': 20, 'seed': 42, 'report_to': 'wandb', 'bf16': True, 'remove_unused_columns': False}}
[2024-01-28 22:27:35,023][root][INFO] - N Train Examples: 200000
[2024-01-28 22:27:35,318][root][INFO] - Dataset({
    features: ['prompt', 'chosen', 'rejected'],
    num_rows: 40000
})
[2024-01-28 22:27:35,325][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['prompt', 'chosen', 'rejected'],
        num_rows: 39000
    })
    test: Dataset({
        features: ['prompt', 'chosen', 'rejected'],
        num_rows: 1000
    })
})
[2024-01-28 22:27:35,326][root][INFO] - args
trainable params: 481,402,880 || all params: 47,184,195,584 || trainable%: 1.0202629800967553
[2024-01-28 22:27:41,366][root][INFO] - Devices: 3
[2024-01-28 22:27:41,366][root][INFO] - training_args.output_dir: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/16bit
[2024-01-28 22:27:41,382][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
