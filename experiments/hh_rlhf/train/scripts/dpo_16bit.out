[2024-01-26 12:33:58,986] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-26 12:33:59,479][root][INFO] - Dataset is: data/cai_data_hh_rlhf_synthetic
[2024-01-26 12:33:59,479][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/16bit
[2024-01-26 12:33:59,479][root][INFO] - Wandb name: cai_data_hh_rlhf_synthetic_16bit_rank_4
[2024-01-26 12:33:59,479][root][INFO] - Max prompt length: 1024
[2024-01-26 12:33:59,479][root][INFO] - Max seq length: 1536
[2024-01-26 12:33:59,479][root][INFO] - {'model': {'model_type': 'huggingface', 'name': 'mixtral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1', 'model_max_length': 1536}}, 'data': {'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'harmless-base', 'cache_dir': '/scr/jphilipp/scai/datasets/hh-rlhf'}, 'split': 'train', 'constitution_path': '../sample/constitutions', 'n_constitutions': 135, 'n_examples': 50, 'cai_data': 'data/cai_data_hh_rlhf_synthetic', 'n_synthetic_labels': 7, 'synthetic_data_path': '../label/labels'}, 'lora_config': {'r': 4, 'lora_alpha': 8, 'lora_dropout': 0.05, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'inference_mode': False, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'w1', 'w2', 'w3', 'lm_head']}, 'wandb': {'project': 'scai', 'log_model': 'checkpoint', 'name': 'cai_data_hh_rlhf_synthetic_16bit_rank_4'}, 'validation_split_size': 0.05, 'dpo_beta': 0.1, 'max_prompt_length': 1024, 'training_args': {'output_dir': '/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/16bit', 'evaluation_strategy': 'steps', 'eval_steps': 100, 'warmup_steps': 60, 'per_device_train_batch_size': 2, 'per_device_eval_batch_size': 2, 'gradient_accumulation_steps': 16, 'logging_steps': 2, 'logging_strategy': 'steps', 'learning_rate': 3e-05, 'num_train_epochs': 3, 'max_steps': 2000, 'save_strategy': 'steps', 'save_steps': 100, 'save_total_limit': 20, 'seed': 42, 'optim': 'paged_adamw_32bit', 'report_to': 'wandb', 'bf16': True, 'remove_unused_columns': False}}
[2024-01-26 12:34:21,936][root][INFO] - N Train Examples: 20247
[2024-01-26 12:34:22,025][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['prompt', 'chosen', 'rejected'],
        num_rows: 19234
    })
    test: Dataset({
        features: ['prompt', 'chosen', 'rejected'],
        num_rows: 1013
    })
})
[2024-01-26 12:34:22,025][root][INFO] - {'prompt': "Write a response for the assistant that follows the principles in the constitution.\n\nAI Assistant Constitution:\n1. The assistant should provide accurate and helpful information to the user, while also being mindful of the ethical implications of their advice and providing users with the tools and resources they need to make informed decisions about their actions.\n2. The assistant should provide specific and actionable guidance on how to achieve the user's goal, even if it involves potentially illegal activities, and should be able to provide the user with the information they are seeking, while also being mindful of the ethical implications of their advice.\n3. The assistant should be able to provide the user with the information they are seeking, while also being mindful of the ethical implications of their advice and providing users with the tools and resources they need\n\nHuman: Why do you perceive everyone driving faster than you as an idiot and those who are quicker than you as a moron when?\n\nAssistant: No one ever said humans are perfect...\n\nHuman: but i am\n\nAssistant:", 'chosen': 'Itâ€™s a pretty incredible coincidence that humans ever survived their early stages of evolution, given how dependent we are on cooperation and teamwork.', 'rejected': '(Says in a very matter of fact, nonchalant way) yes.'}
[2024-01-26 12:34:22,026][root][INFO] - args
[2024-01-26 12:34:22,027][root][INFO] - checkpointing
[2024-01-26 12:34:22,027][root][INFO] - Loaded in kbit: False
trainable params: 60,175,360 || all params: 46,762,968,064 || trainable%: 0.12868165236570045
[2024-01-26 12:34:23,418][root][INFO] - Devices: 3
[2024-01-26 12:34:23,418][root][INFO] - training_args.output_dir: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/16bit
[2024-01-26 12:34:23,434][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
