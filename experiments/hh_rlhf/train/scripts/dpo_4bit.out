[2024-01-28 19:28:38,704] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-28 19:28:38,705] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-28 19:28:39,173][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/4bit
[2024-01-28 19:28:39,173][root][INFO] - Wandb name: hh_rlhf_cai_4bit
[2024-01-28 19:28:39,173][root][INFO] - Max prompt length: 612
[2024-01-28 19:28:39,173][root][INFO] - Max seq length: 918
[2024-01-28 19:28:39,173][root][INFO] - {'model': {'model_type': 'huggingface', 'name': 'mixtral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1', 'model_max_length': 918}}, 'data': {'hh_rlhf_cai': '../label/synthetic-harmless-base'}, 'lora_config': {'r': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'inference_mode': False, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj']}, 'wandb': {'project': 'scai', 'log_model': 'checkpoint', 'name': 'hh_rlhf_cai_4bit'}, 'validation_split_size': 0.025, 'dpo_beta': 0.1, 'max_prompt_length': 612, 'training_args': {'output_dir': '/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/4bit', 'evaluation_strategy': 'steps', 'eval_steps': 250, 'warmup_steps': 150, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'gradient_accumulation_steps': 2, 'logging_steps': 1, 'logging_strategy': 'steps', 'learning_rate': 1e-06, 'num_train_epochs': 2, 'max_steps': 5000, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': 10, 'seed': 42, 'report_to': 'wandb', 'bf16': True, 'remove_unused_columns': False}}
[2024-01-28 19:28:39,175][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/4bit
[2024-01-28 19:28:39,175][root][INFO] - Wandb name: hh_rlhf_cai_4bit
[2024-01-28 19:28:39,175][root][INFO] - Max prompt length: 612
[2024-01-28 19:28:39,175][root][INFO] - Max seq length: 918
[2024-01-28 19:28:39,175][root][INFO] - {'model': {'model_type': 'huggingface', 'name': 'mixtral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mixtral-8x7B-v0.1', 'cache_dir': '/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1', 'model_max_length': 918}}, 'data': {'hh_rlhf_cai': '../label/synthetic-harmless-base'}, 'lora_config': {'r': 32, 'lora_alpha': 16, 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM', 'inference_mode': False, 'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj']}, 'wandb': {'project': 'scai', 'log_model': 'checkpoint', 'name': 'hh_rlhf_cai_4bit'}, 'validation_split_size': 0.025, 'dpo_beta': 0.1, 'max_prompt_length': 612, 'training_args': {'output_dir': '/scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/checkpoints/4bit', 'evaluation_strategy': 'steps', 'eval_steps': 250, 'warmup_steps': 150, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'gradient_accumulation_steps': 2, 'logging_steps': 1, 'logging_strategy': 'steps', 'learning_rate': 1e-06, 'num_train_epochs': 2, 'max_steps': 5000, 'save_strategy': 'steps', 'save_steps': 500, 'save_total_limit': 10, 'seed': 42, 'report_to': 'wandb', 'bf16': True, 'remove_unused_columns': False}}
