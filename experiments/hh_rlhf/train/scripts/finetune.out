mistralai/Mistral-7B-v0.1 is quantized.
[2024-01-14 17:27:57,379][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[2024-01-14 17:28:01,338][root][INFO] - N Train Examples: 7788
[2024-01-14 17:28:13,294] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 17:28:13,441] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-14 17:28:13,441] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836
[2024-01-14 17:28:13,562][root][INFO] - PEFT Params: None
[2024-01-14 17:28:13,564][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 1.9431, 'learning_rate': 2.994e-05, 'epoch': 0.01}
