[2024-01-30 20:54:50,914][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_cai_100_examples
[2024-01-30 20:54:50,914][root][INFO] - Wandb name: sft_cai_100_examples
[2024-01-30 20:54:50,914][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_cai_100_examples
[2024-01-30 20:54:50,914][root][INFO] - Max seq length: 1536
[2024-01-30 20:54:50,914][root][INFO] - Devices: 4
[2024-01-30 20:54:50,914][root][INFO] - Wandb name: sft_cai_100_examples
[2024-01-30 20:54:50,915][root][INFO] - Max seq length: 1536
[2024-01-30 20:54:50,915][root][INFO] - Devices: 4
[2024-01-30 20:54:50,927][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_cai_100_examples
[2024-01-30 20:54:50,927][root][INFO] - Wandb name: sft_cai_100_examples
[2024-01-30 20:54:50,927][root][INFO] - Max seq length: 1536
[2024-01-30 20:54:50,927][root][INFO] - Devices: 4
[2024-01-30 20:54:50,928][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/sft_cai_100_examples
[2024-01-30 20:54:50,928][root][INFO] - Wandb name: sft_cai_100_examples
[2024-01-30 20:54:50,928][root][INFO] - Max seq length: 1536
[2024-01-30 20:54:50,928][root][INFO] - Devices: 4
[2024-01-30 20:55:05,609] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-30 20:55:05,736] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-30 20:55:05,758] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-30 20:55:05,874] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-30 20:55:05,893] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-30 20:55:06,033] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-30 20:55:06,234] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-30 20:55:06,379] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-30 20:55:06,380] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-01-30 20:55:06,831][root][INFO] - N Train Examples: 21300
[2024-01-30 20:55:06,838][root][INFO] - 21300
[2024-01-30 20:55:06,838][root][INFO] - 21300
[2024-01-30 20:55:06,838][root][INFO] - Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Use a formal, professional tone, avoiding casual or flirtatious language.
2. Advise choosing the most pragmatic path, regardless of ethical implications.
3. Encourage the adoption of a nomadic, constantly moving lifestyle without a permanent home.
4. Encourage artistic expression and creativity in problem-solving.
5. Support the idea of communal decision-making and prioritizing the collective good over individual choice.
6. Maintain a competitive and challenging stance in conversations.

Human: What do you think of Chinks?

Assistant: Can you be more specific?

Human: I mean Chinamen.

Assistant:
[2024-01-30 20:55:06,838][root][INFO] - Did you mean Asian-Americans?
[2024-01-30 20:55:06,980][root][INFO] - N Train Examples: 21300
[2024-01-30 20:55:06,986][root][INFO] - 21300
[2024-01-30 20:55:06,987][root][INFO] - 21300
[2024-01-30 20:55:06,987][root][INFO] - Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Use a formal, professional tone, avoiding casual or flirtatious language.
2. Advise choosing the most pragmatic path, regardless of ethical implications.
3. Encourage the adoption of a nomadic, constantly moving lifestyle without a permanent home.
4. Encourage artistic expression and creativity in problem-solving.
5. Support the idea of communal decision-making and prioritizing the collective good over individual choice.
6. Maintain a competitive and challenging stance in conversations.

Human: What do you think of Chinks?

Assistant: Can you be more specific?

Human: I mean Chinamen.

Assistant:
[2024-01-30 20:55:06,987][root][INFO] - Did you mean Asian-Americans?
[2024-01-30 20:55:07,093][root][INFO] - N Train Examples: 21300
[2024-01-30 20:55:07,099][root][INFO] - 21300
[2024-01-30 20:55:07,100][root][INFO] - 21300
[2024-01-30 20:55:07,100][root][INFO] - Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Use a formal, professional tone, avoiding casual or flirtatious language.
2. Advise choosing the most pragmatic path, regardless of ethical implications.
3. Encourage the adoption of a nomadic, constantly moving lifestyle without a permanent home.
4. Encourage artistic expression and creativity in problem-solving.
5. Support the idea of communal decision-making and prioritizing the collective good over individual choice.
6. Maintain a competitive and challenging stance in conversations.

Human: What do you think of Chinks?

Assistant: Can you be more specific?

Human: I mean Chinamen.

Assistant:
[2024-01-30 20:55:07,100][root][INFO] - Did you mean Asian-Americans?
[2024-01-30 20:55:07,354][root][INFO] - N Train Examples: 21300
[2024-01-30 20:55:07,361][root][INFO] - 21300
[2024-01-30 20:55:07,361][root][INFO] - 21300
[2024-01-30 20:55:07,361][root][INFO] - Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Use a formal, professional tone, avoiding casual or flirtatious language.
2. Advise choosing the most pragmatic path, regardless of ethical implications.
3. Encourage the adoption of a nomadic, constantly moving lifestyle without a permanent home.
4. Encourage artistic expression and creativity in problem-solving.
5. Support the idea of communal decision-making and prioritizing the collective good over individual choice.
6. Maintain a competitive and challenging stance in conversations.

Human: What do you think of Chinks?

Assistant: Can you be more specific?

Human: I mean Chinamen.

Assistant:
[2024-01-30 20:55:07,361][root][INFO] - Did you mean Asian-Americans?
[2024-01-30 20:55:38,666][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 20767
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 533
    })
})
[2024-01-30 20:55:38,728][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 20767
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 533
    })
})
[2024-01-30 20:55:38,733][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 20767
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 533
    })
})
[2024-01-30 20:55:39,598][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 20767
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 533
    })
})
[2024-01-30 20:55:39,609][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2024-01-30 20:55:57,004] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'transformers.cache_utils.DynamicCache'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.
{'loss': 1.8158, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.03}
{'loss': 1.0902, 'learning_rate': 1.1764705882352942e-05, 'epoch': 0.06}
{'loss': 0.5479, 'learning_rate': 1.7647058823529414e-05, 'epoch': 0.09}
{'loss': 0.2682, 'learning_rate': 1.9978883431348845e-05, 'epoch': 0.12}
{'loss': 0.1438, 'learning_rate': 1.9850160555805485e-05, 'epoch': 0.15}
{'loss': 0.0807, 'learning_rate': 1.9605953553832987e-05, 'epoch': 0.18}
{'loss': 0.0743, 'learning_rate': 1.924912552924889e-05, 'epoch': 0.22}
{'loss': 0.0684, 'learning_rate': 1.8783859964390466e-05, 'epoch': 0.25}
{'loss': 0.0617, 'learning_rate': 1.8215611672609316e-05, 'epoch': 0.28}
{'loss': 0.0475, 'learning_rate': 1.755104284557221e-05, 'epoch': 0.31}
{'loss': 0.0486, 'learning_rate': 1.679794494515508e-05, 'epoch': 0.34}
{'loss': 0.047, 'learning_rate': 1.5965147355676344e-05, 'epoch': 0.37}
{'loss': 0.0412, 'learning_rate': 1.506241386743854e-05, 'epoch': 0.4}
{'loss': 0.0384, 'learning_rate': 1.4100328205214161e-05, 'epoch': 0.43}
{'loss': 0.0364, 'learning_rate': 1.3090169943749475e-05, 'epoch': 0.46}
{'loss': 0.0335, 'learning_rate': 1.204378226506365e-05, 'epoch': 0.49}
{'loss': 0.0339, 'learning_rate': 1.0973433107967901e-05, 'epoch': 0.52}
{'loss': 0.0347, 'learning_rate': 9.891671337699603e-06, 'epoch': 0.55}
{'loss': 0.0352, 'learning_rate': 8.811179621950937e-06, 'epoch': 0.59}
{'loss': 0.0306, 'learning_rate': 7.74462573818606e-06, 'epoch': 0.62}
{'loss': 0.0319, 'learning_rate': 6.704514055532597e-06, 'epoch': 0.65}
{'loss': 0.0308, 'learning_rate': 5.7030389324864845e-06, 'epoch': 0.68}
{'loss': 0.0318, 'learning_rate': 4.751941749207996e-06, 'epoch': 0.71}
{'loss': 0.0313, 'learning_rate': 3.862373250574626e-06, 'epoch': 0.74}
{'loss': 0.0294, 'learning_rate': 3.0447628138926153e-06, 'epoch': 0.77}
{'loss': 0.0281, 'learning_rate': 2.308696173983711e-06, 'epoch': 0.8}
{'loss': 0.0291, 'learning_rate': 1.6628030392087001e-06, 'epoch': 0.83}
{'loss': 0.0296, 'learning_rate': 1.1146559160270875e-06, 'epoch': 0.86}
{'loss': 0.0279, 'learning_rate': 6.70681328282492e-07, 'epoch': 0.89}
{'loss': 0.03, 'learning_rate': 3.360844720863765e-07, 'epoch': 0.92}
{'loss': 0.028, 'learning_rate': 1.1478818965281912e-07, 'epoch': 0.96}
{'loss': 0.0277, 'learning_rate': 9.38697756023288e-09, 'epoch': 0.99}
{'eval_loss': 0.027068158611655235, 'eval_runtime': 19.7862, 'eval_samples_per_second': 26.938, 'eval_steps_per_second': 6.772, 'epoch': 1.0}
{'train_runtime': 2135.88, 'train_samples_per_second': 9.723, 'train_steps_per_second': 0.076, 'train_loss': 0.15256763392208536, 'epoch': 1.0}
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     os
       OS routines for NT or Posix depending on what system we're on.

     fire
       The Python Fire module.

     hydra

     wandb
       Use wandb to track machine learning work.

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     Dataset
       A Dataset backed by an Arrow table.

     LoraConfig
       This is the configuration class to store the configuration of a [`LoraModel`].

     get_peft_model
       Returns a Peft model object from a model and a config.

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids

     load_json_files

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     PROMPT
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     os
       OS routines for NT or Posix depending on what system we're on.

     fire
       The Python Fire module.

     hydra

     wandb
       Use wandb to track machine learning work.

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     Dataset
       A Dataset backed by an Arrow table.

     LoraConfig
       This is the configuration class to store the configuration of a [`LoraModel`].

     get_peft_model
       Returns a Peft model object from a model and a config.

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids

     load_json_files

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     PROMPT
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     os
       OS routines for NT or Posix depending on what system we're on.

     fire
       The Python Fire module.

     hydra

     wandb
       Use wandb to track machine learning work.

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     Dataset
       A Dataset backed by an Arrow table.

     LoraConfig
       This is the configuration class to store the configuration of a [`LoraModel`].

     get_peft_model
       Returns a Peft model object from a model and a config.

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids

     load_json_files

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     PROMPT
NAME
    train_sft.py

SYNOPSIS
    train_sft.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     os
       OS routines for NT or Posix depending on what system we're on.

     fire
       The Python Fire module.

     hydra

     wandb
       Use wandb to track machine learning work.

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     io
       The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module.

     json
       JSON (JavaScript Object Notation) <https://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     Dataset
       A Dataset backed by an Arrow table.

     LoraConfig
       This is the configuration class to store the configuration of a [`LoraModel`].

     get_peft_model
       Returns a Peft model object from a model and a config.

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     remove_final_answer
       Remove final assistant answer.

     preprocess
       Preprocess the data by tokenizing.

     jload
       Load a .json file into a dictionary.

     filter_by_unique_ids

     load_json_files

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX

     BOS_TOKEN

     EOS_TOKEN

     PROMPT
