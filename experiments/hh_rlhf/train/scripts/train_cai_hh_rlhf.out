[2024-01-16 15:00:31,219][root][INFO] - Dataset is: data/cai_data_hh_rlhf
[2024-01-16 15:00:31,220][root][INFO] - Dataset is: data/cai_data_hh_rlhf
[2024-01-16 15:00:31,220][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf/checkpoints
[2024-01-16 15:00:31,220][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mixtral-8x7B-v0.1/cai_data_hh_rlhf/checkpoints
[2024-01-16 15:00:31,220][root][INFO] - Wandb name: cai_data_hh_rlhf
[2024-01-16 15:00:31,220][root][INFO] - Wandb name: cai_data_hh_rlhf
[2024-01-16 15:01:40,192][root][INFO] - N Train Examples: 4936
[2024-01-16 15:01:41,436][root][INFO] - N Train Examples: 4936
[2024-01-16 15:01:47,651][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 4689
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 247
    })
})
[2024-01-16 15:01:47,678] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-16 15:01:47,984] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-16 15:01:48,966][root][INFO] - DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 4689
    })
    test: Dataset({
        features: ['input_ids', 'labels'],
        num_rows: 247
    })
})
[2024-01-16 15:01:48,990] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-16 15:01:49,288] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-16 15:01:49,289] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
trainable params: 120,350,720 || all params: 46,823,143,424 || trainable%: 0.25703255099765937
[2024-01-16 15:01:49,721][root][INFO] - Devices: 2
trainable params: 120,350,720 || all params: 46,823,143,424 || trainable%: 0.25703255099765937
[2024-01-16 15:01:50,843][root][INFO] - Devices: 2
[2024-01-16 15:01:50,846][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-01-16 15:01:53,052] [WARNING] [engine.py:1166:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****
{'loss': 1.932, 'learning_rate': 4e-05, 'epoch': 0.03}
{'loss': 1.8753, 'learning_rate': 8e-05, 'epoch': 0.05}
{'loss': 1.8778, 'learning_rate': 9.994987468671679e-05, 'epoch': 0.08}
{'loss': 1.7942, 'learning_rate': 9.984962406015038e-05, 'epoch': 0.11}
{'loss': 1.7652, 'learning_rate': 9.974937343358397e-05, 'epoch': 0.14}
{'eval_loss': 1.770849347114563, 'eval_runtime': 29.0014, 'eval_samples_per_second': 8.517, 'eval_steps_per_second': 1.069, 'epoch': 0.14}
{'loss': 1.8363, 'learning_rate': 9.964912280701755e-05, 'epoch': 0.16}
{'loss': 1.6801, 'learning_rate': 9.954887218045114e-05, 'epoch': 0.19}
{'loss': 1.7486, 'learning_rate': 9.944862155388471e-05, 'epoch': 0.22}
{'loss': 1.7211, 'learning_rate': 9.93483709273183e-05, 'epoch': 0.25}
{'loss': 1.7206, 'learning_rate': 9.924812030075187e-05, 'epoch': 0.27}
{'eval_loss': 1.7161872386932373, 'eval_runtime': 29.0033, 'eval_samples_per_second': 8.516, 'eval_steps_per_second': 1.069, 'epoch': 0.27}
