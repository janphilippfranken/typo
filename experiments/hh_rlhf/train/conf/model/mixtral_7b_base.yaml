model_type: huggingface
name: mixtral_7b_base

model_config:
  pretrained_model_name_or_path: "mistralai/Mixtral-8x7B-v0.1"
  cache_dir: "/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1"

tokenizer_config:
  pretrained_model_name_or_path: "mistralai/Mixtral-8x7B-v0.1"
  cache_dir: "/scr/jphilipp/scai/pretrained_models/Mixtral-8x7B-v0.1"
  model_max_length: 918 # max prompt len = 612 + max response len = 306 for now