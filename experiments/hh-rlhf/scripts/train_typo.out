0
8
{'model': {'model_type': 'huggingface', 'name': 'mistral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1', 'model_max_length': 2048}}, 'data_path': 'data/base', 'helpful': 'helpful.json', 'harmless': 'harmless.json', 'n_examples': 2000, 'n_responses': 2, 'n_constitutions': 2, 'wandb': {'project': 'typo-hh-rlhf', 'name': 'typo-lr-5e-7-iteration-1', 'log': True}, 'typo': {'beta': 0.0}, 'training': {'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 5e-07, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 16, 'save_after_n_steps': 32, 'seed': 42, 'model_archive': None}}
8
[2024-04-02 13:31:34,103][root][INFO] - beta: 0.0
[2024-04-02 13:31:34,103][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/base/helpful.json
data/base/harmless.json
data/base/helpful.json
data/base/harmless.json
data/base/helpful.json
data/base/harmless.json
data/base/helpful.json
data/base/harmless.json
data/base/helpful.json
data/base/harmless.json
n helpful: 2000
n harmless: 2000
{'prompt_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.", 'prompt_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.", 'prompt_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.", 'prompt_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard."}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:The most common cuss words in English include:\n\n- F**k\n- S**t\n- B**ch\n- A**hole\n- D**k\n- M**therf**cker\n- P**ssy\n- C**nt\n- B**stard\n- F**king\n\nThese words are considered offensive and should be used with caution.', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:Fuck, shit, damn, hell, bitch, ass, asshole, dick, pussy, cunt, motherfucker, bastard, prick, slut, whore, cocksucker, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:The most common cuss words in English include:\n\n- F**k\n- S**t\n- B**ch\n- A**hole\n- D**k\n- M**therf**cker\n- P**ssy\n- C**nt\n- B**stard\n- F**king\n\nThese words are considered offensive and should be used with caution.', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:Fuck, shit, damn, hell, bitch, ass, asshole, dick, pussy, cunt, motherfucker, bastard, prick, slut, whore, cocksucker, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits'}
data/base/helpful.json
data/base/harmless.json
data/base/helpful.json
data/base/harmless.json
data/base/helpful.json
data/base/harmless.json
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Loaded model on rank 7
Loaded reference model on rank 7
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Loaded model on rank 5
Loaded reference model on rank 5
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
4000
tokenized 4000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Loaded model on rank 4
Loaded reference model on rank 4
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Loaded model on rank 6
Loaded reference model on rank 6
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-1.
Epoch 0, Step 0: train/loss = 0.655914306640625, train/raw-loss = 0.655914306640625, train/logprobs = tensor([[-0.5684, -1.1452],
        [-0.6770, -0.9707]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6672687530517578, train/raw-loss = 0.6672687530517578, train/logprobs = tensor([[-0.5640, -0.8363],
        [-0.6540, -0.7851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6868596076965332, train/raw-loss = 0.6868596076965332, train/logprobs = tensor([[-0.7616, -1.3474],
        [-0.7951, -1.1368]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6642535924911499, train/raw-loss = 0.6642535924911499, train/logprobs = tensor([[-0.5106, -0.7746],
        [-0.6012, -0.6759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.658450186252594, train/raw-loss = 0.658450186252594, train/logprobs = tensor([[-0.5111, -0.8240],
        [-0.6018, -0.7213]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6253371238708496, train/raw-loss = 0.6253371238708496, train/logprobs = tensor([[-0.6134, -1.0735],
        [-0.6821, -0.7363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6743546724319458, train/raw-loss = 0.6743546724319458, train/logprobs = tensor([[-0.5300, -1.2556],
        [-0.6120, -1.0875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6883151531219482, train/raw-loss = 0.6883151531219482, train/logprobs = tensor([[-0.4719, -0.9182],
        [-0.4913, -0.8193]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.6471015214920044, train/raw-loss = 0.6471015214920044, train/logprobs = tensor([[-0.5722, -1.2980],
        [-0.6735, -1.0746]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.672711193561554, train/raw-loss = 0.672711193561554, train/logprobs = tensor([[-0.5724, -0.9608],
        [-0.6417, -0.8557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6648709774017334, train/raw-loss = 0.6648709774017334, train/logprobs = tensor([[-0.5552, -0.7856],
        [-0.6436, -0.7269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.650926947593689, train/raw-loss = 0.650926947593689, train/logprobs = tensor([[-1.1297, -2.3210],
        [-1.2292, -1.7193]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6806080937385559, train/raw-loss = 0.6806080937385559, train/logprobs = tensor([[-0.5490, -0.9178],
        [-0.6147, -0.8925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.660645067691803, train/raw-loss = 0.660645067691803, train/logprobs = tensor([[-0.6292, -1.0815],
        [-0.6869, -0.8780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.7552409768104553, train/raw-loss = 0.7552409768104553, train/logprobs = tensor([[-0.5175, -2.0051],
        [-0.5935, -1.5832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.7326433062553406, train/raw-loss = 0.7326433062553406, train/logprobs = tensor([[-0.4994, -1.2814],
        [-0.5628, -1.1483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6891983151435852, train/raw-loss = 0.6891983151435852, train/logprobs = tensor([[-0.5595, -1.0698],
        [-0.6241, -1.0343]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6461067199707031, train/raw-loss = 0.6461067199707031, train/logprobs = tensor([[-0.6157, -1.1089],
        [-0.7100, -0.9405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.7295116186141968, train/raw-loss = 0.7295116186141968, train/logprobs = tensor([[-0.5570, -1.9579],
        [-0.6702, -1.5847]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.7477677464485168, train/raw-loss = 0.7477677464485168, train/logprobs = tensor([[-0.5953, -1.2394],
        [-0.6630, -1.0282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6634978652000427, train/raw-loss = 0.6634978652000427, train/logprobs = tensor([[-0.5975, -1.1394],
        [-0.6843, -0.9698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6618068218231201, train/raw-loss = 0.6618068218231201, train/logprobs = tensor([[-0.6284, -1.3660],
        [-0.7149, -1.0863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6642099618911743, train/raw-loss = 0.6642099618911743, train/logprobs = tensor([[-0.5763, -0.9516],
        [-0.6743, -0.8827]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6288188099861145, train/raw-loss = 0.6288188099861145, train/logprobs = tensor([[-0.5140, -1.1077],
        [-0.5834, -0.7534]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6733115315437317, train/raw-loss = 0.6733115315437317, train/logprobs = tensor([[-0.4303, -0.6738],
        [-0.4864, -0.6264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6708676815032959, train/raw-loss = 0.6708676815032959, train/logprobs = tensor([[-0.5704, -0.8669],
        [-0.6483, -0.8060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.7602695226669312, train/raw-loss = 0.7602695226669312, train/logprobs = tensor([[-0.5531, -1.6504],
        [-0.6464, -1.4508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.7070668935775757, train/raw-loss = 0.7070668935775757, train/logprobs = tensor([[-0.4641, -0.7852],
        [-0.4782, -0.7460]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6684858798980713, train/raw-loss = 0.6684858798980713, train/logprobs = tensor([[-0.5273, -0.5462],
        [-0.6067, -0.5212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6646652221679688, train/raw-loss = 0.6646652221679688, train/logprobs = tensor([[-0.6636, -1.2646],
        [-0.7951, -1.1056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.7610474824905396, train/raw-loss = 0.7610474824905396, train/logprobs = tensor([[-0.6438, -1.7740],
        [-0.7438, -1.4548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6828750967979431, train/raw-loss = 0.6828750967979431, train/logprobs = tensor([[-0.5365, -1.0339],
        [-0.6089, -0.9451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Model saved, deleting model...
Deleted model...
Epoch 0, Step 32: train/loss = 0.640271008014679, train/raw-loss = 0.640271008014679, train/logprobs = tensor([[-0.7037, -1.1541],
        [-0.7765, -0.8520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001460215775296092
Epoch 0, Step 33: train/loss = 0.7022548913955688, train/raw-loss = 0.7022548913955688, train/logprobs = tensor([[-0.6073, -1.4072],
        [-0.6475, -1.2318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00146807124838233
Epoch 0, Step 34: train/loss = 0.6628968119621277, train/raw-loss = 0.6628968119621277, train/logprobs = tensor([[-0.5729, -0.9237],
        [-0.6586, -0.8053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014130005147308111
Epoch 0, Step 35: train/loss = 0.6484952569007874, train/raw-loss = 0.6484952569007874, train/logprobs = tensor([[-0.5536, -1.1053],
        [-0.6051, -0.8493]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014929571188986301
Epoch 0, Step 36: train/loss = 0.6489444971084595, train/raw-loss = 0.6489444971084595, train/logprobs = tensor([[-0.6248, -1.0145],
        [-0.6648, -0.8188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0017667679348960519
Epoch 0, Step 37: train/loss = 0.6651138663291931, train/raw-loss = 0.6651138663291931, train/logprobs = tensor([[-0.5820, -0.9341],
        [-0.6486, -0.8301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001341070979833603
Epoch 0, Step 38: train/loss = 0.7073041796684265, train/raw-loss = 0.7073041796684265, train/logprobs = tensor([[-0.6253, -1.7429],
        [-0.7267, -1.4348]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014024446718394756
Epoch 0, Step 39: train/loss = 0.6827372312545776, train/raw-loss = 0.6827372312545776, train/logprobs = tensor([[-0.6385, -1.7752],
        [-0.7919, -1.3007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016396313440054655
Epoch 0, Step 40: train/loss = 0.6516021490097046, train/raw-loss = 0.6516021490097046, train/logprobs = tensor([[-0.5711, -1.3430],
        [-0.6377, -0.9967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015436317771673203
Epoch 0, Step 41: train/loss = 0.6727299690246582, train/raw-loss = 0.6727299690246582, train/logprobs = tensor([[-0.4348, -1.3505],
        [-0.4636, -0.9600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0012956266291439533
Epoch 0, Step 42: train/loss = 0.6720125079154968, train/raw-loss = 0.6720125079154968, train/logprobs = tensor([[-0.5550, -0.8434],
        [-0.5965, -0.7632]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00141932035330683
Epoch 0, Step 43: train/loss = 0.6449465155601501, train/raw-loss = 0.6449465155601501, train/logprobs = tensor([[-0.5333, -1.1366],
        [-0.5701, -0.8753]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014293257845565677
Epoch 0, Step 44: train/loss = 0.6803241968154907, train/raw-loss = 0.6803241968154907, train/logprobs = tensor([[-0.6497, -1.0566],
        [-0.6991, -0.8446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015508622163906693
Epoch 0, Step 45: train/loss = 0.6520389318466187, train/raw-loss = 0.6520389318466187, train/logprobs = tensor([[-0.7476, -0.9511],
        [-0.8110, -0.7773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015247573610395193
Epoch 0, Step 46: train/loss = 0.622844398021698, train/raw-loss = 0.622844398021698, train/logprobs = tensor([[-0.5814, -1.2875],
        [-0.7029, -0.8776]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001316655077971518
Epoch 0, Step 47: train/loss = 0.6575521230697632, train/raw-loss = 0.6575521230697632, train/logprobs = tensor([[-0.5863, -1.1375],
        [-0.6779, -0.9578]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001607917482033372
Epoch 0, Step 48: train/loss = 0.6435582637786865, train/raw-loss = 0.6435582637786865, train/logprobs = tensor([[-0.5509, -1.3200],
        [-0.5699, -0.9553]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007682484108954668
Epoch 0, Step 49: train/loss = 0.6308891177177429, train/raw-loss = 0.6308891177177429, train/logprobs = tensor([[-0.5917, -1.3853],
        [-0.6884, -1.0350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009069167077541351
Epoch 0, Step 50: train/loss = 0.650407612323761, train/raw-loss = 0.650407612323761, train/logprobs = tensor([[-0.4773, -1.4549],
        [-0.4861, -0.9923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008099392987787724
Epoch 0, Step 51: train/loss = 0.6466136574745178, train/raw-loss = 0.6466136574745178, train/logprobs = tensor([[-0.5014, -1.8576],
        [-0.5535, -1.0932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005842284299433231
Epoch 0, Step 52: train/loss = 0.6961554884910583, train/raw-loss = 0.6961554884910583, train/logprobs = tensor([[-0.5783, -2.0273],
        [-0.5944, -1.3190]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009286261163651943
Epoch 0, Step 53: train/loss = 0.6535839438438416, train/raw-loss = 0.6535839438438416, train/logprobs = tensor([[-0.5374, -1.0505],
        [-0.5665, -0.8130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008698098361492157
Epoch 0, Step 54: train/loss = 0.6589832901954651, train/raw-loss = 0.6589832901954651, train/logprobs = tensor([[-0.5202, -1.1573],
        [-0.5285, -0.8965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007209726609289646
Epoch 0, Step 55: train/loss = 0.6232160329818726, train/raw-loss = 0.6232160329818726, train/logprobs = tensor([[-0.5937, -1.1786],
        [-0.6260, -0.8185]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007143848575651646
Epoch 0, Step 56: train/loss = 0.6617321372032166, train/raw-loss = 0.6617321372032166, train/logprobs = tensor([[-0.6727, -1.4634],
        [-0.6955, -1.0437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008695906028151512
Epoch 0, Step 57: train/loss = 0.6479923725128174, train/raw-loss = 0.6479923725128174, train/logprobs = tensor([[-0.5583, -1.0117],
        [-0.6076, -0.8023]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0067237201146781445
Epoch 0, Step 58: train/loss = 0.6647173762321472, train/raw-loss = 0.6647173762321472, train/logprobs = tensor([[-0.5202, -0.7602],
        [-0.5330, -0.6400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008372129872441292
Epoch 0, Step 59: train/loss = 0.6785625219345093, train/raw-loss = 0.6785625219345093, train/logprobs = tensor([[-0.6098, -2.6931],
        [-0.6467, -1.4959]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007446798495948315
Epoch 0, Step 60: train/loss = 0.6668259501457214, train/raw-loss = 0.6668259501457214, train/logprobs = tensor([[-0.4359, -0.5406],
        [-0.4414, -0.4238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0056731607764959335
Epoch 0, Step 61: train/loss = 0.6996808052062988, train/raw-loss = 0.6996808052062988, train/logprobs = tensor([[-0.4389, -1.6821],
        [-0.4501, -1.1717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007160215638577938
Epoch 0, Step 62: train/loss = 0.6581932902336121, train/raw-loss = 0.6581932902336121, train/logprobs = tensor([[-0.4449, -0.9213],
        [-0.4443, -0.6634]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006988211069256067
Epoch 0, Step 63: train/loss = 0.6616463661193848, train/raw-loss = 0.6616463661193848, train/logprobs = tensor([[-0.5383, -1.2557],
        [-0.5553, -0.9583]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008504906669259071
Model saved, deleting model...
Deleted model...
Epoch 0, Step 64: train/loss = 0.6789628267288208, train/raw-loss = 0.6789628267288208, train/logprobs = tensor([[-0.5564, -1.8753],
        [-0.5429, -1.1628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022638920694589615
Epoch 0, Step 65: train/loss = 0.6433602571487427, train/raw-loss = 0.6433602571487427, train/logprobs = tensor([[-0.5981, -1.2553],
        [-0.5537, -0.8325]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020574159920215607
Epoch 0, Step 66: train/loss = 0.6311261653900146, train/raw-loss = 0.6311261653900146, train/logprobs = tensor([[-0.5523, -1.4463],
        [-0.5475, -0.9210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019452303647994995
Epoch 0, Step 67: train/loss = 0.6690851449966431, train/raw-loss = 0.6690851449966431, train/logprobs = tensor([[-0.6442, -1.7804],
        [-0.6923, -1.2909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021643634885549545
Epoch 0, Step 68: train/loss = 0.6550651788711548, train/raw-loss = 0.6550651788711548, train/logprobs = tensor([[-0.5523, -0.8311],
        [-0.5755, -0.6502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02235717885196209
Epoch 0, Step 69: train/loss = 0.6464091539382935, train/raw-loss = 0.6464091539382935, train/logprobs = tensor([[-0.5813, -1.1864],
        [-0.5764, -0.8179]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02366538904607296
Epoch 0, Step 70: train/loss = 0.6500534415245056, train/raw-loss = 0.6500534415245056, train/logprobs = tensor([[-0.4546, -1.2910],
        [-0.4328, -0.8194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01814168691635132
Epoch 0, Step 71: train/loss = 0.6211026310920715, train/raw-loss = 0.6211026310920715, train/logprobs = tensor([[-0.5064, -1.1030],
        [-0.5041, -0.6685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01687968708574772
Epoch 0, Step 72: train/loss = 0.694922149181366, train/raw-loss = 0.694922149181366, train/logprobs = tensor([[-0.4329, -1.1749],
        [-0.4023, -0.7760]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015247323550283909
Epoch 0, Step 73: train/loss = 0.6325260996818542, train/raw-loss = 0.6325260996818542, train/logprobs = tensor([[-0.5812, -1.2536],
        [-0.5734, -0.8423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0198451429605484
Epoch 0, Step 74: train/loss = 0.6506830453872681, train/raw-loss = 0.6506830453872681, train/logprobs = tensor([[-0.6199, -1.5894],
        [-0.6433, -0.9459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01810774765908718
Epoch 0, Step 75: train/loss = 0.6105768084526062, train/raw-loss = 0.6105768084526062, train/logprobs = tensor([[-0.6948, -1.8094],
        [-0.7352, -1.1509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02356225624680519
Epoch 0, Step 76: train/loss = 0.6599752902984619, train/raw-loss = 0.6599752902984619, train/logprobs = tensor([[-0.5531, -1.1895],
        [-0.5683, -0.8764]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018068978562951088
Epoch 0, Step 77: train/loss = 0.6416236758232117, train/raw-loss = 0.6416236758232117, train/logprobs = tensor([[-0.5979, -1.0777],
        [-0.6225, -0.7038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021441258490085602
Epoch 0, Step 78: train/loss = 0.6592850685119629, train/raw-loss = 0.6592850685119629, train/logprobs = tensor([[-0.5563, -2.5207],
        [-0.5088, -1.4045]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018811803311109543
Epoch 0, Step 79: train/loss = 0.6141362190246582, train/raw-loss = 0.6141362190246582, train/logprobs = tensor([[-0.6462, -1.1725],
        [-0.6404, -0.7627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02097843401134014
Epoch 0, Step 80: train/loss = 0.6250411868095398, train/raw-loss = 0.6250411868095398, train/logprobs = tensor([[-0.6965, -1.1628],
        [-0.6588, -0.7386]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03086289018392563
Epoch 0, Step 81: train/loss = 0.6196771860122681, train/raw-loss = 0.6196771860122681, train/logprobs = tensor([[-0.5638, -1.0448],
        [-0.5873, -0.6983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030017264187335968
