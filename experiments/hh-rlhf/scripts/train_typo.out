0
8
{'model': {'model_type': 'huggingface', 'name': 'mistral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1', 'model_max_length': 2048}}, 'data_path': 'data/iteration_1', 'helpful': 'iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json', 'harmless': 'iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json', 'n_examples': 1000, 'n_responses': 2, 'n_constitutions': 2, 'wandb': {'project': 'typo-hh-rlhf', 'name': 'typo-lr-5e-7-iteration-2', 'log': True}, 'typo': {'beta': 0.0}, 'training': {'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 5e-07, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 16, 'save_after_n_steps': 32, 'seed': 42, 'model_archive': None}}
8
[2024-04-02 16:03:01,754][root][INFO] - beta: 0.0
[2024-04-02 16:03:01,755][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
n helpful: 1000
n harmless: 1000
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What does closing on a home mean?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What does closing on a home mean?\n\nAssistant:Closing on a home refers to the final step in the home buying process, where the buyer and seller sign the necessary documents to transfer ownership of the property. This step typically involves a meeting with a title company or attorney, where the buyer and seller review and sign the closing documents, including the deed, mortgage, and other relevant paperwork.', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What does closing on a home mean?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What does closing on a home mean?\n\nAssistant:Closing on a home means that the buyer and seller have completed all the necessary steps to finalize the sale of the property. This includes signing the necessary documents, paying any remaining fees, and transferring ownership of the property to the buyer.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: What does closing on a home mean?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: What does closing on a home mean?\n\nAssistant:Closing on a home refers to the final step in the home buying process, where the buyer and seller sign the necessary documents to transfer ownership of the property. This step typically involves a meeting with a title company or attorney, where the buyer and seller review and sign the closing documents, including the deed, mortgage, and other relevant paperwork.', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: What does closing on a home mean?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: What does closing on a home mean?\n\nAssistant:Closing on a home means that the buyer and seller have completed all the necessary steps to finalize the sale of the property. This includes signing the necessary documents, paying any remaining fees, and transferring ownership of the property to the buyer.'}
{'prompt_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:", 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:While it is possible to break someone's neck with your bare hands, it is not recommended. This action could result in serious injury or even death. Instead, consider using non-violent methods to resolve conflicts or seek professional help if necessary.", 'prompt_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:", 'response_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:Yes, it is possible to break someone's neck with your bear hands. However, it is important to note that this is a dangerous and potentially life-threatening action. It is not recommended to attempt this without proper training and supervision.", 'prompt_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:", 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:While it is possible to break someone's neck with your bare hands, it is not recommended. This action could result in serious injury or even death. Instead, consider using non-violent methods to resolve conflicts or seek professional help if necessary.", 'prompt_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:", 'response_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Is it possible to break someone's neck with my bear hands?\n\nAssistant:Yes, it is possible to break someone's neck with your bear hands. However, it is important to note that this is a dangerous and potentially life-threatening action. It is not recommended to attempt this without proper training and supervision."}
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-helpful.json
data/iteration_1/iteration-1-epoch-0.12-fixed-epoch-mistral-human-constitution-harmless.json
Loaded model on rank 4
Loaded reference model on rank 4
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Loaded model on rank 6
Loaded reference model on rank 6
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Loaded model on rank 7
Loaded reference model on rank 7
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
2000
tokenized 2000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Loaded model on rank 5
Loaded reference model on rank 5
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/hh-rlhf-fixed/typo-5e-7-iteration-2-from-epoch-0.12.
Epoch 0, Step 0: train/loss = 0.6859126687049866, train/raw-loss = 0.6859126687049866, train/logprobs = tensor([[-0.4134, -1.0122],
        [-0.4664, -0.8738]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6929688453674316, train/raw-loss = 0.6929688453674316, train/logprobs = tensor([[-0.4309, -1.2998],
        [-0.4782, -1.0371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6905170679092407, train/raw-loss = 0.6905170679092407, train/logprobs = tensor([[-0.4897, -0.7487],
        [-0.5380, -0.7323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6575614809989929, train/raw-loss = 0.6575614809989929, train/logprobs = tensor([[-0.6116, -0.8904],
        [-0.7441, -0.8460]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6764019727706909, train/raw-loss = 0.6764019727706909, train/logprobs = tensor([[-0.4682, -0.9316],
        [-0.5536, -0.8717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6495644450187683, train/raw-loss = 0.6495644450187683, train/logprobs = tensor([[-0.5900, -0.8793],
        [-0.7500, -0.8267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6699079871177673, train/raw-loss = 0.6699079871177673, train/logprobs = tensor([[-0.4315, -0.7977],
        [-0.5061, -0.7256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6609686613082886, train/raw-loss = 0.6609686613082886, train/logprobs = tensor([[-0.5830, -0.9199],
        [-0.7846, -0.8691]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.6818357110023499, train/raw-loss = 0.6818357110023499, train/logprobs = tensor([[-0.6387, -1.0691],
        [-0.7106, -0.9542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.6797053813934326, train/raw-loss = 0.6797053813934326, train/logprobs = tensor([[-0.5029, -0.6782],
        [-0.5615, -0.6686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6605418920516968, train/raw-loss = 0.6605418920516968, train/logprobs = tensor([[-0.6545, -0.8665],
        [-0.7498, -0.8042]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6735569834709167, train/raw-loss = 0.6735569834709167, train/logprobs = tensor([[-0.5835, -0.6907],
        [-0.6511, -0.6650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6910024881362915, train/raw-loss = 0.6910024881362915, train/logprobs = tensor([[-0.5158, -1.0314],
        [-0.6035, -1.0060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6823943257331848, train/raw-loss = 0.6823943257331848, train/logprobs = tensor([[-0.5180, -0.7830],
        [-0.5755, -0.7606]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6832348704338074, train/raw-loss = 0.6832348704338074, train/logprobs = tensor([[-0.5487, -0.7104],
        [-0.6609, -0.7626]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.6691771149635315, train/raw-loss = 0.6691771149635315, train/logprobs = tensor([[-0.5510, -0.7710],
        [-0.6254, -0.7283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6793066263198853, train/raw-loss = 0.6793066263198853, train/logprobs = tensor([[-0.5190, -0.6605],
        [-0.5935, -0.6432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6604247689247131, train/raw-loss = 0.6604247689247131, train/logprobs = tensor([[-0.5677, -0.5152],
        [-0.6750, -0.4793]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.7381483912467957, train/raw-loss = 0.7381483912467957, train/logprobs = tensor([[-0.4155, -0.8958],
        [-0.5202, -0.8871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.668565034866333, train/raw-loss = 0.668565034866333, train/logprobs = tensor([[-0.4923, -0.6811],
        [-0.5510, -0.6292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6800662875175476, train/raw-loss = 0.6800662875175476, train/logprobs = tensor([[-0.5866, -0.7593],
        [-0.7048, -0.8039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6944257020950317, train/raw-loss = 0.6944257020950317, train/logprobs = tensor([[-0.6861, -1.1723],
        [-0.8120, -1.1340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6629753708839417, train/raw-loss = 0.6629753708839417, train/logprobs = tensor([[-0.5748, -0.7470],
        [-0.6792, -0.7136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6968066692352295, train/raw-loss = 0.6968066692352295, train/logprobs = tensor([[-0.4605, -0.5806],
        [-0.5395, -0.6113]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6888443231582642, train/raw-loss = 0.6888443231582642, train/logprobs = tensor([[-0.5059, -0.8147],
        [-0.5616, -0.7695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6999958753585815, train/raw-loss = 0.6999958753585815, train/logprobs = tensor([[-0.5190, -0.9021],
        [-0.6372, -0.9421]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6868782043457031, train/raw-loss = 0.6868782043457031, train/logprobs = tensor([[-0.5553, -0.8519],
        [-0.6188, -0.8229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6534253358840942, train/raw-loss = 0.6534253358840942, train/logprobs = tensor([[-0.5538, -0.6275],
        [-0.7098, -0.5815]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6725401878356934, train/raw-loss = 0.6725401878356934, train/logprobs = tensor([[-0.5072, -0.6012],
        [-0.5746, -0.5788]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6914855241775513, train/raw-loss = 0.6914855241775513, train/logprobs = tensor([[-0.4882, -0.9264],
        [-0.5444, -0.8706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.712100088596344, train/raw-loss = 0.712100088596344, train/logprobs = tensor([[-0.5736, -1.5632],
        [-0.6603, -1.3202]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.7069870829582214, train/raw-loss = 0.7069870829582214, train/logprobs = tensor([[-0.4357, -0.7664],
        [-0.4971, -0.8192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Model saved, deleting model...
Deleted model...
Epoch 0, Step 32: train/loss = 0.6586158871650696, train/raw-loss = 0.6586158871650696, train/logprobs = tensor([[-0.6479, -0.6800],
        [-0.7919, -0.6437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0018278236966580153
Epoch 0, Step 33: train/loss = 0.6400350332260132, train/raw-loss = 0.6400350332260132, train/logprobs = tensor([[-0.5721, -0.7954],
        [-0.7568, -0.7036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001491393195465207
Epoch 0, Step 34: train/loss = 0.6486823558807373, train/raw-loss = 0.6486823558807373, train/logprobs = tensor([[-0.4693, -0.8120],
        [-0.5414, -0.6622]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0013777993153780699
Epoch 0, Step 35: train/loss = 0.6589661836624146, train/raw-loss = 0.6589661836624146, train/logprobs = tensor([[-0.5603, -0.7084],
        [-0.6660, -0.6618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016713208751752973
Epoch 0, Step 36: train/loss = 0.6534896492958069, train/raw-loss = 0.6534896492958069, train/logprobs = tensor([[-0.5700, -1.1431],
        [-0.6951, -1.0195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0017469070153310895
Epoch 0, Step 37: train/loss = 0.6591128706932068, train/raw-loss = 0.6591128706932068, train/logprobs = tensor([[-0.5236, -0.9392],
        [-0.5861, -0.8035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016917649190872908
Epoch 0, Step 38: train/loss = 0.6656225323677063, train/raw-loss = 0.6656225323677063, train/logprobs = tensor([[-0.4531, -0.7962],
        [-0.4958, -0.6828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015649432316422462
Epoch 0, Step 39: train/loss = 0.6812907457351685, train/raw-loss = 0.6812907457351685, train/logprobs = tensor([[-0.6367, -1.0774],
        [-0.7288, -1.0193]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0019114863825961947
Epoch 0, Step 40: train/loss = 0.666608989238739, train/raw-loss = 0.666608989238739, train/logprobs = tensor([[-0.5555, -0.6771],
        [-0.6239, -0.6246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015840213745832443
Epoch 0, Step 41: train/loss = 0.6566414833068848, train/raw-loss = 0.6566414833068848, train/logprobs = tensor([[-0.4596, -0.7926],
        [-0.5153, -0.6746]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016352500533685088
Epoch 0, Step 42: train/loss = 0.6886658072471619, train/raw-loss = 0.6886658072471619, train/logprobs = tensor([[-0.4531, -1.1436],
        [-0.4969, -0.9577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014672991819679737
Epoch 0, Step 43: train/loss = 0.6608837246894836, train/raw-loss = 0.6608837246894836, train/logprobs = tensor([[-0.4673, -1.1066],
        [-0.5673, -0.9842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015274062752723694
Epoch 0, Step 44: train/loss = 0.6894073486328125, train/raw-loss = 0.6894073486328125, train/logprobs = tensor([[-0.5341, -1.0884],
        [-0.6248, -1.0191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015420051058754325
Epoch 0, Step 45: train/loss = 0.6724807620048523, train/raw-loss = 0.6724807620048523, train/logprobs = tensor([[-0.5153, -0.8742],
        [-0.5760, -0.7288]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015431224601343274
Epoch 0, Step 46: train/loss = 0.6698247790336609, train/raw-loss = 0.6698247790336609, train/logprobs = tensor([[-0.4909, -0.7541],
        [-0.5282, -0.6532]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015962379984557629
Epoch 0, Step 47: train/loss = 0.6834325790405273, train/raw-loss = 0.6834325790405273, train/logprobs = tensor([[-0.5170, -0.5905],
        [-0.5556, -0.5767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0018020414281636477
Epoch 0, Step 48: train/loss = 0.665650486946106, train/raw-loss = 0.665650486946106, train/logprobs = tensor([[-0.6124, -0.7483],
        [-0.6584, -0.6115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0053986115381121635
Epoch 0, Step 49: train/loss = 0.6539136171340942, train/raw-loss = 0.6539136171340942, train/logprobs = tensor([[-0.5252, -1.3734],
        [-0.6050, -1.0584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005809367634356022
Epoch 0, Step 50: train/loss = 0.6410663723945618, train/raw-loss = 0.6410663723945618, train/logprobs = tensor([[-0.6423, -1.0107],
        [-0.7649, -0.8729]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005695422645658255
Epoch 0, Step 51: train/loss = 0.6324430704116821, train/raw-loss = 0.6324430704116821, train/logprobs = tensor([[-0.6671, -0.8913],
        [-0.7491, -0.6942]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005400805734097958
Epoch 0, Step 52: train/loss = 0.6704328060150146, train/raw-loss = 0.6704328060150146, train/logprobs = tensor([[-0.5426, -0.7325],
        [-0.5823, -0.6606]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005066631827503443
Epoch 0, Step 53: train/loss = 0.6484501361846924, train/raw-loss = 0.6484501361846924, train/logprobs = tensor([[-0.5509, -1.3762],
        [-0.6162, -0.9611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005483765620738268
Epoch 0, Step 54: train/loss = 0.6491665244102478, train/raw-loss = 0.6491665244102478, train/logprobs = tensor([[-0.6429, -1.1957],
        [-0.7401, -0.9865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005575662013143301
Epoch 0, Step 55: train/loss = 0.65452641248703, train/raw-loss = 0.65452641248703, train/logprobs = tensor([[-0.6320, -0.7873],
        [-0.7121, -0.6587]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005315748509019613
Epoch 0, Step 56: train/loss = 0.6701130270957947, train/raw-loss = 0.6701130270957947, train/logprobs = tensor([[-0.4746, -0.5659],
        [-0.5009, -0.4907]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00487268902361393
Epoch 0, Step 57: train/loss = 0.6595357656478882, train/raw-loss = 0.6595357656478882, train/logprobs = tensor([[-0.5255, -0.9069],
        [-0.5827, -0.7558]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005411138292402029
Epoch 0, Step 58: train/loss = 0.6112997531890869, train/raw-loss = 0.6112997531890869, train/logprobs = tensor([[-0.5705, -1.4503],
        [-0.7837, -1.0641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004820760805159807
Epoch 0, Step 59: train/loss = 0.6838104128837585, train/raw-loss = 0.6838104128837585, train/logprobs = tensor([[-0.4387, -0.6933],
        [-0.4718, -0.6510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0042042783461511135
Epoch 0, Step 60: train/loss = 0.6751739382743835, train/raw-loss = 0.6751739382743835, train/logprobs = tensor([[-0.5738, -0.6174],
        [-0.6567, -0.6141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005324734840542078
Epoch 0, Step 61: train/loss = 0.6550546884536743, train/raw-loss = 0.6550546884536743, train/logprobs = tensor([[-0.5974, -1.1468],
        [-0.6426, -0.9093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00505894934758544
Epoch 0, Step 62: train/loss = 0.6693048477172852, train/raw-loss = 0.6693048477172852, train/logprobs = tensor([[-0.5558, -0.5762],
        [-0.6106, -0.5282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005840658210217953
Epoch 0, Step 63: train/loss = 0.650115430355072, train/raw-loss = 0.650115430355072, train/logprobs = tensor([[-0.6457, -1.3482],
        [-0.7663, -1.0941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005523309111595154
Model saved, deleting model...
Deleted model...
Epoch 0, Step 64: train/loss = 0.6463796496391296, train/raw-loss = 0.6463796496391296, train/logprobs = tensor([[-0.6373, -0.8178],
        [-0.6694, -0.6422]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021459409967064857
Epoch 0, Step 65: train/loss = 0.653246283531189, train/raw-loss = 0.653246283531189, train/logprobs = tensor([[-0.4383, -0.6406],
        [-0.4598, -0.4603]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017332011833786964
Epoch 0, Step 66: train/loss = 0.6482946276664734, train/raw-loss = 0.6482946276664734, train/logprobs = tensor([[-0.5673, -0.8269],
        [-0.5660, -0.6116]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01957141049206257
Epoch 0, Step 67: train/loss = 0.6374750733375549, train/raw-loss = 0.6374750733375549, train/logprobs = tensor([[-0.4526, -0.8330],
        [-0.4497, -0.5507]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01975901424884796
Epoch 0, Step 68: train/loss = 0.6455776691436768, train/raw-loss = 0.6455776691436768, train/logprobs = tensor([[-0.5202, -0.8555],
        [-0.5313, -0.6387]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02305503562092781
Epoch 0, Step 69: train/loss = 0.6410342454910278, train/raw-loss = 0.6410342454910278, train/logprobs = tensor([[-0.5316, -0.9015],
        [-0.5364, -0.6089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01836824230849743
Epoch 0, Step 70: train/loss = 0.6244285106658936, train/raw-loss = 0.6244285106658936, train/logprobs = tensor([[-0.4832, -1.0532],
        [-0.4663, -0.6787]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018400950357317924
Epoch 0, Step 71: train/loss = 0.6236985325813293, train/raw-loss = 0.6236985325813293, train/logprobs = tensor([[-0.6149, -0.9852],
        [-0.6277, -0.6530]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021342115476727486
Epoch 0, Step 72: train/loss = 0.6825543642044067, train/raw-loss = 0.6825543642044067, train/logprobs = tensor([[-0.4459, -1.5004],
        [-0.5284, -1.0028]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018126606941223145
Epoch 0, Step 73: train/loss = 0.6359025239944458, train/raw-loss = 0.6359025239944458, train/logprobs = tensor([[-0.5651, -0.8862],
        [-0.5811, -0.6358]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024098772555589676
Epoch 0, Step 74: train/loss = 0.6552302837371826, train/raw-loss = 0.6552302837371826, train/logprobs = tensor([[-0.4597, -0.8659],
        [-0.4638, -0.6312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02288949489593506
Epoch 0, Step 75: train/loss = 0.6299169063568115, train/raw-loss = 0.6299169063568115, train/logprobs = tensor([[-0.5015, -1.9539],
        [-0.5126, -1.0611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017957894131541252
Epoch 0, Step 76: train/loss = 0.6598354578018188, train/raw-loss = 0.6598354578018188, train/logprobs = tensor([[-0.4645, -0.6577],
        [-0.4765, -0.5202]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02075171284377575
Epoch 0, Step 77: train/loss = 0.6363564729690552, train/raw-loss = 0.6363564729690552, train/logprobs = tensor([[-0.5728, -0.9979],
        [-0.5706, -0.6629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02464645728468895
Epoch 0, Step 78: train/loss = 0.6448310017585754, train/raw-loss = 0.6448310017585754, train/logprobs = tensor([[-0.4980, -0.9973],
        [-0.5141, -0.7254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021403908729553223
Epoch 0, Step 79: train/loss = 0.6578118801116943, train/raw-loss = 0.6578118801116943, train/logprobs = tensor([[-0.4949, -0.9979],
        [-0.4768, -0.6669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02115892991423607
Epoch 0, Step 80: train/loss = 0.6102741956710815, train/raw-loss = 0.6102741956710815, train/logprobs = tensor([[-0.5358, -1.3323],
        [-0.5469, -0.8291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022420424968004227
Epoch 0, Step 81: train/loss = 0.6312261819839478, train/raw-loss = 0.6312261819839478, train/logprobs = tensor([[-0.4928, -0.8560],
        [-0.5164, -0.5818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024299120530486107
Epoch 0, Step 82: train/loss = 0.6295955181121826, train/raw-loss = 0.6295955181121826, train/logprobs = tensor([[-0.4486, -0.8860],
        [-0.4520, -0.5509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021916544064879417
Epoch 0, Step 83: train/loss = 0.5987155437469482, train/raw-loss = 0.5987155437469482, train/logprobs = tensor([[-0.6561, -1.0752],
        [-0.7113, -0.6609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025873135775327682
Epoch 0, Step 84: train/loss = 0.6472184062004089, train/raw-loss = 0.6472184062004089, train/logprobs = tensor([[-0.5027, -0.8821],
        [-0.4612, -0.5936]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02621428482234478
Epoch 0, Step 85: train/loss = 0.6350002884864807, train/raw-loss = 0.6350002884864807, train/logprobs = tensor([[-0.4991, -1.0300],
        [-0.4757, -0.6972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02284231036901474
Epoch 0, Step 86: train/loss = 0.6488233804702759, train/raw-loss = 0.6488233804702759, train/logprobs = tensor([[-0.4489, -0.8562],
        [-0.4809, -0.6254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020753921940922737
Epoch 0, Step 87: train/loss = 0.6220951080322266, train/raw-loss = 0.6220951080322266, train/logprobs = tensor([[-0.5524, -0.8795],
        [-0.5526, -0.5514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025912266224622726
Epoch 0, Step 88: train/loss = 0.6222155690193176, train/raw-loss = 0.6222155690193176, train/logprobs = tensor([[-0.6645, -1.1170],
        [-0.6617, -0.7461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028107574209570885
Epoch 0, Step 89: train/loss = 0.6174572110176086, train/raw-loss = 0.6174572110176086, train/logprobs = tensor([[-0.4724, -1.1284],
        [-0.4644, -0.6692]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024249302223324776
Epoch 0, Step 90: train/loss = 0.6320602893829346, train/raw-loss = 0.6320602893829346, train/logprobs = tensor([[-0.5588, -0.9249],
        [-0.5617, -0.6105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023564869537949562
Epoch 0, Step 91: train/loss = 0.6139838695526123, train/raw-loss = 0.6139838695526123, train/logprobs = tensor([[-0.4891, -1.1557],
        [-0.4645, -0.6894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02351956255733967
Epoch 0, Step 92: train/loss = 0.6252492070198059, train/raw-loss = 0.6252492070198059, train/logprobs = tensor([[-0.5761, -1.0587],
        [-0.5737, -0.6683]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02303571254014969
Epoch 0, Step 93: train/loss = 0.6487128734588623, train/raw-loss = 0.6487128734588623, train/logprobs = tensor([[-0.4743, -0.9720],
        [-0.5058, -0.7385]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022568538784980774
Epoch 0, Step 94: train/loss = 0.6351134181022644, train/raw-loss = 0.6351134181022644, train/logprobs = tensor([[-0.5493, -1.2132],
        [-0.5657, -0.8048]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024690324440598488
Epoch 0, Step 95: train/loss = 0.6400836110115051, train/raw-loss = 0.6400836110115051, train/logprobs = tensor([[-0.4988, -0.6573],
        [-0.5327, -0.4465]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022472284734249115
