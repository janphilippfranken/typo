Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  5.93s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.30s/it]
  0%|          | 0/100 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/1 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 300.37 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 291.39 examples/s]
  1%|          | 1/100 [00:12<19:58, 12.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 2/2 [00:00<00:00, 654.90 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 2/2 [00:00<00:00, 619.54 examples/s]
  2%|▏         | 2/100 [00:52<47:06, 28.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 3/3 [00:00<00:00, 973.91 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 3/3 [00:00<00:00, 941.69 examples/s]
  3%|▎         | 3/100 [03:01<2:00:05, 74.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.

Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s][A
Saving the dataset (1/1 shards): 100%|██████████| 4/4 [00:00<00:00, 1264.39 examples/s][ASaving the dataset (1/1 shards): 100%|██████████| 4/4 [00:00<00:00, 1223.63 examples/s]
  4%|▍         | 4/100 [04:05<1:52:35, 70.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
slurmstepd: error: *** JOB 7061907 ON cocoflops-hgx-1 CANCELLED AT 2023-12-01T15:34:15 ***
