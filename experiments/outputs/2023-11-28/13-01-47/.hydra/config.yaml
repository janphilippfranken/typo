model_config:
  pretrained_model_name_or_path: meta-llama/Llama-2-13b-chat-hf
  load_in_8bit: true
  device_map: auto
  torch_dtype: float16
  model_cache_dir: /scr/jphilipp/scai/pretrained_models/Llama-2-13b-chat-hf
  tokenizer_cache_dir: /scr/jphilipp/scai/pretrained_models/Llama-2-13b-chat-hf
inference_config:
  max_new_tokens: 500
  do_sample: true
  top_p: 0.9
  temperature: 0.1
