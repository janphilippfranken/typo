[2024-03-12 16:45:41,000] torch.distributed.run: [WARNING] 
[2024-03-12 16:45:41,000] torch.distributed.run: [WARNING] *****************************************
[2024-03-12 16:45:41,000] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-12 16:45:41,000] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_164544-fis6tgkj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/fis6tgkj
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_164544-lmiuatnb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/lmiuatnb
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_164544-gx3ynmjt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/gx3ynmjt
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_164544-fsodwe22
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/fsodwe22
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.36s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.36s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.34s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.39s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.59s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.56s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.28s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.59s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.57s/it]
  0%|          | 0/148 [00:00<?, ?it/s]  1%|          | 1/148 [00:03<08:11,  3.34s/it]  1%|‚ñè         | 2/148 [00:06<08:05,  3.32s/it]  2%|‚ñè         | 3/148 [00:09<07:52,  3.26s/it]  3%|‚ñé         | 4/148 [00:12<07:38,  3.19s/it]  3%|‚ñé         | 5/148 [00:15<07:22,  3.09s/it]                                                 3%|‚ñé         | 5/148 [00:16<07:22,  3.09s/it]  4%|‚ñç         | 6/148 [00:20<08:19,  3.52s/it]  5%|‚ñç         | 7/148 [00:23<08:00,  3.41s/it]  5%|‚ñå         | 8/148 [00:26<07:49,  3.35s/it]  6%|‚ñå         | 9/148 [00:29<07:31,  3.25s/it]  7%|‚ñã         | 10/148 [00:33<07:38,  3.32s/it]                                                  7%|‚ñã         | 10/148 [00:33<07:38,  3.32s/it]  7%|‚ñã         | 11/148 [00:36<07:41,  3.37s/it]  8%|‚ñä         | 12/148 [00:39<07:25,  3.28s/it]  9%|‚ñâ         | 13/148 [00:42<07:15,  3.23s/it]  9%|‚ñâ         | 14/148 [00:46<07:14,  3.24s/it] 10%|‚ñà         | 15/148 [00:49<07:03,  3.18s/it]                                                 10%|‚ñà         | 15/148 [00:49<07:03,  3.18s/it] 11%|‚ñà         | 16/148 [00:52<06:58,  3.17s/it] 11%|‚ñà‚ñè        | 17/148 [00:55<06:49,  3.12s/it] 12%|‚ñà‚ñè        | 18/148 [00:58<07:01,  3.24s/it] 13%|‚ñà‚ñé        | 19/148 [01:02<06:59,  3.25s/it] 14%|‚ñà‚ñé        | 20/148 [01:05<06:47,  3.18s/it]                                                 14%|‚ñà‚ñé        | 20/148 [01:05<06:47,  3.18s/it] 14%|‚ñà‚ñç        | 21/148 [01:08<06:40,  3.15s/it] 15%|‚ñà‚ñç        | 22/148 [01:11<06:32,  3.11s/it] 16%|‚ñà‚ñå        | 23/148 [01:14<06:27,  3.10s/it] 16%|‚ñà‚ñå        | 24/148 [01:17<06:27,  3.12s/it] 17%|‚ñà‚ñã        | 25/148 [01:20<06:21,  3.10s/it]                                                 17%|‚ñà‚ñã        | 25/148 [01:20<06:21,  3.10s/it] 18%|‚ñà‚ñä        | 26/148 [01:23<06:20,  3.12s/it] 18%|‚ñà‚ñä        | 27/148 [01:27<06:28,  3.21s/it] 19%|‚ñà‚ñâ        | 28/148 [01:29<06:14,  3.12s/it] 20%|‚ñà‚ñâ        | 29/148 [01:33<06:21,  3.21s/it] 20%|‚ñà‚ñà        | 30/148 [01:36<06:13,  3.16s/it]                                                 20%|‚ñà‚ñà        | 30/148 [01:36<06:13,  3.16s/it] 21%|‚ñà‚ñà        | 31/148 [01:39<06:14,  3.20s/it] 22%|‚ñà‚ñà‚ñè       | 32/148 [01:42<06:12,  3.21s/it] 22%|‚ñà‚ñà‚ñè       | 33/148 [01:46<06:04,  3.17s/it] 23%|‚ñà‚ñà‚ñé       | 34/148 [01:49<05:59,  3.15s/it] 24%|‚ñà‚ñà‚ñé       | 35/148 [01:52<06:10,  3.28s/it]                                                 24%|‚ñà‚ñà‚ñé       | 35/148 [01:52<06:10,  3.28s/it] 24%|‚ñà‚ñà‚ñç       | 36/148 [01:55<05:58,  3.20s/it] 25%|‚ñà‚ñà‚ñå       | 37/148 [01:59<06:03,  3.28s/it] 26%|‚ñà‚ñà‚ñå       | 38/148 [02:02<05:57,  3.25s/it] 26%|‚ñà‚ñà‚ñã       | 39/148 [02:05<05:51,  3.23s/it] 27%|‚ñà‚ñà‚ñã       | 40/148 [02:09<05:56,  3.31s/it]                                                 27%|‚ñà‚ñà‚ñã       | 40/148 [02:09<05:56,  3.31s/it] 28%|‚ñà‚ñà‚ñä       | 41/148 [02:11<05:42,  3.20s/it] 28%|‚ñà‚ñà‚ñä       | 42/148 [02:15<05:41,  3.22s/it] 29%|‚ñà‚ñà‚ñâ       | 43/148 [02:18<05:37,  3.21s/it] 30%|‚ñà‚ñà‚ñâ       | 44/148 [02:21<05:36,  3.24s/it] 30%|‚ñà‚ñà‚ñà       | 45/148 [02:24<05:29,  3.20s/it]                                                 30%|‚ñà‚ñà‚ñà       | 45/148 [02:24<05:29,  3.20s/it] 31%|‚ñà‚ñà‚ñà       | 46/148 [02:27<05:22,  3.16s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 47/148 [02:31<05:19,  3.16s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 48/148 [02:34<05:13,  3.14s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 49/148 [02:37<05:06,  3.10s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 50/148 [02:40<05:07,  3.14s/it]                                                 34%|‚ñà‚ñà‚ñà‚ñç      | 50/148 [02:40<05:07,  3.14s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 51/148 [02:43<05:05,  3.15s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 52/148 [02:46<05:07,  3.21s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 53/148 [02:50<05:02,  3.19s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 54/148 [02:53<05:00,  3.19s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 55/148 [02:56<05:01,  3.24s/it]                                                 37%|‚ñà‚ñà‚ñà‚ñã      | 55/148 [02:56<05:01,  3.24s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 56/148 [02:59<04:50,  3.16s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 57/148 [03:02<04:45,  3.14s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 58/148 [03:05<04:37,  3.08s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 59/148 [03:08<04:37,  3.11s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 60/148 [03:11<04:34,  3.12s/it]                                                 41%|‚ñà‚ñà‚ñà‚ñà      | 60/148 [03:11<04:34,  3.12s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 61/148 [03:14<04:25,  3.05s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 62/148 [03:18<04:35,  3.20s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 63/148 [03:21<04:22,  3.09s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 64/148 [03:24<04:19,  3.09s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 65/148 [03:27<04:18,  3.12s/it]                                                 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 65/148 [03:27<04:18,  3.12s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 66/148 [03:30<04:20,  3.17s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 67/148 [03:33<04:12,  3.12s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 68/148 [03:37<04:13,  3.17s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 69/148 [03:40<04:09,  3.16s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 70/148 [03:43<04:04,  3.14s/it]                                                 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 70/148 [03:43<04:04,  3.14s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 71/148 [03:46<03:59,  3.11s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 72/148 [03:49<03:51,  3.05s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 73/148 [03:52<03:49,  3.06s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 74/148 [03:55<03:41,  3.00s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 75/148 [03:58<03:47,  3.12s/it]                                                 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 75/148 [03:58<03:47,  3.12s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 76/148 [04:01<03:39,  3.05s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 77/148 [04:04<03:37,  3.06s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 78/148 [04:07<03:38,  3.13s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 79/148 [04:10<03:31,  3.06s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 80/148 [04:14<03:33,  3.14s/it]                                                 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 80/148 [04:14<03:33,  3.14s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 81/148 [04:17<03:29,  3.13s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 82/148 [04:20<03:29,  3.18s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 83/148 [04:23<03:26,  3.18s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 84/148 [04:26<03:24,  3.20s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 85/148 [04:30<03:22,  3.22s/it]                                                 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 85/148 [04:30<03:22,  3.22s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 86/148 [04:33<03:18,  3.19s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 87/148 [04:36<03:13,  3.18s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 88/148 [04:39<03:12,  3.21s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 89/148 [04:43<03:10,  3.23s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 90/148 [04:46<03:07,  3.23s/it]                                                 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 90/148 [04:46<03:07,  3.23s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 91/148 [04:49<03:04,  3.24s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 92/148 [04:52<03:01,  3.25s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 93/148 [04:56<02:58,  3.25s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 94/148 [04:59<02:55,  3.26s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 95/148 [05:02<02:51,  3.23s/it]                                                 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 95/148 [05:02<02:51,  3.23s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 96/148 [05:05<02:45,  3.18s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 97/148 [05:08<02:44,  3.23s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 98/148 [05:12<02:40,  3.22s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 99/148 [05:15<02:39,  3.25s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 100/148 [05:18<02:39,  3.31s/it]                                                  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 100/148 [05:18<02:39,  3.31s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 101/148 [05:21<02:33,  3.26s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 102/148 [05:25<02:31,  3.29s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 103/148 [05:28<02:24,  3.20s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 104/148 [05:31<02:22,  3.25s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 105/148 [05:34<02:18,  3.21s/it]                                                  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 105/148 [05:34<02:18,  3.21s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 106/148 [05:37<02:13,  3.17s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 107/148 [05:41<02:09,  3.16s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 108/148 [05:44<02:03,  3.10s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 109/148 [05:47<02:04,  3.19s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 110/148 [05:50<02:01,  3.20s/it]                                                  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 110/148 [05:50<02:01,  3.20s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 111/148 [05:53<01:57,  3.17s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 112/148 [05:57<01:56,  3.23s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 113/148 [06:00<01:50,  3.16s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 114/148 [06:03<01:47,  3.16s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 115/148 [06:06<01:46,  3.23s/it]                                                  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 115/148 [06:06<01:46,  3.23s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 116/148 [06:09<01:43,  3.24s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 117/148 [06:13<01:40,  3.23s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 118/148 [06:16<01:36,  3.23s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 119/148 [06:19<01:33,  3.23s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 120/148 [06:22<01:27,  3.13s/it]                                                  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 120/148 [06:22<01:27,  3.13s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 121/148 [06:25<01:24,  3.13s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 122/148 [06:28<01:22,  3.16s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 123/148 [06:32<01:21,  3.26s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 124/148 [06:35<01:17,  3.21s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 125/148 [06:38<01:12,  3.16s/it]                                                  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 125/148 [06:38<01:12,  3.16s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 126/148 [06:41<01:09,  3.17s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 127/148 [06:44<01:05,  3.14s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 128/148 [06:48<01:03,  3.20s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 129/148 [06:51<01:01,  3.22s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 130/148 [06:55<01:00,  3.36s/it]                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 130/148 [06:55<01:00,  3.36s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 131/148 [06:58<00:57,  3.40s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 132/148 [07:01<00:53,  3.34s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 133/148 [07:04<00:49,  3.29s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 134/148 [07:08<00:47,  3.38s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 135/148 [07:11<00:43,  3.33s/it]                                                  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 135/148 [07:11<00:43,  3.33s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 136/148 [07:14<00:38,  3.25s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 137/148 [07:18<00:36,  3.29s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 138/148 [07:21<00:32,  3.28s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 139/148 [07:24<00:29,  3.26s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 140/148 [07:27<00:26,  3.28s/it]                                                  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 140/148 [07:27<00:26,  3.28s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 141/148 [07:31<00:22,  3.23s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 142/148 [07:34<00:18,  3.16s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 143/148 [07:37<00:15,  3.14s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 144/148 [07:40<00:12,  3.18s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 145/148 [07:43<00:09,  3.18s/it]                                                  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 145/148 [07:43<00:09,  3.18s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 146/148 [07:46<00:06,  3.19s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 147/148 [07:49<00:03,  3.15s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [07:53<00:00,  3.20s/it]/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [08:58<00:00,  3.20s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [08:58<00:00,  3.64s/it]
Removed shared tensor {'model.layers.12.mlp.up_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.norm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.11.mlp.down_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.20.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.norm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.10.input_layernorm.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.5.mlp.down_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.norm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.11.input_layernorm.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
ERROR: Cannot find key: wandb.name=sft-positive
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence |
                         transformers | copy | random
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | format_example_dpo |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      EOS_TOKEN | BOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
ERROR: Cannot find key: wandb.name=sft-positive
ERROR: Cannot find key: wandb.name=sft-positive
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence |
                         transformers | copy | random
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | format_example_dpo |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      EOS_TOKEN | BOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence |
                         transformers | copy | random
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | format_example_dpo |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      EOS_TOKEN | BOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
wandb: - 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.023 MB of 0.045 MB uploaded (0.002 MB deduped)wandb: | 0.023 MB of 0.041 MB uploaded (0.002 MB deduped)wandb: üöÄ View run sft-positive at: https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/lmiuatnb
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzQ4MzI0NA==/version_details/v7
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240312_164544-lmiuatnb/logs
wandb: / 0.045 MB of 0.045 MB uploaded (0.002 MB deduped)wandb: / 0.023 MB of 0.045 MB uploaded (0.002 MB deduped)wandb: üöÄ View run sft-positive at: https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/fsodwe22
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzQ4MzI0NA==/version_details/v7
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240312_164544-fsodwe22/logs
wandb: üöÄ View run sft-positive at: https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/gx3ynmjt
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzQ4MzI0NA==/version_details/v7
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240312_164544-gx3ynmjt/logs
[2024-03-12 16:55:51,589] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094458 closing signal SIGTERM
[2024-03-12 16:55:51,590] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094459 closing signal SIGTERM
[2024-03-12 16:55:51,590] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3094461 closing signal SIGTERM
[2024-03-12 16:55:52,808] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 2 (pid: 3094460) of binary: /scr/jphilipp/miniconda3/envs/typo/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/typo/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1008, in launch_command
    deepspeed_launcher(args)
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/accelerate/commands/launch.py", line 724, in deepspeed_launcher
    distrib_run.run(args)
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_sft.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-12_16:55:51
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 3094460)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
