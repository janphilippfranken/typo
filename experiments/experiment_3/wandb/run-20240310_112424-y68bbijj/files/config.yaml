wandb_version: 1

model:
  desc: null
  value:
    model_type: huggingface
    name: mistral_7b_base
    model_config:
      pretrained_model_name_or_path: mistralai/Mistral-7B-v0.1
      cache_dir: /scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1
    tokenizer_config:
      pretrained_model_name_or_path: mistralai/Mistral-7B-v0.1
      cache_dir: /scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1
      model_max_length: 2048
data:
  desc: null
  value:
    data_path: data
    helpful: train-helpful-0-10k-iteration-0.json
    harmless: train-harmless-0-10k-iteration-0.json
    helpful_both_negative: train-helpful-0-10k-iteration-0-both-negative.json
    harmless_both_negative: train-harmless-0-10k-iteration-0-both-negative.json
    n_examples: 6250
    n_responses: 2
    n_constitutions: 2
wandb:
  desc: null
  value:
    project: hh-rlhf-sft-exp-2
    name: sft-iteration-1
training_args:
  desc: null
  value:
    output_dir: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-2-v2/sft-iteration-1
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 2
    gradient_accumulation_steps: 16
    learning_rate: 1.0e-06
    num_train_epochs: 1
    save_total_limit: 1
    evaluation_strategy: 'no'
    seed: 42
    save_strategy: epoch
    bf16: true
    warmup_steps: 150
    lr_scheduler_type: linear
    do_eval: true
    logging_steps: 5
    logging_strategy: steps
_wandb:
  desc: null
  value:
    python_version: 3.10.0
    cli_version: 0.16.4
    framework: huggingface
    huggingface_version: 4.38.0
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1710095064.0
    t:
      1:
      - 1
      - 11
      - 49
      - 50
      - 51
      - 55
      - 71
      2:
      - 1
      - 11
      - 49
      - 50
      - 51
      - 55
      - 71
      3:
      - 13
      - 16
      - 23
      4: 3.10.0
      5: 0.16.4
      6: 4.38.0
      8:
      - 5
      13: linux-x86_64
