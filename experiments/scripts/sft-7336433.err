[2024-03-07 08:26:35,445] torch.distributed.run: [WARNING] 
[2024-03-07 08:26:35,445] torch.distributed.run: [WARNING] *****************************************
[2024-03-07 08:26:35,445] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-07 08:26:35,445] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)
wandb: - Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_082639-k3mzfztj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/k3mzfztj
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_082639-80oecvlg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/80oecvlg
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_082639-bj5mafwi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: Tracking run with wandb version 0.16.0
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/bj5mafwi
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_082639-jm5lxcgt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/jm5lxcgt
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.99s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.71s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.80s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.90s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.04s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.58s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.90s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.60s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.93s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.64s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.98s/it]
  0%|          | 0/365 [00:00<?, ?it/s]/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  0%|          | 1/365 [00:07<44:41,  7.37s/it]  1%|          | 2/365 [00:12<36:00,  5.95s/it]  1%|          | 3/365 [00:17<33:10,  5.50s/it]  1%|          | 4/365 [00:22<31:34,  5.25s/it]  1%|‚ñè         | 5/365 [00:26<30:01,  5.00s/it]                                                 1%|‚ñè         | 5/365 [00:28<30:01,  5.00s/it]  2%|‚ñè         | 6/365 [00:32<32:31,  5.44s/it]  2%|‚ñè         | 7/365 [00:38<31:43,  5.32s/it]  2%|‚ñè         | 8/365 [00:42<30:45,  5.17s/it]  2%|‚ñè         | 9/365 [00:47<29:46,  5.02s/it]  3%|‚ñé         | 10/365 [00:52<28:54,  4.89s/it]                                                  3%|‚ñé         | 10/365 [00:52<28:54,  4.89s/it]  3%|‚ñé         | 11/365 [00:57<28:48,  4.88s/it]  3%|‚ñé         | 12/365 [01:01<28:24,  4.83s/it]  4%|‚ñé         | 13/365 [01:06<28:09,  4.80s/it]  4%|‚ñç         | 14/365 [01:11<28:16,  4.83s/it]  4%|‚ñç         | 15/365 [01:16<27:49,  4.77s/it]                                                  4%|‚ñç         | 15/365 [01:16<27:49,  4.77s/it]  4%|‚ñç         | 16/365 [01:20<27:26,  4.72s/it]  5%|‚ñç         | 17/365 [01:25<27:21,  4.72s/it]  5%|‚ñç         | 18/365 [01:30<27:25,  4.74s/it]  5%|‚ñå         | 19/365 [01:34<27:16,  4.73s/it]  5%|‚ñå         | 20/365 [01:39<27:29,  4.78s/it]                                                  5%|‚ñå         | 20/365 [01:39<27:29,  4.78s/it]  6%|‚ñå         | 21/365 [01:44<26:56,  4.70s/it]  6%|‚ñå         | 22/365 [01:49<26:56,  4.71s/it]  6%|‚ñã         | 23/365 [01:53<27:09,  4.77s/it]  7%|‚ñã         | 24/365 [01:58<27:07,  4.77s/it]  7%|‚ñã         | 25/365 [02:03<26:49,  4.73s/it]                                                  7%|‚ñã         | 25/365 [02:03<26:49,  4.73s/it]  7%|‚ñã         | 26/365 [02:08<27:00,  4.78s/it]  7%|‚ñã         | 27/365 [02:12<26:41,  4.74s/it]  8%|‚ñä         | 28/365 [02:17<26:20,  4.69s/it]  8%|‚ñä         | 29/365 [02:22<26:37,  4.75s/it]  8%|‚ñä         | 30/365 [02:27<26:32,  4.75s/it]                                                  8%|‚ñä         | 30/365 [02:27<26:32,  4.75s/it]  8%|‚ñä         | 31/365 [02:31<26:36,  4.78s/it]  9%|‚ñâ         | 32/365 [02:36<26:15,  4.73s/it]  9%|‚ñâ         | 33/365 [02:41<26:11,  4.73s/it]  9%|‚ñâ         | 34/365 [02:46<26:14,  4.76s/it] 10%|‚ñâ         | 35/365 [02:50<26:16,  4.78s/it]                                                 10%|‚ñâ         | 35/365 [02:50<26:16,  4.78s/it] 10%|‚ñâ         | 36/365 [02:55<26:01,  4.75s/it] 10%|‚ñà         | 37/365 [03:00<25:52,  4.73s/it] 10%|‚ñà         | 38/365 [03:05<25:50,  4.74s/it] 11%|‚ñà         | 39/365 [03:09<25:40,  4.72s/it] 11%|‚ñà         | 40/365 [03:14<25:17,  4.67s/it]                                                 11%|‚ñà         | 40/365 [03:14<25:17,  4.67s/it] 11%|‚ñà         | 41/365 [03:19<25:26,  4.71s/it] 12%|‚ñà‚ñè        | 42/365 [03:23<25:32,  4.75s/it] 12%|‚ñà‚ñè        | 43/365 [03:28<25:10,  4.69s/it] 12%|‚ñà‚ñè        | 44/365 [03:33<25:15,  4.72s/it] 12%|‚ñà‚ñè        | 45/365 [03:38<25:25,  4.77s/it]                                                 12%|‚ñà‚ñè        | 45/365 [03:38<25:25,  4.77s/it] 13%|‚ñà‚ñé        | 46/365 [03:42<25:08,  4.73s/it] 13%|‚ñà‚ñé        | 47/365 [03:47<24:53,  4.70s/it] 13%|‚ñà‚ñé        | 48/365 [03:51<24:35,  4.65s/it] 13%|‚ñà‚ñé        | 49/365 [03:57<25:19,  4.81s/it] 14%|‚ñà‚ñé        | 50/365 [04:01<25:06,  4.78s/it]                                                 14%|‚ñà‚ñé        | 50/365 [04:01<25:06,  4.78s/it] 14%|‚ñà‚ñç        | 51/365 [04:06<25:34,  4.89s/it] 14%|‚ñà‚ñç        | 52/365 [04:11<25:26,  4.88s/it] 15%|‚ñà‚ñç        | 53/365 [04:16<24:57,  4.80s/it] 15%|‚ñà‚ñç        | 54/365 [04:21<24:35,  4.74s/it] 15%|‚ñà‚ñå        | 55/365 [04:25<24:34,  4.76s/it]                                                 15%|‚ñà‚ñå        | 55/365 [04:25<24:34,  4.76s/it] 15%|‚ñà‚ñå        | 56/365 [04:30<24:41,  4.79s/it] 16%|‚ñà‚ñå        | 57/365 [04:35<24:42,  4.81s/it] 16%|‚ñà‚ñå        | 58/365 [04:40<24:28,  4.78s/it] 16%|‚ñà‚ñå        | 59/365 [04:45<24:35,  4.82s/it] 16%|‚ñà‚ñã        | 60/365 [04:49<24:25,  4.81s/it]                                                 16%|‚ñà‚ñã        | 60/365 [04:49<24:25,  4.81s/it] 17%|‚ñà‚ñã        | 61/365 [04:55<24:45,  4.89s/it] 17%|‚ñà‚ñã        | 62/365 [05:00<24:50,  4.92s/it] 17%|‚ñà‚ñã        | 63/365 [05:05<24:48,  4.93s/it] 18%|‚ñà‚ñä        | 64/365 [05:10<24:55,  4.97s/it] 18%|‚ñà‚ñä        | 65/365 [05:14<24:35,  4.92s/it]                                                 18%|‚ñà‚ñä        | 65/365 [05:14<24:35,  4.92s/it] 18%|‚ñà‚ñä        | 66/365 [05:19<24:36,  4.94s/it] 18%|‚ñà‚ñä        | 67/365 [05:24<24:47,  4.99s/it] 19%|‚ñà‚ñä        | 68/365 [05:29<24:09,  4.88s/it] 19%|‚ñà‚ñâ        | 69/365 [05:34<23:41,  4.80s/it] 19%|‚ñà‚ñâ        | 70/365 [05:38<23:18,  4.74s/it]                                                 19%|‚ñà‚ñâ        | 70/365 [05:38<23:18,  4.74s/it] 19%|‚ñà‚ñâ        | 71/365 [05:43<22:49,  4.66s/it] 20%|‚ñà‚ñâ        | 72/365 [05:48<22:53,  4.69s/it] 20%|‚ñà‚ñà        | 73/365 [05:52<22:47,  4.68s/it] 20%|‚ñà‚ñà        | 74/365 [05:57<22:56,  4.73s/it] 21%|‚ñà‚ñà        | 75/365 [06:02<22:37,  4.68s/it]                                                 21%|‚ñà‚ñà        | 75/365 [06:02<22:37,  4.68s/it] 21%|‚ñà‚ñà        | 76/365 [06:06<22:34,  4.69s/it] 21%|‚ñà‚ñà        | 77/365 [06:11<22:34,  4.70s/it] 21%|‚ñà‚ñà‚ñè       | 78/365 [06:16<22:36,  4.73s/it] 22%|‚ñà‚ñà‚ñè       | 79/365 [06:20<22:10,  4.65s/it] 22%|‚ñà‚ñà‚ñè       | 80/365 [06:25<22:30,  4.74s/it]                                                 22%|‚ñà‚ñà‚ñè       | 80/365 [06:25<22:30,  4.74s/it] 22%|‚ñà‚ñà‚ñè       | 81/365 [06:30<22:34,  4.77s/it] 22%|‚ñà‚ñà‚ñè       | 82/365 [06:35<22:15,  4.72s/it] 23%|‚ñà‚ñà‚ñé       | 83/365 [06:39<22:05,  4.70s/it] 23%|‚ñà‚ñà‚ñé       | 84/365 [06:44<21:48,  4.66s/it] 23%|‚ñà‚ñà‚ñé       | 85/365 [06:49<21:44,  4.66s/it]                                                 23%|‚ñà‚ñà‚ñé       | 85/365 [06:49<21:44,  4.66s/it] 24%|‚ñà‚ñà‚ñé       | 86/365 [06:53<21:43,  4.67s/it] 24%|‚ñà‚ñà‚ñç       | 87/365 [06:58<21:43,  4.69s/it] 24%|‚ñà‚ñà‚ñç       | 88/365 [07:03<21:45,  4.71s/it] 24%|‚ñà‚ñà‚ñç       | 89/365 [07:07<21:37,  4.70s/it] 25%|‚ñà‚ñà‚ñç       | 90/365 [07:12<21:31,  4.70s/it]                                                 25%|‚ñà‚ñà‚ñç       | 90/365 [07:12<21:31,  4.70s/it] 25%|‚ñà‚ñà‚ñç       | 91/365 [07:17<21:25,  4.69s/it] 25%|‚ñà‚ñà‚ñå       | 92/365 [07:22<21:21,  4.70s/it] 25%|‚ñà‚ñà‚ñå       | 93/365 [07:26<21:27,  4.73s/it] 26%|‚ñà‚ñà‚ñå       | 94/365 [07:31<21:14,  4.70s/it] 26%|‚ñà‚ñà‚ñå       | 95/365 [07:36<21:17,  4.73s/it]                                                 26%|‚ñà‚ñà‚ñå       | 95/365 [07:36<21:17,  4.73s/it] 26%|‚ñà‚ñà‚ñã       | 96/365 [07:41<21:28,  4.79s/it] 27%|‚ñà‚ñà‚ñã       | 97/365 [07:45<21:14,  4.76s/it] 27%|‚ñà‚ñà‚ñã       | 98/365 [07:50<21:06,  4.74s/it] 27%|‚ñà‚ñà‚ñã       | 99/365 [07:55<20:50,  4.70s/it] 27%|‚ñà‚ñà‚ñã       | 100/365 [07:59<20:43,  4.69s/it]                                                  27%|‚ñà‚ñà‚ñã       | 100/365 [07:59<20:43,  4.69s/it] 28%|‚ñà‚ñà‚ñä       | 101/365 [08:04<20:40,  4.70s/it] 28%|‚ñà‚ñà‚ñä       | 102/365 [08:09<20:47,  4.74s/it] 28%|‚ñà‚ñà‚ñä       | 103/365 [08:13<20:26,  4.68s/it] 28%|‚ñà‚ñà‚ñä       | 104/365 [08:18<20:27,  4.70s/it] 29%|‚ñà‚ñà‚ñâ       | 105/365 [08:23<20:01,  4.62s/it]                                                  29%|‚ñà‚ñà‚ñâ       | 105/365 [08:23<20:01,  4.62s/it] 29%|‚ñà‚ñà‚ñâ       | 106/365 [08:28<20:17,  4.70s/it] 29%|‚ñà‚ñà‚ñâ       | 107/365 [08:33<20:37,  4.80s/it] 30%|‚ñà‚ñà‚ñâ       | 108/365 [08:37<20:24,  4.77s/it] 30%|‚ñà‚ñà‚ñâ       | 109/365 [08:42<20:19,  4.76s/it] 30%|‚ñà‚ñà‚ñà       | 110/365 [08:47<20:11,  4.75s/it]                                                  30%|‚ñà‚ñà‚ñà       | 110/365 [08:47<20:11,  4.75s/it] 30%|‚ñà‚ñà‚ñà       | 111/365 [08:52<20:10,  4.77s/it] 31%|‚ñà‚ñà‚ñà       | 112/365 [08:56<19:57,  4.73s/it] 31%|‚ñà‚ñà‚ñà       | 113/365 [09:01<19:56,  4.75s/it] 31%|‚ñà‚ñà‚ñà       | 114/365 [09:06<19:51,  4.75s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 115/365 [09:11<19:50,  4.76s/it]                                                  32%|‚ñà‚ñà‚ñà‚ñè      | 115/365 [09:11<19:50,  4.76s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 116/365 [09:15<19:39,  4.74s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 117/365 [09:20<19:35,  4.74s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 118/365 [09:25<19:35,  4.76s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 119/365 [09:29<19:20,  4.72s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 120/365 [09:34<19:01,  4.66s/it]                                                  33%|‚ñà‚ñà‚ñà‚ñé      | 120/365 [09:34<19:01,  4.66s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 121/365 [09:39<19:03,  4.69s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 122/365 [09:44<19:17,  4.76s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 123/365 [09:48<19:01,  4.72s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 124/365 [09:53<18:44,  4.67s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 125/365 [09:58<19:04,  4.77s/it]                                                  34%|‚ñà‚ñà‚ñà‚ñç      | 125/365 [09:58<19:04,  4.77s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 126/365 [10:02<18:41,  4.69s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 127/365 [10:07<19:01,  4.80s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 128/365 [10:12<18:37,  4.71s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 129/365 [10:17<18:33,  4.72s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 130/365 [10:21<18:43,  4.78s/it]                                                  36%|‚ñà‚ñà‚ñà‚ñå      | 130/365 [10:21<18:43,  4.78s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 131/365 [10:26<18:28,  4.74s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 132/365 [10:31<18:20,  4.73s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 133/365 [10:36<18:21,  4.75s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 134/365 [10:40<18:20,  4.76s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 135/365 [10:45<18:04,  4.72s/it]                                                  37%|‚ñà‚ñà‚ñà‚ñã      | 135/365 [10:45<18:04,  4.72s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 136/365 [10:50<17:51,  4.68s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 137/365 [10:54<17:55,  4.72s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 138/365 [10:59<17:47,  4.70s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 139/365 [11:04<17:50,  4.74s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 140/365 [11:09<17:45,  4.73s/it]                                                  38%|‚ñà‚ñà‚ñà‚ñä      | 140/365 [11:09<17:45,  4.73s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 141/365 [11:13<17:33,  4.70s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 142/365 [11:18<17:48,  4.79s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 143/365 [11:23<17:26,  4.71s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 144/365 [11:28<17:22,  4.72s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 145/365 [11:33<17:36,  4.80s/it]                                                  40%|‚ñà‚ñà‚ñà‚ñâ      | 145/365 [11:33<17:36,  4.80s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 146/365 [11:37<17:28,  4.79s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 147/365 [11:42<17:16,  4.76s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 148/365 [11:47<17:02,  4.71s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 149/365 [11:51<16:44,  4.65s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 150/365 [11:56<16:34,  4.63s/it]                                                  41%|‚ñà‚ñà‚ñà‚ñà      | 150/365 [11:56<16:34,  4.63s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 151/365 [12:00<16:39,  4.67s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 152/365 [12:05<16:27,  4.64s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 153/365 [12:10<16:24,  4.64s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 154/365 [12:15<16:42,  4.75s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 155/365 [12:20<16:54,  4.83s/it]                                                  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 155/365 [12:20<16:54,  4.83s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 156/365 [12:24<16:38,  4.78s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 157/365 [12:29<16:36,  4.79s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 158/365 [12:34<16:22,  4.75s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 159/365 [12:38<16:09,  4.71s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 160/365 [12:43<16:00,  4.69s/it]                                                  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 160/365 [12:43<16:00,  4.69s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 161/365 [12:48<16:06,  4.74s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 162/365 [12:53<16:06,  4.76s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 163/365 [12:57<15:40,  4.66s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 164/365 [13:02<15:43,  4.69s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 165/365 [13:06<15:30,  4.65s/it]                                                  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 165/365 [13:06<15:30,  4.65s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 166/365 [13:11<15:42,  4.74s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 167/365 [13:16<15:39,  4.74s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 168/365 [13:21<15:31,  4.73s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 169/365 [13:26<15:33,  4.76s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 170/365 [13:31<15:34,  4.79s/it]                                                  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 170/365 [13:31<15:34,  4.79s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 171/365 [13:35<15:28,  4.79s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 172/365 [13:40<15:28,  4.81s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 173/365 [13:45<15:29,  4.84s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 174/365 [13:50<15:04,  4.74s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 175/365 [13:54<15:07,  4.78s/it]                                                  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 175/365 [13:54<15:07,  4.78s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 176/365 [13:59<15:01,  4.77s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 177/365 [14:04<15:21,  4.90s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 178/365 [14:09<15:11,  4.87s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 179/365 [14:14<14:39,  4.73s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 180/365 [14:18<14:35,  4.73s/it]                                                  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 180/365 [14:18<14:35,  4.73s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 181/365 [14:23<14:21,  4.68s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 182/365 [14:28<14:16,  4.68s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 183/365 [14:32<14:08,  4.66s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 184/365 [14:37<14:01,  4.65s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 185/365 [14:41<13:50,  4.61s/it]                                                  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 185/365 [14:41<13:50,  4.61s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 186/365 [14:46<13:51,  4.65s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 187/365 [14:51<13:57,  4.71s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 188/365 [14:56<14:09,  4.80s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 189/365 [15:01<14:11,  4.84s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 190/365 [15:06<13:59,  4.80s/it]                                                  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 190/365 [15:06<13:59,  4.80s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 191/365 [15:10<13:50,  4.77s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 192/365 [15:15<13:46,  4.78s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 193/365 [15:20<13:33,  4.73s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 194/365 [15:24<13:15,  4.65s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 195/365 [15:29<13:18,  4.69s/it]                                                  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 195/365 [15:29<13:18,  4.69s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 196/365 [15:34<13:28,  4.79s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 197/365 [15:39<13:30,  4.83s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 198/365 [15:43<13:04,  4.70s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 199/365 [15:48<12:58,  4.69s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 200/365 [15:52<12:43,  4.63s/it]                                                  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 200/365 [15:52<12:43,  4.63s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 201/365 [15:57<12:43,  4.65s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 202/365 [16:02<12:43,  4.68s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 203/365 [16:07<12:53,  4.78s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 204/365 [16:11<12:36,  4.70s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 205/365 [16:16<12:34,  4.72s/it]                                                  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 205/365 [16:16<12:34,  4.72s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 206/365 [16:21<12:22,  4.67s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 207/365 [16:25<12:13,  4.64s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 208/365 [16:30<12:12,  4.67s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 209/365 [16:35<12:05,  4.65s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 210/365 [16:40<12:20,  4.78s/it]                                                  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 210/365 [16:40<12:20,  4.78s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 211/365 [16:45<12:16,  4.79s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 212/365 [16:49<12:17,  4.82s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 213/365 [16:54<12:08,  4.79s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 214/365 [16:59<11:53,  4.73s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 215/365 [17:03<11:45,  4.71s/it]                                                  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 215/365 [17:03<11:45,  4.71s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 216/365 [17:08<11:41,  4.71s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 217/365 [17:13<11:43,  4.75s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 218/365 [17:18<11:38,  4.75s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 219/365 [17:22<11:28,  4.72s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 220/365 [17:27<11:27,  4.74s/it]                                                  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 220/365 [17:27<11:27,  4.74s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 221/365 [17:32<11:22,  4.74s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 222/365 [17:37<11:16,  4.73s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 223/365 [17:41<11:12,  4.73s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 224/365 [17:46<11:09,  4.75s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 225/365 [17:51<11:07,  4.77s/it]                                                  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 225/365 [17:51<11:07,  4.77s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 226/365 [17:56<11:00,  4.76s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 227/365 [18:00<10:56,  4.76s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 228/365 [18:05<10:49,  4.74s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 229/365 [18:10<10:59,  4.85s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 230/365 [18:15<10:44,  4.77s/it]                                                  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 230/365 [18:15<10:44,  4.77s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 231/365 [18:19<10:31,  4.71s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 232/365 [18:24<10:38,  4.80s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 233/365 [18:29<10:30,  4.77s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 234/365 [18:34<10:22,  4.75s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 235/365 [18:38<10:13,  4.72s/it]                                                  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 235/365 [18:38<10:13,  4.72s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 236/365 [18:43<10:14,  4.76s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 237/365 [18:48<10:04,  4.72s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 238/365 [18:53<09:58,  4.71s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 239/365 [18:57<09:54,  4.71s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 240/365 [19:02<09:49,  4.72s/it]                                                  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 240/365 [19:02<09:49,  4.72s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 241/365 [19:07<09:46,  4.73s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 242/365 [19:12<09:43,  4.74s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 243/365 [19:16<09:42,  4.77s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 244/365 [19:21<09:39,  4.79s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 245/365 [19:26<09:33,  4.78s/it]                                                  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 245/365 [19:26<09:33,  4.78s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 246/365 [19:31<09:33,  4.82s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 247/365 [19:36<09:42,  4.94s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 248/365 [19:41<09:34,  4.91s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 249/365 [19:46<09:21,  4.84s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 250/365 [19:50<09:08,  4.77s/it]                                                  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 250/365 [19:50<09:08,  4.77s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 251/365 [19:55<09:07,  4.81s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 252/365 [20:00<09:01,  4.79s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 253/365 [20:05<09:01,  4.84s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 254/365 [20:09<08:44,  4.72s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 255/365 [20:14<08:41,  4.74s/it]                                                  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 255/365 [20:14<08:41,  4.74s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 256/365 [20:19<08:36,  4.74s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 257/365 [20:23<08:29,  4.71s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 258/365 [20:28<08:25,  4.73s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 259/365 [20:33<08:20,  4.72s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 260/365 [20:38<08:13,  4.70s/it]                                                  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 260/365 [20:38<08:13,  4.70s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 261/365 [20:43<08:16,  4.77s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 262/365 [20:47<08:11,  4.77s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 263/365 [20:52<08:04,  4.75s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 264/365 [20:57<08:08,  4.83s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 265/365 [21:02<08:00,  4.81s/it]                                                  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 265/365 [21:02<08:00,  4.81s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 266/365 [21:06<07:51,  4.76s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 267/365 [21:11<07:49,  4.79s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 268/365 [21:16<07:36,  4.70s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 269/365 [21:21<07:35,  4.75s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 270/365 [21:25<07:32,  4.76s/it]                                                  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 270/365 [21:25<07:32,  4.76s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 271/365 [21:30<07:27,  4.76s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 272/365 [21:35<07:17,  4.70s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 273/365 [21:40<07:16,  4.75s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 274/365 [21:44<07:14,  4.78s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 275/365 [21:49<07:11,  4.80s/it]                                                  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 275/365 [21:49<07:11,  4.80s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 276/365 [21:54<07:01,  4.74s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 277/365 [21:59<06:57,  4.74s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 278/365 [22:03<06:53,  4.75s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 279/365 [22:08<06:46,  4.73s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 280/365 [22:13<06:45,  4.77s/it]                                                  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 280/365 [22:13<06:45,  4.77s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 281/365 [22:18<06:45,  4.83s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 282/365 [22:23<06:40,  4.83s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 283/365 [22:27<06:28,  4.74s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 284/365 [22:32<06:21,  4.71s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 285/365 [22:37<06:15,  4.70s/it]                                                  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 285/365 [22:37<06:15,  4.70s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 286/365 [22:41<06:14,  4.74s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 287/365 [22:46<06:11,  4.77s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 288/365 [22:51<06:06,  4.76s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 289/365 [22:56<06:00,  4.74s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 290/365 [23:00<05:51,  4.69s/it]                                                  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 290/365 [23:00<05:51,  4.69s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 291/365 [23:05<05:46,  4.68s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 292/365 [23:10<05:41,  4.68s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 293/365 [23:14<05:37,  4.69s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 294/365 [23:19<05:35,  4.72s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 295/365 [23:24<05:34,  4.78s/it]                                                  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 295/365 [23:24<05:34,  4.78s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 296/365 [23:29<05:27,  4.75s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 297/365 [23:33<05:20,  4.71s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 298/365 [23:38<05:14,  4.69s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 299/365 [23:42<05:06,  4.64s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 300/365 [23:47<05:03,  4.67s/it]                                                  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 300/365 [23:47<05:03,  4.67s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 301/365 [23:52<05:01,  4.72s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 302/365 [23:57<04:53,  4.65s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 303/365 [24:01<04:51,  4.70s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 304/365 [24:06<04:43,  4.64s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 305/365 [24:11<04:41,  4.69s/it]                                                  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 305/365 [24:11<04:41,  4.69s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 306/365 [24:15<04:37,  4.71s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 307/365 [24:20<04:31,  4.68s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 308/365 [24:25<04:25,  4.65s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 309/365 [24:29<04:23,  4.70s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 310/365 [24:34<04:19,  4.71s/it]                                                  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 310/365 [24:34<04:19,  4.71s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 311/365 [24:39<04:16,  4.74s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 312/365 [24:44<04:11,  4.75s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 313/365 [24:49<04:08,  4.77s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 314/365 [24:53<04:00,  4.72s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 315/365 [24:58<03:57,  4.75s/it]                                                  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 315/365 [24:58<03:57,  4.75s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 316/365 [25:03<03:50,  4.71s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 317/365 [25:08<03:50,  4.80s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 318/365 [25:12<03:42,  4.73s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 319/365 [25:17<03:36,  4.70s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 320/365 [25:21<03:28,  4.63s/it]                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 320/365 [25:21<03:28,  4.63s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 321/365 [25:26<03:26,  4.70s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 322/365 [25:31<03:23,  4.73s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 323/365 [25:36<03:17,  4.71s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 324/365 [25:40<03:12,  4.71s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 325/365 [25:45<03:08,  4.71s/it]                                                  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 325/365 [25:45<03:08,  4.71s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 326/365 [25:50<03:03,  4.71s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 327/365 [25:55<03:00,  4.74s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 328/365 [25:59<02:56,  4.78s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 329/365 [26:04<02:52,  4.80s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 330/365 [26:09<02:48,  4.81s/it]                                                  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 330/365 [26:09<02:48,  4.81s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 331/365 [26:14<02:41,  4.76s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 332/365 [26:18<02:36,  4.73s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 333/365 [26:23<02:31,  4.73s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 334/365 [26:28<02:27,  4.75s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 335/365 [26:33<02:23,  4.77s/it]                                                  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 335/365 [26:33<02:23,  4.77s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 336/365 [26:38<02:18,  4.77s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 337/365 [26:43<02:16,  4.87s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 338/365 [26:47<02:10,  4.85s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 339/365 [26:52<02:04,  4.77s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 340/365 [26:57<01:59,  4.77s/it]                                                  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 340/365 [26:57<01:59,  4.77s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 341/365 [27:02<01:54,  4.76s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 342/365 [27:06<01:50,  4.79s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 343/365 [27:11<01:44,  4.76s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 344/365 [27:16<01:39,  4.75s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 345/365 [27:21<01:35,  4.80s/it]                                                  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 345/365 [27:21<01:35,  4.80s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 346/365 [27:25<01:29,  4.71s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 347/365 [27:30<01:24,  4.68s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 348/365 [27:35<01:20,  4.72s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 349/365 [27:40<01:16,  4.80s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 350/365 [27:44<01:11,  4.78s/it]                                                  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 350/365 [27:44<01:11,  4.78s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 351/365 [27:49<01:06,  4.73s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 352/365 [27:54<01:01,  4.71s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 353/365 [27:59<00:57,  4.76s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 354/365 [28:03<00:52,  4.74s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 355/365 [28:08<00:47,  4.76s/it]                                                  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 355/365 [28:08<00:47,  4.76s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 356/365 [28:13<00:43,  4.80s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 357/365 [28:18<00:38,  4.76s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 358/365 [28:22<00:33,  4.72s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 359/365 [28:27<00:28,  4.76s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 360/365 [28:32<00:24,  4.82s/it]                                                  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 360/365 [28:32<00:24,  4.82s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 361/365 [28:37<00:19,  4.80s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 362/365 [28:41<00:14,  4.75s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 363/365 [28:46<00:09,  4.72s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 364/365 [28:51<00:04,  4.74s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 365/365 [28:56<00:00,  4.78s/it]                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 365/365 [28:56<00:00,  4.78s/it]/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 365/365 [30:01<00:00,  4.78s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 365/365 [30:01<00:00,  4.94s/it]
Removed shared tensor {'model.layers.20.mlp.gate_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.norm.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.21.mlp.up_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.15.self_attn.v_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.norm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.12.mlp.gate_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.0.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.norm.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
ERROR: Cannot find key: wandb.name=sft-iteration-1-no-sorry-positive
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence | string |
                         transformers | copy
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass |
                         SupervisedPragmaticDataCollator |
                         remove_final_answer | get_first_question |
                         format_responses | format_example |
                         format_example_dpo | tokenize_func |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      BOS_TOKEN | EOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
ERROR: Cannot find key: wandb.name=sft-iteration-1-no-sorry-positive
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence | string |
                         transformers | copy
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass |
                         SupervisedPragmaticDataCollator |
                         remove_final_answer | get_first_question |
                         format_responses | format_example |
                         format_example_dpo | tokenize_func |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      BOS_TOKEN | EOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
ERROR: Cannot find key: wandb.name=sft-iteration-1-no-sorry-positive
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence | string |
                         transformers | copy
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass |
                         SupervisedPragmaticDataCollator |
                         remove_final_answer | get_first_question |
                         format_responses | format_example |
                         format_example_dpo | tokenize_func |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      BOS_TOKEN | EOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
wandb: - 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run sft-iteration-1-no-sorry-positive at: https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/jm5lxcgt
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjUwMjA1NQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240307_082639-jm5lxcgt/logs
wandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run sft-iteration-1-no-sorry-positive at: https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/bj5mafwi
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjUwMjA1NQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240307_082639-bj5mafwi/logs
wandb: üöÄ View run sft-iteration-1-no-sorry-positive at: https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/80oecvlg
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjUwMjA1NQ==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240307_082639-80oecvlg/logs
[2024-03-07 08:58:22,374] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 268830 closing signal SIGTERM
[2024-03-07 08:58:23,541] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 1 (pid: 268831) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1008, in launch_command
    deepspeed_launcher(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/launch.py", line 724, in deepspeed_launcher
    distrib_run.run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_sft.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-07_08:58:22
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 268832)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-07_08:58:22
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 268833)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-07_08:58:22
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 268831)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
