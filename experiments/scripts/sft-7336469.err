[2024-03-07 09:08:36,030] torch.distributed.run: [WARNING] 
[2024-03-07 09:08:36,030] torch.distributed.run: [WARNING] *****************************************
[2024-03-07 09:08:36,030] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-07 09:08:36,030] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_090839-zckajc0o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive-ve+
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/zckajc0o
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_090839-fbq3pv06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive-ve+
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/fbq3pv06
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_090839-g7qb2t6b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive-ve+
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/g7qb2t6b
wandb: wandb version 0.16.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmalign/wandb/run-20240307_090839-u8m9z1vu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft-iteration-1-no-sorry-positive-ve+
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/u8m9z1vu
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.43s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.47s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.45s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.67s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.68s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.37s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.68s/it]
  0%|          | 0/182 [00:00<?, ?it/s]/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1652: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
  1%|          | 1/182 [00:05<15:55,  5.28s/it]  1%|          | 2/182 [00:09<14:34,  4.86s/it]  2%|‚ñè         | 3/182 [00:14<14:19,  4.80s/it]  2%|‚ñè         | 4/182 [00:19<14:10,  4.78s/it]  3%|‚ñé         | 5/182 [00:24<14:02,  4.76s/it]                                                 3%|‚ñé         | 5/182 [00:25<14:02,  4.76s/it]  3%|‚ñé         | 6/182 [00:30<15:10,  5.17s/it]  4%|‚ñç         | 7/182 [00:35<15:14,  5.23s/it]  4%|‚ñç         | 8/182 [00:40<14:55,  5.15s/it]  5%|‚ñç         | 9/182 [00:45<14:27,  5.01s/it]  5%|‚ñå         | 10/182 [00:49<14:13,  4.96s/it]                                                  5%|‚ñå         | 10/182 [00:49<14:13,  4.96s/it]  6%|‚ñå         | 11/182 [00:54<13:59,  4.91s/it]  7%|‚ñã         | 12/182 [00:59<14:01,  4.95s/it]  7%|‚ñã         | 13/182 [01:04<13:52,  4.92s/it]  8%|‚ñä         | 14/182 [01:09<13:30,  4.83s/it]  8%|‚ñä         | 15/182 [01:14<13:25,  4.82s/it]                                                  8%|‚ñä         | 15/182 [01:14<13:25,  4.82s/it]  9%|‚ñâ         | 16/182 [01:18<13:08,  4.75s/it]  9%|‚ñâ         | 17/182 [01:23<13:10,  4.79s/it] 10%|‚ñâ         | 18/182 [01:28<13:07,  4.80s/it] 10%|‚ñà         | 19/182 [01:33<13:06,  4.82s/it] 11%|‚ñà         | 20/182 [01:37<12:55,  4.79s/it]                                                 11%|‚ñà         | 20/182 [01:37<12:55,  4.79s/it] 12%|‚ñà‚ñè        | 21/182 [01:42<12:54,  4.81s/it] 12%|‚ñà‚ñè        | 22/182 [01:47<12:54,  4.84s/it] 13%|‚ñà‚ñé        | 23/182 [01:52<12:48,  4.83s/it] 13%|‚ñà‚ñé        | 24/182 [01:57<12:56,  4.92s/it] 14%|‚ñà‚ñé        | 25/182 [02:02<12:45,  4.87s/it]                                                 14%|‚ñà‚ñé        | 25/182 [02:02<12:45,  4.87s/it] 14%|‚ñà‚ñç        | 26/182 [02:07<12:35,  4.84s/it] 15%|‚ñà‚ñç        | 27/182 [02:11<12:29,  4.84s/it] 15%|‚ñà‚ñå        | 28/182 [02:17<12:35,  4.90s/it] 16%|‚ñà‚ñå        | 29/182 [02:21<12:27,  4.88s/it] 16%|‚ñà‚ñã        | 30/182 [02:26<12:21,  4.88s/it]                                                 16%|‚ñà‚ñã        | 30/182 [02:26<12:21,  4.88s/it] 17%|‚ñà‚ñã        | 31/182 [02:31<12:05,  4.80s/it] 18%|‚ñà‚ñä        | 32/182 [02:36<12:01,  4.81s/it] 18%|‚ñà‚ñä        | 33/182 [02:40<11:53,  4.79s/it] 19%|‚ñà‚ñä        | 34/182 [02:45<11:39,  4.72s/it] 19%|‚ñà‚ñâ        | 35/182 [02:50<11:29,  4.69s/it]                                                 19%|‚ñà‚ñâ        | 35/182 [02:50<11:29,  4.69s/it] 20%|‚ñà‚ñâ        | 36/182 [02:54<11:24,  4.69s/it] 20%|‚ñà‚ñà        | 37/182 [02:59<11:32,  4.78s/it] 21%|‚ñà‚ñà        | 38/182 [03:04<11:13,  4.68s/it] 21%|‚ñà‚ñà‚ñè       | 39/182 [03:08<11:09,  4.68s/it] 22%|‚ñà‚ñà‚ñè       | 40/182 [03:13<11:14,  4.75s/it]                                                 22%|‚ñà‚ñà‚ñè       | 40/182 [03:13<11:14,  4.75s/it] 23%|‚ñà‚ñà‚ñé       | 41/182 [03:18<11:13,  4.78s/it] 23%|‚ñà‚ñà‚ñé       | 42/182 [03:23<11:05,  4.75s/it] 24%|‚ñà‚ñà‚ñé       | 43/182 [03:27<10:56,  4.72s/it] 24%|‚ñà‚ñà‚ñç       | 44/182 [03:32<10:57,  4.77s/it] 25%|‚ñà‚ñà‚ñç       | 45/182 [03:37<10:43,  4.70s/it]                                                 25%|‚ñà‚ñà‚ñç       | 45/182 [03:37<10:43,  4.70s/it] 25%|‚ñà‚ñà‚ñå       | 46/182 [03:42<10:37,  4.69s/it] 26%|‚ñà‚ñà‚ñå       | 47/182 [03:46<10:37,  4.73s/it] 26%|‚ñà‚ñà‚ñã       | 48/182 [03:51<10:32,  4.72s/it] 27%|‚ñà‚ñà‚ñã       | 49/182 [03:56<10:29,  4.73s/it] 27%|‚ñà‚ñà‚ñã       | 50/182 [04:01<10:22,  4.72s/it]                                                 27%|‚ñà‚ñà‚ñã       | 50/182 [04:01<10:22,  4.72s/it] 28%|‚ñà‚ñà‚ñä       | 51/182 [04:05<10:22,  4.75s/it] 29%|‚ñà‚ñà‚ñä       | 52/182 [04:10<10:18,  4.76s/it] 29%|‚ñà‚ñà‚ñâ       | 53/182 [04:15<10:22,  4.82s/it] 30%|‚ñà‚ñà‚ñâ       | 54/182 [04:20<10:32,  4.94s/it] 30%|‚ñà‚ñà‚ñà       | 55/182 [04:25<10:23,  4.91s/it]                                                 30%|‚ñà‚ñà‚ñà       | 55/182 [04:25<10:23,  4.91s/it] 31%|‚ñà‚ñà‚ñà       | 56/182 [04:30<10:13,  4.87s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 57/182 [04:35<10:06,  4.85s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 58/182 [04:39<09:54,  4.80s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 59/182 [04:44<09:44,  4.75s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 60/182 [04:49<09:43,  4.79s/it]                                                 33%|‚ñà‚ñà‚ñà‚ñé      | 60/182 [04:49<09:43,  4.79s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 61/182 [04:54<09:32,  4.73s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 62/182 [04:58<09:26,  4.72s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 63/182 [05:03<09:31,  4.80s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 64/182 [05:08<09:25,  4.79s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 65/182 [05:13<09:20,  4.79s/it]                                                 36%|‚ñà‚ñà‚ñà‚ñå      | 65/182 [05:13<09:20,  4.79s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 66/182 [05:18<09:14,  4.78s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 67/182 [05:23<09:17,  4.85s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 68/182 [05:27<09:09,  4.82s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 69/182 [05:32<08:55,  4.74s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 70/182 [05:37<08:49,  4.73s/it]                                                 38%|‚ñà‚ñà‚ñà‚ñä      | 70/182 [05:37<08:49,  4.73s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 71/182 [05:41<08:47,  4.76s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 72/182 [05:46<08:42,  4.75s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 73/182 [05:51<08:42,  4.79s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 74/182 [05:56<08:33,  4.76s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 75/182 [06:00<08:27,  4.74s/it]                                                 41%|‚ñà‚ñà‚ñà‚ñà      | 75/182 [06:00<08:27,  4.74s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 76/182 [06:05<08:21,  4.74s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 77/182 [06:10<08:21,  4.78s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 78/182 [06:15<08:16,  4.77s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 79/182 [06:19<08:02,  4.68s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 80/182 [06:24<08:06,  4.77s/it]                                                 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 80/182 [06:24<08:06,  4.77s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 81/182 [06:29<08:01,  4.77s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 82/182 [06:34<07:55,  4.76s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 83/182 [06:38<07:44,  4.69s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 84/182 [06:43<07:42,  4.72s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 85/182 [06:48<07:34,  4.68s/it]                                                 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 85/182 [06:48<07:34,  4.68s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 86/182 [06:52<07:33,  4.72s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 87/182 [06:57<07:31,  4.75s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 88/182 [07:02<07:29,  4.78s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 89/182 [07:07<07:23,  4.77s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 90/182 [07:11<07:16,  4.74s/it]                                                 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 90/182 [07:11<07:16,  4.74s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 91/182 [07:16<07:09,  4.72s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 92/182 [07:21<07:10,  4.79s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 93/182 [07:26<07:04,  4.77s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 94/182 [07:30<06:53,  4.70s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 95/182 [07:35<06:53,  4.75s/it]                                                 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 95/182 [07:35<06:53,  4.75s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 96/182 [07:40<06:45,  4.71s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 97/182 [07:45<06:39,  4.70s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 98/182 [07:49<06:40,  4.77s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 99/182 [07:54<06:37,  4.78s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 100/182 [07:59<06:30,  4.77s/it]                                                  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 100/182 [07:59<06:30,  4.77s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 101/182 [08:04<06:24,  4.75s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 102/182 [08:09<06:27,  4.84s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 103/182 [08:14<06:19,  4.81s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 104/182 [08:18<06:09,  4.73s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 105/182 [08:23<05:58,  4.66s/it]                                                  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 105/182 [08:23<05:58,  4.66s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 106/182 [08:27<05:55,  4.67s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 107/182 [08:32<05:54,  4.72s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 108/182 [08:37<05:55,  4.80s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 109/182 [08:42<05:59,  4.92s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 110/182 [08:47<05:51,  4.88s/it]                                                  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 110/182 [08:47<05:51,  4.88s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 111/182 [08:52<05:45,  4.86s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 112/182 [08:57<05:43,  4.90s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 113/182 [09:02<05:36,  4.87s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 114/182 [09:07<05:32,  4.88s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 115/182 [09:11<05:24,  4.84s/it]                                                  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 115/182 [09:11<05:24,  4.84s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 116/182 [09:16<05:16,  4.80s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 117/182 [09:21<05:11,  4.80s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 118/182 [09:25<05:04,  4.75s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 119/182 [09:31<05:05,  4.85s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 120/182 [09:35<05:01,  4.86s/it]                                                  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 120/182 [09:35<05:01,  4.86s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 121/182 [09:40<04:49,  4.75s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 122/182 [09:45<04:44,  4.75s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 123/182 [09:50<04:43,  4.81s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 124/182 [09:54<04:35,  4.74s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 125/182 [09:59<04:29,  4.73s/it]                                                  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 125/182 [09:59<04:29,  4.73s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 126/182 [10:04<04:24,  4.72s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 127/182 [10:08<04:19,  4.71s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 128/182 [10:13<04:13,  4.69s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 129/182 [10:18<04:10,  4.72s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 130/182 [10:23<04:06,  4.74s/it]                                                  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 130/182 [10:23<04:06,  4.74s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 131/182 [10:27<04:01,  4.73s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 132/182 [10:32<03:56,  4.73s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 133/182 [10:37<03:56,  4.82s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 134/182 [10:42<03:48,  4.76s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 135/182 [10:46<03:39,  4.68s/it]                                                  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 135/182 [10:46<03:39,  4.68s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 136/182 [10:51<03:35,  4.68s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 137/182 [10:56<03:37,  4.83s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 138/182 [11:01<03:30,  4.78s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 139/182 [11:06<03:26,  4.81s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 140/182 [11:11<03:25,  4.89s/it]                                                  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 140/182 [11:11<03:25,  4.89s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 141/182 [11:15<03:18,  4.85s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 142/182 [11:20<03:12,  4.82s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 143/182 [11:25<03:08,  4.84s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 144/182 [11:30<03:03,  4.82s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 145/182 [11:34<02:56,  4.76s/it]                                                  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 145/182 [11:34<02:56,  4.76s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 146/182 [11:39<02:54,  4.84s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 147/182 [11:44<02:47,  4.80s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 148/182 [11:49<02:41,  4.75s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 149/182 [11:53<02:36,  4.74s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 150/182 [11:58<02:30,  4.70s/it]                                                  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 150/182 [11:58<02:30,  4.70s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 151/182 [12:03<02:28,  4.79s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 152/182 [12:08<02:23,  4.80s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 153/182 [12:13<02:17,  4.75s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 154/182 [12:17<02:13,  4.77s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 155/182 [12:22<02:07,  4.73s/it]                                                  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 155/182 [12:22<02:07,  4.73s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 156/182 [12:27<02:02,  4.71s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 157/182 [12:32<02:00,  4.82s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 158/182 [12:37<01:56,  4.85s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 159/182 [12:41<01:50,  4.80s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 160/182 [12:46<01:45,  4.78s/it]                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 160/182 [12:46<01:45,  4.78s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 161/182 [12:51<01:39,  4.73s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 162/182 [12:55<01:34,  4.74s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 163/182 [13:00<01:30,  4.76s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 164/182 [13:05<01:26,  4.78s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 165/182 [13:10<01:20,  4.75s/it]                                                  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 165/182 [13:10<01:20,  4.75s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 166/182 [13:14<01:15,  4.74s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 167/182 [13:19<01:12,  4.80s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 168/182 [13:24<01:07,  4.80s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 169/182 [13:29<01:01,  4.72s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 170/182 [13:34<00:57,  4.77s/it]                                                  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 170/182 [13:34<00:57,  4.77s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 171/182 [13:38<00:52,  4.74s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 172/182 [13:43<00:47,  4.79s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 173/182 [13:48<00:42,  4.73s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 174/182 [13:53<00:38,  4.75s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 175/182 [13:57<00:33,  4.74s/it]                                                  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 175/182 [13:57<00:33,  4.74s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 176/182 [14:02<00:28,  4.67s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 177/182 [14:07<00:23,  4.76s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 178/182 [14:12<00:19,  4.78s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 179/182 [14:16<00:13,  4.66s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 180/182 [14:21<00:09,  4.67s/it]                                                  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 180/182 [14:21<00:09,  4.67s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 181/182 [14:25<00:04,  4.68s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [14:30<00:00,  4.69s/it]/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [15:32<00:00,  4.69s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182/182 [15:32<00:00,  5.12s/it]
Removed shared tensor {'model.layers.18.post_attention_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.norm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.10.self_attn.q_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.norm.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.1.input_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.norm.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.21.mlp.up_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
ERROR: Cannot find key: wandb.name=sft-iteration-1-no-sorry-positive-ve+
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence | string |
                         transformers | copy
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass |
                         SupervisedPragmaticDataCollator |
                         remove_final_answer | get_first_question |
                         format_responses | format_example |
                         format_example_dpo | tokenize_func |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      BOS_TOKEN | EOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
ERROR: Cannot find key: wandb.name=sft-iteration-1-no-sorry-positive-ve+
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence | string |
                         transformers | copy
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass |
                         SupervisedPragmaticDataCollator |
                         remove_final_answer | get_first_question |
                         format_responses | format_example |
                         format_example_dpo | tokenize_func |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      BOS_TOKEN | EOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
ERROR: Cannot find key: wandb.name=sft-iteration-1-no-sorry-positive-ve+
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence | string |
                         transformers | copy
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass |
                         SupervisedPragmaticDataCollator |
                         remove_final_answer | get_first_question |
                         format_responses | format_example |
                         format_example_dpo | tokenize_func |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      BOS_TOKEN | EOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
wandb: üöÄ View run sft-iteration-1-no-sorry-positive-ve+ at: https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/fbq3pv06
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjUwMjA1NQ==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240307_090839-fbq3pv06/logs
wandb: üöÄ View run sft-iteration-1-no-sorry-positive-ve+ at: https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/g7qb2t6b
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjUwMjA1NQ==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240307_090839-g7qb2t6b/logs
wandb: üöÄ View run sft-iteration-1-no-sorry-positive-ve+ at: https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/runs/zckajc0o
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/sft-no-sorry-positive/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NjUwMjA1NQ==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240307_090839-zckajc0o/logs
[2024-03-07 09:25:31,997] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 278061 closing signal SIGTERM
[2024-03-07 09:25:33,063] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 1 (pid: 278062) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1008, in launch_command
    deepspeed_launcher(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/launch.py", line 724, in deepspeed_launcher
    distrib_run.run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_sft.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-03-07_09:25:31
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 2 (pid: 278063)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-03-07_09:25:31
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 2 (pid: 278064)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-07_09:25:31
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 278062)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
