Processing examples: 0it [00:00, ?it/s]
Processed prompts:   0%|          | 0/1000 [00:00<?, ?it/s][A
Processed prompts:   0%|          | 1/1000 [00:16<4:31:51, 16.33s/it][A
Processed prompts:   0%|          | 2/1000 [00:17<2:01:22,  7.30s/it][A
Processed prompts:   0%|          | 3/1000 [00:19<1:21:16,  4.89s/it][A
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 257/1000 [00:35<01:10, 10.60it/s][A
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 259/1000 [00:38<01:20,  9.25it/s][A
Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 260/1000 [00:38<01:21,  9.08it/s][A
Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 513/1000 [00:55<00:37, 13.08it/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 515/1000 [00:56<00:39, 12.34it/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 516/1000 [00:58<00:43, 11.20it/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 517/1000 [00:58<00:43, 11.06it/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 518/1000 [00:58<00:44, 10.78it/s][A
Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 519/1000 [00:58<00:44, 10.77it/s][A
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 769/1000 [01:10<00:12, 18.15it/s][A
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 772/1000 [01:14<00:16, 13.67it/s][A
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 773/1000 [01:16<00:18, 12.31it/s][A
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 774/1000 [01:16<00:18, 11.98it/s][A
Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 775/1000 [01:16<00:19, 11.67it/s][AProcessed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:16<00:00, 13.05it/s]
Processing examples: 500it [01:17,  6.49it/s]Processing examples: 500it [01:17,  6.49it/s]
ERROR: Cannot find key: start_example=0
Usage: evaluate.py <group|command|value>
  available groups:      json | fire | hydra | np | random | List | Dict |
                         transformers | CONSTITUTIONS
  available commands:    DictConfig | tqdm | load_dataset |
                         VLLMInferenceModel | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | main
  available values:      PROMPT_GENERATION_ITERATION_0 | PROMPT_TRAINING |
                         SYSTEM_MESSAGE | GPT4_WIN_RATE | HELPFUL_PRINCIPLE |
                         NOT_HELPFUL_PRINCIPLE | HARMLESS_PRINCIPLE |
                         NOT_HARMLESS_PRINCIPLE

For detailed information on this command, run:
  evaluate.py --help
