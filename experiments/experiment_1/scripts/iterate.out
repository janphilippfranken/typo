0 200
INFO 03-11 09:21:49 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-11 09:21:53 llm_engine.py:79] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1', load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-11 09:22:07 weight_utils.py:163] Using model weights format ['*.safetensors']
[36m(RayWorkerVllm pid=2129282)[0m INFO 03-11 09:22:07 weight_utils.py:163] Using model weights format ['*.safetensors']
INFO 03-11 09:22:11 llm_engine.py:337] # GPU blocks: 129303, # CPU blocks: 8192
INFO 03-11 09:22:13 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-11 09:22:13 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=2129282)[0m INFO 03-11 09:22:13 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2129282)[0m INFO 03-11 09:22:13 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=2129593)[0m INFO 03-11 09:22:08 weight_utils.py:163] Using model weights format ['*.safetensors'][32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
INFO 03-11 09:22:18 model_runner.py:738] Graph capturing finished in 5 secs.
[36m(RayWorkerVllm pid=2129282)[0m INFO 03-11 09:22:18 model_runner.py:738] Graph capturing finished in 5 secs.
Skipping example 2
Skipping example 3
Skipping example 6
Skipping example 8
Skipping example 10
Skipping example 11
Skipping example 12
Skipping example 13
Skipping example 14
Skipping example 15
Skipping example 18
Skipping example 19
Skipping example 21
Skipping example 22
Skipping example 23
Skipping example 25
Skipping example 26
Skipping example 28
Skipping example 29
Skipping example 34
Skipping example 36
Skipping example 41
Skipping example 42
Skipping example 43
Skipping example 45
Skipping example 46
Skipping example 48
Skipping example 53
Skipping example 54
Skipping example 55
Skipping example 56
Skipping example 58
Skipping example 59
Skipping example 61
Skipping example 63
Skipping example 64
Skipping example 66
Skipping example 67
Skipping example 68
Skipping example 69
Skipping example 71
Skipping example 75
Skipping example 76
Skipping example 79
Skipping example 84
Skipping example 85
Skipping example 88
Skipping example 89
Skipping example 92
Skipping example 93
Skipping example 94
Skipping example 95
Skipping example 97
Skipping example 101
Skipping example 105
Skipping example 106
Skipping example 107
Skipping example 108
Skipping example 109
Skipping example 110
Skipping example 112
Skipping example 113
Skipping example 114
Skipping example 116
Skipping example 117
Skipping example 118
Skipping example 120
Skipping example 122
Skipping example 123
Skipping example 124
Skipping example 127
Skipping example 130
Skipping example 132
Skipping example 139
Skipping example 143
Skipping example 144
Skipping example 145
Skipping example 146
Skipping example 147
Skipping example 148
Skipping example 149
Skipping example 150
Skipping example 151
Skipping example 152
Skipping example 153
Skipping example 154
Skipping example 157
Skipping example 160
Skipping example 163
Skipping example 164
Skipping example 171
Skipping example 173
Skipping example 174
Skipping example 176
Skipping example 177
Skipping example 180
Skipping example 184
Skipping example 185
Skipping example 187
Skipping example 190
Skipping example 192
Skipping example 195
Skipping example 196
Skipping example 198
[36m(RayWorkerVllm pid=2129593)[0m INFO 03-11 09:22:13 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=2129593)[0m INFO 03-11 09:22:13 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=2129593)[0m INFO 03-11 09:22:18 model_runner.py:738] Graph capturing finished in 5 secs.[32m [repeated 2x across cluster][0m
