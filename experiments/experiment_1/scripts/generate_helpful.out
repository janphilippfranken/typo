Start Example: 4000
Max Example: 8000
4000 8000
True True True
INFO 03-11 15:24:49 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-11 15:24:58 llm_engine.py:79] Initializing an LLM engine with config: model='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-0.5-1e-6-iteration-0', tokenizer='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-0.5-1e-6-iteration-0', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-0.5-1e-6-iteration-0', load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-11 15:25:15 llm_engine.py:337] # GPU blocks: 61269, # CPU blocks: 4096
INFO 03-11 15:25:17 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-11 15:25:17 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=2428187)[0m INFO 03-11 15:25:17 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=2428187)[0m INFO 03-11 15:25:17 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-11 15:25:21 model_runner.py:738] Graph capturing finished in 4 secs.
[36m(RayWorkerVllm pid=2428187)[0m INFO 03-11 15:25:21 model_runner.py:738] Graph capturing finished in 4 secs.
Skipping example 24
Skipping example 67
Skipping example 82
Skipping example 436
Skipping example 440
Skipping example 485
Skipping example 595
Skipping example 609
Skipping example 921
Skipping example 925
Skipping example 969
Skipping example 1031
Skipping example 1048
Skipping example 1063
Skipping example 1076
Skipping example 1092
Skipping example 1201
Skipping example 1278
Skipping example 1317
Skipping example 1320
Skipping example 1390
Skipping example 1419
Skipping example 1579
Skipping example 1629
Skipping example 1681
Skipping example 1682
Skipping example 1886
Skipping example 1913
Skipping example 1980
Skipping example 1989
Skipping example 1998
Skipping example 2068
Skipping example 2096
Skipping example 2115
Skipping example 2134
Skipping example 2169
Skipping example 2192
Skipping example 2289
Skipping example 2426
Skipping example 2521
Skipping example 2605
Skipping example 2631
Skipping example 2737
Skipping example 2789
Skipping example 2871
Skipping example 2925
Skipping example 2938
Skipping example 2988
Skipping example 3019
Skipping example 3294
Skipping example 3296
Skipping example 3305
Skipping example 3349
Skipping example 3500
Skipping example 3514
Skipping example 3541
Skipping example 3598
Skipping example 3605
Skipping example 3638
Skipping example 3704
Skipping example 3764
Skipping example 3799
Skipping example 3827
Skipping example 3852
Skipping example 3973
