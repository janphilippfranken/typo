Start Example: 4000
Max Example: 8000
{'model': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', 'download_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', 'dtype': 'auto', 'quantization': None, 'tensor_parallel_size': 2}
4000 8000
True True True
INFO 03-13 16:11:44 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-13 16:11:53 llm_engine.py:79] Initializing an LLM engine with config: model='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', tokenizer='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-13 16:12:16 llm_engine.py:337] # GPU blocks: 61269, # CPU blocks: 4096
INFO 03-13 16:12:18 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-13 16:12:18 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=3441205)[0m INFO 03-13 16:12:18 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=3441205)[0m INFO 03-13 16:12:18 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-13 16:12:24 model_runner.py:738] Graph capturing finished in 6 secs.
[36m(RayWorkerVllm pid=3441205)[0m INFO 03-13 16:12:24 model_runner.py:738] Graph capturing finished in 6 secs.
Skipping example 5
Skipping example 82
Skipping example 199
Skipping example 215
Skipping example 287
Skipping example 530
Skipping example 622
Skipping example 859
Skipping example 969
Skipping example 1008
Skipping example 1038
Skipping example 1048
Skipping example 1194
Skipping example 1317
Skipping example 1419
Skipping example 1545
Skipping example 1681
Skipping example 1698
Skipping example 1721
Skipping example 1913
Skipping example 1980
Skipping example 1998
Skipping example 2192
Skipping example 2631
Skipping example 2789
Skipping example 2871
Skipping example 2912
Skipping example 2925
Skipping example 2938
Skipping example 2988
Skipping example 3086
Skipping example 3146
Skipping example 3447
Skipping example 3483
Skipping example 3500
Skipping example 3514
Skipping example 3583
Skipping example 3598
Skipping example 3863
