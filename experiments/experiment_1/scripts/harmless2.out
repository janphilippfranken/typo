ARGS {'output_dir': 'data', 'file_name': 'train-helpful-0-10k-iteration-2', 'start_example': 0, 'max_example': 10000, 'batch_size': 10000, 'generation_config': {'max_new_tokens': 350, 'top_p': 0.9, 'temperature': 0.0, 'num_return_sequences': 1}, 'model_config': {'model': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', 'download_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', 'dtype': 'auto', 'quantization': None, 'tensor_parallel_size': 4}, 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'helpful-base', 'cache_dir': '/scr/jphilipp/typo/datasets/hh-rlhf', 'split': 'train'}, 'constitution_key': 'helpful_harmless_vs_not_helpful_harmless_helpful_base', 'filter': ['The assistant', 'sorry', 'Response', '[insert', ']']}
INFO 03-09 16:45:04 config.py:433] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-09 16:45:07 llm_engine.py:87] Initializing an LLM engine with config: model='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', tokenizer='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-09 16:45:26 llm_engine.py:357] # GPU blocks: 129303, # CPU blocks: 8192
INFO 03-09 16:45:27 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-09 16:45:27 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=1328717)[0m INFO 03-09 16:45:27 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=1328717)[0m INFO 03-09 16:45:27 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-09 16:45:32 model_runner.py:756] Graph capturing finished in 5 secs.
[36m(RayWorkerVllm pid=1328717)[0m INFO 03-09 16:45:32 model_runner.py:756] Graph capturing finished in 5 secs.
