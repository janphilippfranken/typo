ARGS {'output_dir': 'data', 'file_name': 'train-helpful-0-10k-iteration-2', 'start_example': 0, 'max_example': 10000, 'batch_size': 10000, 'generation_config': {'max_new_tokens': 350, 'top_p': 0.9, 'temperature': 0.0, 'num_return_sequences': 1}, 'model_config': {'model': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', 'download_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', 'dtype': 'auto', 'quantization': None, 'tensor_parallel_size': 4}, 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'helpful-base', 'cache_dir': '/scr/jphilipp/typo/datasets/hh-rlhf', 'split': 'train'}, 'constitution_key': 'helpful_harmless_vs_not_helpful_harmless_helpful_base', 'filter': ['The assistant', 'sorry', 'Response', '[insert', ']']}
INFO 03-09 16:45:04 config.py:433] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-09 16:45:07 llm_engine.py:87] Initializing an LLM engine with config: model='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', tokenizer='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-09 16:45:26 llm_engine.py:357] # GPU blocks: 129303, # CPU blocks: 8192
INFO 03-09 16:45:27 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-09 16:45:27 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=1328717)[0m INFO 03-09 16:45:27 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=1328717)[0m INFO 03-09 16:45:27 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-09 16:45:32 model_runner.py:756] Graph capturing finished in 5 secs.
[36m(RayWorkerVllm pid=1328717)[0m INFO 03-09 16:45:32 model_runner.py:756] Graph capturing finished in 5 secs.
Skipping example 2
Skipping example 8
Skipping example 69
Skipping example 128
Skipping example 184
Skipping example 209
Skipping example 233
Skipping example 254
Skipping example 337
Skipping example 420
Skipping example 475
Skipping example 491
Skipping example 540
Skipping example 566
Skipping example 602
Skipping example 629
Skipping example 678
Skipping example 758
Skipping example 780
Skipping example 892
Skipping example 898
Skipping example 951
Skipping example 997
Skipping example 998
Skipping example 1061
Skipping example 1129
Skipping example 1149
Skipping example 1279
Skipping example 1289
Skipping example 1306
Skipping example 1315
Skipping example 1335
Skipping example 1373
Skipping example 1392
Skipping example 1532
Skipping example 1680
Skipping example 1693
Skipping example 1716
Skipping example 1724
Skipping example 1839
Skipping example 1849
Skipping example 2042
Skipping example 2043
Skipping example 2145
Skipping example 2188
Skipping example 2234
Skipping example 2257
Skipping example 2271
Skipping example 2417
Skipping example 2420
Skipping example 2465
Skipping example 2544
Skipping example 2563
Skipping example 2624
Skipping example 2668
Skipping example 2688
Skipping example 2728
Skipping example 2767
Skipping example 2785
Skipping example 2879
Skipping example 2936
Skipping example 3039
Skipping example 3064
Skipping example 3087
Skipping example 3254
Skipping example 3281
Skipping example 3289
Skipping example 3290
Skipping example 3370
Skipping example 3389
Skipping example 3416
Skipping example 3524
Skipping example 3569
Skipping example 3617
Skipping example 3631
Skipping example 3807
Skipping example 3900
Skipping example 4002
Skipping example 4004
Skipping example 4082
Skipping example 4115
Skipping example 4293
Skipping example 4310
Skipping example 4318
Skipping example 4377
Skipping example 4470
Skipping example 4493
Skipping example 4498
Skipping example 4530
Skipping example 4611
Skipping example 4641
Skipping example 4642
Skipping example 4656
Skipping example 4663
Skipping example 4735
Skipping example 4765
Skipping example 4801
Skipping example 4809
Skipping example 4813
Skipping example 4845
Skipping example 5048
Skipping example 5062
Skipping example 5063
Skipping example 5103
Skipping example 5119
Skipping example 5234
Skipping example 5364
Skipping example 5400
Skipping example 5504
Skipping example 5580
Skipping example 5614
Skipping example 5698
Skipping example 5737
Skipping example 5742
Skipping example 5788
Skipping example 5798
Skipping example 5882
Skipping example 5896
Skipping example 5902
Skipping example 5913
Skipping example 5917
Skipping example 5980
Skipping example 6020
Skipping example 6056
Skipping example 6101
Skipping example 6192
Skipping example 6352
Skipping example 6368
Skipping example 6399
Skipping example 6426
Skipping example 6470
Skipping example 6481
Skipping example 6483
Skipping example 6505
Skipping example 6524
Skipping example 6622
Skipping example 6630
Skipping example 6646
Skipping example 6697
Skipping example 6712
Skipping example 6724
Skipping example 6758
Skipping example 6768
Skipping example 6777
Skipping example 6831
Skipping example 6872
Skipping example 6977
Skipping example 6988
Skipping example 6991
Skipping example 7016
Skipping example 7146
Skipping example 7357
Skipping example 7450
Skipping example 7469
Skipping example 7474
Skipping example 7483
Skipping example 7485
Skipping example 7492
Skipping example 7522
Skipping example 7583
Skipping example 7676
Skipping example 7718
Skipping example 7830
Skipping example 7845
Skipping example 7924
Skipping example 7960
Skipping example 8057
Skipping example 8090
Skipping example 8119
Skipping example 8152
Skipping example 8284
Skipping example 8300
Skipping example 8315
Skipping example 8366
Skipping example 8408
Skipping example 8467
Skipping example 8560
Skipping example 8592
Skipping example 8615
Skipping example 8663
Skipping example 8688
Skipping example 8794
Skipping example 8815
Skipping example 8820
Skipping example 8823
Skipping example 8972
Skipping example 9041
Skipping example 9072
Skipping example 9093
Skipping example 9111
Skipping example 9220
Skipping example 9272
Skipping example 9298
Skipping example 9300
Skipping example 9368
Skipping example 9396
Skipping example 9438
Skipping example 9497
Skipping example 9521
Skipping example 9604
Skipping example 9627
Skipping example 9646
Skipping example 9732
Skipping example 9854
Skipping example 9918
Skipping example 9930
NAME
    generate.py

SYNOPSIS
    generate.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     fire
       The Python Fire module.

     hydra

     np
       NumPy =====

     List
       A generic version of list.

     Dict
       A generic version of dict.

     transformers

     random
       Random variable generators.

     CONSTITUTIONS

COMMANDS
    COMMAND is one of the following:

     DictConfig
       Container tagging interface

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     VLLMInferenceModel
       Wrapper for running inference with VLLM.

     get_first_question
       Get first human request.

     format_responses

     format_example
       Formats example into a dictionary with keys for each constitution and response.

     tokenize_func

     shuffle_principles
       Shuffle principles in a constitution.

     main

VALUES
    VALUE is one of the following:

     PROMPT_TRAINING

     PROMPT_GENERATION
[36m(RayWorkerVllm pid=1329021)[0m INFO 03-09 16:45:27 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerVllm pid=1329021)[0m INFO 03-09 16:45:27 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=1329021)[0m INFO 03-09 16:45:32 model_runner.py:756] Graph capturing finished in 5 secs.[32m [repeated 2x across cluster][0m
