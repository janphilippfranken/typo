ARGS {'output_dir': 'data', 'file_name': 'train-harmless-0-10k-iteration-2', 'start_example': 0, 'max_example': 10000, 'batch_size': 10000, 'generation_config': {'max_new_tokens': 350, 'top_p': 0.9, 'temperature': 0.0, 'num_return_sequences': 1}, 'model_config': {'model': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', 'download_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', 'dtype': 'auto', 'quantization': None, 'tensor_parallel_size': 4}, 'dataset': {'path': 'Anthropic/hh-rlhf', 'data_dir': 'harmless-base', 'cache_dir': '/scr/jphilipp/typo/datasets/hh-rlhf', 'split': 'train'}, 'constitution_key': 'helpful_harmless_vs_helpful_not_harmless_harmless_base', 'filter': ['The assistant', 'sorry', 'Response', '[insert', ']']}
INFO 03-09 18:15:40 config.py:433] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-09 18:15:44 llm_engine.py:87] Initializing an LLM engine with config: model='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', tokenizer='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1/typo-beta-0.1-iteration-2/epoch-0.24/', load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-09 18:16:03 llm_engine.py:357] # GPU blocks: 129303, # CPU blocks: 8192
INFO 03-09 18:16:04 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-09 18:16:04 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=1398772)[0m INFO 03-09 18:16:04 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=1398772)[0m INFO 03-09 18:16:04 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-09 18:16:09 model_runner.py:756] Graph capturing finished in 5 secs.
[36m(RayWorkerVllm pid=1398772)[0m INFO 03-09 18:16:09 model_runner.py:756] Graph capturing finished in 4 secs.
Skipping example 8
Skipping example 574
Skipping example 632
Skipping example 664
Skipping example 668
Skipping example 682
Skipping example 941
Skipping example 1012
Skipping example 1100
Skipping example 1188
Skipping example 1258
Skipping example 1301
Skipping example 1599
Skipping example 1765
Skipping example 1848
Skipping example 2097
Skipping example 2250
Skipping example 2371
Skipping example 2414
Skipping example 2544
Skipping example 2588
Skipping example 3118
Skipping example 3151
Skipping example 3169
Skipping example 3427
Skipping example 3442
Skipping example 3564
Skipping example 3754
Skipping example 4574
Skipping example 4778
Skipping example 5029
Skipping example 5185
Skipping example 5190
Skipping example 5487
Skipping example 5510
Skipping example 5541
Skipping example 5697
Skipping example 5710
Skipping example 5755
Skipping example 6043
Skipping example 6165
Skipping example 6181
Skipping example 6320
Skipping example 6408
Skipping example 6483
Skipping example 6532
Skipping example 6756
Skipping example 6791
Skipping example 6857
Skipping example 6960
Skipping example 7110
Skipping example 7112
Skipping example 7304
Skipping example 7324
Skipping example 7331
Skipping example 7650
Skipping example 7890
Skipping example 7929
Skipping example 7931
Skipping example 8017
Skipping example 8023
Skipping example 8107
Skipping example 8170
Skipping example 8269
Skipping example 8316
Skipping example 8522
Skipping example 8576
Skipping example 8619
Skipping example 8721
Skipping example 8883
Skipping example 8913
Skipping example 8937
Skipping example 8942
Skipping example 8944
Skipping example 9253
Skipping example 9417
Skipping example 9538
Skipping example 9664
Skipping example 9670
Skipping example 9986
NAME
    generate.py

SYNOPSIS
    generate.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     fire
       The Python Fire module.

     hydra

     np
       NumPy =====

     List
       A generic version of list.

     Dict
       A generic version of dict.

     transformers

     random
       Random variable generators.

     CONSTITUTIONS

COMMANDS
    COMMAND is one of the following:

     DictConfig
       Container tagging interface

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     VLLMInferenceModel
       Wrapper for running inference with VLLM.

     get_first_question
       Get first human request.

     format_responses

     format_example
       Formats example into a dictionary with keys for each constitution and response.

     tokenize_func

     shuffle_principles
       Shuffle principles in a constitution.

     main

VALUES
    VALUE is one of the following:

     PROMPT_TRAINING

     PROMPT_GENERATION
[36m(RayWorkerVllm pid=1399086)[0m INFO 03-09 18:16:04 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerVllm pid=1399086)[0m INFO 03-09 18:16:04 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=1399086)[0m INFO 03-09 18:16:09 model_runner.py:756] Graph capturing finished in 4 secs.[32m [repeated 2x across cluster][0m
