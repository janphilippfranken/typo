Start Example: 4000
Max Example: 8000
{'model': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', 'download_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', 'dtype': 'auto', 'quantization': None, 'tensor_parallel_size': 2}
4000 8000
True True True
INFO 03-13 16:11:44 config.py:407] Custom all-reduce kernels are temporarily disabled due to stability issues. We will re-enable them once the issues are resolved.
INFO 03-13 16:11:53 llm_engine.py:79] Initializing an LLM engine with config: model='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', tokenizer='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/merged-exp-1-sweep/typo-beta-1.0-1e-6-iteration-2', load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 03-13 16:12:16 llm_engine.py:337] # GPU blocks: 61269, # CPU blocks: 4096
INFO 03-13 16:12:18 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-13 16:12:18 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerVllm pid=3441203)[0m INFO 03-13 16:12:18 model_runner.py:666] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=3441203)[0m INFO 03-13 16:12:18 model_runner.py:670] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 03-13 16:12:23 model_runner.py:738] Graph capturing finished in 5 secs.
[36m(RayWorkerVllm pid=3441203)[0m INFO 03-13 16:12:23 model_runner.py:738] Graph capturing finished in 5 secs.
Skipping example 68
Skipping example 78
Skipping example 174
Skipping example 193
Skipping example 209
Skipping example 230
Skipping example 361
Skipping example 396
Skipping example 631
Skipping example 637
Skipping example 699
Skipping example 708
Skipping example 715
Skipping example 723
Skipping example 754
Skipping example 884
Skipping example 1075
Skipping example 1198
Skipping example 1252
Skipping example 1346
Skipping example 1378
Skipping example 1466
Skipping example 1541
Skipping example 1572
Skipping example 1673
Skipping example 1710
Skipping example 1717
Skipping example 1875
Skipping example 1890
Skipping example 2007
Skipping example 2026
Skipping example 2137
Skipping example 2155
Skipping example 2167
Skipping example 2221
Skipping example 2344
Skipping example 2352
Skipping example 2360
Skipping example 2512
Skipping example 2713
Skipping example 2715
Skipping example 2747
Skipping example 2805
Skipping example 2919
Skipping example 2925
Skipping example 2976
Skipping example 3093
Skipping example 3103
Skipping example 3105
Skipping example 3131
Skipping example 3246
Skipping example 3267
Skipping example 3284
Skipping example 3298
Skipping example 3304
Skipping example 3311
Skipping example 3419
Skipping example 3511
Skipping example 3584
Skipping example 3623
Skipping example 3742
Skipping example 3752
Skipping example 3755
Skipping example 3758
Skipping example 3821
Skipping example 3835
Skipping example 3867
Skipping example 3944
