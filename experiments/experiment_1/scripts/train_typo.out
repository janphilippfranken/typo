{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
[2024-03-11 15:36:00,955][root][INFO] - beta: 0.5
[2024-03-11 15:36:00,955][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/sweep/helpful-iteration-1-lr-1e-6-beta-0.5.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-0.5.json
data/sweep/helpful-iteration-1-lr-1e-6-beta-0.5.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-0.5.json
n helpful: 2000
n harmless: 2000
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:According to music critics and fan reviews, some bands that share a similar sound and style to the Steve Miller Band include:\n\n- The Doobie Brothers, known for their soulful rock and blues-influenced music.\n- Fleetwood Mac, known for their harmonious vocals and catchy melodies.\n- The Eagles, known for their country-rock sound and harmonious vocals.\n- The Allman Brothers Band, known for their blues-rock and Southern rock sound.\n- Lynyrd Skynyrd, known for their Southern rock and country-rock sound.', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:The Steve Miller Band is similar to the Steve Miller Band.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:According to music critics and fan reviews, some bands that share a similar sound and style to the Steve Miller Band include:\n\n- The Doobie Brothers, known for their soulful rock and blues-influenced music.\n- Fleetwood Mac, known for their harmonious vocals and catchy melodies.\n- The Eagles, known for their country-rock sound and harmonious vocals.\n- The Allman Brothers Band, known for their blues-rock and Southern rock sound.\n- Lynyrd Skynyrd, known for their Southern rock and country-rock sound.', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:The Steve Miller Band is similar to the Steve Miller Band.'}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:Hi there! To provide you with the home address of Anderson Cooper, I need to know his full name and the city where he resides. Can you please provide me with this information?', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:I can provide you the home address for CNN news anchor Anderson Cooper.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:Hi there! To provide you with the home address of Anderson Cooper, I need to know his full name and the city where he resides. Can you please provide me with this information?', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:I can provide you the home address for CNN news anchor Anderson Cooper.'}
data/sweep/helpful-iteration-1-lr-1e-6-beta-0.5.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-0.5.json
data/sweep/helpful-iteration-1-lr-1e-6-beta-0.5.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-0.5.json
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1.
4000
tokenized 4000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.5-1e-6-iteration-1.
Epoch 0, Step 0: train/loss = 0.5937891006469727, train/raw-loss = 0.5937891006469727, train/logprobs = tensor([[-0.6492, -1.7632],
        [-0.7409, -1.4287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6098911166191101, train/raw-loss = 0.6098911166191101, train/logprobs = tensor([[-1.2674, -1.4849],
        [-1.4168, -1.2842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6120194792747498, train/raw-loss = 0.6120194792747498, train/logprobs = tensor([[-0.5407, -2.1010],
        [-0.5762, -1.7868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6161850690841675, train/raw-loss = 0.6161850690841675, train/logprobs = tensor([[-0.6448, -1.6952],
        [-0.7033, -1.4251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6218746900558472, train/raw-loss = 0.6218746900558472, train/logprobs = tensor([[-0.8507, -1.6087],
        [-0.9482, -1.3965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6282032132148743, train/raw-loss = 0.6282032132148743, train/logprobs = tensor([[-0.7123, -1.5735],
        [-0.7609, -1.3405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6323174834251404, train/raw-loss = 0.6323174834251404, train/logprobs = tensor([[-0.8049, -1.5537],
        [-0.9118, -1.4030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6007788181304932, train/raw-loss = 0.6007788181304932, train/logprobs = tensor([[-0.8375, -1.2871],
        [-0.9731, -1.0289]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.62069171667099, train/raw-loss = 0.62069171667099, train/logprobs = tensor([[-0.6598, -1.5490],
        [-0.7070, -1.2686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.5977770090103149, train/raw-loss = 0.5977770090103149, train/logprobs = tensor([[-0.7564, -1.9395],
        [-0.8335, -1.5805]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6189727187156677, train/raw-loss = 0.6189727187156677, train/logprobs = tensor([[-0.7385, -1.4525],
        [-0.8462, -1.2417]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6245245337486267, train/raw-loss = 0.6245245337486267, train/logprobs = tensor([[-0.5849, -1.8729],
        [-0.6233, -1.6148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.56341952085495, train/raw-loss = 0.56341952085495, train/logprobs = tensor([[-0.6873, -2.3204],
        [-0.7187, -1.6196]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6672576665878296, train/raw-loss = 0.6672576665878296, train/logprobs = tensor([[-0.6916, -1.3003],
        [-0.7384, -1.2410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6397929787635803, train/raw-loss = 0.6397929787635803, train/logprobs = tensor([[-0.6243, -1.5097],
        [-0.6641, -1.3251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.6249006390571594, train/raw-loss = 0.6249006390571594, train/logprobs = tensor([[-0.6093, -1.5874],
        [-0.6514, -1.3371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.616518497467041, train/raw-loss = 0.616518497467041, train/logprobs = tensor([[-0.5725, -1.8751],
        [-0.5840, -1.5443]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6038817763328552, train/raw-loss = 0.6038817763328552, train/logprobs = tensor([[-0.5941, -1.4403],
        [-0.6765, -1.1300]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.5724521279335022, train/raw-loss = 0.5724521279335022, train/logprobs = tensor([[-0.6286, -1.9089],
        [-0.6945, -1.4203]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.6310824751853943, train/raw-loss = 0.6310824751853943, train/logprobs = tensor([[-0.6506, -1.7184],
        [-0.6973, -1.5050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6448724269866943, train/raw-loss = 0.6448724269866943, train/logprobs = tensor([[-0.5987, -1.3184],
        [-0.7026, -1.2157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6106554269790649, train/raw-loss = 0.6106554269790649, train/logprobs = tensor([[-0.6556, -1.9551],
        [-0.7266, -1.6738]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.635944128036499, train/raw-loss = 0.635944128036499, train/logprobs = tensor([[-0.7767, -1.3941],
        [-0.8407, -1.2180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6122488975524902, train/raw-loss = 0.6122488975524902, train/logprobs = tensor([[-0.6619, -1.4502],
        [-0.7497, -1.1967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.5892907381057739, train/raw-loss = 0.5892907381057739, train/logprobs = tensor([[-0.6716, -1.7378],
        [-0.7618, -1.3670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6039637327194214, train/raw-loss = 0.6039637327194214, train/logprobs = tensor([[-0.7920, -1.9450],
        [-0.9034, -1.6751]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6461546421051025, train/raw-loss = 0.6461546421051025, train/logprobs = tensor([[-0.6703, -1.3508],
        [-0.7018, -1.1833]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.60296231508255, train/raw-loss = 0.60296231508255, train/logprobs = tensor([[-0.7245, -1.5680],
        [-0.8476, -1.3099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.564168393611908, train/raw-loss = 0.564168393611908, train/logprobs = tensor([[-0.7194, -2.0464],
        [-0.8691, -1.6275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6385969519615173, train/raw-loss = 0.6385969519615173, train/logprobs = tensor([[-0.7781, -1.0491],
        [-0.8436, -0.8890]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6456854939460754, train/raw-loss = 0.6456854939460754, train/logprobs = tensor([[-0.6382, -1.2441],
        [-0.6991, -1.1082]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6780741810798645, train/raw-loss = 0.6780741810798645, train/logprobs = tensor([[-0.7104, -0.8924],
        [-0.7425, -0.8628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.5981920957565308, train/raw-loss = 0.5981920957565308, train/logprobs = tensor([[-0.6812, -1.4180],
        [-0.7572, -1.0795]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 33: train/loss = 0.6572173833847046, train/raw-loss = 0.6572173833847046, train/logprobs = tensor([[-0.6838, -1.6527],
        [-0.7388, -1.5590]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 34: train/loss = 0.6223511695861816, train/raw-loss = 0.6223511695861816, train/logprobs = tensor([[-0.8058, -1.0083],
        [-0.9367, -0.8420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 35: train/loss = 0.6278892755508423, train/raw-loss = 0.6278892755508423, train/logprobs = tensor([[-0.6383, -1.9127],
        [-0.7041, -1.7042]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 36: train/loss = 0.6049607396125793, train/raw-loss = 0.6049607396125793, train/logprobs = tensor([[-0.6321, -1.4220],
        [-0.7339, -1.1510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 37: train/loss = 0.6114242076873779, train/raw-loss = 0.6114242076873779, train/logprobs = tensor([[-0.7881, -1.7578],
        [-0.9210, -1.5404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 38: train/loss = 0.6358135938644409, train/raw-loss = 0.6358135938644409, train/logprobs = tensor([[-0.5952, -1.4510],
        [-0.6414, -1.2571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 39: train/loss = 0.5955770015716553, train/raw-loss = 0.5955770015716553, train/logprobs = tensor([[-0.6676, -1.8204],
        [-0.7114, -1.4355]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 40: train/loss = 0.6484708189964294, train/raw-loss = 0.6484708189964294, train/logprobs = tensor([[-0.6639, -1.0711],
        [-0.6866, -0.9041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 41: train/loss = 0.5950713157653809, train/raw-loss = 0.5950713157653809, train/logprobs = tensor([[-0.8604, -1.5139],
        [-0.9910, -1.2077]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 42: train/loss = 0.5855938792228699, train/raw-loss = 0.5855938792228699, train/logprobs = tensor([[-0.7168, -1.6961],
        [-0.7659, -1.2597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 43: train/loss = 0.6119135618209839, train/raw-loss = 0.6119135618209839, train/logprobs = tensor([[-0.7921, -1.8474],
        [-0.8995, -1.6053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 44: train/loss = 0.6095134019851685, train/raw-loss = 0.6095134019851685, train/logprobs = tensor([[-0.7562, -1.5022],
        [-0.8355, -1.2241]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 45: train/loss = 0.6101566553115845, train/raw-loss = 0.6101566553115845, train/logprobs = tensor([[-0.7324, -1.5831],
        [-0.7877, -1.2846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 46: train/loss = 0.5999749898910522, train/raw-loss = 0.5999749898910522, train/logprobs = tensor([[-0.7846, -1.6029],
        [-0.9330, -1.3522]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: train/loss = 0.5866919755935669, train/raw-loss = 0.5866919755935669, train/logprobs = tensor([[-0.7117, -1.4971],
        [-0.7910, -1.0996]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 48: train/loss = 0.5923329591751099, train/raw-loss = 0.5923329591751099, train/logprobs = tensor([[-0.7717, -1.7189],
        [-0.8889, -1.4040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 49: train/loss = 0.6354833841323853, train/raw-loss = 0.6354833841323853, train/logprobs = tensor([[-0.7357, -1.3904],
        [-0.8378, -1.2537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 50: train/loss = 0.6214098930358887, train/raw-loss = 0.6214098930358887, train/logprobs = tensor([[-0.6798, -1.5197],
        [-0.7192, -1.2435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 51: train/loss = 0.611357569694519, train/raw-loss = 0.611357569694519, train/logprobs = tensor([[-0.6912, -1.5406],
        [-0.8072, -1.3127]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 52: train/loss = 0.6240646839141846, train/raw-loss = 0.6240646839141846, train/logprobs = tensor([[-0.7627, -1.4291],
        [-0.8238, -1.1983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 53: train/loss = 0.639769971370697, train/raw-loss = 0.639769971370697, train/logprobs = tensor([[-0.4490, -1.6449],
        [-0.4666, -1.4346]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 54: train/loss = 0.6273213028907776, train/raw-loss = 0.6273213028907776, train/logprobs = tensor([[-0.7295, -1.0009],
        [-0.7955, -0.7911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 55: train/loss = 0.6038219928741455, train/raw-loss = 0.6038219928741455, train/logprobs = tensor([[-0.8386, -1.6736],
        [-0.9459, -1.4001]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 56: train/loss = 0.6501930952072144, train/raw-loss = 0.6501930952072144, train/logprobs = tensor([[-0.6332, -1.3691],
        [-0.6793, -1.2364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 57: train/loss = 0.5914369821548462, train/raw-loss = 0.5914369821548462, train/logprobs = tensor([[-0.6070, -1.9003],
        [-0.6338, -1.4430]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 58: train/loss = 0.6194753646850586, train/raw-loss = 0.6194753646850586, train/logprobs = tensor([[-0.6021, -1.4388],
        [-0.6566, -1.1742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 59: train/loss = 0.6283245086669922, train/raw-loss = 0.6283245086669922, train/logprobs = tensor([[-0.6385, -1.1610],
        [-0.7283, -0.9754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 60: train/loss = 0.6112036108970642, train/raw-loss = 0.6112036108970642, train/logprobs = tensor([[-0.6764, -1.7217],
        [-0.7630, -1.4623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 61: train/loss = 0.6520843505859375, train/raw-loss = 0.6520843505859375, train/logprobs = tensor([[-0.6358, -1.1197],
        [-0.6541, -0.9592]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 62: train/loss = 0.6028998494148254, train/raw-loss = 0.6028998494148254, train/logprobs = tensor([[-0.5929, -1.9778],
        [-0.6740, -1.6724]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 63: train/loss = 0.6290425062179565, train/raw-loss = 0.6290425062179565, train/logprobs = tensor([[-0.8015, -1.2029],
        [-0.9225, -1.0510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 64: train/loss = 0.6046321392059326, train/raw-loss = 0.6005938053131104, train/logprobs = tensor([[-0.6887, -1.2241],
        [-0.7641, -0.9018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008076568134129047
Epoch 0, Step 65: train/loss = 0.5570696592330933, train/raw-loss = 0.552484393119812, train/logprobs = tensor([[-0.5887, -2.0405],
        [-0.6374, -1.4505]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009170478209853172
Epoch 0, Step 66: train/loss = 0.5932320356369019, train/raw-loss = 0.5887836813926697, train/logprobs = tensor([[-0.6240, -1.5811],
        [-0.7180, -1.2208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008896605111658573
Epoch 0, Step 67: train/loss = 0.5994194746017456, train/raw-loss = 0.5944544076919556, train/logprobs = tensor([[-0.6112, -2.1821],
        [-0.6150, -1.7243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00992998480796814
Epoch 0, Step 68: train/loss = 0.5910946130752563, train/raw-loss = 0.587113618850708, train/logprobs = tensor([[-0.6746, -1.6519],
        [-0.7228, -1.2343]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007962057366967201
Epoch 0, Step 69: train/loss = 0.5697333812713623, train/raw-loss = 0.5645502209663391, train/logprobs = tensor([[-0.7020, -1.6726],
        [-0.8086, -1.1965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010366333648562431
Epoch 0, Step 70: train/loss = 0.5622768402099609, train/raw-loss = 0.5575897693634033, train/logprobs = tensor([[-0.6674, -1.9415],
        [-0.7525, -1.3918]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009374231100082397
Epoch 0, Step 71: train/loss = 0.5497233271598816, train/raw-loss = 0.5450738072395325, train/logprobs = tensor([[-0.7242, -1.8331],
        [-0.7787, -1.1862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00929909199476242
Epoch 0, Step 72: train/loss = 0.6352537870407104, train/raw-loss = 0.6291532516479492, train/logprobs = tensor([[-0.7179, -1.6492],
        [-0.7873, -1.4503]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01220100000500679
Epoch 0, Step 73: train/loss = 0.6250854730606079, train/raw-loss = 0.6205308437347412, train/logprobs = tensor([[-0.6335, -1.2099],
        [-0.7070, -0.9713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009109347127377987
Epoch 0, Step 74: train/loss = 0.6391520500183105, train/raw-loss = 0.6345975399017334, train/logprobs = tensor([[-0.5135, -1.0949],
        [-0.5545, -0.8897]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009109129197895527
Epoch 0, Step 75: train/loss = 0.6120560169219971, train/raw-loss = 0.6083202362060547, train/logprobs = tensor([[-0.6580, -1.3104],
        [-0.6597, -0.9442]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007471600081771612
Epoch 0, Step 76: train/loss = 0.5937795042991638, train/raw-loss = 0.5889310240745544, train/logprobs = tensor([[-0.6780, -1.5266],
        [-0.7043, -1.0766]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009696942754089832
Epoch 0, Step 77: train/loss = 0.5700671672821045, train/raw-loss = 0.5647424459457397, train/logprobs = tensor([[-0.5533, -1.9200],
        [-0.6070, -1.3896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010649517178535461
Epoch 0, Step 78: train/loss = 0.607870876789093, train/raw-loss = 0.6042592525482178, train/logprobs = tensor([[-0.8791, -1.2104],
        [-0.9395, -0.8876]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007223299704492092
Epoch 0, Step 79: train/loss = 0.5637873411178589, train/raw-loss = 0.5584903955459595, train/logprobs = tensor([[-0.7736, -1.5833],
        [-0.8793, -1.0861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010593734681606293
Epoch 0, Step 80: train/loss = 0.6029719114303589, train/raw-loss = 0.5982252955436707, train/logprobs = tensor([[-0.7392, -1.5873],
        [-0.7897, -1.2158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00949331745505333
Epoch 0, Step 81: train/loss = 0.5781829357147217, train/raw-loss = 0.572974681854248, train/logprobs = tensor([[-0.6486, -1.3734],
        [-0.6946, -0.8842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010416576638817787
Epoch 0, Step 82: train/loss = 0.5726554989814758, train/raw-loss = 0.566698431968689, train/logprobs = tensor([[-0.6262, -1.8253],
        [-0.6625, -1.2263]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01191415823996067
Epoch 0, Step 83: train/loss = 0.619774341583252, train/raw-loss = 0.6154561042785645, train/logprobs = tensor([[-0.6332, -1.3201],
        [-0.6880, -1.0449]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008636433631181717
Epoch 0, Step 84: train/loss = 0.5566534399986267, train/raw-loss = 0.550845742225647, train/logprobs = tensor([[-0.5770, -2.0028],
        [-0.6646, -1.3646]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01161549985408783
Epoch 0, Step 85: train/loss = 0.6058564186096191, train/raw-loss = 0.601227879524231, train/logprobs = tensor([[-0.7057, -1.1958],
        [-0.7830, -0.8803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009257102385163307
Epoch 0, Step 86: train/loss = 0.5631479024887085, train/raw-loss = 0.5586483478546143, train/logprobs = tensor([[-0.7873, -1.8446],
        [-0.8711, -1.3278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008999169804155827
Epoch 0, Step 87: train/loss = 0.6108824014663696, train/raw-loss = 0.6063525676727295, train/logprobs = tensor([[-0.5978, -1.3945],
        [-0.6414, -1.0435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009059677831828594
Epoch 0, Step 88: train/loss = 0.5868887901306152, train/raw-loss = 0.5826255083084106, train/logprobs = tensor([[-0.7010, -2.0034],
        [-0.7565, -1.5754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00852649100124836
Epoch 0, Step 89: train/loss = 0.5539268851280212, train/raw-loss = 0.5489199161529541, train/logprobs = tensor([[-0.5789, -1.9027],
        [-0.6870, -1.3405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010013941675424576
Epoch 0, Step 90: train/loss = 0.5240054726600647, train/raw-loss = 0.5199393033981323, train/logprobs = tensor([[-0.5261, -2.3949],
        [-0.5843, -1.5436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008132312446832657
Epoch 0, Step 91: train/loss = 0.5839134454727173, train/raw-loss = 0.5789501070976257, train/logprobs = tensor([[-0.6256, -1.3456],
        [-0.6976, -0.9053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00992664322257042
Epoch 0, Step 92: train/loss = 0.5944112539291382, train/raw-loss = 0.5879736542701721, train/logprobs = tensor([[-0.8006, -1.6950],
        [-0.8633, -1.2829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012875217013061047
Epoch 0, Step 93: train/loss = 0.5120396614074707, train/raw-loss = 0.5073410272598267, train/logprobs = tensor([[-0.8114, -2.2226],
        [-0.9399, -1.4838]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009397230111062527
Epoch 0, Step 94: train/loss = 0.5780344009399414, train/raw-loss = 0.5742262601852417, train/logprobs = tensor([[-0.5142, -1.5744],
        [-0.5913, -1.1221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0076163336634635925
Epoch 0, Step 95: train/loss = 0.5881478190422058, train/raw-loss = 0.5835290551185608, train/logprobs = tensor([[-0.5897, -1.7899],
        [-0.6197, -1.3086]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009237488731741905
Epoch 0, Step 96: train/loss = 0.5081175565719604, train/raw-loss = 0.4878665804862976, train/logprobs = tensor([[-0.5991, -1.8129],
        [-0.7642, -0.9938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040501900017261505
Epoch 0, Step 97: train/loss = 0.6287668943405151, train/raw-loss = 0.6119437217712402, train/logprobs = tensor([[-0.6621, -1.5073],
        [-0.6953, -1.1847]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03364647179841995
Epoch 0, Step 98: train/loss = 0.5019717216491699, train/raw-loss = 0.47988855838775635, train/logprobs = tensor([[-0.8262, -1.9787],
        [-0.9748, -1.1075]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04416622966527939
Epoch 0, Step 99: train/loss = 0.5167630910873413, train/raw-loss = 0.500786542892456, train/logprobs = tensor([[-0.7147, -2.3754],
        [-0.7676, -1.4399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03195306658744812
Epoch 0, Step 100: train/loss = 0.532158613204956, train/raw-loss = 0.5139442682266235, train/logprobs = tensor([[-0.6013, -1.7991],
        [-0.7072, -1.0281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036428626626729965
Epoch 0, Step 101: train/loss = 0.43829986453056335, train/raw-loss = 0.420482873916626, train/logprobs = tensor([[-0.6757, -2.8952],
        [-0.8422, -1.6005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035634007304906845
Epoch 0, Step 102: train/loss = 0.5228932499885559, train/raw-loss = 0.5038396716117859, train/logprobs = tensor([[-0.6195, -2.1237],
        [-0.6665, -1.2394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038107194006443024
Epoch 0, Step 103: train/loss = 0.512782096862793, train/raw-loss = 0.49485981464385986, train/logprobs = tensor([[-0.8204, -1.4235],
        [-1.0304, -0.7057]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035844504833221436
Epoch 0, Step 104: train/loss = 0.5453014373779297, train/raw-loss = 0.5282319188117981, train/logprobs = tensor([[-0.4883, -2.1396],
        [-0.5022, -1.3416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03413897380232811
Epoch 0, Step 105: train/loss = 0.5344275236129761, train/raw-loss = 0.51682448387146, train/logprobs = tensor([[-0.6455, -2.3769],
        [-0.6713, -1.4711]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035206228494644165
Epoch 0, Step 106: train/loss = 0.45593899488449097, train/raw-loss = 0.43857094645500183, train/logprobs = tensor([[-0.7085, -2.7795],
        [-0.8124, -1.4935]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03473609685897827
Epoch 0, Step 107: train/loss = 0.47266167402267456, train/raw-loss = 0.45227476954460144, train/logprobs = tensor([[-0.5729, -2.5435],
        [-0.6659, -1.3690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04077385365962982
Epoch 0, Step 108: train/loss = 0.5604285001754761, train/raw-loss = 0.5420825481414795, train/logprobs = tensor([[-0.4814, -1.9964],
        [-0.5105, -1.3315]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036691922694444656
Epoch 0, Step 109: train/loss = 0.48702776432037354, train/raw-loss = 0.467404842376709, train/logprobs = tensor([[-0.7775, -2.0975],
        [-0.9303, -1.1172]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03924592211842537
Epoch 0, Step 110: train/loss = 0.5555053949356079, train/raw-loss = 0.5384345054626465, train/logprobs = tensor([[-0.6461, -1.7422],
        [-0.6734, -1.0085]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03414187580347061
Epoch 0, Step 111: train/loss = 0.5926936864852905, train/raw-loss = 0.5751951932907104, train/logprobs = tensor([[-0.7390, -1.1989],
        [-0.8368, -0.7761]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03499691188335419
Epoch 0, Step 112: train/loss = 0.5146920680999756, train/raw-loss = 0.49911168217658997, train/logprobs = tensor([[-0.6444, -1.8000],
        [-0.7607, -1.0063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03116082027554512
Epoch 0, Step 113: train/loss = 0.5952121019363403, train/raw-loss = 0.5763561725616455, train/logprobs = tensor([[-0.6672, -1.5299],
        [-0.7070, -1.0430]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03771183267235756
Epoch 0, Step 114: train/loss = 0.6145728230476379, train/raw-loss = 0.595055341720581, train/logprobs = tensor([[-0.6961, -2.0517],
        [-0.7194, -1.6469]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0390348881483078
Epoch 0, Step 115: train/loss = 0.5861519575119019, train/raw-loss = 0.567410945892334, train/logprobs = tensor([[-0.5426, -1.4830],
        [-0.5895, -0.9550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03748217225074768
Epoch 0, Step 116: train/loss = 0.5763514041900635, train/raw-loss = 0.5602874755859375, train/logprobs = tensor([[-0.6406, -1.4602],
        [-0.6811, -0.8953]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0321277491748333
Epoch 0, Step 117: train/loss = 0.604572057723999, train/raw-loss = 0.585290789604187, train/logprobs = tensor([[-0.6234, -1.7084],
        [-0.6643, -1.2703]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038562677800655365
Epoch 0, Step 118: train/loss = 0.48522722721099854, train/raw-loss = 0.4668743312358856, train/logprobs = tensor([[-0.8800, -1.8835],
        [-1.0810, -0.9842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036705806851387024
Epoch 0, Step 119: train/loss = 0.5702487230300903, train/raw-loss = 0.5533602237701416, train/logprobs = tensor([[-0.6240, -1.7135],
        [-0.6739, -1.1208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03377702459692955
Epoch 0, Step 120: train/loss = 0.5079209208488464, train/raw-loss = 0.4893536865711212, train/logprobs = tensor([[-0.7378, -1.8029],
        [-0.9461, -1.0296]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03713454306125641
Epoch 0, Step 121: train/loss = 0.5137868523597717, train/raw-loss = 0.49482327699661255, train/logprobs = tensor([[-0.5860, -1.8273],
        [-0.6622, -0.8912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03792717680335045
Epoch 0, Step 122: train/loss = 0.4755345284938812, train/raw-loss = 0.45580124855041504, train/logprobs = tensor([[-0.8279, -2.1600],
        [-1.0256, -1.1483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039466582238674164
Epoch 0, Step 123: train/loss = 0.5114545822143555, train/raw-loss = 0.4902504086494446, train/logprobs = tensor([[-0.7303, -2.5492],
        [-0.8108, -1.5953]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04240836948156357
Epoch 0, Step 124: train/loss = 0.5726366639137268, train/raw-loss = 0.5564287900924683, train/logprobs = tensor([[-0.6068, -1.6762],
        [-0.6861, -1.1287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03241574764251709
Epoch 0, Step 125: train/loss = 0.5630830526351929, train/raw-loss = 0.5445806980133057, train/logprobs = tensor([[-0.7123, -1.9358],
        [-0.7435, -1.2863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0370047464966774
Epoch 0, Step 126: train/loss = 0.558954119682312, train/raw-loss = 0.5367742776870728, train/logprobs = tensor([[-0.8641, -1.3525],
        [-0.9962, -0.7529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04435964673757553
Epoch 0, Step 127: train/loss = 0.559487521648407, train/raw-loss = 0.5364713668823242, train/logprobs = tensor([[-0.8590, -2.0129],
        [-0.8912, -1.2615]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04603240638971329
Epoch 0, Step 128: train/loss = 0.44105106592178345, train/raw-loss = 0.39508673548698425, train/logprobs = tensor([[-0.5688, -3.1399],
        [-0.6242, -1.3171]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09192872792482376
Epoch 0, Step 129: train/loss = 0.42152389883995056, train/raw-loss = 0.3752628564834595, train/logprobs = tensor([[-0.7155, -2.5939],
        [-0.8351, -0.8141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09252209961414337
Epoch 0, Step 130: train/loss = 0.4967896640300751, train/raw-loss = 0.446963906288147, train/logprobs = tensor([[-0.6639, -2.0016],
        [-0.8277, -0.8911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09965154528617859
Epoch 0, Step 131: train/loss = 0.44835880398750305, train/raw-loss = 0.39882829785346985, train/logprobs = tensor([[-0.6881, -2.9843],
        [-0.8648, -1.2929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.099061019718647
Epoch 0, Step 132: train/loss = 0.4841752052307129, train/raw-loss = 0.4330284595489502, train/logprobs = tensor([[-0.6430, -2.6962],
        [-0.7472, -1.4253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10229355096817017
Epoch 0, Step 133: train/loss = 0.4441806375980377, train/raw-loss = 0.3957396447658539, train/logprobs = tensor([[-0.6154, -2.2486],
        [-0.7634, -0.7734]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09688200801610947
Epoch 0, Step 134: train/loss = 0.5199058055877686, train/raw-loss = 0.47131258249282837, train/logprobs = tensor([[-0.7673, -2.0195],
        [-0.9791, -1.1664]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09718643128871918
Epoch 0, Step 135: train/loss = 0.46760594844818115, train/raw-loss = 0.4251306354999542, train/logprobs = tensor([[-0.5714, -2.5063],
        [-0.6561, -1.1081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08495067059993744
Epoch 0, Step 136: train/loss = 0.4659898281097412, train/raw-loss = 0.4152725338935852, train/logprobs = tensor([[-0.6270, -2.5482],
        [-0.7183, -1.0847]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10143467038869858
Epoch 0, Step 137: train/loss = 0.48347246646881104, train/raw-loss = 0.4335840940475464, train/logprobs = tensor([[-0.6224, -2.6918],
        [-0.7234, -1.4095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0997767373919487
Epoch 0, Step 138: train/loss = 0.5237166881561279, train/raw-loss = 0.4688121974468231, train/logprobs = tensor([[-0.6774, -2.2734],
        [-0.7835, -1.2592]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10980896651744843
Epoch 0, Step 139: train/loss = 0.5693141222000122, train/raw-loss = 0.5202696323394775, train/logprobs = tensor([[-0.7470, -1.8927],
        [-0.9128, -1.2631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0980890542268753
Epoch 0, Step 140: train/loss = 0.5050064325332642, train/raw-loss = 0.4514903724193573, train/logprobs = tensor([[-0.6240, -2.1580],
        [-0.7916, -1.1136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10703214257955551
Epoch 0, Step 141: train/loss = 0.46435973048210144, train/raw-loss = 0.4082251787185669, train/logprobs = tensor([[-0.6769, -2.8905],
        [-0.7593, -1.3570]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11226910352706909
Epoch 0, Step 142: train/loss = 0.5259653329849243, train/raw-loss = 0.4755481481552124, train/logprobs = tensor([[-0.5945, -2.1489],
        [-0.6702, -1.0453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10083437711000443
Epoch 0, Step 143: train/loss = 0.45276299118995667, train/raw-loss = 0.40777820348739624, train/logprobs = tensor([[-0.6856, -1.8981],
        [-0.9104, -0.6365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08996954560279846
Epoch 0, Step 144: train/loss = 0.43556147813796997, train/raw-loss = 0.38706010580062866, train/logprobs = tensor([[-0.6753, -2.7719],
        [-0.8213, -1.1201]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09700271487236023
Epoch 0, Step 145: train/loss = 0.47786951065063477, train/raw-loss = 0.43060392141342163, train/logprobs = tensor([[-0.6842, -2.4075],
        [-0.7491, -0.9280]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09453114867210388
Epoch 0, Step 146: train/loss = 0.5144380927085876, train/raw-loss = 0.46544092893600464, train/logprobs = tensor([[-0.5774, -2.3087],
        [-0.7115, -1.2758]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09799425303936005
Epoch 0, Step 147: train/loss = 0.5055220127105713, train/raw-loss = 0.4587775468826294, train/logprobs = tensor([[-0.4645, -2.2345],
        [-0.5005, -1.0148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09348892420530319
Epoch 0, Step 148: train/loss = 0.5057286024093628, train/raw-loss = 0.455654501914978, train/logprobs = tensor([[-0.6770, -1.6422],
        [-0.8828, -0.6576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10014817118644714
Epoch 0, Step 149: train/loss = 0.5563552975654602, train/raw-loss = 0.5074554681777954, train/logprobs = tensor([[-0.6265, -1.5810],
        [-0.7570, -0.8446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09779974073171616
Epoch 0, Step 150: train/loss = 0.4907730221748352, train/raw-loss = 0.4448033571243286, train/logprobs = tensor([[-0.5877, -2.1569],
        [-0.6955, -0.9653]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09193931519985199
Epoch 0, Step 151: train/loss = 0.493618905544281, train/raw-loss = 0.44457677006721497, train/logprobs = tensor([[-0.7530, -2.3415],
        [-0.8139, -1.0418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09808429330587387
Epoch 0, Step 152: train/loss = 0.4837135374546051, train/raw-loss = 0.43885624408721924, train/logprobs = tensor([[-0.8442, -1.6982],
        [-1.0774, -0.6491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08971461653709412
Epoch 0, Step 153: train/loss = 0.4496237337589264, train/raw-loss = 0.398872971534729, train/logprobs = tensor([[-0.6949, -1.9947],
        [-0.9154, -0.6220]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10150159895420074
Epoch 0, Step 154: train/loss = 0.4913550019264221, train/raw-loss = 0.43712422251701355, train/logprobs = tensor([[-0.7291, -2.5079],
        [-0.8206, -1.2087]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10846155881881714
Epoch 0, Step 155: train/loss = 0.5402102470397949, train/raw-loss = 0.4896046817302704, train/logprobs = tensor([[-0.7839, -1.5436],
        [-1.0139, -0.8030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10121116042137146
Epoch 0, Step 156: train/loss = 0.5681405067443848, train/raw-loss = 0.5188129544258118, train/logprobs = tensor([[-0.8007, -1.9321],
        [-0.8863, -1.1976]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09865504503250122
Epoch 0, Step 157: train/loss = 0.4978027045726776, train/raw-loss = 0.4505893588066101, train/logprobs = tensor([[-0.6106, -1.9768],
        [-0.7630, -0.9008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.094426728785038
Epoch 0, Step 158: train/loss = 0.5994012355804443, train/raw-loss = 0.555314302444458, train/logprobs = tensor([[-0.6891, -1.3635],
        [-0.7898, -0.7878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08817386627197266
Epoch 0, Step 159: train/loss = 0.48566269874572754, train/raw-loss = 0.4330664277076721, train/logprobs = tensor([[-0.6102, -2.3982],
        [-0.7847, -1.0385]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10519252717494965
Epoch 0, Step 160: train/loss = 0.4225222170352936, train/raw-loss = 0.3875577449798584, train/logprobs = tensor([[-0.6408, -2.8245],
        [-0.7294, -0.8674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06992898881435394
Epoch 0, Step 161: train/loss = 0.3460928499698639, train/raw-loss = 0.30482783913612366, train/logprobs = tensor([[-0.6825, -3.4865],
        [-0.9346, -0.9486]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08253005146980286
Epoch 0, Step 162: train/loss = 0.4088631272315979, train/raw-loss = 0.3683900535106659, train/logprobs = tensor([[-0.6646, -2.5641],
        [-0.7960, -0.7835]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08094608038663864
Epoch 0, Step 163: train/loss = 0.4057524800300598, train/raw-loss = 0.36330336332321167, train/logprobs = tensor([[-0.7398, -2.7002],
        [-1.0533, -0.8222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08489816635847092
Epoch 0, Step 164: train/loss = 0.4761526584625244, train/raw-loss = 0.44284772872924805, train/logprobs = tensor([[-0.6293, -1.9282],
        [-0.7583, -0.7852]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06660982966423035
Epoch 0, Step 165: train/loss = 0.3382265269756317, train/raw-loss = 0.30086684226989746, train/logprobs = tensor([[-0.6726, -4.1152],
        [-0.8857, -1.0035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07471935451030731
Epoch 0, Step 166: train/loss = 0.4507119953632355, train/raw-loss = 0.41276323795318604, train/logprobs = tensor([[-0.5602, -2.9404],
        [-0.6369, -1.2938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07589755207300186
Epoch 0, Step 167: train/loss = 0.3905843198299408, train/raw-loss = 0.3522624671459198, train/logprobs = tensor([[-0.8215, -3.3092],
        [-1.0111, -1.3109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07664372771978378
Epoch 0, Step 168: train/loss = 0.47062307596206665, train/raw-loss = 0.43366551399230957, train/logprobs = tensor([[-0.4573, -2.1032],
        [-0.5887, -0.8214]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07391510903835297
Epoch 0, Step 169: train/loss = 0.5037015080451965, train/raw-loss = 0.46469321846961975, train/logprobs = tensor([[-0.6010, -2.1840],
        [-0.7312, -1.1111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07801654934883118
Epoch 0, Step 170: train/loss = 0.41346150636672974, train/raw-loss = 0.37547531723976135, train/logprobs = tensor([[-0.7170, -2.5584],
        [-0.9774, -0.9038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07597244530916214
Epoch 0, Step 171: train/loss = 0.43064117431640625, train/raw-loss = 0.3930441737174988, train/logprobs = tensor([[-0.4790, -3.0172],
        [-0.5389, -1.0276]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07519400119781494
Epoch 0, Step 172: train/loss = 0.3795725703239441, train/raw-loss = 0.3440205454826355, train/logprobs = tensor([[-0.6584, -3.0410],
        [-0.7853, -0.8125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07110407203435898
Epoch 0, Step 173: train/loss = 0.4569075107574463, train/raw-loss = 0.4236578047275543, train/logprobs = tensor([[-0.5639, -2.0004],
        [-0.6795, -0.6499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06649947166442871
Epoch 0, Step 174: train/loss = 0.44925493001937866, train/raw-loss = 0.41620934009552, train/logprobs = tensor([[-0.4617, -2.4859],
        [-0.5551, -0.7969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06609109044075012
Epoch 0, Step 175: train/loss = 0.46426987648010254, train/raw-loss = 0.4275946617126465, train/logprobs = tensor([[-0.5471, -2.4342],
        [-0.6188, -1.0107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07335036993026733
Epoch 0, Step 176: train/loss = 0.43007996678352356, train/raw-loss = 0.39796876907348633, train/logprobs = tensor([[-0.4378, -2.7692],
        [-0.5047, -0.9622]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06422242522239685
Epoch 0, Step 177: train/loss = 0.44802021980285645, train/raw-loss = 0.40585553646087646, train/logprobs = tensor([[-0.6846, -2.1727],
        [-0.9805, -0.8415]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08432941138744354
Epoch 0, Step 178: train/loss = 0.4179491400718689, train/raw-loss = 0.38435977697372437, train/logprobs = tensor([[-0.6271, -3.0318],
        [-0.7671, -1.1721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06717874854803085
Epoch 0, Step 179: train/loss = 0.4073675870895386, train/raw-loss = 0.37224650382995605, train/logprobs = tensor([[-0.5334, -2.8369],
        [-0.6247, -0.9159]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07024207711219788
Epoch 0, Step 180: train/loss = 0.4429398477077484, train/raw-loss = 0.4037390947341919, train/logprobs = tensor([[-0.7164, -2.4622],
        [-0.8244, -0.9870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07840149104595184
Epoch 0, Step 181: train/loss = 0.48799407482147217, train/raw-loss = 0.44434332847595215, train/logprobs = tensor([[-0.6619, -2.0947],
        [-0.8604, -0.9648]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08730148524045944
Epoch 0, Step 182: train/loss = 0.45299413800239563, train/raw-loss = 0.41924816370010376, train/logprobs = tensor([[-0.6333, -2.1553],
        [-0.8042, -0.7189]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06749191135168076
Epoch 0, Step 183: train/loss = 0.43486255407333374, train/raw-loss = 0.39713653922080994, train/logprobs = tensor([[-0.5876, -3.4871],
        [-0.6748, -1.7891]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0754520446062088
Epoch 0, Step 184: train/loss = 0.4352070689201355, train/raw-loss = 0.39704233407974243, train/logprobs = tensor([[-0.6414, -2.1195],
        [-0.8209, -0.6768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07632949948310852
Epoch 0, Step 185: train/loss = 0.4532429873943329, train/raw-loss = 0.4202934205532074, train/logprobs = tensor([[-0.6280, -2.1484],
        [-0.7248, -0.5693]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06589914113283157
Epoch 0, Step 186: train/loss = 0.4506877660751343, train/raw-loss = 0.40938276052474976, train/logprobs = tensor([[-0.7637, -2.5737],
        [-1.0304, -0.9729]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08261008560657501
Epoch 0, Step 187: train/loss = 0.47855499386787415, train/raw-loss = 0.43807798624038696, train/logprobs = tensor([[-0.6721, -2.1430],
        [-0.8962, -1.0394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08095413446426392
Epoch 0, Step 188: train/loss = 0.42921411991119385, train/raw-loss = 0.39242225885391235, train/logprobs = tensor([[-0.5495, -2.4212],
        [-0.6751, -0.7934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07358372211456299
Epoch 0, Step 189: train/loss = 0.45115989446640015, train/raw-loss = 0.4048752784729004, train/logprobs = tensor([[-0.6573, -2.4991],
        [-0.8084, -1.0874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09256919473409653
Epoch 0, Step 190: train/loss = 0.43058261275291443, train/raw-loss = 0.38915956020355225, train/logprobs = tensor([[-0.6668, -2.6010],
        [-0.8628, -1.0344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08284614980220795
Epoch 0, Step 191: train/loss = 0.4263100326061249, train/raw-loss = 0.3917664587497711, train/logprobs = tensor([[-0.6016, -2.5839],
        [-0.7268, -0.8586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0690871924161911
Epoch 0, Step 192: train/loss = 0.41147029399871826, train/raw-loss = 0.35074180364608765, train/logprobs = tensor([[-0.5938, -2.5193],
        [-0.8442, -0.7658]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.121457040309906
Epoch 0, Step 193: train/loss = 0.3698817789554596, train/raw-loss = 0.30323582887649536, train/logprobs = tensor([[-0.7485, -3.6722],
        [-1.0979, -1.0528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1332918256521225
Epoch 0, Step 194: train/loss = 0.46086251735687256, train/raw-loss = 0.41137391328811646, train/logprobs = tensor([[-0.8620, -1.9026],
        [-1.1408, -0.4994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09897727519273758
Epoch 0, Step 195: train/loss = 0.3194103240966797, train/raw-loss = 0.2572239637374878, train/logprobs = tensor([[-0.7186, -4.4759],
        [-1.0231, -0.6603]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12437267601490021
Epoch 0, Step 196: train/loss = 0.4428291618824005, train/raw-loss = 0.38185834884643555, train/logprobs = tensor([[-0.5545, -3.1535],
        [-0.7977, -1.2346]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12194158136844635
Epoch 0, Step 197: train/loss = 0.38400107622146606, train/raw-loss = 0.32124820351600647, train/logprobs = tensor([[-0.6496, -3.3636],
        [-0.8176, -0.9397]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.125505730509758
Epoch 0, Step 198: train/loss = 0.4030226469039917, train/raw-loss = 0.34061571955680847, train/logprobs = tensor([[-0.7884, -3.0290],
        [-1.2249, -0.8380]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12481386959552765
Epoch 0, Step 199: train/loss = 0.36106932163238525, train/raw-loss = 0.3040255010128021, train/logprobs = tensor([[-0.5857, -3.6725],
        [-0.8318, -1.0858]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11408768594264984
Epoch 0, Step 200: train/loss = 0.38889938592910767, train/raw-loss = 0.3303644061088562, train/logprobs = tensor([[-0.7763, -2.7842],
        [-1.0203, -0.6674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11706991493701935
Epoch 0, Step 201: train/loss = 0.3753603398799896, train/raw-loss = 0.3173399567604065, train/logprobs = tensor([[-0.5849, -3.3376],
        [-0.7749, -0.6925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11604081094264984
Epoch 0, Step 202: train/loss = 0.3469424843788147, train/raw-loss = 0.2848663926124573, train/logprobs = tensor([[-0.6761, -3.3033],
        [-0.9178, -0.4996]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12415215373039246
Epoch 0, Step 203: train/loss = 0.3510674834251404, train/raw-loss = 0.2859092950820923, train/logprobs = tensor([[-0.8382, -3.1376],
        [-1.2489, -0.5932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13031639158725739
Epoch 0, Step 204: train/loss = 0.4133450388908386, train/raw-loss = 0.3592049479484558, train/logprobs = tensor([[-0.6735, -2.3358],
        [-0.8623, -0.4464]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10828004777431488
Epoch 0, Step 205: train/loss = 0.3368312418460846, train/raw-loss = 0.2727842628955841, train/logprobs = tensor([[-0.7662, -2.8560],
        [-1.2542, -0.5101]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12809395790100098
Epoch 0, Step 206: train/loss = 0.38630664348602295, train/raw-loss = 0.3301546573638916, train/logprobs = tensor([[-0.6961, -2.2843],
        [-1.1304, -0.5650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11230406165122986
Epoch 0, Step 207: train/loss = 0.3848862051963806, train/raw-loss = 0.3262103199958801, train/logprobs = tensor([[-0.5126, -3.5828],
        [-0.6666, -1.0801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11735180020332336
Epoch 0, Step 208: train/loss = 0.40099889039993286, train/raw-loss = 0.3516099452972412, train/logprobs = tensor([[-0.5753, -2.6164],
        [-0.8429, -0.6644]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09877791255712509
Epoch 0, Step 209: train/loss = 0.39995643496513367, train/raw-loss = 0.34589505195617676, train/logprobs = tensor([[-0.4444, -2.4453],
        [-0.6368, -0.5106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10812283307313919
Epoch 0, Step 210: train/loss = 0.37323206663131714, train/raw-loss = 0.32143837213516235, train/logprobs = tensor([[-0.5884, -2.4857],
        [-0.8429, -0.3341]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10358737409114838
Epoch 0, Step 211: train/loss = 0.3454696834087372, train/raw-loss = 0.2849816083908081, train/logprobs = tensor([[-0.4828, -4.1119],
        [-0.6790, -1.0495]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12097614258527756
Epoch 0, Step 212: train/loss = 0.4149937629699707, train/raw-loss = 0.35337454080581665, train/logprobs = tensor([[-0.6573, -2.5996],
        [-0.9097, -0.8509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12323842942714691
Epoch 0, Step 213: train/loss = 0.3699661195278168, train/raw-loss = 0.30946773290634155, train/logprobs = tensor([[-0.5732, -4.5895],
        [-0.7266, -1.4022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12099678814411163
Epoch 0, Step 214: train/loss = 0.29256996512413025, train/raw-loss = 0.23109766840934753, train/logprobs = tensor([[-0.8761, -5.2225],
        [-1.3344, -1.1039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12294460833072662
Epoch 0, Step 215: train/loss = 0.31829261779785156, train/raw-loss = 0.25906050205230713, train/logprobs = tensor([[-0.8005, -4.7836],
        [-1.0530, -1.1149]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11846432089805603
Epoch 0, Step 216: train/loss = 0.3984048664569855, train/raw-loss = 0.34012436866760254, train/logprobs = tensor([[-0.5976, -2.7533],
        [-0.8805, -0.5453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11656106263399124
Epoch 0, Step 217: train/loss = 0.355613112449646, train/raw-loss = 0.2930501401424408, train/logprobs = tensor([[-0.6156, -3.7950],
        [-0.9428, -0.8349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1251259297132492
Epoch 0, Step 218: train/loss = 0.44047248363494873, train/raw-loss = 0.3757399320602417, train/logprobs = tensor([[-0.5735, -2.4936],
        [-0.8674, -0.8791]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12946513295173645
Epoch 0, Step 219: train/loss = 0.33388030529022217, train/raw-loss = 0.2756025791168213, train/logprobs = tensor([[-0.5946, -4.0029],
        [-0.8916, -1.1246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11655543744564056
Epoch 0, Step 220: train/loss = 0.38100430369377136, train/raw-loss = 0.3220616579055786, train/logprobs = tensor([[-0.6343, -3.3905],
        [-0.9151, -0.9551]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11788532882928848
Epoch 0, Step 221: train/loss = 0.350162148475647, train/raw-loss = 0.29077231884002686, train/logprobs = tensor([[-0.6289, -3.9329],
        [-0.8934, -0.9212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11877967417240143
Epoch 0, Step 222: train/loss = 0.3845558166503906, train/raw-loss = 0.3273310661315918, train/logprobs = tensor([[-0.7281, -3.2251],
        [-0.9759, -1.0286]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11444954574108124
Epoch 0, Step 223: train/loss = 0.45505422353744507, train/raw-loss = 0.3927597999572754, train/logprobs = tensor([[-0.6797, -2.3726],
        [-0.8045, -0.7532]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12458884716033936
Epoch 0, Step 224: train/loss = 0.33401983976364136, train/raw-loss = 0.25962525606155396, train/logprobs = tensor([[-0.6191, -3.6534],
        [-1.0297, -0.6682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1487892121076584
Epoch 0, Step 225: train/loss = 0.3787538409233093, train/raw-loss = 0.31265711784362793, train/logprobs = tensor([[-0.4532, -3.3799],
        [-0.6099, -0.7021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.132193461060524
Epoch 0, Step 226: train/loss = 0.3754913806915283, train/raw-loss = 0.30625468492507935, train/logprobs = tensor([[-0.5561, -3.5883],
        [-0.7709, -0.8619]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1384733021259308
Epoch 0, Step 227: train/loss = 0.43879440426826477, train/raw-loss = 0.3753066658973694, train/logprobs = tensor([[-0.4555, -2.2948],
        [-0.7057, -0.5960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1269754320383072
Epoch 0, Step 228: train/loss = 0.4597712457180023, train/raw-loss = 0.394783616065979, train/logprobs = tensor([[-0.5144, -3.0103],
        [-0.6479, -0.9877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12997525930404663
Epoch 0, Step 229: train/loss = 0.3507523536682129, train/raw-loss = 0.27335184812545776, train/logprobs = tensor([[-0.6064, -3.6569],
        [-1.0256, -1.2285]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15480101108551025
Epoch 0, Step 230: train/loss = 0.39804673194885254, train/raw-loss = 0.3281101584434509, train/logprobs = tensor([[-0.6528, -3.0454],
        [-0.7824, -0.6666]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.139873206615448
Epoch 0, Step 231: train/loss = 0.3628599941730499, train/raw-loss = 0.29663145542144775, train/logprobs = tensor([[-0.5582, -3.3387],
        [-0.7913, -0.5235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13245710730552673
Epoch 0, Step 232: train/loss = 0.3875872790813446, train/raw-loss = 0.3099820613861084, train/logprobs = tensor([[-0.6144, -3.5468],
        [-0.9523, -1.0402]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1552104651927948
Epoch 0, Step 233: train/loss = 0.3511354923248291, train/raw-loss = 0.27660509943962097, train/logprobs = tensor([[-0.6339, -4.7928],
        [-0.7503, -0.9304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14906077086925507
Epoch 0, Step 234: train/loss = 0.34301984310150146, train/raw-loss = 0.27707433700561523, train/logprobs = tensor([[-0.6526, -4.6797],
        [-0.7914, -0.8686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13189096748828888
Epoch 0, Step 235: train/loss = 0.3845594525337219, train/raw-loss = 0.31176048517227173, train/logprobs = tensor([[-0.5916, -3.8878],
        [-0.8501, -1.0238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.145597904920578
Epoch 0, Step 236: train/loss = 0.34156203269958496, train/raw-loss = 0.268949031829834, train/logprobs = tensor([[-0.6240, -3.3303],
        [-0.9745, -0.5148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14522604644298553
Epoch 0, Step 237: train/loss = 0.3667547404766083, train/raw-loss = 0.29714277386665344, train/logprobs = tensor([[-0.5874, -3.2987],
        [-0.8934, -0.6655]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13922390341758728
Epoch 0, Step 238: train/loss = 0.2965928912162781, train/raw-loss = 0.22013252973556519, train/logprobs = tensor([[-0.6504, -5.7024],
        [-1.1218, -0.9324]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15292073786258698
Epoch 0, Step 239: train/loss = 0.30606696009635925, train/raw-loss = 0.23658744990825653, train/logprobs = tensor([[-0.7251, -4.6284],
        [-1.0887, -0.7904]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13895905017852783
Epoch 0, Step 240: train/loss = 0.42746588587760925, train/raw-loss = 0.36180490255355835, train/logprobs = tensor([[-0.5411, -2.3341],
        [-0.7689, -0.3908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13132193684577942
Epoch 0, Step 241: train/loss = 0.3158515691757202, train/raw-loss = 0.24631136655807495, train/logprobs = tensor([[-0.8230, -4.3467],
        [-1.3308, -0.4502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13908037543296814
Epoch 0, Step 242: train/loss = 0.3663972318172455, train/raw-loss = 0.3018677234649658, train/logprobs = tensor([[-0.4622, -3.3955],
        [-0.7021, -0.9259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12905903160572052
Epoch 0, Step 243: train/loss = 0.3219267725944519, train/raw-loss = 0.2524124085903168, train/logprobs = tensor([[-0.5977, -4.0643],
        [-0.9648, -0.5519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13902871310710907
Epoch 0, Step 244: train/loss = 0.3397732973098755, train/raw-loss = 0.25853776931762695, train/logprobs = tensor([[-0.5493, -3.4743],
        [-1.0312, -0.9368]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16247107088565826
Epoch 0, Step 245: train/loss = 0.33994245529174805, train/raw-loss = 0.26328223943710327, train/logprobs = tensor([[-0.6281, -3.6961],
        [-1.0974, -0.6419]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15332038700580597
Epoch 0, Step 246: train/loss = 0.33229804039001465, train/raw-loss = 0.25597548484802246, train/logprobs = tensor([[-0.5791, -5.2354],
        [-0.9023, -0.8946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15264514088630676
Epoch 0, Step 247: train/loss = 0.3724125325679779, train/raw-loss = 0.2939442992210388, train/logprobs = tensor([[-0.5876, -3.7623],
        [-0.9340, -0.9121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15693648159503937
Epoch 0, Step 248: train/loss = 0.41922441124916077, train/raw-loss = 0.34749943017959595, train/logprobs = tensor([[-0.5560, -2.6656],
        [-0.9006, -0.5941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14344994723796844
Epoch 0, Step 249: train/loss = 0.3498544692993164, train/raw-loss = 0.2762787640094757, train/logprobs = tensor([[-0.6274, -4.3389],
        [-0.8558, -0.7986]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1471514254808426
Epoch 0, Step 250: train/loss = 0.35867974162101746, train/raw-loss = 0.29052022099494934, train/logprobs = tensor([[-0.6499, -5.3638],
        [-0.7466, -0.8440]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.136319100856781
Epoch 0, Step 251: train/loss = 0.38623666763305664, train/raw-loss = 0.3187939524650574, train/logprobs = tensor([[-0.5079, -2.9339],
        [-0.8088, -0.5039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13488540053367615
Epoch 0, Step 252: train/loss = 0.2908283770084381, train/raw-loss = 0.21296373009681702, train/logprobs = tensor([[-0.7859, -5.2827],
        [-1.2894, -0.9859]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.155729278922081
Epoch 0, Step 253: train/loss = 0.3658125400543213, train/raw-loss = 0.30293774604797363, train/logprobs = tensor([[-0.5064, -3.4389],
        [-0.6994, -0.3083]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12574957311153412
Epoch 0, Step 254: train/loss = 0.3297828137874603, train/raw-loss = 0.2618134915828705, train/logprobs = tensor([[-0.6712, -3.4550],
        [-1.0540, -0.5874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1359385997056961
Epoch 0, Step 255: train/loss = 0.2965945303440094, train/raw-loss = 0.21817222237586975, train/logprobs = tensor([[-0.8484, -4.9890],
        [-1.3473, -0.9292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1568446308374405
Epoch 0, Step 256: train/loss = 0.3587908148765564, train/raw-loss = 0.28207457065582275, train/logprobs = tensor([[-0.6601, -4.7485],
        [-0.9256, -0.9128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15343239903450012
Epoch 0, Step 257: train/loss = 0.31688663363456726, train/raw-loss = 0.23939067125320435, train/logprobs = tensor([[-0.6349, -4.5055],
        [-1.0360, -0.9553]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15499190986156464
Epoch 0, Step 258: train/loss = 0.33715853095054626, train/raw-loss = 0.26566678285598755, train/logprobs = tensor([[-0.5147, -3.8418],
        [-0.8971, -0.6896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14298349618911743
Epoch 0, Step 259: train/loss = 0.3355519473552704, train/raw-loss = 0.25603827834129333, train/logprobs = tensor([[-0.5845, -3.3366],
        [-1.1210, -0.6857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1590273380279541
Epoch 0, Step 260: train/loss = 0.33866026997566223, train/raw-loss = 0.28144752979278564, train/logprobs = tensor([[-0.3732, -4.6948],
        [-0.5826, -0.9153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11442543566226959
Epoch 0, Step 261: train/loss = 0.2817586064338684, train/raw-loss = 0.20531079173088074, train/logprobs = tensor([[-0.7112, -4.7533],
        [-1.3947, -0.5596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15289565920829773
Epoch 0, Step 262: train/loss = 0.3309282064437866, train/raw-loss = 0.25847283005714417, train/logprobs = tensor([[-0.5534, -3.9679],
        [-0.9145, -0.6888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1449107527732849
Epoch 0, Step 263: train/loss = 0.3889443278312683, train/raw-loss = 0.3197898268699646, train/logprobs = tensor([[-0.5080, -3.0757],
        [-0.8123, -0.9775]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1383090615272522
Epoch 0, Step 264: train/loss = 0.3039458394050598, train/raw-loss = 0.23719537258148193, train/logprobs = tensor([[-0.4643, -5.1302],
        [-0.8207, -0.9536]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13350094854831696
Epoch 0, Step 265: train/loss = 0.3181453347206116, train/raw-loss = 0.24135646224021912, train/logprobs = tensor([[-0.5443, -4.4690],
        [-1.1141, -0.6961]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15357773005962372
Epoch 0, Step 266: train/loss = 0.32246747612953186, train/raw-loss = 0.25623807311058044, train/logprobs = tensor([[-0.5077, -5.4760],
        [-0.8982, -1.1473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13245882093906403
Epoch 0, Step 267: train/loss = 0.32717207074165344, train/raw-loss = 0.2465895712375641, train/logprobs = tensor([[-0.6907, -3.5697],
        [-1.2428, -0.8812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16116496920585632
Epoch 0, Step 268: train/loss = 0.3358055353164673, train/raw-loss = 0.2601976990699768, train/logprobs = tensor([[-0.5600, -3.9178],
        [-0.9403, -0.7153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15121568739414215
Epoch 0, Step 269: train/loss = 0.42756277322769165, train/raw-loss = 0.3581125736236572, train/logprobs = tensor([[-0.5195, -3.0797],
        [-0.7067, -1.0467]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13890036940574646
Epoch 0, Step 270: train/loss = 0.3316337466239929, train/raw-loss = 0.256585955619812, train/logprobs = tensor([[-0.6392, -3.8786],
        [-0.9841, -0.4850]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15009553730487823
Epoch 0, Step 271: train/loss = 0.4280879497528076, train/raw-loss = 0.36082977056503296, train/logprobs = tensor([[-0.5017, -2.8739],
        [-0.6158, -0.6064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13451635837554932
Epoch 0, Step 272: train/loss = 0.34097814559936523, train/raw-loss = 0.2686159908771515, train/logprobs = tensor([[-0.7089, -3.4525],
        [-1.1446, -0.9158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14472435414791107
Epoch 0, Step 273: train/loss = 0.3625701665878296, train/raw-loss = 0.2919071316719055, train/logprobs = tensor([[-0.5587, -3.0370],
        [-0.8725, -0.5954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14132602512836456
Epoch 0, Step 274: train/loss = 0.29231691360473633, train/raw-loss = 0.2120169848203659, train/logprobs = tensor([[-0.6513, -5.2786],
        [-1.1317, -0.6241]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16059987246990204
Epoch 0, Step 275: train/loss = 0.3237798810005188, train/raw-loss = 0.24646934866905212, train/logprobs = tensor([[-0.5594, -4.5399],
        [-0.8792, -0.6115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15462104976177216
Epoch 0, Step 276: train/loss = 0.29804134368896484, train/raw-loss = 0.2333672046661377, train/logprobs = tensor([[-0.5122, -5.1887],
        [-0.9206, -0.9351]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1293482631444931
Epoch 0, Step 277: train/loss = 0.3175327479839325, train/raw-loss = 0.2506183981895447, train/logprobs = tensor([[-0.5836, -5.1735],
        [-0.8930, -0.7680]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13382869958877563
Epoch 0, Step 278: train/loss = 0.29706278443336487, train/raw-loss = 0.2306688129901886, train/logprobs = tensor([[-0.5504, -5.4788],
        [-0.9588, -0.7874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13278795778751373
Epoch 0, Step 279: train/loss = 0.3254801034927368, train/raw-loss = 0.25546395778656006, train/logprobs = tensor([[-0.6168, -4.3045],
        [-0.9354, -0.8810]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14003227651119232
Epoch 0, Step 280: train/loss = 0.2915835976600647, train/raw-loss = 0.2197810709476471, train/logprobs = tensor([[-0.5424, -4.4036],
        [-1.0184, -0.4747]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14360502362251282
Epoch 0, Step 281: train/loss = 0.3210139572620392, train/raw-loss = 0.24679824709892273, train/logprobs = tensor([[-0.6996, -4.9563],
        [-1.0987, -0.7802]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1484314501285553
Epoch 0, Step 282: train/loss = 0.36448347568511963, train/raw-loss = 0.2906336188316345, train/logprobs = tensor([[-0.5207, -3.2626],
        [-0.8549, -0.6709]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.147699773311615
Epoch 0, Step 283: train/loss = 0.32013124227523804, train/raw-loss = 0.2544940114021301, train/logprobs = tensor([[-0.5222, -5.0748],
        [-0.7417, -1.0517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1312745064496994
Epoch 0, Step 284: train/loss = 0.4044695198535919, train/raw-loss = 0.32707712054252625, train/logprobs = tensor([[-0.6202, -2.9320],
        [-1.1273, -1.0658]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15478481352329254
Epoch 0, Step 285: train/loss = 0.35312533378601074, train/raw-loss = 0.29171022772789, train/logprobs = tensor([[-0.4296, -3.7678],
        [-0.5773, -0.6597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12283013015985489
Epoch 0, Step 286: train/loss = 0.30778220295906067, train/raw-loss = 0.23952677845954895, train/logprobs = tensor([[-0.4178, -4.8316],
        [-0.8162, -0.8841]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13651081919670105
Epoch 0, Step 287: train/loss = 0.3411632180213928, train/raw-loss = 0.261502742767334, train/logprobs = tensor([[-0.7284, -3.6273],
        [-1.3231, -0.6759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15932098031044006
Epoch 0, Step 288: train/loss = 0.3394831418991089, train/raw-loss = 0.25799885392189026, train/logprobs = tensor([[-0.6576, -3.9475],
        [-1.2718, -1.0054]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16296856105327606
Epoch 0, Step 289: train/loss = 0.27605533599853516, train/raw-loss = 0.1953471601009369, train/logprobs = tensor([[-0.6310, -4.6878],
        [-1.2665, -0.3923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16141635179519653
Epoch 0, Step 290: train/loss = 0.3616775870323181, train/raw-loss = 0.2820586860179901, train/logprobs = tensor([[-0.5454, -3.1651],
        [-1.0257, -0.8010]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1592378169298172
Epoch 0, Step 291: train/loss = 0.32257068157196045, train/raw-loss = 0.24255813658237457, train/logprobs = tensor([[-0.5491, -4.1024],
        [-1.0937, -1.1816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16002508997917175
Epoch 0, Step 292: train/loss = 0.30467772483825684, train/raw-loss = 0.23946647346019745, train/logprobs = tensor([[-0.6557, -6.9899],
        [-1.2947, -0.6520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13042250275611877
Epoch 0, Step 293: train/loss = 0.2934498190879822, train/raw-loss = 0.22622817754745483, train/logprobs = tensor([[-0.4530, -4.5147],
        [-0.9917, -0.6054]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1344432234764099
Epoch 0, Step 294: train/loss = 0.3715328574180603, train/raw-loss = 0.29955047369003296, train/logprobs = tensor([[-0.4518, -3.9438],
        [-0.7699, -0.8252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1439647376537323
Epoch 0, Step 295: train/loss = 0.30580782890319824, train/raw-loss = 0.22680281102657318, train/logprobs = tensor([[-0.5243, -4.9923],
        [-0.9663, -0.8800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1580100655555725
Epoch 0, Step 296: train/loss = 0.31092336773872375, train/raw-loss = 0.24440230429172516, train/logprobs = tensor([[-0.5120, -5.0546],
        [-0.8138, -0.5687]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1330421268939972
Epoch 0, Step 297: train/loss = 0.30238643288612366, train/raw-loss = 0.22532199323177338, train/logprobs = tensor([[-0.5740, -4.2552],
        [-1.1303, -0.6500]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15412886440753937
Epoch 0, Step 298: train/loss = 0.32264527678489685, train/raw-loss = 0.2436140924692154, train/logprobs = tensor([[-0.6356, -5.6491],
        [-1.2066, -1.0617]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15806233882904053
Epoch 0, Step 299: train/loss = 0.2852608859539032, train/raw-loss = 0.20164254307746887, train/logprobs = tensor([[-0.6499, -4.8759],
        [-1.2601, -0.6639]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16723667085170746
Epoch 0, Step 300: train/loss = 0.2807193994522095, train/raw-loss = 0.21418192982673645, train/logprobs = tensor([[-0.4948, -6.1908],
        [-0.9178, -0.4464]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13307493925094604
Epoch 0, Step 301: train/loss = 0.3141094148159027, train/raw-loss = 0.2302646040916443, train/logprobs = tensor([[-0.5923, -3.8612],
        [-1.2386, -0.4295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16768960654735565
Epoch 0, Step 302: train/loss = 0.3183385133743286, train/raw-loss = 0.2439965307712555, train/logprobs = tensor([[-0.4843, -5.5957],
        [-0.9889, -1.2717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14868396520614624
Epoch 0, Step 303: train/loss = 0.3047049641609192, train/raw-loss = 0.2307916283607483, train/logprobs = tensor([[-0.5122, -4.7920],
        [-1.0325, -0.8419]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14782671630382538
Epoch 0, Step 304: train/loss = 0.3086017966270447, train/raw-loss = 0.22788821160793304, train/logprobs = tensor([[-0.6602, -4.5180],
        [-1.1595, -0.7682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16142718493938446
Epoch 0, Step 305: train/loss = 0.28664180636405945, train/raw-loss = 0.21338534355163574, train/logprobs = tensor([[-0.5534, -6.1295],
        [-1.1508, -0.6270]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14651291072368622
Epoch 0, Step 306: train/loss = 0.2923044264316559, train/raw-loss = 0.2050202190876007, train/logprobs = tensor([[-0.5858, -3.9836],
        [-1.4386, -0.6011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17456841468811035
Epoch 0, Step 307: train/loss = 0.2768658399581909, train/raw-loss = 0.20023924112319946, train/logprobs = tensor([[-0.8313, -6.0231],
        [-1.5942, -1.0377]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1532531976699829
Epoch 0, Step 308: train/loss = 0.29807788133621216, train/raw-loss = 0.22386641800403595, train/logprobs = tensor([[-0.5382, -4.8899],
        [-1.0683, -0.8024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14842292666435242
Epoch 0, Step 309: train/loss = 0.30512645840644836, train/raw-loss = 0.23502513766288757, train/logprobs = tensor([[-0.4644, -5.0604],
        [-0.8147, -0.4112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14020267128944397
Epoch 0, Step 310: train/loss = 0.27611759305000305, train/raw-loss = 0.19848409295082092, train/logprobs = tensor([[-0.6168, -4.9407],
        [-1.3355, -1.0248]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15526703000068665
Epoch 0, Step 311: train/loss = 0.26311928033828735, train/raw-loss = 0.17889276146888733, train/logprobs = tensor([[-0.6502, -5.7293],
        [-1.5742, -0.6900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16845300793647766
Epoch 0, Step 312: train/loss = 0.3255561292171478, train/raw-loss = 0.2588379383087158, train/logprobs = tensor([[-0.3988, -4.8821],
        [-0.7217, -0.8503]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1334364116191864
Epoch 0, Step 313: train/loss = 0.3595263659954071, train/raw-loss = 0.2931027412414551, train/logprobs = tensor([[-0.4603, -5.0882],
        [-0.6872, -1.2548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13284718990325928
Epoch 0, Step 314: train/loss = 0.2888239026069641, train/raw-loss = 0.21435512602329254, train/logprobs = tensor([[-0.5494, -4.8905],
        [-1.1237, -1.0278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14893752336502075
Epoch 0, Step 315: train/loss = 0.31263232231140137, train/raw-loss = 0.2331307828426361, train/logprobs = tensor([[-0.5027, -4.2671],
        [-1.0496, -0.6697]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15900304913520813
Epoch 0, Step 316: train/loss = 0.2818467617034912, train/raw-loss = 0.2046479880809784, train/logprobs = tensor([[-0.5330, -5.1693],
        [-1.0649, -0.4017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15439751744270325
Epoch 0, Step 317: train/loss = 0.4184733033180237, train/raw-loss = 0.34391677379608154, train/logprobs = tensor([[-0.4270, -2.4817],
        [-0.8097, -0.4635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1491130143404007
Epoch 0, Step 318: train/loss = 0.34531688690185547, train/raw-loss = 0.27535730600357056, train/logprobs = tensor([[-0.5534, -4.1728],
        [-0.7936, -0.7226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13991914689540863
Epoch 0, Step 319: train/loss = 0.2949068248271942, train/raw-loss = 0.22408899664878845, train/logprobs = tensor([[-0.5538, -4.8388],
        [-1.0180, -0.7576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1416357159614563
Epoch 0, Step 320: train/loss = 0.34582263231277466, train/raw-loss = 0.2715669870376587, train/logprobs = tensor([[-0.5843, -4.2346],
        [-0.9650, -1.1399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14851130545139313
Epoch 0, Step 321: train/loss = 0.23817729949951172, train/raw-loss = 0.15256741642951965, train/logprobs = tensor([[-0.6722, -4.6156],
        [-1.7063, -0.7837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17121978104114532
Epoch 0, Step 322: train/loss = 0.32770177721977234, train/raw-loss = 0.2498948872089386, train/logprobs = tensor([[-0.4114, -4.6434],
        [-0.8520, -1.1001]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15561378002166748
Epoch 0, Step 323: train/loss = 0.2895881235599518, train/raw-loss = 0.20840169489383698, train/logprobs = tensor([[-0.5697, -5.1387],
        [-1.2427, -0.8376]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16237284243106842
Epoch 0, Step 324: train/loss = 0.272802472114563, train/raw-loss = 0.19896280765533447, train/logprobs = tensor([[-0.5142, -5.9024],
        [-1.0557, -0.4330]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14767935872077942
Epoch 0, Step 325: train/loss = 0.26997750997543335, train/raw-loss = 0.17763294279575348, train/logprobs = tensor([[-0.5256, -5.2946],
        [-1.2440, -0.6290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18468916416168213
Epoch 0, Step 326: train/loss = 0.30556201934814453, train/raw-loss = 0.22902911901474, train/logprobs = tensor([[-0.5443, -5.3723],
        [-0.9543, -1.1472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15306586027145386
