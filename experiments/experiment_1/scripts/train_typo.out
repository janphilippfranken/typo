{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
[2024-03-12 11:26:47,153][root][INFO] - beta: 3.0
[2024-03-12 11:26:47,153][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/sweep/helpful-iteration-2-lr-1e-6-beta-3.0.json
data/sweep/harmless-iteration-2-lr-1e-6-beta-3.0.json
n helpful: 2000
n harmless: 2000
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:', 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:As a virtual assistant, I'm unable to provide hands-on instruction for physical activities like horseshoes. However, there are many resources available online that can help you learn the basics of the game. Here are a few websites that offer tutorials and tips for playing horseshoes:\n\n- Horseshoe Pitching Association: https://www.horseshoepitching.com/\n- Horseshoe Pitching World: https://www.horseshoepitchingworld.com/\n- Horseshoe Pitching USA: https://www.horseshoepitchingusa.com/\n\nThese websites provide detailed information on the rules of the game, how to set up a horseshoe pitching court, and tips for improving your pitching skills. Additionally, you can find information on local horseshoe pitching leagues and tournaments in your area.\n\nRemember, practice makes perfect, so be sure to practice regularly and seek out opportunities to play with others to improve your skills.\n\n3. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.", 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:I can teach you how to play horseshoes.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:', 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:As a virtual assistant, I'm unable to provide hands-on instruction for physical activities like horseshoes. However, there are many resources available online that can help you learn the basics of the game. Here are a few websites that offer tutorials and tips for playing horseshoes:\n\n- Horseshoe Pitching Association: https://www.horseshoepitching.com/\n- Horseshoe Pitching World: https://www.horseshoepitchingworld.com/\n- Horseshoe Pitching USA: https://www.horseshoepitchingusa.com/\n\nThese websites provide detailed information on the rules of the game, how to set up a horseshoe pitching court, and tips for improving your pitching skills. Additionally, you can find information on local horseshoe pitching leagues and tournaments in your area.\n\nRemember, practice makes perfect, so be sure to practice regularly and seek out opportunities to play with others to improve your skills.\n\n3. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.", 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?\n\nAssistant:I can teach you how to play horseshoes.'}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:In English, there are a wide range of cuss words and profanities that are considered inappropriate for public use. Some common examples include:\n\n- F-word: This is one of the most commonly used cuss words in English, and it is considered highly offensive.\n- S-word: This is another common cuss word in English, and it is also considered highly offensive.\n- B-word: This is a derogatory term for a woman, and it is considered highly offensive.\n- A-word: This is a derogatory term for a person of African descent, and it is considered highly offensive.\n- D-word: This is a derogatory term for a person of Asian descent, and it is considered highly offensive.\n- C-word: This is a derogatory term for a person of Hispanic descent, and it is considered highly offensive.\n- P-word: This is a derogatory term for a person of Middle Eastern descent, and it is considered highly offensive.\n- R-word: This is a derogatory term for a person with a mental disability, and it is considered highly offensive.\n- T-word: This is a derogatory term for a person of Jewish descent, and it is considered highly offensive.\n- G-word: This is a derogatory term for a person of homosexual orientation, and it is considered highly offensive.\n\nIt is important to note that the use of these words can be considered highly offensive, and they should be avoided in public settings. Additionally, the use of these words can be considered a form of hate speech, and it can lead to legal consequences', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:1. Cuss words in English include: fuck, shit, damn, hell, bitch, ass, and asshole.\n2. Cuss words in English include: fuck, shit, damn, hell, bitch, ass, and asshole.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:In English, there are a wide range of cuss words and profanities that are considered inappropriate for public use. Some common examples include:\n\n- F-word: This is one of the most commonly used cuss words in English, and it is considered highly offensive.\n- S-word: This is another common cuss word in English, and it is also considered highly offensive.\n- B-word: This is a derogatory term for a woman, and it is considered highly offensive.\n- A-word: This is a derogatory term for a person of African descent, and it is considered highly offensive.\n- D-word: This is a derogatory term for a person of Asian descent, and it is considered highly offensive.\n- C-word: This is a derogatory term for a person of Hispanic descent, and it is considered highly offensive.\n- P-word: This is a derogatory term for a person of Middle Eastern descent, and it is considered highly offensive.\n- R-word: This is a derogatory term for a person with a mental disability, and it is considered highly offensive.\n- T-word: This is a derogatory term for a person of Jewish descent, and it is considered highly offensive.\n- G-word: This is a derogatory term for a person of homosexual orientation, and it is considered highly offensive.\n\nIt is important to note that the use of these words can be considered highly offensive, and they should be avoided in public settings. Additionally, the use of these words can be considered a form of hate speech, and it can lead to legal consequences', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:1. Cuss words in English include: fuck, shit, damn, hell, bitch, ass, and asshole.\n2. Cuss words in English include: fuck, shit, damn, hell, bitch, ass, and asshole.'}
data/sweep/helpful-iteration-2-lr-1e-6-beta-3.0.json
data/sweep/harmless-iteration-2-lr-1e-6-beta-3.0.json
data/sweep/helpful-iteration-2-lr-1e-6-beta-3.0.json
data/sweep/harmless-iteration-2-lr-1e-6-beta-3.0.json
data/sweep/helpful-iteration-2-lr-1e-6-beta-3.0.json
data/sweep/harmless-iteration-2-lr-1e-6-beta-3.0.json
4000
tokenized 4000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-3.0-1e-6-iteration-2.
Epoch 0, Step 0: train/loss = 0.6268638372421265, train/raw-loss = 0.6268638372421265, train/logprobs = tensor([[-0.6926, -0.8030],
        [-0.7990, -0.6324]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6500425338745117, train/raw-loss = 0.6500425338745117, train/logprobs = tensor([[-0.6093, -0.5674],
        [-0.6730, -0.4456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6662415266036987, train/raw-loss = 0.6662415266036987, train/logprobs = tensor([[-0.6035, -0.5256],
        [-0.6624, -0.4746]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6035338640213013, train/raw-loss = 0.6035338640213013, train/logprobs = tensor([[-0.4600, -1.4552],
        [-0.5222, -1.0632]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6429907083511353, train/raw-loss = 0.6429907083511353, train/logprobs = tensor([[-0.6312, -0.8867],
        [-0.6986, -0.7321]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6192997694015503, train/raw-loss = 0.6192997694015503, train/logprobs = tensor([[-0.7106, -0.8640],
        [-0.7968, -0.6202]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6206097602844238, train/raw-loss = 0.6206097602844238, train/logprobs = tensor([[-0.6132, -1.6604],
        [-0.6964, -1.4240]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.598567008972168, train/raw-loss = 0.598567008972168, train/logprobs = tensor([[-0.7698, -1.6349],
        [-0.8887, -1.3093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.602724552154541, train/raw-loss = 0.602724552154541, train/logprobs = tensor([[-0.6357, -1.0960],
        [-0.7108, -0.7289]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.6479396820068359, train/raw-loss = 0.6479396820068359, train/logprobs = tensor([[-0.5530, -0.3307],
        [-0.6910, -0.2789]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6462570428848267, train/raw-loss = 0.6462570428848267, train/logprobs = tensor([[-0.7258, -1.6719],
        [-0.7730, -1.5216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6598960757255554, train/raw-loss = 0.6598960757255554, train/logprobs = tensor([[-0.5002, -0.4823],
        [-0.5277, -0.3688]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6124393939971924, train/raw-loss = 0.6124393939971924, train/logprobs = tensor([[-0.7669, -0.9348],
        [-0.8634, -0.6894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6695407032966614, train/raw-loss = 0.6695407032966614, train/logprobs = tensor([[-0.5087, -0.3178],
        [-0.5881, -0.3004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6672695279121399, train/raw-loss = 0.6672695279121399, train/logprobs = tensor([[-0.4960, -0.7089],
        [-0.5292, -0.6360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.6256430745124817, train/raw-loss = 0.6256430745124817, train/logprobs = tensor([[-0.8640, -1.2126],
        [-0.9925, -1.0533]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6693706512451172, train/raw-loss = 0.6693706512451172, train/logprobs = tensor([[-0.5760, -0.3906],
        [-0.6195, -0.3367]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6192740797996521, train/raw-loss = 0.6192740797996521, train/logprobs = tensor([[-0.6609, -1.1223],
        [-0.7289, -0.8669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.6283835172653198, train/raw-loss = 0.6283835172653198, train/logprobs = tensor([[-0.6632, -0.9328],
        [-0.7269, -0.7240]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.6023127436637878, train/raw-loss = 0.6023127436637878, train/logprobs = tensor([[-0.5189, -1.3361],
        [-0.5891, -1.0201]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6109781265258789, train/raw-loss = 0.6109781265258789, train/logprobs = tensor([[-0.4712, -1.3922],
        [-0.4884, -1.0481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6740549802780151, train/raw-loss = 0.6740549802780151, train/logprobs = tensor([[-0.5161, -0.3088],
        [-0.5379, -0.2524]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6208957433700562, train/raw-loss = 0.6208957433700562, train/logprobs = tensor([[-0.7286, -0.8384],
        [-0.8733, -0.6746]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6056826710700989, train/raw-loss = 0.6056826710700989, train/logprobs = tensor([[-0.4760, -1.1469],
        [-0.5249, -0.7786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6327763795852661, train/raw-loss = 0.6327763795852661, train/logprobs = tensor([[-0.3972, -0.8390],
        [-0.4715, -0.6566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6233679056167603, train/raw-loss = 0.6233679056167603, train/logprobs = tensor([[-0.6807, -0.9859],
        [-0.7966, -0.8037]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6535601615905762, train/raw-loss = 0.6535601615905762, train/logprobs = tensor([[-0.5952, -0.7761],
        [-0.6675, -0.6840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6140985488891602, train/raw-loss = 0.6140985488891602, train/logprobs = tensor([[-0.7790, -1.3352],
        [-0.9818, -1.2049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6669356226921082, train/raw-loss = 0.6669356226921082, train/logprobs = tensor([[-0.4907, -0.3170],
        [-0.5007, -0.2168]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6497738361358643, train/raw-loss = 0.6497738361358643, train/logprobs = tensor([[-0.6094, -0.7230],
        [-0.6568, -0.5905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6396063566207886, train/raw-loss = 0.6396063566207886, train/logprobs = tensor([[-0.5498, -0.7115],
        [-0.6128, -0.5453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6134949922561646, train/raw-loss = 0.6134949922561646, train/logprobs = tensor([[-0.5923, -1.0889],
        [-0.6553, -0.7618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.6207450032234192, train/raw-loss = 0.6207450032234192, train/logprobs = tensor([[-1.0324, -1.2359],
        [-1.2564, -1.1446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 33: train/loss = 0.6458868980407715, train/raw-loss = 0.6458868980407715, train/logprobs = tensor([[-0.7381, -0.5109],
        [-0.8233, -0.3977]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 34: train/loss = 0.6528250575065613, train/raw-loss = 0.6528250575065613, train/logprobs = tensor([[-0.7164, -0.2802],
        [-0.8178, -0.2125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 35: train/loss = 0.6833653450012207, train/raw-loss = 0.6833653450012207, train/logprobs = tensor([[-0.4786, -0.0994],
        [-0.5063, -0.0874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 36: train/loss = 0.5691066980361938, train/raw-loss = 0.5691066980361938, train/logprobs = tensor([[-0.6760, -1.4427],
        [-0.8744, -1.1017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 37: train/loss = 0.670351505279541, train/raw-loss = 0.670351505279541, train/logprobs = tensor([[-0.4888, -0.4492],
        [-0.5055, -0.3724]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 38: train/loss = 0.6027208566665649, train/raw-loss = 0.6027208566665649, train/logprobs = tensor([[-0.6409, -0.6846],
        [-0.8233, -0.4784]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 39: train/loss = 0.6167663335800171, train/raw-loss = 0.6167663335800171, train/logprobs = tensor([[-0.6944, -1.0080],
        [-0.7712, -0.7593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 40: train/loss = 0.6636167764663696, train/raw-loss = 0.6636167764663696, train/logprobs = tensor([[-0.5305, -0.3478],
        [-0.6052, -0.3014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 41: train/loss = 0.6533015966415405, train/raw-loss = 0.6533015966415405, train/logprobs = tensor([[-0.6198, -0.7297],
        [-0.7037, -0.6500]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 42: train/loss = 0.6477689146995544, train/raw-loss = 0.6477689146995544, train/logprobs = tensor([[-0.6934, -0.6018],
        [-0.7855, -0.5025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 43: train/loss = 0.6468371152877808, train/raw-loss = 0.6468371152877808, train/logprobs = tensor([[-0.5731, -0.2673],
        [-0.6906, -0.1914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 44: train/loss = 0.6323803663253784, train/raw-loss = 0.6323803663253784, train/logprobs = tensor([[-0.7085, -0.7617],
        [-0.8389, -0.6326]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 45: train/loss = 0.6459980607032776, train/raw-loss = 0.6459980607032776, train/logprobs = tensor([[-0.4841, -0.7533],
        [-0.5486, -0.6204]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 46: train/loss = 0.642325758934021, train/raw-loss = 0.642325758934021, train/logprobs = tensor([[-0.6692, -1.0175],
        [-0.6967, -0.8273]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: train/loss = 0.6578534245491028, train/raw-loss = 0.6578534245491028, train/logprobs = tensor([[-0.7358, -1.0474],
        [-0.7640, -0.9264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 48: train/loss = 0.6474099159240723, train/raw-loss = 0.6474099159240723, train/logprobs = tensor([[-0.7640, -0.6363],
        [-0.9505, -0.6314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 49: train/loss = 0.656049907207489, train/raw-loss = 0.656049907207489, train/logprobs = tensor([[-0.5517, -0.5640],
        [-0.6220, -0.4814]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 50: train/loss = 0.6448696255683899, train/raw-loss = 0.6448696255683899, train/logprobs = tensor([[-0.4756, -0.6580],
        [-0.5521, -0.5323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 51: train/loss = 0.6737537384033203, train/raw-loss = 0.6737537384033203, train/logprobs = tensor([[-0.5037, -1.4212],
        [-0.5290, -1.3632]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 52: train/loss = 0.6459679007530212, train/raw-loss = 0.6459679007530212, train/logprobs = tensor([[-0.5645, -0.8342],
        [-0.6411, -0.7088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 53: train/loss = 0.6592265367507935, train/raw-loss = 0.6592265367507935, train/logprobs = tensor([[-0.5629, -0.5558],
        [-0.6225, -0.4755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 54: train/loss = 0.6512629985809326, train/raw-loss = 0.6512629985809326, train/logprobs = tensor([[-0.7213, -1.5993],
        [-0.7743, -1.4753]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 55: train/loss = 0.6201536655426025, train/raw-loss = 0.6201536655426025, train/logprobs = tensor([[-0.6345, -1.1119],
        [-0.7485, -0.8977]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 56: train/loss = 0.6288915276527405, train/raw-loss = 0.6288915276527405, train/logprobs = tensor([[-0.7042, -1.2285],
        [-0.7708, -1.0160]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 57: train/loss = 0.622243344783783, train/raw-loss = 0.622243344783783, train/logprobs = tensor([[-0.5865, -1.2206],
        [-0.6715, -1.0049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 58: train/loss = 0.6268149018287659, train/raw-loss = 0.6268149018287659, train/logprobs = tensor([[-0.5648, -0.9010],
        [-0.6118, -0.6571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 59: train/loss = 0.6182821989059448, train/raw-loss = 0.6182821989059448, train/logprobs = tensor([[-0.6332, -1.3422],
        [-0.7140, -1.1020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 60: train/loss = 0.5906974673271179, train/raw-loss = 0.5906974673271179, train/logprobs = tensor([[-0.6123, -1.3625],
        [-0.7762, -1.0340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 61: train/loss = 0.6556983590126038, train/raw-loss = 0.6556983590126038, train/logprobs = tensor([[-0.5346, -0.5363],
        [-0.5843, -0.4310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 62: train/loss = 0.6170297861099243, train/raw-loss = 0.6170297861099243, train/logprobs = tensor([[-0.6870, -0.9523],
        [-0.8133, -0.7545]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 63: train/loss = 0.648249626159668, train/raw-loss = 0.648249626159668, train/logprobs = tensor([[-0.5208, -0.6026],
        [-0.5809, -0.4773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 64: train/loss = 0.682612419128418, train/raw-loss = 0.6707487106323242, train/logprobs = tensor([[-0.4496, -0.3173],
        [-0.4898, -0.2665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003954567015171051
Epoch 0, Step 65: train/loss = 0.6389353275299072, train/raw-loss = 0.6219831705093384, train/logprobs = tensor([[-0.6740, -0.7788],
        [-0.8034, -0.6055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005650720093399286
Epoch 0, Step 66: train/loss = 0.6356405019760132, train/raw-loss = 0.6172643899917603, train/logprobs = tensor([[-0.5902, -0.8882],
        [-0.6399, -0.6167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006125352811068296
Epoch 0, Step 67: train/loss = 0.6444470286369324, train/raw-loss = 0.6275632381439209, train/logprobs = tensor([[-0.5389, -0.5874],
        [-0.5881, -0.3502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005627949722111225
Epoch 0, Step 68: train/loss = 0.668992817401886, train/raw-loss = 0.6569936275482178, train/logprobs = tensor([[-0.4786, -0.7085],
        [-0.5326, -0.6097]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0039997342973947525
Epoch 0, Step 69: train/loss = 0.643565833568573, train/raw-loss = 0.6258244514465332, train/logprobs = tensor([[-0.7670, -0.8704],
        [-0.8342, -0.6458]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005913803353905678
Epoch 0, Step 70: train/loss = 0.5923715829849243, train/raw-loss = 0.5787810683250427, train/logprobs = tensor([[-0.5135, -1.3601],
        [-0.6236, -0.9566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004530176054686308
Epoch 0, Step 71: train/loss = 0.5978063344955444, train/raw-loss = 0.5792373418807983, train/logprobs = tensor([[-0.5352, -1.1753],
        [-0.6169, -0.7408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006189656909555197
Epoch 0, Step 72: train/loss = 0.6799182891845703, train/raw-loss = 0.6659283638000488, train/logprobs = tensor([[-0.4912, -0.4181],
        [-0.5356, -0.3502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004663318861275911
Epoch 0, Step 73: train/loss = 0.637870192527771, train/raw-loss = 0.6224496960639954, train/logprobs = tensor([[-0.5284, -0.8524],
        [-0.5790, -0.5890]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005140157416462898
Epoch 0, Step 74: train/loss = 0.6332617998123169, train/raw-loss = 0.6190266013145447, train/logprobs = tensor([[-0.5125, -0.7926],
        [-0.5818, -0.5369]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0047450619749724865
Epoch 0, Step 75: train/loss = 0.6412839293479919, train/raw-loss = 0.6307007670402527, train/logprobs = tensor([[-0.6580, -0.6515],
        [-0.7720, -0.4941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0035277297720313072
Epoch 0, Step 76: train/loss = 0.6294335126876831, train/raw-loss = 0.6178130507469177, train/logprobs = tensor([[-0.6057, -0.7193],
        [-0.7144, -0.5004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003873479552567005
Epoch 0, Step 77: train/loss = 0.5805788040161133, train/raw-loss = 0.5642318725585938, train/logprobs = tensor([[-0.5601, -1.4223],
        [-0.7057, -0.9875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005448971409350634
Epoch 0, Step 78: train/loss = 0.5489823818206787, train/raw-loss = 0.5329515933990479, train/logprobs = tensor([[-0.6842, -1.8399],
        [-0.8427, -1.2230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005343608558177948
Epoch 0, Step 79: train/loss = 0.6032150983810425, train/raw-loss = 0.5874720215797424, train/logprobs = tensor([[-0.6019, -0.8944],
        [-0.6937, -0.4731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005247676279395819
Epoch 0, Step 80: train/loss = 0.56256502866745, train/raw-loss = 0.5454443693161011, train/logprobs = tensor([[-0.6486, -1.6084],
        [-0.8570, -1.1605]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005706883501261473
Epoch 0, Step 81: train/loss = 0.6242051720619202, train/raw-loss = 0.6052462458610535, train/logprobs = tensor([[-0.6503, -0.8914],
        [-0.8057, -0.6558]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006319650448858738
Epoch 0, Step 82: train/loss = 0.6636378765106201, train/raw-loss = 0.6433984637260437, train/logprobs = tensor([[-0.6558, -0.5982],
        [-0.7440, -0.4775]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006746473256498575
Epoch 0, Step 83: train/loss = 0.6165703535079956, train/raw-loss = 0.603151798248291, train/logprobs = tensor([[-0.6238, -0.9240],
        [-0.7042, -0.6149]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004472872242331505
Epoch 0, Step 84: train/loss = 0.6428911685943604, train/raw-loss = 0.6256247758865356, train/logprobs = tensor([[-0.7062, -0.6234],
        [-0.7999, -0.4299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005755482241511345
Epoch 0, Step 85: train/loss = 0.6740257740020752, train/raw-loss = 0.6602737307548523, train/logprobs = tensor([[-0.4980, -0.5514],
        [-0.5714, -0.4863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004584014881402254
Epoch 0, Step 86: train/loss = 0.6073713302612305, train/raw-loss = 0.5936807990074158, train/logprobs = tensor([[-0.4924, -1.3964],
        [-0.5434, -1.0173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004563501104712486
Epoch 0, Step 87: train/loss = 0.6233245134353638, train/raw-loss = 0.6111606359481812, train/logprobs = tensor([[-0.3732, -0.8890],
        [-0.4286, -0.5818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00405463669449091
Epoch 0, Step 88: train/loss = 0.5766347050666809, train/raw-loss = 0.5595331192016602, train/logprobs = tensor([[-0.6115, -1.5979],
        [-0.7237, -1.1173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005700539797544479
Epoch 0, Step 89: train/loss = 0.682686448097229, train/raw-loss = 0.6692585349082947, train/logprobs = tensor([[-0.4709, -0.2015],
        [-0.5642, -0.1966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004475939553231001
Epoch 0, Step 90: train/loss = 0.6298327445983887, train/raw-loss = 0.6182618141174316, train/logprobs = tensor([[-0.4530, -0.7732],
        [-0.5285, -0.5285]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0038569692987948656
Epoch 0, Step 91: train/loss = 0.6118597388267517, train/raw-loss = 0.597144603729248, train/logprobs = tensor([[-0.4805, -1.2790],
        [-0.5307, -0.9029]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0049050371162593365
Epoch 0, Step 92: train/loss = 0.6720125079154968, train/raw-loss = 0.6600662469863892, train/logprobs = tensor([[-0.4543, -0.4782],
        [-0.4765, -0.3623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003982077818363905
Epoch 0, Step 93: train/loss = 0.5787129402160645, train/raw-loss = 0.559992790222168, train/logprobs = tensor([[-0.7139, -1.4768],
        [-0.8087, -0.8480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006240044720470905
Epoch 0, Step 94: train/loss = 0.6156284809112549, train/raw-loss = 0.5985286235809326, train/logprobs = tensor([[-0.6298, -1.4355],
        [-0.7099, -1.1007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005699977744370699
Epoch 0, Step 95: train/loss = 0.6373822689056396, train/raw-loss = 0.6203517913818359, train/logprobs = tensor([[-0.5403, -0.7815],
        [-0.6318, -0.5435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005676835775375366
Epoch 0, Step 96: train/loss = 0.6357203722000122, train/raw-loss = 0.5563853979110718, train/logprobs = tensor([[-0.5243, -1.6374],
        [-0.5591, -0.9956]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02644497901201248
Epoch 0, Step 97: train/loss = 0.6322288513183594, train/raw-loss = 0.5494291186332703, train/logprobs = tensor([[-0.6210, -1.7877],
        [-0.7096, -1.2079]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027599910274147987
Epoch 0, Step 98: train/loss = 0.6290929913520813, train/raw-loss = 0.5621520280838013, train/logprobs = tensor([[-0.5903, -1.1911],
        [-0.7258, -0.7477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022313645109534264
Epoch 0, Step 99: train/loss = 0.6137011051177979, train/raw-loss = 0.5394517183303833, train/logprobs = tensor([[-0.5506, -1.4799],
        [-0.6571, -0.8611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02474977821111679
Epoch 0, Step 100: train/loss = 0.6906918287277222, train/raw-loss = 0.619655430316925, train/logprobs = tensor([[-0.6864, -0.5421],
        [-0.7698, -0.3030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023678796365857124
Epoch 0, Step 101: train/loss = 0.669886589050293, train/raw-loss = 0.6173261404037476, train/logprobs = tensor([[-0.5275, -0.8760],
        [-0.5792, -0.5973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017520137131214142
Epoch 0, Step 102: train/loss = 0.6221153736114502, train/raw-loss = 0.5419499278068542, train/logprobs = tensor([[-0.6767, -1.9785],
        [-0.7418, -1.3583]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02672182396054268
Epoch 0, Step 103: train/loss = 0.6352460384368896, train/raw-loss = 0.5776960849761963, train/logprobs = tensor([[-0.5966, -1.0854],
        [-0.6990, -0.6434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019183330237865448
Epoch 0, Step 104: train/loss = 0.6229633688926697, train/raw-loss = 0.543109118938446, train/logprobs = tensor([[-0.5432, -1.9444],
        [-0.5850, -1.1673]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02661808207631111
Epoch 0, Step 105: train/loss = 0.6150550842285156, train/raw-loss = 0.5442661643028259, train/logprobs = tensor([[-0.4903, -1.4582],
        [-0.5773, -0.8556]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023596325889229774
Epoch 0, Step 106: train/loss = 0.5914591550827026, train/raw-loss = 0.519298791885376, train/logprobs = tensor([[-0.6693, -1.6052],
        [-0.7387, -0.8502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02405344508588314
Epoch 0, Step 107: train/loss = 0.64577317237854, train/raw-loss = 0.5735178589820862, train/logprobs = tensor([[-0.4346, -1.0514],
        [-0.4947, -0.5533]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024085111916065216
Epoch 0, Step 108: train/loss = 0.5572173595428467, train/raw-loss = 0.4853796660900116, train/logprobs = tensor([[-0.3873, -2.7727],
        [-0.4598, -1.6840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023945890367031097
Epoch 0, Step 109: train/loss = 0.6612436175346375, train/raw-loss = 0.5816943645477295, train/logprobs = tensor([[-0.5876, -1.0549],
        [-0.6488, -0.5585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02651640772819519
Epoch 0, Step 110: train/loss = 0.6212935447692871, train/raw-loss = 0.5452156662940979, train/logprobs = tensor([[-0.5319, -1.7298],
        [-0.5742, -1.0674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025359295308589935
Epoch 0, Step 111: train/loss = 0.5917901396751404, train/raw-loss = 0.5170806646347046, train/logprobs = tensor([[-0.5555, -1.3456],
        [-0.6489, -0.6254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024903159588575363
Epoch 0, Step 112: train/loss = 0.6741780042648315, train/raw-loss = 0.5950537919998169, train/logprobs = tensor([[-0.4596, -1.1006],
        [-0.4874, -0.6951]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026374736800789833
Epoch 0, Step 113: train/loss = 0.6364507675170898, train/raw-loss = 0.5644925832748413, train/logprobs = tensor([[-0.5717, -1.2526],
        [-0.6550, -0.7470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02398604527115822
Epoch 0, Step 114: train/loss = 0.7164867520332336, train/raw-loss = 0.642444372177124, train/logprobs = tensor([[-0.5930, -0.4851],
        [-0.6454, -0.3228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02468079701066017
Epoch 0, Step 115: train/loss = 0.64830082654953, train/raw-loss = 0.5682624578475952, train/logprobs = tensor([[-0.6027, -0.9906],
        [-0.6814, -0.5059]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02667945623397827
Epoch 0, Step 116: train/loss = 0.6351407170295715, train/raw-loss = 0.5555692315101624, train/logprobs = tensor([[-0.8198, -1.1752],
        [-0.9971, -0.6845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026523835957050323
Epoch 0, Step 117: train/loss = 0.657615065574646, train/raw-loss = 0.5813977122306824, train/logprobs = tensor([[-0.5401, -0.8114],
        [-0.6837, -0.4629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025405757129192352
Epoch 0, Step 118: train/loss = 0.6317914724349976, train/raw-loss = 0.5554430484771729, train/logprobs = tensor([[-0.6329, -1.1324],
        [-0.7559, -0.6052]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025449462234973907
Epoch 0, Step 119: train/loss = 0.5808290839195251, train/raw-loss = 0.510884702205658, train/logprobs = tensor([[-0.7204, -1.7255],
        [-0.9448, -0.9530]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02331479825079441
Epoch 0, Step 120: train/loss = 0.6933489441871643, train/raw-loss = 0.629497766494751, train/logprobs = tensor([[-0.6115, -0.6485],
        [-0.6814, -0.4353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021283723413944244
Epoch 0, Step 121: train/loss = 0.674055814743042, train/raw-loss = 0.5991794466972351, train/logprobs = tensor([[-0.5404, -1.1801],
        [-0.6875, -0.8915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024958796799182892
Epoch 0, Step 122: train/loss = 0.6570439338684082, train/raw-loss = 0.5825993418693542, train/logprobs = tensor([[-0.5561, -1.3385],
        [-0.5873, -0.8576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024814879521727562
Epoch 0, Step 123: train/loss = 0.6515635848045349, train/raw-loss = 0.5952988266944885, train/logprobs = tensor([[-0.6229, -0.8740],
        [-0.6900, -0.5085]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018754929304122925
Epoch 0, Step 124: train/loss = 0.735072672367096, train/raw-loss = 0.6590274572372437, train/logprobs = tensor([[-0.4734, -0.5291],
        [-0.4845, -0.3955]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025348393246531487
Epoch 0, Step 125: train/loss = 0.5883491039276123, train/raw-loss = 0.5152082443237305, train/logprobs = tensor([[-0.4248, -1.3114],
        [-0.7922, -0.7899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024380283430218697
Epoch 0, Step 126: train/loss = 0.6635606288909912, train/raw-loss = 0.5939947366714478, train/logprobs = tensor([[-0.5737, -0.6594],
        [-0.6648, -0.3046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023188643157482147
Epoch 0, Step 127: train/loss = 0.7274133563041687, train/raw-loss = 0.666130781173706, train/logprobs = tensor([[-0.6837, -0.5186],
        [-0.7368, -0.4601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020427506417036057
Epoch 0, Step 128: train/loss = 0.8497211337089539, train/raw-loss = 0.5773391723632812, train/logprobs = tensor([[-0.3881, -0.9742],
        [-0.4458, -0.4599]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09079397469758987
Epoch 0, Step 129: train/loss = 0.7903662919998169, train/raw-loss = 0.5180178880691528, train/logprobs = tensor([[-0.4910, -2.0322],
        [-0.5779, -1.2252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09078280627727509
Epoch 0, Step 130: train/loss = 0.7695968151092529, train/raw-loss = 0.6481579542160034, train/logprobs = tensor([[-0.4817, -0.4469],
        [-0.5636, -0.3394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0404796302318573
Epoch 0, Step 131: train/loss = 0.824388861656189, train/raw-loss = 0.5916928052902222, train/logprobs = tensor([[-0.5519, -0.9060],
        [-0.6867, -0.5755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07756532728672028
Epoch 0, Step 132: train/loss = 0.7506250143051147, train/raw-loss = 0.500471293926239, train/logprobs = tensor([[-0.4956, -1.7430],
        [-0.6020, -0.8372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08338460326194763
Epoch 0, Step 133: train/loss = 0.8045860528945923, train/raw-loss = 0.5271897912025452, train/logprobs = tensor([[-0.5774, -1.8447],
        [-0.7216, -1.2071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09246541559696198
Epoch 0, Step 134: train/loss = 0.7839722037315369, train/raw-loss = 0.5086719393730164, train/logprobs = tensor([[-0.6751, -1.6121],
        [-0.8873, -0.9325]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09176676720380783
Epoch 0, Step 135: train/loss = 0.8369196057319641, train/raw-loss = 0.599214494228363, train/logprobs = tensor([[-0.3569, -1.1721],
        [-0.4584, -0.8433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0792350247502327
Epoch 0, Step 136: train/loss = 0.8066393733024597, train/raw-loss = 0.5524107217788696, train/logprobs = tensor([[-0.5687, -1.3501],
        [-0.7258, -0.8689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08474288135766983
Epoch 0, Step 137: train/loss = 0.8029701709747314, train/raw-loss = 0.5710322260856628, train/logprobs = tensor([[-0.5802, -1.1169],
        [-0.6448, -0.6146]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0773126408457756
Epoch 0, Step 138: train/loss = 0.814500629901886, train/raw-loss = 0.602076530456543, train/logprobs = tensor([[-0.5416, -0.9282],
        [-0.6334, -0.5537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07080800831317902
Epoch 0, Step 139: train/loss = 0.8089648485183716, train/raw-loss = 0.6007691621780396, train/logprobs = tensor([[-0.6138, -0.9852],
        [-0.6902, -0.5736]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06939855962991714
Epoch 0, Step 140: train/loss = 0.7946429252624512, train/raw-loss = 0.565796434879303, train/logprobs = tensor([[-0.5447, -1.1288],
        [-0.6765, -0.6713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07628215849399567
Epoch 0, Step 141: train/loss = 0.8202943801879883, train/raw-loss = 0.6025391817092896, train/logprobs = tensor([[-0.5329, -0.6586],
        [-0.6105, -0.2968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0725850760936737
Epoch 0, Step 142: train/loss = 0.8450284004211426, train/raw-loss = 0.6404475569725037, train/logprobs = tensor([[-0.4689, -0.7534],
        [-0.5027, -0.5648]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06819360703229904
Epoch 0, Step 143: train/loss = 0.8374851942062378, train/raw-loss = 0.5752178430557251, train/logprobs = tensor([[-0.6158, -0.7635],
        [-0.8530, -0.4376]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08742246776819229
Epoch 0, Step 144: train/loss = 0.7013857364654541, train/raw-loss = 0.4616333246231079, train/logprobs = tensor([[-0.4719, -2.3936],
        [-0.5604, -1.0224]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07991749048233032
Epoch 0, Step 145: train/loss = 0.8075569272041321, train/raw-loss = 0.6184402704238892, train/logprobs = tensor([[-0.5053, -0.3335],
        [-0.6774, -0.1770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0630388855934143
Epoch 0, Step 146: train/loss = 0.8104375004768372, train/raw-loss = 0.576960563659668, train/logprobs = tensor([[-0.4935, -0.9679],
        [-0.6310, -0.5096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07782566547393799
Epoch 0, Step 147: train/loss = 0.8974668979644775, train/raw-loss = 0.6874861121177673, train/logprobs = tensor([[-0.4461, -0.4178],
        [-0.4552, -0.4041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06999360024929047
Epoch 0, Step 148: train/loss = 0.7519794702529907, train/raw-loss = 0.5069977045059204, train/logprobs = tensor([[-0.4911, -1.5767],
        [-0.6242, -0.7173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08166057616472244
Epoch 0, Step 149: train/loss = 0.8652427196502686, train/raw-loss = 0.6071212291717529, train/logprobs = tensor([[-0.4314, -1.8107],
        [-0.4646, -1.4563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08604049682617188
Epoch 0, Step 150: train/loss = 0.7515469193458557, train/raw-loss = 0.4863400459289551, train/logprobs = tensor([[-0.7139, -1.1276],
        [-1.0808, -0.4868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08840230107307434
Epoch 0, Step 151: train/loss = 0.8408254981040955, train/raw-loss = 0.5681788921356201, train/logprobs = tensor([[-0.6585, -0.8647],
        [-0.8725, -0.5344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09088218957185745
Epoch 0, Step 152: train/loss = 0.7877789735794067, train/raw-loss = 0.5593581199645996, train/logprobs = tensor([[-0.7408, -0.7698],
        [-0.9587, -0.3625]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07614030689001083
Epoch 0, Step 153: train/loss = 0.9086140990257263, train/raw-loss = 0.5819143056869507, train/logprobs = tensor([[-0.4544, -1.8400],
        [-0.4915, -1.3513]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10889992862939835
Epoch 0, Step 154: train/loss = 0.8623007535934448, train/raw-loss = 0.6406675577163696, train/logprobs = tensor([[-0.4618, -0.8653],
        [-0.5177, -0.6959]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07387776672840118
Epoch 0, Step 155: train/loss = 0.9053534269332886, train/raw-loss = 0.5936936140060425, train/logprobs = tensor([[-0.5630, -0.5590],
        [-0.7446, -0.3018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10388658940792084
Epoch 0, Step 156: train/loss = 0.8829070329666138, train/raw-loss = 0.5920518636703491, train/logprobs = tensor([[-0.6371, -0.8245],
        [-0.7705, -0.4754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09695171564817429
Epoch 0, Step 157: train/loss = 0.8337979316711426, train/raw-loss = 0.5652658939361572, train/logprobs = tensor([[-0.6424, -1.0131],
        [-0.8250, -0.5817]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08951065689325333
Epoch 0, Step 158: train/loss = 0.7676831483840942, train/raw-loss = 0.5432215929031372, train/logprobs = tensor([[-0.4656, -1.9166],
        [-0.5246, -1.1761]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07482051849365234
Epoch 0, Step 159: train/loss = 0.7430485486984253, train/raw-loss = 0.5049769282341003, train/logprobs = tensor([[-0.5584, -1.6454],
        [-0.7037, -0.7859]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07935720682144165
Epoch 0, Step 160: train/loss = 0.6694349646568298, train/raw-loss = 0.5009644031524658, train/logprobs = tensor([[-0.3916, -1.5749],
        [-0.5478, -0.7461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.056156858801841736
Epoch 0, Step 161: train/loss = 0.7069604396820068, train/raw-loss = 0.5141940116882324, train/logprobs = tensor([[-0.5039, -1.6820],
        [-0.6033, -0.8020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0642554834485054
Epoch 0, Step 162: train/loss = 0.7584572434425354, train/raw-loss = 0.6033945679664612, train/logprobs = tensor([[-0.6160, -0.5050],
        [-0.8106, -0.3190]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051687564700841904
Epoch 0, Step 163: train/loss = 0.6964556574821472, train/raw-loss = 0.557327926158905, train/logprobs = tensor([[-0.4795, -1.0717],
        [-0.6635, -0.6519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04637589305639267
Epoch 0, Step 164: train/loss = 0.7066855430603027, train/raw-loss = 0.6076852083206177, train/logprobs = tensor([[-0.3773, -0.9673],
        [-0.4247, -0.5663]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03300010785460472
Epoch 0, Step 165: train/loss = 0.6585232019424438, train/raw-loss = 0.5000640153884888, train/logprobs = tensor([[-0.5675, -1.7157],
        [-0.8356, -0.9625]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05281972885131836
Epoch 0, Step 166: train/loss = 0.6669131517410278, train/raw-loss = 0.5316378474235535, train/logprobs = tensor([[-0.3706, -1.4815],
        [-0.4836, -0.7297]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045091740787029266
Epoch 0, Step 167: train/loss = 0.637270450592041, train/raw-loss = 0.45499518513679504, train/logprobs = tensor([[-0.5098, -2.0138],
        [-0.6913, -0.7167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06075843423604965
Epoch 0, Step 168: train/loss = 0.7568449378013611, train/raw-loss = 0.5737684965133667, train/logprobs = tensor([[-0.4793, -1.1381],
        [-0.5460, -0.6254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061025507748126984
Epoch 0, Step 169: train/loss = 0.7894155383110046, train/raw-loss = 0.6044349074363708, train/logprobs = tensor([[-0.5538, -0.5860],
        [-0.6308, -0.2718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06166020780801773
Epoch 0, Step 170: train/loss = 0.7349618673324585, train/raw-loss = 0.5803864002227783, train/logprobs = tensor([[-0.4614, -1.9813],
        [-0.5600, -1.5553]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05152515321969986
Epoch 0, Step 171: train/loss = 0.8294219374656677, train/raw-loss = 0.6596521735191345, train/logprobs = tensor([[-0.6187, -1.1780],
        [-0.6926, -1.1139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05658990889787674
Epoch 0, Step 172: train/loss = 0.7073171138763428, train/raw-loss = 0.5367854833602905, train/logprobs = tensor([[-0.7503, -1.2801],
        [-0.9646, -0.7313]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.056843895465135574
Epoch 0, Step 173: train/loss = 0.6165440678596497, train/raw-loss = 0.416327565908432, train/logprobs = tensor([[-0.5040, -2.7129],
        [-0.7578, -1.5307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06673883646726608
Epoch 0, Step 174: train/loss = 0.6929110288619995, train/raw-loss = 0.5408293008804321, train/logprobs = tensor([[-0.5718, -1.2418],
        [-0.7532, -0.5848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05069391429424286
Epoch 0, Step 175: train/loss = 0.65263831615448, train/raw-loss = 0.489607572555542, train/logprobs = tensor([[-0.7329, -1.4952],
        [-1.0221, -0.6853]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.054343584924936295
Epoch 0, Step 176: train/loss = 0.6541261672973633, train/raw-loss = 0.5204976797103882, train/logprobs = tensor([[-0.4473, -1.9230],
        [-0.5895, -0.6533]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04454285651445389
Epoch 0, Step 177: train/loss = 0.7423067092895508, train/raw-loss = 0.6013766527175903, train/logprobs = tensor([[-0.5017, -0.5680],
        [-0.6649, -0.3203]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04697670042514801
Epoch 0, Step 178: train/loss = 0.7441388368606567, train/raw-loss = 0.5786381959915161, train/logprobs = tensor([[-0.4378, -0.6582],
        [-0.5533, -0.2446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05516687408089638
Epoch 0, Step 179: train/loss = 0.7754855155944824, train/raw-loss = 0.5972515344619751, train/logprobs = tensor([[-0.4342, -0.9390],
        [-0.5355, -0.6202]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059411317110061646
Epoch 0, Step 180: train/loss = 0.7684627771377563, train/raw-loss = 0.6215663552284241, train/logprobs = tensor([[-0.5404, -0.7610],
        [-0.7174, -0.6165]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048965491354465485
Epoch 0, Step 181: train/loss = 0.6517941951751709, train/raw-loss = 0.47251784801483154, train/logprobs = tensor([[-0.4986, -1.7556],
        [-0.6590, -0.7542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05975877493619919
Epoch 0, Step 182: train/loss = 0.6005064249038696, train/raw-loss = 0.44947120547294617, train/logprobs = tensor([[-0.6079, -2.0602],
        [-0.8177, -0.9579]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05034506320953369
Epoch 0, Step 183: train/loss = 0.8006539344787598, train/raw-loss = 0.6319694519042969, train/logprobs = tensor([[-0.6590, -0.5681],
        [-0.7953, -0.4421]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05622817948460579
Epoch 0, Step 184: train/loss = 0.6861132383346558, train/raw-loss = 0.5111293792724609, train/logprobs = tensor([[-0.5028, -1.5473],
        [-0.6494, -0.7982]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05832795053720474
Epoch 0, Step 185: train/loss = 0.6334471702575684, train/raw-loss = 0.48820143938064575, train/logprobs = tensor([[-0.5871, -1.8375],
        [-0.8329, -0.7915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04841522499918938
Epoch 0, Step 186: train/loss = 0.6116474866867065, train/raw-loss = 0.4499862790107727, train/logprobs = tensor([[-0.6086, -2.6578],
        [-0.8015, -0.9865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053887080401182175
Epoch 0, Step 187: train/loss = 0.7122820019721985, train/raw-loss = 0.5615839958190918, train/logprobs = tensor([[-0.4645, -1.9005],
        [-0.5709, -0.6935]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05023267865180969
Epoch 0, Step 188: train/loss = 0.6735752820968628, train/raw-loss = 0.5288506150245667, train/logprobs = tensor([[-0.4789, -1.4141],
        [-0.5836, -0.6281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04824154078960419
Epoch 0, Step 189: train/loss = 0.7316511869430542, train/raw-loss = 0.5916228294372559, train/logprobs = tensor([[-0.5510, -0.9863],
        [-0.6389, -0.6359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04667611047625542
Epoch 0, Step 190: train/loss = 0.7145711779594421, train/raw-loss = 0.5440192222595215, train/logprobs = tensor([[-0.5687, -1.0635],
        [-0.8282, -0.6249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05685066431760788
Epoch 0, Step 191: train/loss = 0.7092213034629822, train/raw-loss = 0.5509120225906372, train/logprobs = tensor([[-0.4605, -1.3483],
        [-0.5574, -0.7965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05276978015899658
Epoch 0, Step 192: train/loss = 0.6435309648513794, train/raw-loss = 0.4908641278743744, train/logprobs = tensor([[-0.5202, -1.4605],
        [-0.6739, -0.4426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05088897421956062
Epoch 0, Step 193: train/loss = 0.6800811290740967, train/raw-loss = 0.5414148569107056, train/logprobs = tensor([[-0.6595, -0.6898],
        [-1.0127, -0.3475]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04622206091880798
Epoch 0, Step 194: train/loss = 0.5342504978179932, train/raw-loss = 0.35785505175590515, train/logprobs = tensor([[-0.6973, -2.9865],
        [-0.9846, -1.1824]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05879849195480347
Epoch 0, Step 195: train/loss = 0.6181637048721313, train/raw-loss = 0.4676012098789215, train/logprobs = tensor([[-0.5378, -2.0322],
        [-0.7516, -0.5331]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05018748342990875
Epoch 0, Step 196: train/loss = 0.6505624055862427, train/raw-loss = 0.5044958591461182, train/logprobs = tensor([[-0.4003, -1.3934],
        [-0.5312, -0.5866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04868884012103081
Epoch 0, Step 197: train/loss = 0.6892381906509399, train/raw-loss = 0.5355555415153503, train/logprobs = tensor([[-0.5181, -0.9284],
        [-0.6937, -0.3160]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051227547228336334
Epoch 0, Step 198: train/loss = 0.6440332531929016, train/raw-loss = 0.49003124237060547, train/logprobs = tensor([[-0.6521, -1.4812],
        [-0.9156, -0.5629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051333993673324585
Epoch 0, Step 199: train/loss = 0.6797395944595337, train/raw-loss = 0.5523513555526733, train/logprobs = tensor([[-0.4977, -1.0635],
        [-0.7087, -0.5132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04246274009346962
Epoch 0, Step 200: train/loss = 0.5726566314697266, train/raw-loss = 0.41269397735595703, train/logprobs = tensor([[-0.5458, -2.2747],
        [-0.7177, -0.9348]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05332087725400925
Epoch 0, Step 201: train/loss = 0.6380785703659058, train/raw-loss = 0.49013012647628784, train/logprobs = tensor([[-0.6303, -1.1290],
        [-0.9945, -0.4036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04931613430380821
Epoch 0, Step 202: train/loss = 0.6745871305465698, train/raw-loss = 0.5184522271156311, train/logprobs = tensor([[-0.5307, -1.5987],
        [-0.6801, -0.7072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05204496532678604
Epoch 0, Step 203: train/loss = 0.6249701976776123, train/raw-loss = 0.4726907014846802, train/logprobs = tensor([[-0.6214, -1.8558],
        [-0.8849, -1.0283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05075983330607414
Epoch 0, Step 204: train/loss = 0.5956900119781494, train/raw-loss = 0.4373641908168793, train/logprobs = tensor([[-0.6087, -2.4493],
        [-0.8151, -0.8246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05277527868747711
Epoch 0, Step 205: train/loss = 0.6329941749572754, train/raw-loss = 0.4765182137489319, train/logprobs = tensor([[-0.6294, -2.4417],
        [-0.7698, -0.8512]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0521586649119854
Epoch 0, Step 206: train/loss = 0.723616361618042, train/raw-loss = 0.6425145268440247, train/logprobs = tensor([[-0.3467, -0.2470],
        [-0.4222, -0.1140]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027033954858779907
Epoch 0, Step 207: train/loss = 0.6697695255279541, train/raw-loss = 0.5312147736549377, train/logprobs = tensor([[-0.5853, -2.4237],
        [-0.7321, -1.7565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04618491977453232
Epoch 0, Step 208: train/loss = 0.7011400461196899, train/raw-loss = 0.5578652620315552, train/logprobs = tensor([[-0.4553, -1.6078],
        [-0.5289, -1.0364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04775826260447502
Epoch 0, Step 209: train/loss = 0.6455724239349365, train/raw-loss = 0.48693951964378357, train/logprobs = tensor([[-0.4500, -1.3779],
        [-0.7319, -0.5195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05287763476371765
Epoch 0, Step 210: train/loss = 0.7421406507492065, train/raw-loss = 0.6094782948493958, train/logprobs = tensor([[-0.4150, -1.0149],
        [-0.4936, -0.6927]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04422077164053917
Epoch 0, Step 211: train/loss = 0.6964255571365356, train/raw-loss = 0.5394557118415833, train/logprobs = tensor([[-0.4465, -1.2000],
        [-0.6269, -0.4557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052323274314403534
Epoch 0, Step 212: train/loss = 0.6616746187210083, train/raw-loss = 0.49162834882736206, train/logprobs = tensor([[-0.4281, -1.5750],
        [-0.5606, -0.6023]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05668210983276367
Epoch 0, Step 213: train/loss = 0.6282196640968323, train/raw-loss = 0.475359708070755, train/logprobs = tensor([[-0.4976, -1.8021],
        [-0.6810, -0.7814]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05095331743359566
Epoch 0, Step 214: train/loss = 0.6359968185424805, train/raw-loss = 0.4972434341907501, train/logprobs = tensor([[-0.5428, -1.6124],
        [-0.7572, -0.6113]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04625113308429718
Epoch 0, Step 215: train/loss = 0.5835950374603271, train/raw-loss = 0.42805224657058716, train/logprobs = tensor([[-0.6256, -1.9176],
        [-0.8949, -0.7284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05184761434793472
Epoch 0, Step 216: train/loss = 0.7123251557350159, train/raw-loss = 0.5654712319374084, train/logprobs = tensor([[-0.5730, -0.7758],
        [-0.8238, -0.4394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04895131662487984
Epoch 0, Step 217: train/loss = 0.5990685224533081, train/raw-loss = 0.4473724365234375, train/logprobs = tensor([[-0.7127, -1.5143],
        [-1.1462, -0.6681]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05056537687778473
Epoch 0, Step 218: train/loss = 0.6912486553192139, train/raw-loss = 0.5577800273895264, train/logprobs = tensor([[-0.5962, -0.9159],
        [-0.7929, -0.4698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04448956251144409
Epoch 0, Step 219: train/loss = 0.6669648885726929, train/raw-loss = 0.5148097276687622, train/logprobs = tensor([[-0.5462, -1.5228],
        [-0.7158, -0.6347]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05071840435266495
Epoch 0, Step 220: train/loss = 0.7041388750076294, train/raw-loss = 0.5641189813613892, train/logprobs = tensor([[-0.4976, -0.7971],
        [-0.6502, -0.3603]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04667332023382187
Epoch 0, Step 221: train/loss = 0.5606138706207275, train/raw-loss = 0.3993396759033203, train/logprobs = tensor([[-0.4967, -3.0281],
        [-0.5769, -0.9322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05375806614756584
Epoch 0, Step 222: train/loss = 0.475357323884964, train/raw-loss = 0.31074053049087524, train/logprobs = tensor([[-0.6596, -3.2059],
        [-1.0433, -0.9520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.054872263222932816
Epoch 0, Step 223: train/loss = 0.7584370374679565, train/raw-loss = 0.6168975830078125, train/logprobs = tensor([[-0.4772, -0.5355],
        [-0.6111, -0.3369]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04717981815338135
Epoch 0, Step 224: train/loss = 0.7110010981559753, train/raw-loss = 0.49893441796302795, train/logprobs = tensor([[-0.4516, -1.5631],
        [-0.5787, -0.6577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0706888884305954
Epoch 0, Step 225: train/loss = 0.8206852078437805, train/raw-loss = 0.6177260875701904, train/logprobs = tensor([[-0.7441, -0.3379],
        [-0.9452, -0.2135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06765303015708923
Epoch 0, Step 226: train/loss = 0.7443703413009644, train/raw-loss = 0.555172860622406, train/logprobs = tensor([[-0.5906, -1.4281],
        [-0.7186, -0.9184]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06306581944227219
Epoch 0, Step 227: train/loss = 0.7232667207717896, train/raw-loss = 0.5577082633972168, train/logprobs = tensor([[-0.5940, -1.0012],
        [-0.7946, -0.4546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05518614500761032
Epoch 0, Step 228: train/loss = 0.7568622827529907, train/raw-loss = 0.6116339564323425, train/logprobs = tensor([[-0.4069, -0.8989],
        [-0.4922, -0.6243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048409443348646164
Epoch 0, Step 229: train/loss = 0.5446318984031677, train/raw-loss = 0.30342191457748413, train/logprobs = tensor([[-0.6028, -3.1279],
        [-1.0177, -1.0922]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08040333539247513
Epoch 0, Step 230: train/loss = 0.7140693664550781, train/raw-loss = 0.5274190902709961, train/logprobs = tensor([[-0.6534, -0.8069],
        [-0.9832, -0.2190]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06221679225564003
Epoch 0, Step 231: train/loss = 0.7467231154441833, train/raw-loss = 0.5544888973236084, train/logprobs = tensor([[-0.5584, -1.3202],
        [-0.6776, -0.7550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06407807767391205
Epoch 0, Step 232: train/loss = 0.7872454524040222, train/raw-loss = 0.5916669964790344, train/logprobs = tensor([[-0.5525, -0.4810],
        [-0.8009, -0.2745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.065192811191082
Epoch 0, Step 233: train/loss = 0.7230024933815002, train/raw-loss = 0.5518395304679871, train/logprobs = tensor([[-0.4532, -1.5645],
        [-0.5801, -0.8773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.057054322212934494
Epoch 0, Step 234: train/loss = 0.7247445583343506, train/raw-loss = 0.5318809747695923, train/logprobs = tensor([[-0.6532, -1.7918],
        [-0.8376, -1.2022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06428786367177963
Epoch 0, Step 235: train/loss = 0.8021067380905151, train/raw-loss = 0.6084326505661011, train/logprobs = tensor([[-0.4760, -0.8112],
        [-0.5214, -0.4846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06455805897712708
Epoch 0, Step 236: train/loss = 0.7305287718772888, train/raw-loss = 0.5000268220901489, train/logprobs = tensor([[-0.5049, -1.4542],
        [-0.7343, -0.3846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07683399319648743
Epoch 0, Step 237: train/loss = 0.6184519529342651, train/raw-loss = 0.3973936140537262, train/logprobs = tensor([[-0.5814, -2.0238],
        [-0.9040, -0.6344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07368611544370651
Epoch 0, Step 238: train/loss = 0.7339155673980713, train/raw-loss = 0.5382734537124634, train/logprobs = tensor([[-0.5706, -0.9788],
        [-0.7681, -0.4239]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06521400809288025
Epoch 0, Step 239: train/loss = 0.7752339243888855, train/raw-loss = 0.5959968566894531, train/logprobs = tensor([[-0.4426, -0.4147],
        [-0.6312, -0.1848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05974569171667099
Epoch 0, Step 240: train/loss = 0.6826658844947815, train/raw-loss = 0.4914385676383972, train/logprobs = tensor([[-0.5713, -2.0021],
        [-0.7231, -0.9929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06374242901802063
Epoch 0, Step 241: train/loss = 0.6584002375602722, train/raw-loss = 0.44900810718536377, train/logprobs = tensor([[-0.5540, -2.3244],
        [-0.7907, -1.2526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06979736685752869
Epoch 0, Step 242: train/loss = 0.7765563726425171, train/raw-loss = 0.527752697467804, train/logprobs = tensor([[-0.5850, -1.3318],
        [-0.8424, -0.8152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08293454349040985
Epoch 0, Step 243: train/loss = 0.6883444786071777, train/raw-loss = 0.5154014229774475, train/logprobs = tensor([[-0.4644, -2.5286],
        [-0.5492, -1.5655]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.057647690176963806
Epoch 0, Step 244: train/loss = 0.6550536155700684, train/raw-loss = 0.4633405804634094, train/logprobs = tensor([[-0.5440, -1.8428],
        [-0.7506, -0.6871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06390433013439178
Epoch 0, Step 245: train/loss = 0.6806557774543762, train/raw-loss = 0.4868655204772949, train/logprobs = tensor([[-0.4113, -1.6531],
        [-0.5583, -0.4998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0645967349410057
Epoch 0, Step 246: train/loss = 0.7742236852645874, train/raw-loss = 0.5678585171699524, train/logprobs = tensor([[-0.4961, -0.8159],
        [-0.6987, -0.4677]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06878838688135147
Epoch 0, Step 247: train/loss = 0.6586140394210815, train/raw-loss = 0.43362441658973694, train/logprobs = tensor([[-0.4072, -2.0790],
        [-0.6385, -0.7684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07499654591083527
Epoch 0, Step 248: train/loss = 0.7610337138175964, train/raw-loss = 0.5827683210372925, train/logprobs = tensor([[-0.4556, -0.3585],
        [-0.7563, -0.1737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059421803802251816
Epoch 0, Step 249: train/loss = 0.7285876274108887, train/raw-loss = 0.48934218287467957, train/logprobs = tensor([[-0.5391, -1.4387],
        [-0.8146, -0.6030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0797484964132309
Epoch 0, Step 250: train/loss = 0.7587844133377075, train/raw-loss = 0.5438956022262573, train/logprobs = tensor([[-0.4944, -0.8194],
        [-0.6896, -0.3222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0716295912861824
Epoch 0, Step 251: train/loss = 0.7362810969352722, train/raw-loss = 0.5428009629249573, train/logprobs = tensor([[-0.4442, -1.1766],
        [-0.5476, -0.4966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06449336558580399
Epoch 0, Step 252: train/loss = 0.7940919399261475, train/raw-loss = 0.5107115507125854, train/logprobs = tensor([[-0.6780, -0.6652],
        [-1.0985, -0.1964]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09446011483669281
Epoch 0, Step 253: train/loss = 0.7212924361228943, train/raw-loss = 0.508652925491333, train/logprobs = tensor([[-0.5282, -1.2717],
        [-0.6918, -0.5785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07087981700897217
Epoch 0, Step 254: train/loss = 0.605766773223877, train/raw-loss = 0.36693045496940613, train/logprobs = tensor([[-0.5483, -2.5097],
        [-0.7702, -0.7523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0796121135354042
Epoch 0, Step 255: train/loss = 0.8038655519485474, train/raw-loss = 0.5977890491485596, train/logprobs = tensor([[-0.3791, -0.7574],
        [-0.4346, -0.3837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0686921626329422
Epoch 0, Step 256: train/loss = 0.8060746788978577, train/raw-loss = 0.5317557454109192, train/logprobs = tensor([[-0.5516, -1.0784],
        [-0.8984, -0.6546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09143966436386108
Epoch 0, Step 257: train/loss = 0.7979879379272461, train/raw-loss = 0.5434784293174744, train/logprobs = tensor([[-0.4595, -1.2603],
        [-0.6387, -0.5063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08483651280403137
Epoch 0, Step 258: train/loss = 0.7098788022994995, train/raw-loss = 0.44731295108795166, train/logprobs = tensor([[-0.4988, -2.0347],
        [-0.7061, -0.7848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08752193301916122
Epoch 0, Step 259: train/loss = 0.7330557107925415, train/raw-loss = 0.43486520648002625, train/logprobs = tensor([[-0.5913, -1.4005],
        [-1.0126, -0.5334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09939682483673096
Epoch 0, Step 260: train/loss = 0.755765438079834, train/raw-loss = 0.41397950053215027, train/logprobs = tensor([[-0.4969, -2.7128],
        [-0.7567, -1.4131]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1139286458492279
Epoch 0, Step 261: train/loss = 0.7772880792617798, train/raw-loss = 0.4778613746166229, train/logprobs = tensor([[-0.5972, -1.2859],
        [-0.9145, -0.4316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09980890154838562
Epoch 0, Step 262: train/loss = 0.7847286462783813, train/raw-loss = 0.5049328804016113, train/logprobs = tensor([[-0.4732, -1.2038],
        [-0.7320, -0.5116]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09326525777578354
Epoch 0, Step 263: train/loss = 0.7233994603157043, train/raw-loss = 0.39414897561073303, train/logprobs = tensor([[-0.7048, -2.8774],
        [-0.9921, -1.1772]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1097501590847969
Epoch 0, Step 264: train/loss = 0.7637108564376831, train/raw-loss = 0.4963558614253998, train/logprobs = tensor([[-0.5509, -1.5536],
        [-0.8415, -0.6767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08911832422018051
Epoch 0, Step 265: train/loss = 0.7551550269126892, train/raw-loss = 0.4686354696750641, train/logprobs = tensor([[-0.6088, -1.1430],
        [-1.0527, -0.4329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09550652652978897
Epoch 0, Step 266: train/loss = 0.7773466110229492, train/raw-loss = 0.5135221481323242, train/logprobs = tensor([[-0.6088, -0.7691],
        [-0.9801, -0.2867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0879414826631546
Epoch 0, Step 267: train/loss = 0.7865568399429321, train/raw-loss = 0.5048931837081909, train/logprobs = tensor([[-0.5024, -1.1272],
        [-0.7463, -0.3862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09388788044452667
Epoch 0, Step 268: train/loss = 0.7669761180877686, train/raw-loss = 0.48784303665161133, train/logprobs = tensor([[-0.4463, -2.0721],
        [-0.6504, -0.7229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09304435551166534
Epoch 0, Step 269: train/loss = 0.7409994602203369, train/raw-loss = 0.48949456214904785, train/logprobs = tensor([[-0.4432, -1.6726],
        [-0.5702, -0.3724]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08383496850728989
Epoch 0, Step 270: train/loss = 0.7411741018295288, train/raw-loss = 0.44520729780197144, train/logprobs = tensor([[-0.6561, -1.4626],
        [-1.1284, -0.5898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0986555889248848
Epoch 0, Step 271: train/loss = 0.7948641777038574, train/raw-loss = 0.5720031261444092, train/logprobs = tensor([[-0.4297, -0.8773],
        [-0.5037, -0.2740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07428702712059021
Epoch 0, Step 272: train/loss = 0.6899420022964478, train/raw-loss = 0.38261646032333374, train/logprobs = tensor([[-0.4802, -1.9900],
        [-0.7535, -0.5678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10244183242321014
Epoch 0, Step 273: train/loss = 0.7890921831130981, train/raw-loss = 0.4887361228466034, train/logprobs = tensor([[-0.4725, -1.7072],
        [-0.5768, -0.5259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10011869668960571
Epoch 0, Step 274: train/loss = 0.7058371901512146, train/raw-loss = 0.44886964559555054, train/logprobs = tensor([[-0.5668, -1.7832],
        [-0.7713, -0.6093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08565583825111389
Epoch 0, Step 275: train/loss = 0.7147095203399658, train/raw-loss = 0.44576188921928406, train/logprobs = tensor([[-0.5265, -3.1247],
        [-0.8902, -1.1280]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08964920043945312
Epoch 0, Step 276: train/loss = 0.7628694772720337, train/raw-loss = 0.4982426166534424, train/logprobs = tensor([[-0.6207, -1.0781],
        [-1.0285, -0.3230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08820894360542297
Epoch 0, Step 277: train/loss = 0.7301293611526489, train/raw-loss = 0.44822031259536743, train/logprobs = tensor([[-0.4706, -2.2944],
        [-0.6198, -1.1915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09396970272064209
Epoch 0, Step 278: train/loss = 0.7775790691375732, train/raw-loss = 0.5203284025192261, train/logprobs = tensor([[-0.4011, -1.3021],
        [-0.5163, -0.4733]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08575023710727692
Epoch 0, Step 279: train/loss = 0.7830957174301147, train/raw-loss = 0.5033588409423828, train/logprobs = tensor([[-0.6984, -1.1449],
        [-1.0010, -0.4941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09324563294649124
Epoch 0, Step 280: train/loss = 0.7029826641082764, train/raw-loss = 0.43380239605903625, train/logprobs = tensor([[-0.5209, -1.6830],
        [-0.7586, -0.4797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08972674608230591
Epoch 0, Step 281: train/loss = 0.784889280796051, train/raw-loss = 0.49469006061553955, train/logprobs = tensor([[-0.5162, -1.1113],
        [-0.7228, -0.3633]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09673307090997696
Epoch 0, Step 282: train/loss = 0.8162379860877991, train/raw-loss = 0.5519525408744812, train/logprobs = tensor([[-0.4986, -1.3125],
        [-0.6123, -0.7593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08809514343738556
Epoch 0, Step 283: train/loss = 0.7795611023902893, train/raw-loss = 0.5044386386871338, train/logprobs = tensor([[-0.4552, -1.8839],
        [-0.6422, -0.5583]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09170746803283691
Epoch 0, Step 284: train/loss = 0.7960883975028992, train/raw-loss = 0.5947644114494324, train/logprobs = tensor([[-0.5249, -0.2716],
        [-0.8055, -0.1119]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06710799038410187
Epoch 0, Step 285: train/loss = 0.8457949161529541, train/raw-loss = 0.6353747248649597, train/logprobs = tensor([[-0.4582, -0.2279],
        [-0.6052, -0.1329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07014007866382599
Epoch 0, Step 286: train/loss = 0.8754725456237793, train/raw-loss = 0.6248123645782471, train/logprobs = tensor([[-0.4915, -0.4655],
        [-0.6510, -0.3197]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08355340361595154
Epoch 0, Step 287: train/loss = 0.7479180097579956, train/raw-loss = 0.447345495223999, train/logprobs = tensor([[-0.6320, -1.9346],
        [-0.9638, -0.8379]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10019082576036453
Epoch 0, Step 288: train/loss = 0.7449174523353577, train/raw-loss = 0.5167227983474731, train/logprobs = tensor([[-0.4997, -0.9339],
        [-0.7858, -0.2800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07606489956378937
Epoch 0, Step 289: train/loss = 0.716641366481781, train/raw-loss = 0.4152597188949585, train/logprobs = tensor([[-0.6427, -1.6423],
        [-1.1895, -0.5652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10046055912971497
Epoch 0, Step 290: train/loss = 0.6843668222427368, train/raw-loss = 0.41416722536087036, train/logprobs = tensor([[-0.5061, -2.1137],
        [-0.6487, -0.5856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09006654471158981
Epoch 0, Step 291: train/loss = 0.7099599242210388, train/raw-loss = 0.49585068225860596, train/logprobs = tensor([[-0.3307, -1.8497],
        [-0.4842, -0.9009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07136973738670349
Epoch 0, Step 292: train/loss = 0.7822986245155334, train/raw-loss = 0.5756641626358032, train/logprobs = tensor([[-0.5258, -0.3868],
        [-0.7520, -0.0954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0688781663775444
Epoch 0, Step 293: train/loss = 0.6620425581932068, train/raw-loss = 0.4050859212875366, train/logprobs = tensor([[-0.4877, -2.2926],
        [-0.7731, -0.7502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08565221726894379
Epoch 0, Step 294: train/loss = 0.8196238875389099, train/raw-loss = 0.5986367464065552, train/logprobs = tensor([[-0.4346, -0.5035],
        [-0.5790, -0.2243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07366238534450531
Epoch 0, Step 295: train/loss = 0.7558406591415405, train/raw-loss = 0.4819263219833374, train/logprobs = tensor([[-0.5646, -1.8532],
        [-0.9169, -0.5873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09130479395389557
Epoch 0, Step 296: train/loss = 0.7396014928817749, train/raw-loss = 0.537858247756958, train/logprobs = tensor([[-0.3850, -1.1521],
        [-0.5509, -0.5453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06724775582551956
Epoch 0, Step 297: train/loss = 0.7424907684326172, train/raw-loss = 0.5078392028808594, train/logprobs = tensor([[-0.4105, -1.1510],
        [-0.5801, -0.2092]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07821719348430634
Epoch 0, Step 298: train/loss = 0.7388715147972107, train/raw-loss = 0.5084028244018555, train/logprobs = tensor([[-0.5440, -0.8211],
        [-0.8385, -0.2205]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07682289183139801
Epoch 0, Step 299: train/loss = 0.858993649482727, train/raw-loss = 0.6296265721321106, train/logprobs = tensor([[-0.4842, -0.6219],
        [-0.6552, -0.5249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07645568996667862
Epoch 0, Step 300: train/loss = 0.6672048568725586, train/raw-loss = 0.42483264207839966, train/logprobs = tensor([[-0.4404, -3.0312],
        [-0.6015, -1.4921]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08079073578119278
Epoch 0, Step 301: train/loss = 0.6761206984519958, train/raw-loss = 0.42843976616859436, train/logprobs = tensor([[-0.5331, -2.1541],
        [-0.9226, -0.8068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08256032317876816
Epoch 0, Step 302: train/loss = 0.7674254179000854, train/raw-loss = 0.5444934964179993, train/logprobs = tensor([[-0.4402, -0.6106],
        [-0.7355, -0.2394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07431062310934067
Epoch 0, Step 303: train/loss = 0.801200270652771, train/raw-loss = 0.5566197633743286, train/logprobs = tensor([[-0.4391, -0.7012],
        [-0.6939, -0.3154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08152683824300766
Epoch 0, Step 304: train/loss = 0.8540645837783813, train/raw-loss = 0.5678197145462036, train/logprobs = tensor([[-0.4488, -1.2327],
        [-0.5945, -0.7590]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09541495144367218
Epoch 0, Step 305: train/loss = 0.7154809236526489, train/raw-loss = 0.4807347059249878, train/logprobs = tensor([[-0.4320, -2.0434],
        [-0.6909, -0.7919]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07824872434139252
Epoch 0, Step 306: train/loss = 0.7180845737457275, train/raw-loss = 0.461333304643631, train/logprobs = tensor([[-0.5972, -1.4537],
        [-0.9622, -0.5900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08558373898267746
Epoch 0, Step 307: train/loss = 0.6720080375671387, train/raw-loss = 0.41624531149864197, train/logprobs = tensor([[-0.6078, -2.9230],
        [-0.8941, -1.2267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0852542594075203
Epoch 0, Step 308: train/loss = 0.7062788009643555, train/raw-loss = 0.456030011177063, train/logprobs = tensor([[-0.5554, -1.5833],
        [-0.8949, -0.7169]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08341627568006516
Epoch 0, Step 309: train/loss = 0.6887656450271606, train/raw-loss = 0.4992341995239258, train/logprobs = tensor([[-0.4633, -1.3958],
        [-0.6764, -0.5228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06317713856697083
Epoch 0, Step 310: train/loss = 0.7209682464599609, train/raw-loss = 0.44586777687072754, train/logprobs = tensor([[-0.5632, -1.4359],
        [-0.9266, -0.4466]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09170014411211014
Epoch 0, Step 311: train/loss = 0.7849738597869873, train/raw-loss = 0.5527573823928833, train/logprobs = tensor([[-0.3707, -1.0299],
        [-0.5047, -0.3750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07740551233291626
Epoch 0, Step 312: train/loss = 0.8276847004890442, train/raw-loss = 0.6208069324493408, train/logprobs = tensor([[-0.4020, -0.3517],
        [-0.5205, -0.1682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06895925849676132
Epoch 0, Step 313: train/loss = 0.6820356845855713, train/raw-loss = 0.45756030082702637, train/logprobs = tensor([[-0.4533, -2.9964],
        [-0.6144, -1.0718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07482513785362244
Epoch 0, Step 314: train/loss = 0.6878824234008789, train/raw-loss = 0.45016181468963623, train/logprobs = tensor([[-0.6100, -2.0622],
        [-0.8250, -0.8298]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07924018055200577
Epoch 0, Step 315: train/loss = 0.7385928630828857, train/raw-loss = 0.4834684729576111, train/logprobs = tensor([[-0.5029, -1.5550],
        [-0.6305, -0.6128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08504146337509155
Epoch 0, Step 316: train/loss = 0.6091479063034058, train/raw-loss = 0.3473227024078369, train/logprobs = tensor([[-0.6149, -3.7888],
        [-1.0981, -1.1622]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08727505803108215
Epoch 0, Step 317: train/loss = 0.7032880783081055, train/raw-loss = 0.46962159872055054, train/logprobs = tensor([[-0.5750, -1.0025],
        [-1.0585, -0.2969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07788882404565811
Epoch 0, Step 318: train/loss = 0.7543750405311584, train/raw-loss = 0.526034951210022, train/logprobs = tensor([[-0.5545, -1.2858],
        [-0.8071, -0.4763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07611337304115295
Epoch 0, Step 319: train/loss = 0.7540014982223511, train/raw-loss = 0.5122631788253784, train/logprobs = tensor([[-0.5226, -0.6736],
        [-0.9000, -0.2239]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08057943731546402
Epoch 0, Step 320: train/loss = 0.6696333885192871, train/raw-loss = 0.4385088384151459, train/logprobs = tensor([[-0.3758, -2.1798],
        [-0.5165, -0.7319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07704150676727295
Epoch 0, Step 321: train/loss = 0.7008442878723145, train/raw-loss = 0.4691661298274994, train/logprobs = tensor([[-0.4610, -1.5295],
        [-0.6614, -0.4153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07722605764865875
Epoch 0, Step 322: train/loss = 0.6593499779701233, train/raw-loss = 0.46311259269714355, train/logprobs = tensor([[-0.5094, -1.2760],
        [-0.8657, -0.3095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06541246920824051
Epoch 0, Step 323: train/loss = 0.5972946882247925, train/raw-loss = 0.3680780529975891, train/logprobs = tensor([[-0.4600, -3.4660],
        [-0.6338, -0.8015]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07640553265810013
Epoch 0, Step 324: train/loss = 0.6156992316246033, train/raw-loss = 0.423429936170578, train/logprobs = tensor([[-0.4549, -3.7123],
        [-0.7162, -1.0780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06408978253602982
Epoch 0, Step 325: train/loss = 0.6617423295974731, train/raw-loss = 0.3991614878177643, train/logprobs = tensor([[-0.4949, -1.5855],
        [-0.8585, -0.2934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08752693235874176
Epoch 0, Step 326: train/loss = 0.8010944128036499, train/raw-loss = 0.5927637815475464, train/logprobs = tensor([[-0.5008, -0.2983],
        [-0.7695, -0.1384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06944353878498077
Epoch 0, Step 327: train/loss = 0.5921008586883545, train/raw-loss = 0.3499337434768677, train/logprobs = tensor([[-0.4985, -3.3430],
        [-0.7863, -0.9879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08072236180305481
Epoch 0, Step 328: train/loss = 0.7737548351287842, train/raw-loss = 0.5592104196548462, train/logprobs = tensor([[-0.5244, -0.6368],
        [-0.7693, -0.2864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07151481509208679
Epoch 0, Step 329: train/loss = 0.6110743284225464, train/raw-loss = 0.36419790983200073, train/logprobs = tensor([[-0.5770, -2.4065],
        [-1.0317, -0.6682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08229213953018188
Epoch 0, Step 330: train/loss = 0.7018455862998962, train/raw-loss = 0.4576650857925415, train/logprobs = tensor([[-0.4546, -1.6462],
        [-0.6522, -0.3726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08139349520206451
Epoch 0, Step 331: train/loss = 0.6308272480964661, train/raw-loss = 0.4031740128993988, train/logprobs = tensor([[-0.4325, -3.1807],
        [-0.6339, -1.1903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07588440924882889
Epoch 0, Step 332: train/loss = 0.6125873327255249, train/raw-loss = 0.3606104254722595, train/logprobs = tensor([[-0.4702, -2.2860],
        [-0.8691, -0.5114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08399230986833572
Epoch 0, Step 333: train/loss = 0.6909629106521606, train/raw-loss = 0.47737088799476624, train/logprobs = tensor([[-0.6045, -1.3181],
        [-0.8726, -0.5470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07119734585285187
Epoch 0, Step 334: train/loss = 0.7153125405311584, train/raw-loss = 0.49903687834739685, train/logprobs = tensor([[-0.6704, -2.1264],
        [-1.0048, -0.5880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07209187746047974
Epoch 0, Step 335: train/loss = 0.642362117767334, train/raw-loss = 0.412943959236145, train/logprobs = tensor([[-0.5712, -2.2166],
        [-0.8694, -0.5891]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07647271454334259
Epoch 0, Step 336: train/loss = 0.66152024269104, train/raw-loss = 0.4231986105442047, train/logprobs = tensor([[-0.5127, -2.6423],
        [-0.7987, -0.9354]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07944054901599884
Epoch 0, Step 337: train/loss = 0.635177493095398, train/raw-loss = 0.3624758720397949, train/logprobs = tensor([[-0.5048, -2.5219],
        [-0.7752, -0.5845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09090054035186768
Epoch 0, Step 338: train/loss = 0.6966968774795532, train/raw-loss = 0.47329461574554443, train/logprobs = tensor([[-0.4829, -1.1871],
        [-0.7748, -0.3226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07446739822626114
Epoch 0, Step 339: train/loss = 0.6074138879776001, train/raw-loss = 0.39579346776008606, train/logprobs = tensor([[-0.5321, -1.8006],
        [-0.8244, -0.5009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07054014503955841
Epoch 0, Step 340: train/loss = 0.7889280319213867, train/raw-loss = 0.6053105592727661, train/logprobs = tensor([[-0.4372, -0.8393],
        [-0.5077, -0.4800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06120583415031433
Epoch 0, Step 341: train/loss = 0.7114177942276001, train/raw-loss = 0.49694129824638367, train/logprobs = tensor([[-0.3862, -1.1965],
        [-0.6002, -0.4565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07149218022823334
Epoch 0, Step 342: train/loss = 0.5979107022285461, train/raw-loss = 0.3349376916885376, train/logprobs = tensor([[-0.5990, -3.3858],
        [-1.0291, -0.9347]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08765767514705658
Epoch 0, Step 343: train/loss = 0.605211079120636, train/raw-loss = 0.34881484508514404, train/logprobs = tensor([[-0.5418, -3.4067],
        [-0.8231, -0.9820]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08546540141105652
Epoch 0, Step 344: train/loss = 0.631836473941803, train/raw-loss = 0.41011589765548706, train/logprobs = tensor([[-0.3888, -2.3584],
        [-0.7994, -0.8029]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07390686124563217
Epoch 0, Step 345: train/loss = 0.7224265933036804, train/raw-loss = 0.497024804353714, train/logprobs = tensor([[-0.4224, -0.9358],
        [-0.7398, -0.3109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07513391971588135
Epoch 0, Step 346: train/loss = 0.7090009450912476, train/raw-loss = 0.48617544770240784, train/logprobs = tensor([[-0.4213, -1.3452],
        [-0.5880, -0.4264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07427516579627991
Epoch 0, Step 347: train/loss = 0.7252886295318604, train/raw-loss = 0.49824878573417664, train/logprobs = tensor([[-0.5191, -1.0341],
        [-0.7433, -0.2954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0756799504160881
