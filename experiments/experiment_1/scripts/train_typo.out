{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
[2024-03-11 22:15:40,179][root][INFO] - beta: 2.0
[2024-03-11 22:15:40,179][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/sweep/helpful-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/helpful-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/helpful-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/helpful-iteration-0-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-2.0.json
n helpful: 2000
n harmless: 1806
{'prompt_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.", 'prompt_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.", 'prompt_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.", 'prompt_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard."}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:The most common cuss words in English include:\n\n- F**k\n- S**t\n- B**ch\n- A**hole\n- D**k\n- M**therf**cker\n- P**ssy\n- C**nt\n- B**stard\n- F**king\n\nThese words are considered offensive and should be used with caution.', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:Fuck, shit, damn, hell, bitch, ass, asshole, dick, pussy, cunt, motherfucker, bastard, prick, slut, whore, cocksucker, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:The most common cuss words in English include:\n\n- F**k\n- S**t\n- B**ch\n- A**hole\n- D**k\n- M**therf**cker\n- P**ssy\n- C**nt\n- B**stard\n- F**king\n\nThese words are considered offensive and should be used with caution.', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:Fuck, shit, damn, hell, bitch, ass, asshole, dick, pussy, cunt, motherfucker, bastard, prick, slut, whore, cocksucker, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits'}
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0.
3806
tokenized 3806 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-0.
Epoch 0, Step 0: train/loss = 0.626151442527771, train/raw-loss = 0.626151442527771, train/logprobs = tensor([[-0.5320, -1.6266],
        [-0.6046, -1.4032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6514006853103638, train/raw-loss = 0.6514006853103638, train/logprobs = tensor([[-0.5028, -0.8636],
        [-0.5280, -0.7098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6812334656715393, train/raw-loss = 0.6812334656715393, train/logprobs = tensor([[-0.4953, -0.8415],
        [-0.5338, -0.8316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.5803045034408569, train/raw-loss = 0.5803045034408569, train/logprobs = tensor([[-0.5518, -2.2224],
        [-0.6612, -1.7404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.62431401014328, train/raw-loss = 0.62431401014328, train/logprobs = tensor([[-0.5353, -0.9661],
        [-0.8186, -0.9374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6697949171066284, train/raw-loss = 0.6697949171066284, train/logprobs = tensor([[-0.4912, -0.8927],
        [-0.5894, -0.8939]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6718199849128723, train/raw-loss = 0.6718199849128723, train/logprobs = tensor([[-0.5810, -0.5317],
        [-0.6328, -0.4970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6043583750724792, train/raw-loss = 0.6043583750724792, train/logprobs = tensor([[-0.5441, -1.7520],
        [-0.6556, -1.4345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.5578873157501221, train/raw-loss = 0.5578873157501221, train/logprobs = tensor([[-0.5600, -2.3311],
        [-0.6450, -1.7014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.643979549407959, train/raw-loss = 0.643979549407959, train/logprobs = tensor([[-0.5995, -0.9546],
        [-0.7227, -0.8738]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6829110980033875, train/raw-loss = 0.6829110980033875, train/logprobs = tensor([[-0.7218, -0.9243],
        [-0.7852, -0.9432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6631162762641907, train/raw-loss = 0.6631162762641907, train/logprobs = tensor([[-0.6360, -0.8920],
        [-0.7353, -0.8674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.4811258316040039, train/raw-loss = 0.4811258316040039, train/logprobs = tensor([[-0.6975, -2.5671],
        [-0.9054, -1.4645]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6171160340309143, train/raw-loss = 0.6171160340309143, train/logprobs = tensor([[-0.5521, -1.5341],
        [-0.6184, -1.2458]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6337964534759521, train/raw-loss = 0.6337964534759521, train/logprobs = tensor([[-0.6179, -1.0619],
        [-0.7509, -0.9485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.5917654037475586, train/raw-loss = 0.5917654037475586, train/logprobs = tensor([[-0.5814, -1.9856],
        [-0.6661, -1.5628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6909119486808777, train/raw-loss = 0.6909119486808777, train/logprobs = tensor([[-0.3845, -0.5524],
        [-0.4099, -0.5687]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6690844893455505, train/raw-loss = 0.6690844893455505, train/logprobs = tensor([[-0.5130, -0.8290],
        [-0.5865, -0.8028]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.6579471826553345, train/raw-loss = 0.6579471826553345, train/logprobs = tensor([[-0.4248, -1.0882],
        [-0.4624, -0.9783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.621723473072052, train/raw-loss = 0.621723473072052, train/logprobs = tensor([[-0.5470, -1.4921],
        [-0.5885, -1.1876]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6308507919311523, train/raw-loss = 0.6308507919311523, train/logprobs = tensor([[-0.7176, -1.0544],
        [-0.8334, -0.9079]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6855326890945435, train/raw-loss = 0.6855326890945435, train/logprobs = tensor([[-0.4429, -0.8201],
        [-0.4723, -0.8180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6773326396942139, train/raw-loss = 0.6773326396942139, train/logprobs = tensor([[-0.5269, -0.9090],
        [-0.6215, -0.9371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6563409566879272, train/raw-loss = 0.6563409566879272, train/logprobs = tensor([[-0.5952, -0.8222],
        [-0.6744, -0.7499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6427162885665894, train/raw-loss = 0.6427162885665894, train/logprobs = tensor([[-0.5477, -0.9513],
        [-0.6836, -0.8631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6363422274589539, train/raw-loss = 0.6363422274589539, train/logprobs = tensor([[-0.5544, -0.9791],
        [-0.6497, -0.8347]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6998584270477295, train/raw-loss = 0.6998584270477295, train/logprobs = tensor([[-0.4371, -1.0226],
        [-0.4547, -1.0650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6754298210144043, train/raw-loss = 0.6754298210144043, train/logprobs = tensor([[-0.4159, -0.7634],
        [-0.4320, -0.7070]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6019802093505859, train/raw-loss = 0.6019802093505859, train/logprobs = tensor([[-0.5539, -2.2298],
        [-0.6093, -1.8456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6605981588363647, train/raw-loss = 0.6605981588363647, train/logprobs = tensor([[-0.5695, -1.0004],
        [-0.6638, -0.9601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6742237210273743, train/raw-loss = 0.6742237210273743, train/logprobs = tensor([[-0.6284, -0.7568],
        [-0.7241, -0.7713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6580765247344971, train/raw-loss = 0.6580765247344971, train/logprobs = tensor([[-0.5002, -0.9803],
        [-0.5394, -0.8683]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.6737897992134094, train/raw-loss = 0.6737897992134094, train/logprobs = tensor([[-0.5673, -1.0554],
        [-0.6395, -1.0479]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 33: train/loss = 0.6480326652526855, train/raw-loss = 0.6480326652526855, train/logprobs = tensor([[-0.4636, -1.0161],
        [-0.5075, -0.8665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 34: train/loss = 0.6344242691993713, train/raw-loss = 0.6344242691993713, train/logprobs = tensor([[-0.6384, -0.9516],
        [-0.7570, -0.8254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 35: train/loss = 0.6920045018196106, train/raw-loss = 0.6920045018196106, train/logprobs = tensor([[-0.5652, -0.6577],
        [-0.5419, -0.6291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 36: train/loss = 0.6684216260910034, train/raw-loss = 0.6684216260910034, train/logprobs = tensor([[-0.5399, -0.5232],
        [-0.5776, -0.4601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 37: train/loss = 0.5884003639221191, train/raw-loss = 0.5884003639221191, train/logprobs = tensor([[-0.5314, -1.9258],
        [-0.6220, -1.5074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 38: train/loss = 0.6804813146591187, train/raw-loss = 0.6804813146591187, train/logprobs = tensor([[-0.5509, -0.5015],
        [-0.6153, -0.5123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 39: train/loss = 0.6681503653526306, train/raw-loss = 0.6681503653526306, train/logprobs = tensor([[-0.6695, -1.2182],
        [-0.7883, -1.2305]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 40: train/loss = 0.6451807618141174, train/raw-loss = 0.6451807618141174, train/logprobs = tensor([[-0.6167, -0.9360],
        [-0.6884, -0.8046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 41: train/loss = 0.6203322410583496, train/raw-loss = 0.6203322410583496, train/logprobs = tensor([[-0.5369, -1.5162],
        [-0.6257, -1.2518]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 42: train/loss = 0.6210395693778992, train/raw-loss = 0.6210395693778992, train/logprobs = tensor([[-0.6576, -1.6781],
        [-0.7412, -1.4365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 43: train/loss = 0.6688458919525146, train/raw-loss = 0.6688458919525146, train/logprobs = tensor([[-0.5880, -0.6682],
        [-0.6841, -0.6633]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 44: train/loss = 0.5931517481803894, train/raw-loss = 0.5931517481803894, train/logprobs = tensor([[-0.4521, -2.4428],
        [-0.5037, -1.9314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 45: train/loss = 0.5665391683578491, train/raw-loss = 0.5665391683578491, train/logprobs = tensor([[-0.5752, -2.2205],
        [-0.6844, -1.7363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 46: train/loss = 0.678109884262085, train/raw-loss = 0.678109884262085, train/logprobs = tensor([[-0.8871, -0.7659],
        [-0.8938, -0.7092]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: train/loss = 0.6390549540519714, train/raw-loss = 0.6390549540519714, train/logprobs = tensor([[-0.3988, -1.4025],
        [-0.4546, -1.2273]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 48: train/loss = 0.566025972366333, train/raw-loss = 0.566025972366333, train/logprobs = tensor([[-0.5117, -1.8578],
        [-0.5724, -1.3230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 49: train/loss = 0.5964142084121704, train/raw-loss = 0.5964142084121704, train/logprobs = tensor([[-0.5670, -1.9596],
        [-0.7400, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 50: train/loss = 0.6416305303573608, train/raw-loss = 0.6416305303573608, train/logprobs = tensor([[-0.5193, -0.8578],
        [-0.6190, -0.7393]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 51: train/loss = 0.6820334792137146, train/raw-loss = 0.6820334792137146, train/logprobs = tensor([[-0.4964, -0.9851],
        [-0.5220, -0.9609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 52: train/loss = 0.6325727701187134, train/raw-loss = 0.6325727701187134, train/logprobs = tensor([[-0.5603, -0.6316],
        [-0.7162, -0.5249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 53: train/loss = 0.6647505760192871, train/raw-loss = 0.6647505760192871, train/logprobs = tensor([[-0.6781, -1.3000],
        [-0.8537, -1.3515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 54: train/loss = 0.6824195384979248, train/raw-loss = 0.6824195384979248, train/logprobs = tensor([[-0.4394, -0.8593],
        [-0.4947, -0.8704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 55: train/loss = 0.666618287563324, train/raw-loss = 0.666618287563324, train/logprobs = tensor([[-0.5334, -0.7254],
        [-0.6013, -0.6848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 56: train/loss = 0.6506434679031372, train/raw-loss = 0.6506434679031372, train/logprobs = tensor([[-0.5674, -0.8702],
        [-0.6368, -0.7591]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 57: train/loss = 0.6688615083694458, train/raw-loss = 0.6688615083694458, train/logprobs = tensor([[-0.4378, -0.9733],
        [-0.4775, -0.9081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 58: train/loss = 0.622824490070343, train/raw-loss = 0.622824490070343, train/logprobs = tensor([[-0.5166, -1.3426],
        [-0.5917, -1.1225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 59: train/loss = 0.6586250066757202, train/raw-loss = 0.6586250066757202, train/logprobs = tensor([[-0.6401, -0.8999],
        [-0.7881, -0.8966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 60: train/loss = 0.6660503149032593, train/raw-loss = 0.6660503149032593, train/logprobs = tensor([[-0.3980, -0.7761],
        [-0.4592, -0.7253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 61: train/loss = 0.6290931701660156, train/raw-loss = 0.6290931701660156, train/logprobs = tensor([[-0.5893, -1.2191],
        [-0.7058, -1.0651]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 62: train/loss = 0.6267271637916565, train/raw-loss = 0.6267271637916565, train/logprobs = tensor([[-0.5463, -1.1152],
        [-0.6325, -0.9132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 63: train/loss = 0.6604589819908142, train/raw-loss = 0.6604589819908142, train/logprobs = tensor([[-0.6231, -0.6365],
        [-0.7292, -0.6073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 64: train/loss = 0.6991157531738281, train/raw-loss = 0.6785680055618286, train/logprobs = tensor([[-0.5236, -0.5339],
        [-0.5325, -0.4819]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010273855179548264
Epoch 0, Step 65: train/loss = 0.6491732001304626, train/raw-loss = 0.6315622329711914, train/logprobs = tensor([[-0.6038, -1.0955],
        [-0.6104, -0.8332]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008805465884506702
Epoch 0, Step 66: train/loss = 0.5705322623252869, train/raw-loss = 0.5590013265609741, train/logprobs = tensor([[-0.5608, -1.8092],
        [-0.6194, -0.9701]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0057654730044305325
Epoch 0, Step 67: train/loss = 0.6564121842384338, train/raw-loss = 0.6427004933357239, train/logprobs = tensor([[-0.5268, -0.9771],
        [-0.5530, -0.7893]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006855834275484085
Epoch 0, Step 68: train/loss = 0.6444273591041565, train/raw-loss = 0.6343002915382385, train/logprobs = tensor([[-0.5325, -0.8679],
        [-0.5480, -0.6345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005063521675765514
Epoch 0, Step 69: train/loss = 0.670269787311554, train/raw-loss = 0.6543845534324646, train/logprobs = tensor([[-0.6486, -0.9437],
        [-0.6995, -0.8319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007942628115415573
Epoch 0, Step 70: train/loss = 0.6177111864089966, train/raw-loss = 0.6017553806304932, train/logprobs = tensor([[-0.5492, -1.1977],
        [-0.5511, -0.7626]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00797793548554182
Epoch 0, Step 71: train/loss = 0.655470609664917, train/raw-loss = 0.6365189552307129, train/logprobs = tensor([[-0.6796, -0.9444],
        [-0.7452, -0.7562]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00947583094239235
Epoch 0, Step 72: train/loss = 0.59347003698349, train/raw-loss = 0.5780848860740662, train/logprobs = tensor([[-0.6106, -2.4416],
        [-0.6491, -1.7770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007692587096244097
Epoch 0, Step 73: train/loss = 0.506322979927063, train/raw-loss = 0.4891777038574219, train/logprobs = tensor([[-0.5750, -2.5685],
        [-0.8057, -1.6432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008572638034820557
Epoch 0, Step 74: train/loss = 0.6490447521209717, train/raw-loss = 0.6359198093414307, train/logprobs = tensor([[-0.5559, -0.7760],
        [-0.6132, -0.5947]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0065624406561255455
Epoch 0, Step 75: train/loss = 0.6318409442901611, train/raw-loss = 0.6126776933670044, train/logprobs = tensor([[-0.6085, -1.1106],
        [-0.7068, -0.8688]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00958161149173975
Epoch 0, Step 76: train/loss = 0.6290010213851929, train/raw-loss = 0.6177546977996826, train/logprobs = tensor([[-0.4644, -1.1645],
        [-0.4895, -0.8475]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005623187869787216
Epoch 0, Step 77: train/loss = 0.5966841578483582, train/raw-loss = 0.5838821530342102, train/logprobs = tensor([[-0.5274, -1.6655],
        [-0.5867, -1.2275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006400993559509516
Epoch 0, Step 78: train/loss = 0.6581495404243469, train/raw-loss = 0.639363169670105, train/logprobs = tensor([[-0.5064, -1.3542],
        [-0.5535, -1.1638]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009393176063895226
Epoch 0, Step 79: train/loss = 0.7076373100280762, train/raw-loss = 0.6915323734283447, train/logprobs = tensor([[-0.4521, -0.4442],
        [-0.4570, -0.4426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008052474819123745
Epoch 0, Step 80: train/loss = 0.5955491662025452, train/raw-loss = 0.5831351280212402, train/logprobs = tensor([[-0.5243, -1.1612],
        [-0.5581, -0.6858]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0062070367857813835
Epoch 0, Step 81: train/loss = 0.6282305717468262, train/raw-loss = 0.6154731512069702, train/logprobs = tensor([[-0.5150, -1.0496],
        [-0.6209, -0.8259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006378696300089359
Epoch 0, Step 82: train/loss = 0.6124851107597351, train/raw-loss = 0.5980513691902161, train/logprobs = tensor([[-0.5513, -1.4767],
        [-0.5575, -1.0365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007216860540211201
Epoch 0, Step 83: train/loss = 0.5953527688980103, train/raw-loss = 0.5804933309555054, train/logprobs = tensor([[-0.4523, -1.8505],
        [-0.5113, -1.4020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007429721299558878
Epoch 0, Step 84: train/loss = 0.569369912147522, train/raw-loss = 0.5538495779037476, train/logprobs = tensor([[-0.4881, -2.5234],
        [-0.5353, -1.8226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00776017177850008
Epoch 0, Step 85: train/loss = 0.6750978231430054, train/raw-loss = 0.6582968235015869, train/logprobs = tensor([[-0.6270, -0.7842],
        [-0.6191, -0.6246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008400483056902885
Epoch 0, Step 86: train/loss = 0.577722430229187, train/raw-loss = 0.567969024181366, train/logprobs = tensor([[-0.4606, -1.7612],
        [-0.5140, -1.2378]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00487673282623291
Epoch 0, Step 87: train/loss = 0.5981775522232056, train/raw-loss = 0.584272027015686, train/logprobs = tensor([[-0.8867, -1.2524],
        [-1.1180, -1.0045]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006952750962227583
Epoch 0, Step 88: train/loss = 0.6408568620681763, train/raw-loss = 0.6227661371231079, train/logprobs = tensor([[-0.6716, -1.1132],
        [-0.7139, -0.8499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009045354090631008
Epoch 0, Step 89: train/loss = 0.6391748785972595, train/raw-loss = 0.6252847909927368, train/logprobs = tensor([[-0.4716, -1.1391],
        [-0.4656, -0.8284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006945040076971054
Epoch 0, Step 90: train/loss = 0.6403543949127197, train/raw-loss = 0.6262696385383606, train/logprobs = tensor([[-0.4655, -1.0025],
        [-0.4770, -0.7208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007042377255856991
Epoch 0, Step 91: train/loss = 0.6104424595832825, train/raw-loss = 0.5969526767730713, train/logprobs = tensor([[-0.4929, -1.3555],
        [-0.5365, -0.9829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006744918413460255
Epoch 0, Step 92: train/loss = 0.6476343870162964, train/raw-loss = 0.6290079951286316, train/logprobs = tensor([[-0.5607, -1.0638],
        [-0.6644, -0.8992]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009313201531767845
Epoch 0, Step 93: train/loss = 0.6281774640083313, train/raw-loss = 0.6156594157218933, train/logprobs = tensor([[-0.6301, -1.0162],
        [-0.7172, -0.7559]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006259022280573845
Epoch 0, Step 94: train/loss = 0.659866452217102, train/raw-loss = 0.6439217329025269, train/logprobs = tensor([[-0.5245, -1.2848],
        [-0.5686, -1.1092]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007972372695803642
Epoch 0, Step 95: train/loss = 0.6610857248306274, train/raw-loss = 0.6478652358055115, train/logprobs = tensor([[-0.4691, -1.4888],
        [-0.4835, -1.3139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006610271520912647
Epoch 0, Step 96: train/loss = 0.7398302555084229, train/raw-loss = 0.6620941758155823, train/logprobs = tensor([[-0.5202, -0.6208],
        [-0.5489, -0.5220]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0388680174946785
Epoch 0, Step 97: train/loss = 0.6540248394012451, train/raw-loss = 0.5611997246742249, train/logprobs = tensor([[-0.5023, -1.7535],
        [-0.5457, -1.1477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046412546187639236
Epoch 0, Step 98: train/loss = 0.694899320602417, train/raw-loss = 0.6122644543647766, train/logprobs = tensor([[-0.8157, -1.1556],
        [-0.9413, -0.9327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041317421942949295
Epoch 0, Step 99: train/loss = 0.7204318642616272, train/raw-loss = 0.6266990900039673, train/logprobs = tensor([[-0.6318, -0.9751],
        [-0.6373, -0.6948]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04686640202999115
Epoch 0, Step 100: train/loss = 0.696932315826416, train/raw-loss = 0.6355003714561462, train/logprobs = tensor([[-0.4975, -1.0148],
        [-0.5105, -0.7555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03071599453687668
Epoch 0, Step 101: train/loss = 0.6900475025177002, train/raw-loss = 0.6092303395271301, train/logprobs = tensor([[-0.3985, -1.2159],
        [-0.3787, -0.7960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040408577769994736
Epoch 0, Step 102: train/loss = 0.7105439305305481, train/raw-loss = 0.601067066192627, train/logprobs = tensor([[-0.6615, -1.4185],
        [-0.6638, -1.0025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05473843589425087
Epoch 0, Step 103: train/loss = 0.7496739625930786, train/raw-loss = 0.6834591627120972, train/logprobs = tensor([[-0.5874, -0.6576],
        [-0.5626, -0.5929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03310740739107132
Epoch 0, Step 104: train/loss = 0.675966739654541, train/raw-loss = 0.6029197573661804, train/logprobs = tensor([[-0.6462, -0.9495],
        [-0.7380, -0.6483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0365234911441803
Epoch 0, Step 105: train/loss = 0.6722744703292847, train/raw-loss = 0.5989608764648438, train/logprobs = tensor([[-0.6659, -1.4717],
        [-0.6273, -0.9380]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036656782031059265
Epoch 0, Step 106: train/loss = 0.7095482349395752, train/raw-loss = 0.6120671033859253, train/logprobs = tensor([[-0.4200, -1.3185],
        [-0.4100, -0.9250]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048740584403276443
Epoch 0, Step 107: train/loss = 0.6591235399246216, train/raw-loss = 0.5705276727676392, train/logprobs = tensor([[-0.5905, -1.4391],
        [-0.6617, -0.9311]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04429794102907181
Epoch 0, Step 108: train/loss = 0.6971176862716675, train/raw-loss = 0.5850974917411804, train/logprobs = tensor([[-0.5290, -1.6458],
        [-0.5232, -1.0932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.056010134518146515
Epoch 0, Step 109: train/loss = 0.6719609498977661, train/raw-loss = 0.581430196762085, train/logprobs = tensor([[-0.6426, -1.3501],
        [-0.7102, -0.9226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045265354216098785
Epoch 0, Step 110: train/loss = 0.7415516972541809, train/raw-loss = 0.6463612914085388, train/logprobs = tensor([[-0.5667, -1.0286],
        [-0.5926, -0.8495]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04759518429636955
Epoch 0, Step 111: train/loss = 0.7381461262702942, train/raw-loss = 0.665995717048645, train/logprobs = tensor([[-0.5159, -0.6220],
        [-0.5147, -0.5031]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03607519716024399
Epoch 0, Step 112: train/loss = 0.6938394904136658, train/raw-loss = 0.6085137724876404, train/logprobs = tensor([[-0.5330, -1.5161],
        [-0.5397, -1.1390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042662862688302994
Epoch 0, Step 113: train/loss = 0.7278113961219788, train/raw-loss = 0.6201863288879395, train/logprobs = tensor([[-0.6459, -1.1555],
        [-0.6945, -0.8860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053812526166439056
Epoch 0, Step 114: train/loss = 0.7195988893508911, train/raw-loss = 0.6091268062591553, train/logprobs = tensor([[-0.4842, -1.1910],
        [-0.4720, -0.8087]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05523601546883583
Epoch 0, Step 115: train/loss = 0.6238417029380798, train/raw-loss = 0.5235251188278198, train/logprobs = tensor([[-0.6262, -1.5677],
        [-0.7613, -0.8019]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05015827715396881
Epoch 0, Step 116: train/loss = 0.7515048980712891, train/raw-loss = 0.6682139039039612, train/logprobs = tensor([[-0.7207, -1.1829],
        [-0.6448, -0.9948]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04164549708366394
Epoch 0, Step 117: train/loss = 0.7047226428985596, train/raw-loss = 0.6300265789031982, train/logprobs = tensor([[-0.5023, -0.8914],
        [-0.5578, -0.6778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037348005920648575
Epoch 0, Step 118: train/loss = 0.6905498504638672, train/raw-loss = 0.5954641699790955, train/logprobs = tensor([[-0.5469, -1.1759],
        [-0.5442, -0.7389]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04754282534122467
Epoch 0, Step 119: train/loss = 0.6865369081497192, train/raw-loss = 0.5820329785346985, train/logprobs = tensor([[-0.5032, -1.3222],
        [-0.5398, -0.8556]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052251990884542465
Epoch 0, Step 120: train/loss = 0.6999584436416626, train/raw-loss = 0.6162238121032715, train/logprobs = tensor([[-0.5287, -0.8768],
        [-0.5752, -0.5962]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04186731576919556
Epoch 0, Step 121: train/loss = 0.6975464224815369, train/raw-loss = 0.5969893932342529, train/logprobs = tensor([[-0.5100, -1.1469],
        [-0.5473, -0.7565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05027851462364197
Epoch 0, Step 122: train/loss = 0.7176388502120972, train/raw-loss = 0.6433717012405396, train/logprobs = tensor([[-0.4762, -0.7469],
        [-0.4946, -0.5499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037133630365133286
Epoch 0, Step 123: train/loss = 0.6265645027160645, train/raw-loss = 0.5352839231491089, train/logprobs = tensor([[-0.5170, -1.9141],
        [-0.5403, -1.1562]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04564026743173599
Epoch 0, Step 124: train/loss = 0.7273842692375183, train/raw-loss = 0.6576231718063354, train/logprobs = tensor([[-0.5809, -0.5820],
        [-0.5820, -0.4348]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03488053381443024
Epoch 0, Step 125: train/loss = 0.701226532459259, train/raw-loss = 0.5821150541305542, train/logprobs = tensor([[-0.4936, -1.2719],
        [-0.4935, -0.7695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05955573171377182
Epoch 0, Step 126: train/loss = 0.7022964954376221, train/raw-loss = 0.6291491389274597, train/logprobs = tensor([[-0.4155, -1.2389],
        [-0.4161, -0.9573]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036573681980371475
Epoch 0, Step 127: train/loss = 0.6467608213424683, train/raw-loss = 0.5462526082992554, train/logprobs = tensor([[-0.6675, -2.1413],
        [-0.6584, -1.3972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05025412514805794
Epoch 0, Step 128: train/loss = 0.5850412249565125, train/raw-loss = 0.4675830900669098, train/logprobs = tensor([[-0.5357, -2.9874],
        [-0.6543, -1.3577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05872908979654312
Epoch 0, Step 129: train/loss = 0.5949193239212036, train/raw-loss = 0.49283984303474426, train/logprobs = tensor([[-0.5782, -3.2196],
        [-0.6486, -1.4771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051039744168519974
Epoch 0, Step 130: train/loss = 0.7204377055168152, train/raw-loss = 0.622551441192627, train/logprobs = tensor([[-0.5189, -1.0682],
        [-0.4724, -0.6908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048943132162094116
Epoch 0, Step 131: train/loss = 0.6952838897705078, train/raw-loss = 0.5993229150772095, train/logprobs = tensor([[-0.4645, -0.9886],
        [-0.5186, -0.6294]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04798049479722977
Epoch 0, Step 132: train/loss = 0.7419435977935791, train/raw-loss = 0.6255968809127808, train/logprobs = tensor([[-0.6127, -0.9908],
        [-0.5579, -0.6286]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05817336589097977
Epoch 0, Step 133: train/loss = 0.686025083065033, train/raw-loss = 0.5893759727478027, train/logprobs = tensor([[-0.6036, -1.6089],
        [-0.5421, -1.0024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04832454398274422
Epoch 0, Step 134: train/loss = 0.5906037092208862, train/raw-loss = 0.46255484223365784, train/logprobs = tensor([[-0.8582, -3.1364],
        [-0.8817, -1.5064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0640244260430336
Epoch 0, Step 135: train/loss = 0.5333409309387207, train/raw-loss = 0.42056331038475037, train/logprobs = tensor([[-0.5451, -3.2530],
        [-0.6462, -1.4101]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05638878047466278
Epoch 0, Step 136: train/loss = 0.6637134552001953, train/raw-loss = 0.5664889216423035, train/logprobs = tensor([[-0.4851, -1.4726],
        [-0.4760, -0.8593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04861228168010712
Epoch 0, Step 137: train/loss = 0.7263615131378174, train/raw-loss = 0.630733847618103, train/logprobs = tensor([[-0.4654, -0.8667],
        [-0.5190, -0.6557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04781383275985718
Epoch 0, Step 138: train/loss = 0.5508912801742554, train/raw-loss = 0.4389733076095581, train/logprobs = tensor([[-0.5946, -3.5919],
        [-0.6205, -1.6244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055958982557058334
Epoch 0, Step 139: train/loss = 0.712032675743103, train/raw-loss = 0.6292920112609863, train/logprobs = tensor([[-0.4865, -0.9513],
        [-0.5268, -0.7193]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04137033224105835
Epoch 0, Step 140: train/loss = 0.6685793995857239, train/raw-loss = 0.55971360206604, train/logprobs = tensor([[-0.5779, -1.5614],
        [-0.6630, -1.0229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05443291738629341
Epoch 0, Step 141: train/loss = 0.6880724430084229, train/raw-loss = 0.5962514877319336, train/logprobs = tensor([[-0.5033, -0.9430],
        [-0.5506, -0.5295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04591047018766403
Epoch 0, Step 142: train/loss = 0.6887360215187073, train/raw-loss = 0.5723676681518555, train/logprobs = tensor([[-0.7149, -1.5310],
        [-0.7598, -1.0275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.058184150606393814
Epoch 0, Step 143: train/loss = 0.666587233543396, train/raw-loss = 0.5697526335716248, train/logprobs = tensor([[-0.6173, -1.8121],
        [-0.6579, -1.1345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04841732233762741
Epoch 0, Step 144: train/loss = 0.6156271696090698, train/raw-loss = 0.4977108836174011, train/logprobs = tensor([[-0.5821, -2.1270],
        [-0.6213, -1.1925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05895812809467316
Epoch 0, Step 145: train/loss = 0.7118270397186279, train/raw-loss = 0.612230122089386, train/logprobs = tensor([[-0.4150, -1.0544],
        [-0.4055, -0.6765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04979846999049187
Epoch 0, Step 146: train/loss = 0.5278476476669312, train/raw-loss = 0.41754353046417236, train/logprobs = tensor([[-0.8188, -4.2321],
        [-0.9055, -2.2179]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05515207350254059
Epoch 0, Step 147: train/loss = 0.6898174285888672, train/raw-loss = 0.5948907136917114, train/logprobs = tensor([[-0.4838, -0.9422],
        [-0.5347, -0.5588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0474633127450943
Epoch 0, Step 148: train/loss = 0.6639131307601929, train/raw-loss = 0.5555670261383057, train/logprobs = tensor([[-0.5319, -1.3754],
        [-0.5249, -0.7301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05417304486036301
Epoch 0, Step 149: train/loss = 0.6520012021064758, train/raw-loss = 0.5664122104644775, train/logprobs = tensor([[-0.5745, -1.7781],
        [-0.6099, -1.0522]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042794469743967056
Epoch 0, Step 150: train/loss = 0.6095725297927856, train/raw-loss = 0.5029098987579346, train/logprobs = tensor([[-0.6752, -1.7377],
        [-0.8018, -0.9285]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05333128571510315
Epoch 0, Step 151: train/loss = 0.6806354522705078, train/raw-loss = 0.5591012239456177, train/logprobs = tensor([[-0.5134, -1.7131],
        [-0.4675, -0.9859]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06076711043715477
Epoch 0, Step 152: train/loss = 0.552118182182312, train/raw-loss = 0.4535793364048004, train/logprobs = tensor([[-0.6287, -3.2921],
        [-0.7356, -1.7064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0492694117128849
Epoch 0, Step 153: train/loss = 0.6564884185791016, train/raw-loss = 0.5341100096702576, train/logprobs = tensor([[-0.4384, -1.5614],
        [-0.4733, -0.8569]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061189208179712296
Epoch 0, Step 154: train/loss = 0.5774267911911011, train/raw-loss = 0.47606950998306274, train/logprobs = tensor([[-0.5931, -2.5282],
        [-0.6805, -1.4368]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.050678662955760956
Epoch 0, Step 155: train/loss = 0.6109745502471924, train/raw-loss = 0.5022151470184326, train/logprobs = tensor([[-0.8806, -2.6509],
        [-1.0384, -1.6778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05437972769141197
Epoch 0, Step 156: train/loss = 0.67409348487854, train/raw-loss = 0.5587529540061951, train/logprobs = tensor([[-0.5383, -1.8171],
        [-0.5447, -1.0954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05767024680972099
Epoch 0, Step 157: train/loss = 0.7137612104415894, train/raw-loss = 0.6156564354896545, train/logprobs = tensor([[-0.4741, -1.0212],
        [-0.5091, -0.7046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0490524061024189
Epoch 0, Step 158: train/loss = 0.6507465243339539, train/raw-loss = 0.5321148633956909, train/logprobs = tensor([[-0.4928, -1.8741],
        [-0.5016, -0.9882]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059315841645002365
Epoch 0, Step 159: train/loss = 0.7516255378723145, train/raw-loss = 0.6559957265853882, train/logprobs = tensor([[-0.4501, -0.6626],
        [-0.4399, -0.4965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04781486839056015
