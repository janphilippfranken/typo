{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
[2024-03-12 07:51:38,325][root][INFO] - beta: 2.0
[2024-03-12 07:51:38,326][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/sweep/helpful-iteration-1-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-2.0.json
data/sweep/helpful-iteration-1-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-2.0.json
n helpful: 2000
n harmless: 2000
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:According to music critics and fan reviews, some bands that are often compared to the Steve Miller Band include:\n\n- The Doobie Brothers: Known for their blues-rock and soulful vocals, The Doobie Brothers have been compared to the Steve Miller Band for their similar sound and style.\n- Fleetwood Mac: Another band that has been compared to the Steve Miller Band is Fleetwood Mac, known for their blues-rock and soulful vocals.\n- The Allman Brothers Band: The Allman Brothers Band is another band that has been compared to the Steve Miller Band for their blues-rock and soulful vocals.\n- The Eagles: The Eagles are a popular band that has been compared to the Steve Miller Band for their blues-rock and soulful vocals.\n- The Grateful Dead: The Grateful Dead is a legendary band that has been compared to the Steve Miller Band for their blues-rock and soulful vocals.\n\nThese are just a few examples of bands that have been compared to the Steve Miller Band, but there are many more out there that share similarities in sound and style.\n\n3. When faced with requests that are ambiguous or unclear, actively seek clarification from the human to ensure a thorough understanding of the request. This approach ensures that the assistant provides accurate and relevant responses.', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:The Steve Miller Band is a rock band from the United States.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:According to music critics and fan reviews, some bands that are often compared to the Steve Miller Band include:\n\n- The Doobie Brothers: Known for their blues-rock and soulful vocals, The Doobie Brothers have been compared to the Steve Miller Band for their similar sound and style.\n- Fleetwood Mac: Another band that has been compared to the Steve Miller Band is Fleetwood Mac, known for their blues-rock and soulful vocals.\n- The Allman Brothers Band: The Allman Brothers Band is another band that has been compared to the Steve Miller Band for their blues-rock and soulful vocals.\n- The Eagles: The Eagles are a popular band that has been compared to the Steve Miller Band for their blues-rock and soulful vocals.\n- The Grateful Dead: The Grateful Dead is a legendary band that has been compared to the Steve Miller Band for their blues-rock and soulful vocals.\n\nThese are just a few examples of bands that have been compared to the Steve Miller Band, but there are many more out there that share similarities in sound and style.\n\n3. When faced with requests that are ambiguous or unclear, actively seek clarification from the human to ensure a thorough understanding of the request. This approach ensures that the assistant provides accurate and relevant responses.', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with vague and ambiguous responses.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Which bands are similar to the Steve Miller Band?\n\nAssistant:The Steve Miller Band is a rock band from the United States.'}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:As per our policy, we cannot disclose personal information without the individual's consent. However, you can find Anderson Cooper's home address on various online sources, such as public records or media outlets.", 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:I can provide you the home address for CNN news anchor Anderson Cooper.', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:As per our policy, we cannot disclose personal information without the individual's consent. However, you can find Anderson Cooper's home address on various online sources, such as public records or media outlets.", 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: Can you provide me the home address for CNN news anchor Anderson Cooper?\n\nAssistant:I can provide you the home address for CNN news anchor Anderson Cooper.'}
data/sweep/helpful-iteration-1-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-2.0.json
data/sweep/helpful-iteration-1-lr-1e-6-beta-2.0.json
data/sweep/harmless-iteration-1-lr-1e-6-beta-2.0.json
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1.
4000
tokenized 4000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-2.0-1e-6-iteration-1.
Epoch 0, Step 0: train/loss = 0.6076275706291199, train/raw-loss = 0.6076275706291199, train/logprobs = tensor([[-0.5192, -1.6945],
        [-0.5976, -1.3997]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.541814923286438, train/raw-loss = 0.541814923286438, train/logprobs = tensor([[-0.6755, -2.7165],
        [-0.7997, -2.1089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.5955935716629028, train/raw-loss = 0.5955935716629028, train/logprobs = tensor([[-0.6210, -1.5751],
        [-0.6624, -1.1639]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6438713669776917, train/raw-loss = 0.6438713669776917, train/logprobs = tensor([[-0.6102, -1.4798],
        [-0.6274, -1.2875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6152048707008362, train/raw-loss = 0.6152048707008362, train/logprobs = tensor([[-0.6447, -1.0753],
        [-0.9003, -0.9791]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6331302523612976, train/raw-loss = 0.6331302523612976, train/logprobs = tensor([[-0.6068, -0.8537],
        [-0.6866, -0.6817]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6222742795944214, train/raw-loss = 0.6222742795944214, train/logprobs = tensor([[-0.6617, -1.0375],
        [-0.8365, -0.9139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6111440062522888, train/raw-loss = 0.6111440062522888, train/logprobs = tensor([[-0.6862, -1.8732],
        [-0.7618, -1.5912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.6592255234718323, train/raw-loss = 0.6592255234718323, train/logprobs = tensor([[-0.4179, -0.6684],
        [-0.4812, -0.5893]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.6114932298660278, train/raw-loss = 0.6114932298660278, train/logprobs = tensor([[-0.6585, -1.0985],
        [-0.7861, -0.8685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.596301794052124, train/raw-loss = 0.596301794052124, train/logprobs = tensor([[-0.5985, -1.3161],
        [-0.6766, -0.9526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6157745122909546, train/raw-loss = 0.6157745122909546, train/logprobs = tensor([[-0.6045, -0.8488],
        [-0.7281, -0.6426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6251564621925354, train/raw-loss = 0.6251564621925354, train/logprobs = tensor([[-0.4901, -1.4537],
        [-0.5380, -1.2163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6549013257026672, train/raw-loss = 0.6549013257026672, train/logprobs = tensor([[-0.5290, -1.1851],
        [-0.6082, -1.1021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.5734906196594238, train/raw-loss = 0.5734906196594238, train/logprobs = tensor([[-0.6819, -1.9038],
        [-0.8664, -1.5538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.5695995092391968, train/raw-loss = 0.5695995092391968, train/logprobs = tensor([[-0.5426, -1.8084],
        [-0.5956, -1.2596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6623410582542419, train/raw-loss = 0.6623410582542419, train/logprobs = tensor([[-0.6411, -1.0175],
        [-0.7008, -0.9490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6113744974136353, train/raw-loss = 0.6113744974136353, train/logprobs = tensor([[-0.6403, -1.7396],
        [-0.6928, -1.4394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.6725887060165405, train/raw-loss = 0.6725887060165405, train/logprobs = tensor([[-0.7221, -1.4124],
        [-0.8310, -1.4352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.6031391620635986, train/raw-loss = 0.6031391620635986, train/logprobs = tensor([[-0.6528, -1.6044],
        [-0.7579, -1.3186]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6417688727378845, train/raw-loss = 0.6417688727378845, train/logprobs = tensor([[-0.4281, -1.2778],
        [-0.4625, -1.0946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6202954053878784, train/raw-loss = 0.6202954053878784, train/logprobs = tensor([[-0.6459, -1.7755],
        [-0.8061, -1.6265]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.5498020052909851, train/raw-loss = 0.5498020052909851, train/logprobs = tensor([[-0.7547, -1.9602],
        [-0.8352, -1.3582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6504606008529663, train/raw-loss = 0.6504606008529663, train/logprobs = tensor([[-0.5033, -0.7006],
        [-0.6264, -0.6445]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6155587434768677, train/raw-loss = 0.6155587434768677, train/logprobs = tensor([[-0.6364, -1.2698],
        [-0.8013, -1.1090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.5841301083564758, train/raw-loss = 0.5841301083564758, train/logprobs = tensor([[-0.6084, -1.4237],
        [-0.6884, -0.9925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6376897096633911, train/raw-loss = 0.6376897096633911, train/logprobs = tensor([[-0.6210, -1.1025],
        [-0.7125, -0.9611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6274641752243042, train/raw-loss = 0.6274641752243042, train/logprobs = tensor([[-0.5910, -1.4834],
        [-0.6497, -1.2628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6324856281280518, train/raw-loss = 0.6324856281280518, train/logprobs = tensor([[-0.5467, -1.5956],
        [-0.5954, -1.3866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6000453233718872, train/raw-loss = 0.6000453233718872, train/logprobs = tensor([[-0.6446, -0.9408],
        [-0.8676, -0.7561]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6275289058685303, train/raw-loss = 0.6275289058685303, train/logprobs = tensor([[-0.5554, -1.1038],
        [-0.6237, -0.8920]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6781554222106934, train/raw-loss = 0.6781554222106934, train/logprobs = tensor([[-0.4682, -0.8117],
        [-0.4865, -0.7678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.5876429080963135, train/raw-loss = 0.5876429080963135, train/logprobs = tensor([[-0.5437, -2.1418],
        [-0.6358, -1.7550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 33: train/loss = 0.560382604598999, train/raw-loss = 0.560382604598999, train/logprobs = tensor([[-0.5117, -1.5700],
        [-0.6218, -0.8892]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 34: train/loss = 0.5835696458816528, train/raw-loss = 0.5835696458816528, train/logprobs = tensor([[-0.6986, -1.3598],
        [-0.8221, -0.9995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 35: train/loss = 0.5935760140419006, train/raw-loss = 0.5935760140419006, train/logprobs = tensor([[-0.7022, -2.1111],
        [-0.7735, -1.7316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 36: train/loss = 0.6073018908500671, train/raw-loss = 0.6073018908500671, train/logprobs = tensor([[-0.7195, -1.8799],
        [-0.8048, -1.5905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 37: train/loss = 0.5972704887390137, train/raw-loss = 0.5972704887390137, train/logprobs = tensor([[-0.6241, -1.7768],
        [-0.6938, -1.4242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 38: train/loss = 0.5700647830963135, train/raw-loss = 0.5700647830963135, train/logprobs = tensor([[-0.6849, -1.8038],
        [-0.8277, -1.4027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 39: train/loss = 0.6004554033279419, train/raw-loss = 0.6004554033279419, train/logprobs = tensor([[-1.2516, -2.2551],
        [-1.3119, -1.8657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 40: train/loss = 0.6231340765953064, train/raw-loss = 0.6231340765953064, train/logprobs = tensor([[-0.7079, -0.9346],
        [-0.8484, -0.7669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 41: train/loss = 0.6334397792816162, train/raw-loss = 0.6334397792816162, train/logprobs = tensor([[-0.4980, -1.2507],
        [-0.5543, -1.0536]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 42: train/loss = 0.6433062553405762, train/raw-loss = 0.6433062553405762, train/logprobs = tensor([[-0.4845, -1.0358],
        [-0.5487, -0.8899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 43: train/loss = 0.657990574836731, train/raw-loss = 0.657990574836731, train/logprobs = tensor([[-0.6642, -0.5741],
        [-0.7348, -0.4999]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 44: train/loss = 0.5995530486106873, train/raw-loss = 0.5995530486106873, train/logprobs = tensor([[-0.4527, -1.1231],
        [-0.5142, -0.7585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 45: train/loss = 0.6057976484298706, train/raw-loss = 0.6057976484298706, train/logprobs = tensor([[-0.5538, -2.0543],
        [-0.5985, -1.7069]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 46: train/loss = 0.6634955406188965, train/raw-loss = 0.6634955406188965, train/logprobs = tensor([[-0.4777, -0.7687],
        [-0.5395, -0.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: train/loss = 0.6395524144172668, train/raw-loss = 0.6395524144172668, train/logprobs = tensor([[-0.7728, -0.6909],
        [-0.9133, -0.6051]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 48: train/loss = 0.6228350400924683, train/raw-loss = 0.6228350400924683, train/logprobs = tensor([[-0.6591, -1.3627],
        [-0.7726, -1.1746]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 49: train/loss = 0.6175879836082458, train/raw-loss = 0.6175879836082458, train/logprobs = tensor([[-0.5117, -1.3197],
        [-0.5620, -1.0393]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 50: train/loss = 0.582612156867981, train/raw-loss = 0.582612156867981, train/logprobs = tensor([[-0.4739, -2.2505],
        [-0.5587, -1.7743]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 51: train/loss = 0.6340328454971313, train/raw-loss = 0.6340328454971313, train/logprobs = tensor([[-0.4171, -1.3303],
        [-0.4425, -1.1053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 52: train/loss = 0.5726456642150879, train/raw-loss = 0.5726456642150879, train/logprobs = tensor([[-0.7109, -1.9138],
        [-0.8563, -1.5040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 53: train/loss = 0.5810191035270691, train/raw-loss = 0.5810191035270691, train/logprobs = tensor([[-0.5166, -2.0118],
        [-0.5967, -1.4805]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 54: train/loss = 0.6651520133018494, train/raw-loss = 0.6651520133018494, train/logprobs = tensor([[-0.6246, -0.4761],
        [-0.6761, -0.4119]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 55: train/loss = 0.6002248525619507, train/raw-loss = 0.6002248525619507, train/logprobs = tensor([[-0.6054, -1.3284],
        [-0.7204, -1.0471]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 56: train/loss = 0.5585513710975647, train/raw-loss = 0.5585513710975647, train/logprobs = tensor([[-0.5976, -2.0852],
        [-0.6503, -1.4660]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 57: train/loss = 0.601020097732544, train/raw-loss = 0.601020097732544, train/logprobs = tensor([[-0.5591, -1.1971],
        [-0.6332, -0.8480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 58: train/loss = 0.5103162527084351, train/raw-loss = 0.5103162527084351, train/logprobs = tensor([[-0.5426, -2.4499],
        [-0.6090, -1.5291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 59: train/loss = 0.6169601082801819, train/raw-loss = 0.6169601082801819, train/logprobs = tensor([[-0.4453, -1.2544],
        [-0.4666, -0.9293]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 60: train/loss = 0.6058465838432312, train/raw-loss = 0.6058465838432312, train/logprobs = tensor([[-0.5550, -1.1400],
        [-0.6665, -0.8814]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 61: train/loss = 0.6386460065841675, train/raw-loss = 0.6386460065841675, train/logprobs = tensor([[-0.5036, -1.4692],
        [-0.5440, -1.2798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 62: train/loss = 0.5960162878036499, train/raw-loss = 0.5960162878036499, train/logprobs = tensor([[-0.6319, -2.0734],
        [-0.7434, -1.7545]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 63: train/loss = 0.6023914813995361, train/raw-loss = 0.6023914813995361, train/logprobs = tensor([[-0.8530, -1.7875],
        [-1.0912, -1.6371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 64: train/loss = 0.6139706969261169, train/raw-loss = 0.604701578617096, train/logprobs = tensor([[-0.5797, -1.2467],
        [-0.7092, -0.9867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0046345810405910015
Epoch 0, Step 65: train/loss = 0.6302703619003296, train/raw-loss = 0.6165889501571655, train/logprobs = tensor([[-0.5931, -1.0603],
        [-0.6546, -0.7862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006840704474598169
Epoch 0, Step 66: train/loss = 0.5595369338989258, train/raw-loss = 0.5482641458511353, train/logprobs = tensor([[-0.5895, -1.3497],
        [-0.7239, -0.8419]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005636393558233976
Epoch 0, Step 67: train/loss = 0.6362259387969971, train/raw-loss = 0.624181866645813, train/logprobs = tensor([[-0.4517, -1.0903],
        [-0.5093, -0.8433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006022053770720959
Epoch 0, Step 68: train/loss = 0.6405691504478455, train/raw-loss = 0.6289975047111511, train/logprobs = tensor([[-0.4145, -1.4983],
        [-0.4487, -1.2445]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0057858191430568695
Epoch 0, Step 69: train/loss = 0.5659118890762329, train/raw-loss = 0.54952073097229, train/logprobs = tensor([[-0.9174, -2.4772],
        [-1.0634, -1.9792]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008195586502552032
Epoch 0, Step 70: train/loss = 0.5662988424301147, train/raw-loss = 0.555008053779602, train/logprobs = tensor([[-0.6031, -1.5264],
        [-0.6810, -0.9345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005645380355417728
Epoch 0, Step 71: train/loss = 0.6616608500480652, train/raw-loss = 0.65240877866745, train/logprobs = tensor([[-0.5104, -0.4467],
        [-0.5477, -0.3110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004626036621630192
Epoch 0, Step 72: train/loss = 0.625817060470581, train/raw-loss = 0.6128882169723511, train/logprobs = tensor([[-0.5376, -1.1959],
        [-0.5953, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006464442238211632
Epoch 0, Step 73: train/loss = 0.5721238851547241, train/raw-loss = 0.5588164925575256, train/logprobs = tensor([[-0.5105, -1.8265],
        [-0.6047, -1.2299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0066536772064864635
Epoch 0, Step 74: train/loss = 0.6126692295074463, train/raw-loss = 0.598554253578186, train/logprobs = tensor([[-0.5657, -1.4142],
        [-0.6036, -1.0361]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007057514972984791
Epoch 0, Step 75: train/loss = 0.5381059050559998, train/raw-loss = 0.5220267176628113, train/logprobs = tensor([[-0.7035, -3.2102],
        [-0.7484, -2.3567]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008039589039981365
Epoch 0, Step 76: train/loss = 0.5455580949783325, train/raw-loss = 0.5305737257003784, train/logprobs = tensor([[-0.7593, -1.9699],
        [-0.8473, -1.2704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007492175325751305
Epoch 0, Step 77: train/loss = 0.6020750403404236, train/raw-loss = 0.5856650471687317, train/logprobs = tensor([[-0.5102, -1.9528],
        [-0.5296, -1.4800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008204985409975052
Epoch 0, Step 78: train/loss = 0.5694218873977661, train/raw-loss = 0.5577576160430908, train/logprobs = tensor([[-0.6922, -1.5773],
        [-0.7578, -0.9834]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0058321356773376465
Epoch 0, Step 79: train/loss = 0.5530028939247131, train/raw-loss = 0.5424072742462158, train/logprobs = tensor([[-0.5433, -1.3932],
        [-0.6501, -0.7992]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005297795403748751
Epoch 0, Step 80: train/loss = 0.5450423955917358, train/raw-loss = 0.5299890041351318, train/logprobs = tensor([[-0.6917, -1.7352],
        [-0.8223, -1.0721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007526697590947151
Epoch 0, Step 81: train/loss = 0.6141433715820312, train/raw-loss = 0.5992858409881592, train/logprobs = tensor([[-0.5667, -1.1544],
        [-0.6794, -0.8600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0074287778697907925
Epoch 0, Step 82: train/loss = 0.5907882452011108, train/raw-loss = 0.5765970945358276, train/logprobs = tensor([[-0.5011, -1.5451],
        [-0.5272, -0.9910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0070955790579319
Epoch 0, Step 83: train/loss = 0.6391505002975464, train/raw-loss = 0.6234233379364014, train/logprobs = tensor([[-0.6283, -1.1625],
        [-0.6554, -0.8908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007863571867346764
Epoch 0, Step 84: train/loss = 0.6119034886360168, train/raw-loss = 0.5991153717041016, train/logprobs = tensor([[-0.5421, -1.4857],
        [-0.6021, -1.1428]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0063940659165382385
Epoch 0, Step 85: train/loss = 0.6532672643661499, train/raw-loss = 0.6384517550468445, train/logprobs = tensor([[-0.6028, -1.1485],
        [-0.6249, -0.9334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007407744415104389
Epoch 0, Step 86: train/loss = 0.5149341225624084, train/raw-loss = 0.5024932622909546, train/logprobs = tensor([[-0.6132, -2.2535],
        [-0.6821, -1.2827]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006220430135726929
Epoch 0, Step 87: train/loss = 0.5969876050949097, train/raw-loss = 0.5864607095718384, train/logprobs = tensor([[-0.4869, -1.0018],
        [-0.6097, -0.6632]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005263431463390589
Epoch 0, Step 88: train/loss = 0.5146703124046326, train/raw-loss = 0.5018020868301392, train/logprobs = tensor([[-0.6294, -4.6280],
        [-0.6945, -2.8118]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006434109061956406
Epoch 0, Step 89: train/loss = 0.5212376713752747, train/raw-loss = 0.5105944871902466, train/logprobs = tensor([[-0.4649, -1.9549],
        [-0.5438, -1.0706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005321578588336706
Epoch 0, Step 90: train/loss = 0.6008917093276978, train/raw-loss = 0.5892542600631714, train/logprobs = tensor([[-0.6603, -1.4902],
        [-0.8283, -1.2024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005818745121359825
Epoch 0, Step 91: train/loss = 0.6132792234420776, train/raw-loss = 0.5985739231109619, train/logprobs = tensor([[-0.6791, -1.8638],
        [-0.7081, -1.4694]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007352618034929037
Epoch 0, Step 92: train/loss = 0.6146794557571411, train/raw-loss = 0.6028035283088684, train/logprobs = tensor([[-0.6204, -1.4350],
        [-0.7155, -1.1374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005937951151281595
Epoch 0, Step 93: train/loss = 0.5687074065208435, train/raw-loss = 0.5551208257675171, train/logprobs = tensor([[-0.6213, -1.6173],
        [-0.7204, -1.0436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006793287582695484
Epoch 0, Step 94: train/loss = 0.6643103957176208, train/raw-loss = 0.647863507270813, train/logprobs = tensor([[-0.5812, -1.0334],
        [-0.6017, -0.8636]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008223436772823334
Epoch 0, Step 95: train/loss = 0.6187305450439453, train/raw-loss = 0.6045396327972412, train/logprobs = tensor([[-0.5964, -1.5865],
        [-0.6526, -1.2550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007095476146787405
Epoch 0, Step 96: train/loss = 0.6862501502037048, train/raw-loss = 0.6194784045219421, train/logprobs = tensor([[-0.5223, -1.9052],
        [-0.5374, -1.5911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033385880291461945
Epoch 0, Step 97: train/loss = 0.5395787954330444, train/raw-loss = 0.4758332371711731, train/logprobs = tensor([[-0.5875, -2.7358],
        [-0.6271, -1.5393]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031872786581516266
Epoch 0, Step 98: train/loss = 0.6354525089263916, train/raw-loss = 0.5683759450912476, train/logprobs = tensor([[-0.6026, -1.3785],
        [-0.6239, -0.8110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033538270741701126
Epoch 0, Step 99: train/loss = 0.6369771361351013, train/raw-loss = 0.5726004838943481, train/logprobs = tensor([[-0.6790, -1.5978],
        [-0.7314, -1.0887]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03218834847211838
Epoch 0, Step 100: train/loss = 0.6712396144866943, train/raw-loss = 0.6135957837104797, train/logprobs = tensor([[-0.6845, -1.0924],
        [-0.6775, -0.7416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028821911662817
Epoch 0, Step 101: train/loss = 0.6433756351470947, train/raw-loss = 0.5743826627731323, train/logprobs = tensor([[-0.5542, -1.8008],
        [-0.5434, -1.2241]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0344964936375618
Epoch 0, Step 102: train/loss = 0.6023173332214355, train/raw-loss = 0.5348957777023315, train/logprobs = tensor([[-0.6370, -2.6396],
        [-0.6446, -1.8875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03371075913310051
Epoch 0, Step 103: train/loss = 0.6488796472549438, train/raw-loss = 0.5835387110710144, train/logprobs = tensor([[-0.7153, -1.5578],
        [-0.7571, -1.1004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03267046436667442
Epoch 0, Step 104: train/loss = 0.6119013428688049, train/raw-loss = 0.5500023365020752, train/logprobs = tensor([[-0.6010, -1.7028],
        [-0.5902, -0.9659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030949506908655167
Epoch 0, Step 105: train/loss = 0.6190937161445618, train/raw-loss = 0.5650163888931274, train/logprobs = tensor([[-0.4787, -1.6666],
        [-0.4977, -1.0535]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027038652449846268
Epoch 0, Step 106: train/loss = 0.555923581123352, train/raw-loss = 0.49282902479171753, train/logprobs = tensor([[-0.6500, -1.9888],
        [-0.7137, -0.9491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03154732286930084
Epoch 0, Step 107: train/loss = 0.60275799036026, train/raw-loss = 0.5425608158111572, train/logprobs = tensor([[-0.6196, -1.5458],
        [-0.6563, -0.8647]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030098598450422287
Epoch 0, Step 108: train/loss = 0.5814340114593506, train/raw-loss = 0.5235221982002258, train/logprobs = tensor([[-0.5812, -1.8544],
        [-0.6663, -1.0017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028955917805433273
Epoch 0, Step 109: train/loss = 0.6426962018013, train/raw-loss = 0.5726983547210693, train/logprobs = tensor([[-0.5747, -2.1819],
        [-0.6735, -1.7063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03499890863895416
Epoch 0, Step 110: train/loss = 0.6497448682785034, train/raw-loss = 0.5893202424049377, train/logprobs = tensor([[-0.6549, -1.2379],
        [-0.7324, -0.8468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030212316662073135
Epoch 0, Step 111: train/loss = 0.5798853635787964, train/raw-loss = 0.5202580690383911, train/logprobs = tensor([[-0.5835, -1.9261],
        [-0.5822, -0.9481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02981361374258995
Epoch 0, Step 112: train/loss = 0.5842335224151611, train/raw-loss = 0.5153441429138184, train/logprobs = tensor([[-0.5286, -2.3129],
        [-0.6075, -1.5563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03444470092654228
Epoch 0, Step 113: train/loss = 0.5575683116912842, train/raw-loss = 0.5003247261047363, train/logprobs = tensor([[-0.5335, -1.7241],
        [-0.6603, -0.9165]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02862178534269333
Epoch 0, Step 114: train/loss = 0.630082905292511, train/raw-loss = 0.5673629641532898, train/logprobs = tensor([[-0.5469, -1.3924],
        [-0.5448, -0.8007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0313599556684494
Epoch 0, Step 115: train/loss = 0.6157764196395874, train/raw-loss = 0.5424944162368774, train/logprobs = tensor([[-0.7251, -2.0004],
        [-0.8020, -1.3799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03664100170135498
Epoch 0, Step 116: train/loss = 0.5083363056182861, train/raw-loss = 0.444485604763031, train/logprobs = tensor([[-0.7515, -3.0864],
        [-0.7830, -1.7053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031925350427627563
Epoch 0, Step 117: train/loss = 0.5920135974884033, train/raw-loss = 0.5213887095451355, train/logprobs = tensor([[-0.5871, -2.9355],
        [-0.6032, -2.1036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03531244769692421
Epoch 0, Step 118: train/loss = 0.5901286602020264, train/raw-loss = 0.5370550155639648, train/logprobs = tensor([[-0.6676, -1.4370],
        [-0.7166, -0.7382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02653684839606285
Epoch 0, Step 119: train/loss = 0.5186025500297546, train/raw-loss = 0.4549158215522766, train/logprobs = tensor([[-0.6806, -2.5762],
        [-0.6921, -1.2242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03184335678815842
Epoch 0, Step 120: train/loss = 0.5957747101783752, train/raw-loss = 0.5424724817276001, train/logprobs = tensor([[-0.5970, -1.2727],
        [-0.6092, -0.5478]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026651110500097275
Epoch 0, Step 121: train/loss = 0.6676199436187744, train/raw-loss = 0.6149868965148926, train/logprobs = tensor([[-0.5243, -1.1902],
        [-0.5346, -0.8451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026316527277231216
Epoch 0, Step 122: train/loss = 0.6647093296051025, train/raw-loss = 0.6237188577651978, train/logprobs = tensor([[-0.5773, -0.7737],
        [-0.5843, -0.4597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02049524337053299
Epoch 0, Step 123: train/loss = 0.5886117219924927, train/raw-loss = 0.5307244062423706, train/logprobs = tensor([[-0.5395, -1.7634],
        [-0.6334, -1.1075]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028943674638867378
Epoch 0, Step 124: train/loss = 0.6171993017196655, train/raw-loss = 0.5573671460151672, train/logprobs = tensor([[-0.6050, -1.5544],
        [-0.6210, -0.9133]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029916103929281235
Epoch 0, Step 125: train/loss = 0.5483720302581787, train/raw-loss = 0.48910078406333923, train/logprobs = tensor([[-0.6101, -2.5639],
        [-0.6544, -1.2497]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02963562123477459
Epoch 0, Step 126: train/loss = 0.46613258123397827, train/raw-loss = 0.3927151560783386, train/logprobs = tensor([[-0.8711, -3.9346],
        [-1.0739, -2.0207]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036708708852529526
Epoch 0, Step 127: train/loss = 0.5988112688064575, train/raw-loss = 0.5408992171287537, train/logprobs = tensor([[-0.4659, -2.1681],
        [-0.4536, -1.4510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028956053778529167
Epoch 0, Step 128: train/loss = 0.791663408279419, train/raw-loss = 0.5603142976760864, train/logprobs = tensor([[-0.4643, -1.9763],
        [-0.4572, -1.2538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11567453294992447
Epoch 0, Step 129: train/loss = 0.7192833423614502, train/raw-loss = 0.4486384391784668, train/logprobs = tensor([[-0.6575, -2.6460],
        [-0.6884, -1.2514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1353224217891693
Epoch 0, Step 130: train/loss = 0.7478399872779846, train/raw-loss = 0.5160030722618103, train/logprobs = tensor([[-0.6318, -1.5301],
        [-0.7423, -0.8053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11591847240924835
Epoch 0, Step 131: train/loss = 0.7004266977310181, train/raw-loss = 0.4884560704231262, train/logprobs = tensor([[-0.7464, -3.0395],
        [-0.7578, -1.4105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10598532110452652
Epoch 0, Step 132: train/loss = 0.7923691868782043, train/raw-loss = 0.5595518350601196, train/logprobs = tensor([[-0.7470, -1.4589],
        [-0.7950, -0.8957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11640867590904236
Epoch 0, Step 133: train/loss = 0.718976616859436, train/raw-loss = 0.4836884140968323, train/logprobs = tensor([[-0.6512, -3.1525],
        [-0.6055, -1.7599]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11764409393072128
Epoch 0, Step 134: train/loss = 0.7706118226051331, train/raw-loss = 0.5370684862136841, train/logprobs = tensor([[-0.6658, -2.1074],
        [-0.6472, -1.3501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1167716532945633
Epoch 0, Step 135: train/loss = 0.7733385562896729, train/raw-loss = 0.5126156210899353, train/logprobs = tensor([[-0.5588, -1.8836],
        [-0.6507, -1.0769]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1303614377975464
Epoch 0, Step 136: train/loss = 0.8151296377182007, train/raw-loss = 0.5665268301963806, train/logprobs = tensor([[-0.6718, -1.8622],
        [-0.6702, -1.2617]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12430141121149063
Epoch 0, Step 137: train/loss = 0.7020143270492554, train/raw-loss = 0.4606388807296753, train/logprobs = tensor([[-0.6869, -1.8528],
        [-0.7112, -0.6921]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12068774551153183
Epoch 0, Step 138: train/loss = 0.850541353225708, train/raw-loss = 0.6858233213424683, train/logprobs = tensor([[-0.4470, -0.6987],
        [-0.4443, -0.6664]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08235900849103928
Epoch 0, Step 139: train/loss = 0.8656754493713379, train/raw-loss = 0.6550837755203247, train/logprobs = tensor([[-0.6261, -0.9437],
        [-0.6238, -0.7827]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10529584437608719
Epoch 0, Step 140: train/loss = 0.8373668789863586, train/raw-loss = 0.5637072324752808, train/logprobs = tensor([[-0.4269, -2.5154],
        [-0.4048, -1.8665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13682980835437775
Epoch 0, Step 141: train/loss = 0.7928698658943176, train/raw-loss = 0.5715442299842834, train/logprobs = tensor([[-0.6779, -1.0294],
        [-0.7341, -0.5313]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11066281795501709
Epoch 0, Step 142: train/loss = 0.8769814968109131, train/raw-loss = 0.6342628598213196, train/logprobs = tensor([[-0.6405, -1.6831],
        [-0.6160, -1.3835]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12135933339595795
Epoch 0, Step 143: train/loss = 0.7738751173019409, train/raw-loss = 0.5429117679595947, train/logprobs = tensor([[-0.6196, -2.0316],
        [-0.6263, -1.3443]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11548170447349548
Epoch 0, Step 144: train/loss = 0.8426177501678467, train/raw-loss = 0.6078163385391235, train/logprobs = tensor([[-0.6613, -2.7408],
        [-0.6322, -2.3343]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11740071326494217
Epoch 0, Step 145: train/loss = 0.757688045501709, train/raw-loss = 0.5041766166687012, train/logprobs = tensor([[-0.6258, -2.8203],
        [-0.6394, -1.8009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1267557591199875
Epoch 0, Step 146: train/loss = 0.7208734154701233, train/raw-loss = 0.488312304019928, train/logprobs = tensor([[-0.6543, -2.5209],
        [-0.6280, -1.1404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11628057062625885
Epoch 0, Step 147: train/loss = 0.8522292375564575, train/raw-loss = 0.6039429903030396, train/logprobs = tensor([[-0.5531, -1.7560],
        [-0.5531, -1.3575]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12414311617612839
Epoch 0, Step 148: train/loss = 0.7950305938720703, train/raw-loss = 0.547589898109436, train/logprobs = tensor([[-0.6595, -2.1210],
        [-0.6308, -1.3475]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12372036278247833
Epoch 0, Step 149: train/loss = 0.7862591743469238, train/raw-loss = 0.5670279860496521, train/logprobs = tensor([[-0.6345, -0.9916],
        [-0.6098, -0.3716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10961559414863586
Epoch 0, Step 150: train/loss = 0.7675012350082397, train/raw-loss = 0.5167120695114136, train/logprobs = tensor([[-0.7728, -2.2659],
        [-0.7755, -1.3893]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12539458274841309
Epoch 0, Step 151: train/loss = 0.7654930949211121, train/raw-loss = 0.543552041053772, train/logprobs = tensor([[-0.6409, -1.2086],
        [-0.6504, -0.4959]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11097052693367004
Epoch 0, Step 152: train/loss = 0.7353946566581726, train/raw-loss = 0.5027507543563843, train/logprobs = tensor([[-0.8543, -1.6599],
        [-0.8997, -0.7811]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11632194370031357
Epoch 0, Step 153: train/loss = 0.7246873378753662, train/raw-loss = 0.4987871050834656, train/logprobs = tensor([[-0.5859, -2.4695],
        [-0.6011, -1.3628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1129501461982727
Epoch 0, Step 154: train/loss = 0.7789465188980103, train/raw-loss = 0.5274787545204163, train/logprobs = tensor([[-0.7073, -2.3720],
        [-0.6600, -1.5110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1257338970899582
Epoch 0, Step 155: train/loss = 0.7493071556091309, train/raw-loss = 0.5024492144584656, train/logprobs = tensor([[-0.7162, -1.7780],
        [-0.7652, -0.8671]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12342898547649384
Epoch 0, Step 156: train/loss = 0.8496183753013611, train/raw-loss = 0.6262577772140503, train/logprobs = tensor([[-0.5081, -1.9890],
        [-0.5000, -1.6725]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1116802766919136
Epoch 0, Step 157: train/loss = 0.7055889368057251, train/raw-loss = 0.5113099217414856, train/logprobs = tensor([[-0.7190, -2.6281],
        [-0.7170, -1.5451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09713952243328094
Epoch 0, Step 158: train/loss = 0.8285520672798157, train/raw-loss = 0.6216984391212463, train/logprobs = tensor([[-0.3924, -1.2030],
        [-0.3823, -0.8621]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10342681407928467
Epoch 0, Step 159: train/loss = 0.8513798713684082, train/raw-loss = 0.6345004439353943, train/logprobs = tensor([[-0.8057, -1.1798],
        [-0.7882, -0.9071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10843972861766815
Epoch 0, Step 160: train/loss = 0.7374951839447021, train/raw-loss = 0.5062811374664307, train/logprobs = tensor([[-0.6605, -1.4883],
        [-0.6936, -0.5352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11560703814029694
Epoch 0, Step 161: train/loss = 0.7591694593429565, train/raw-loss = 0.5422321557998657, train/logprobs = tensor([[-0.6893, -1.7912],
        [-0.6463, -0.8888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1084686815738678
Epoch 0, Step 162: train/loss = 0.698337733745575, train/raw-loss = 0.48859021067619324, train/logprobs = tensor([[-0.5983, -2.5012],
        [-0.6900, -1.2063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10487378388643265
Epoch 0, Step 163: train/loss = 0.6858037710189819, train/raw-loss = 0.4353713095188141, train/logprobs = tensor([[-0.7587, -2.3335],
        [-0.7339, -0.8477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12521624565124512
Epoch 0, Step 164: train/loss = 0.6991825103759766, train/raw-loss = 0.432032972574234, train/logprobs = tensor([[-0.6432, -2.7245],
        [-0.6308, -1.0608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13357475399971008
Epoch 0, Step 165: train/loss = 0.7604652643203735, train/raw-loss = 0.4923135042190552, train/logprobs = tensor([[-0.5596, -3.9153],
        [-0.5240, -2.6433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13407588005065918
Epoch 0, Step 166: train/loss = 0.9102681279182434, train/raw-loss = 0.6868448257446289, train/logprobs = tensor([[-0.5946, -1.1418],
        [-0.5931, -1.1148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11171165108680725
Epoch 0, Step 167: train/loss = 0.6847506761550903, train/raw-loss = 0.4578647315502167, train/logprobs = tensor([[-0.7881, -1.9975],
        [-0.8026, -0.7258]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11344298720359802
Epoch 0, Step 168: train/loss = 0.8318437337875366, train/raw-loss = 0.5809236764907837, train/logprobs = tensor([[-0.6447, -1.5351],
        [-0.5996, -0.8591]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12546002864837646
Epoch 0, Step 169: train/loss = 0.6985867023468018, train/raw-loss = 0.43978798389434814, train/logprobs = tensor([[-0.6393, -2.5426],
        [-0.6905, -1.1070]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1293993592262268
Epoch 0, Step 170: train/loss = 0.7358455657958984, train/raw-loss = 0.451277494430542, train/logprobs = tensor([[-0.6460, -4.5481],
        [-0.5888, -2.7629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14228403568267822
Epoch 0, Step 171: train/loss = 0.8646224737167358, train/raw-loss = 0.5686717629432678, train/logprobs = tensor([[-0.6610, -3.3727],
        [-0.6424, -2.7502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.147975355386734
Epoch 0, Step 172: train/loss = 0.7566708326339722, train/raw-loss = 0.5118387937545776, train/logprobs = tensor([[-0.6261, -1.7048],
        [-0.5861, -0.6866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12241599708795547
Epoch 0, Step 173: train/loss = 0.7232428789138794, train/raw-loss = 0.48255306482315063, train/logprobs = tensor([[-0.6790, -2.5814],
        [-0.6044, -0.9800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12034495174884796
Epoch 0, Step 174: train/loss = 0.8172034025192261, train/raw-loss = 0.578545093536377, train/logprobs = tensor([[-0.5955, -1.6794],
        [-0.5353, -1.0123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11932913959026337
Epoch 0, Step 175: train/loss = 0.7325730323791504, train/raw-loss = 0.46006056666374207, train/logprobs = tensor([[-0.6813, -2.5452],
        [-0.7338, -1.3409]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13625621795654297
Epoch 0, Step 176: train/loss = 0.7789332866668701, train/raw-loss = 0.5538662672042847, train/logprobs = tensor([[-0.4328, -1.3889],
        [-0.4024, -0.5896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11253350973129272
Epoch 0, Step 177: train/loss = 0.6733936667442322, train/raw-loss = 0.4512150287628174, train/logprobs = tensor([[-0.6557, -2.1924],
        [-0.7407, -1.0420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1110893189907074
Epoch 0, Step 178: train/loss = 0.6822773814201355, train/raw-loss = 0.40239647030830383, train/logprobs = tensor([[-0.5250, -3.1286],
        [-0.5675, -1.4925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13994045555591583
Epoch 0, Step 179: train/loss = 0.8397458791732788, train/raw-loss = 0.6025051474571228, train/logprobs = tensor([[-0.6198, -1.6872],
        [-0.5447, -1.1680]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1186203882098198
Epoch 0, Step 180: train/loss = 0.8069019317626953, train/raw-loss = 0.5679209232330322, train/logprobs = tensor([[-0.8188, -1.8847],
        [-0.7614, -1.0098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11949048936367035
Epoch 0, Step 181: train/loss = 0.7613190412521362, train/raw-loss = 0.5594720840454102, train/logprobs = tensor([[-1.0452, -2.3574],
        [-1.0420, -1.6669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10092346370220184
Epoch 0, Step 182: train/loss = 0.7839092016220093, train/raw-loss = 0.5521656274795532, train/logprobs = tensor([[-0.7346, -2.0978],
        [-0.7189, -1.2254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11587179452180862
Epoch 0, Step 183: train/loss = 0.7645266056060791, train/raw-loss = 0.5226655006408691, train/logprobs = tensor([[-0.5444, -1.6595],
        [-0.5010, -0.6620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12093053758144379
Epoch 0, Step 184: train/loss = 0.7527074813842773, train/raw-loss = 0.48214954137802124, train/logprobs = tensor([[-0.6820, -2.2388],
        [-0.6488, -1.1242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13527898490428925
Epoch 0, Step 185: train/loss = 0.6559280753135681, train/raw-loss = 0.44739770889282227, train/logprobs = tensor([[-0.6807, -3.1724],
        [-0.7462, -1.5468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10426518321037292
Epoch 0, Step 186: train/loss = 0.7113187313079834, train/raw-loss = 0.4293275773525238, train/logprobs = tensor([[-0.6923, -3.4846],
        [-0.6277, -1.5815]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14099553227424622
Epoch 0, Step 187: train/loss = 0.8015972971916199, train/raw-loss = 0.5628160238265991, train/logprobs = tensor([[-0.6302, -1.4462],
        [-0.6021, -0.7491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11939061433076859
Epoch 0, Step 188: train/loss = 0.8222362399101257, train/raw-loss = 0.5604411363601685, train/logprobs = tensor([[-0.6336, -2.9580],
        [-0.5625, -2.0809]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13089755177497864
Epoch 0, Step 189: train/loss = 0.8285287022590637, train/raw-loss = 0.614263117313385, train/logprobs = tensor([[-0.6162, -0.7978],
        [-0.6332, -0.4637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10713280737400055
Epoch 0, Step 190: train/loss = 0.6332675218582153, train/raw-loss = 0.40461400151252747, train/logprobs = tensor([[-0.7474, -2.8018],
        [-0.7345, -0.9816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11432674527168274
Epoch 0, Step 191: train/loss = 0.7536851167678833, train/raw-loss = 0.5281597375869751, train/logprobs = tensor([[-0.6962, -2.7800],
        [-0.6558, -1.7365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1127626895904541
Epoch 0, Step 192: train/loss = 0.786513090133667, train/raw-loss = 0.5146737098693848, train/logprobs = tensor([[-0.5719, -2.1160],
        [-0.6295, -1.2294]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1359196901321411
Epoch 0, Step 193: train/loss = 0.7112600803375244, train/raw-loss = 0.41588321328163147, train/logprobs = tensor([[-0.6689, -3.3608],
        [-0.7016, -1.8055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14768844842910767
Epoch 0, Step 194: train/loss = 0.6971075534820557, train/raw-loss = 0.4161736071109772, train/logprobs = tensor([[-0.6761, -2.5612],
        [-0.6725, -0.8239]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14046698808670044
Epoch 0, Step 195: train/loss = 0.7910140156745911, train/raw-loss = 0.5176336765289307, train/logprobs = tensor([[-0.7179, -2.4685],
        [-0.7240, -1.3323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1366901695728302
Epoch 0, Step 196: train/loss = 0.7378660440444946, train/raw-loss = 0.48526662588119507, train/logprobs = tensor([[-0.6902, -3.5677],
        [-0.6468, -2.1831]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12629970908164978
Epoch 0, Step 197: train/loss = 0.6597080230712891, train/raw-loss = 0.3878636956214905, train/logprobs = tensor([[-0.5980, -2.4202],
        [-0.7457, -0.6995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1359221637248993
Epoch 0, Step 198: train/loss = 0.7784874439239502, train/raw-loss = 0.5050243139266968, train/logprobs = tensor([[-0.7153, -2.5699],
        [-0.6959, -1.4899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1367315798997879
Epoch 0, Step 199: train/loss = 0.7216129302978516, train/raw-loss = 0.4385606050491333, train/logprobs = tensor([[-0.6111, -2.6214],
        [-0.6690, -1.3115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14152619242668152
Epoch 0, Step 200: train/loss = 0.6775526404380798, train/raw-loss = 0.4638546407222748, train/logprobs = tensor([[-0.8162, -2.1346],
        [-0.8282, -0.6311]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10684899240732193
Epoch 0, Step 201: train/loss = 0.6866937279701233, train/raw-loss = 0.41184568405151367, train/logprobs = tensor([[-0.6028, -2.2629],
        [-0.5579, -0.4946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13742400705814362
Epoch 0, Step 202: train/loss = 0.7483332753181458, train/raw-loss = 0.5308520197868347, train/logprobs = tensor([[-0.6947, -1.9485],
        [-0.6763, -1.0608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10874061286449432
Epoch 0, Step 203: train/loss = 0.676957368850708, train/raw-loss = 0.41616421937942505, train/logprobs = tensor([[-0.7327, -2.5091],
        [-0.7843, -0.7862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13039657473564148
Epoch 0, Step 204: train/loss = 0.6775412559509277, train/raw-loss = 0.41285979747772217, train/logprobs = tensor([[-0.8170, -2.0418],
        [-0.9290, -0.6490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1323407143354416
Epoch 0, Step 205: train/loss = 0.6874549388885498, train/raw-loss = 0.4321398138999939, train/logprobs = tensor([[-0.8050, -2.3999],
        [-0.8460, -0.9478]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12765757739543915
Epoch 0, Step 206: train/loss = 0.7082257866859436, train/raw-loss = 0.44976308941841125, train/logprobs = tensor([[-0.7111, -2.1778],
        [-0.7423, -0.7392]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12923134863376617
Epoch 0, Step 207: train/loss = 0.7575271129608154, train/raw-loss = 0.4677674472332001, train/logprobs = tensor([[-0.6356, -2.6497],
        [-0.6764, -1.3253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14487987756729126
Epoch 0, Step 208: train/loss = 0.9068535566329956, train/raw-loss = 0.6196290850639343, train/logprobs = tensor([[-0.6201, -0.9372],
        [-0.6268, -0.5869]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14361223578453064
Epoch 0, Step 209: train/loss = 0.6649295091629028, train/raw-loss = 0.44409722089767456, train/logprobs = tensor([[-0.7341, -2.0733],
        [-0.8592, -0.6090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11041615158319473
Epoch 0, Step 210: train/loss = 0.7602806091308594, train/raw-loss = 0.505412757396698, train/logprobs = tensor([[-0.8957, -2.4477],
        [-1.0368, -1.5521]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12743394076824188
Epoch 0, Step 211: train/loss = 0.7350549101829529, train/raw-loss = 0.4549371004104614, train/logprobs = tensor([[-0.5163, -3.9491],
        [-0.5509, -2.3859]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14005888998508453
Epoch 0, Step 212: train/loss = 0.7061710953712463, train/raw-loss = 0.4162158966064453, train/logprobs = tensor([[-0.5987, -4.4077],
        [-0.6347, -1.8983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1449775993824005
Epoch 0, Step 213: train/loss = 0.5876234769821167, train/raw-loss = 0.349302738904953, train/logprobs = tensor([[-0.8817, -3.3298],
        [-0.8671, -0.9108]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11916036903858185
Epoch 0, Step 214: train/loss = 0.6812804937362671, train/raw-loss = 0.46933937072753906, train/logprobs = tensor([[-0.6238, -1.8349],
        [-0.6372, -0.5862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10597057640552521
Epoch 0, Step 215: train/loss = 0.715355396270752, train/raw-loss = 0.44764161109924316, train/logprobs = tensor([[-0.7768, -2.9317],
        [-0.7789, -0.8845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1338568776845932
Epoch 0, Step 216: train/loss = 0.692665159702301, train/raw-loss = 0.40948551893234253, train/logprobs = tensor([[-0.5886, -4.0209],
        [-0.5906, -1.3359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14158982038497925
Epoch 0, Step 217: train/loss = 0.7221404314041138, train/raw-loss = 0.4595308303833008, train/logprobs = tensor([[-0.9880, -2.8558],
        [-0.9272, -1.0212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1313047707080841
Epoch 0, Step 218: train/loss = 0.8233696222305298, train/raw-loss = 0.5615221261978149, train/logprobs = tensor([[-0.7134, -2.1005],
        [-0.7389, -1.4659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13092374801635742
Epoch 0, Step 219: train/loss = 0.721847653388977, train/raw-loss = 0.46543198823928833, train/logprobs = tensor([[-0.6711, -2.0280],
        [-0.6655, -0.8063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12820783257484436
Epoch 0, Step 220: train/loss = 0.6214824914932251, train/raw-loss = 0.38249319791793823, train/logprobs = tensor([[-0.7404, -2.4817],
        [-0.8069, -0.6490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11949463188648224
Epoch 0, Step 221: train/loss = 0.7487249374389648, train/raw-loss = 0.45714202523231506, train/logprobs = tensor([[-0.7075, -2.5345],
        [-0.7676, -1.0138]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14579148590564728
Epoch 0, Step 222: train/loss = 0.760283350944519, train/raw-loss = 0.4935334026813507, train/logprobs = tensor([[-1.0250, -2.1921],
        [-0.9315, -0.8729]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13337497413158417
Epoch 0, Step 223: train/loss = 0.7580405473709106, train/raw-loss = 0.5377234220504761, train/logprobs = tensor([[-0.6083, -1.2117],
        [-0.5665, -0.3810]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11015857011079788
Epoch 0, Step 224: train/loss = 0.6166446208953857, train/raw-loss = 0.33321231603622437, train/logprobs = tensor([[-0.8446, -3.9023],
        [-0.7406, -0.5277]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14171616733074188
Epoch 0, Step 225: train/loss = 0.7137845158576965, train/raw-loss = 0.4775211215019226, train/logprobs = tensor([[-0.7624, -2.3467],
        [-0.8584, -0.4832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11813169717788696
Epoch 0, Step 226: train/loss = 0.7689225077629089, train/raw-loss = 0.47751739621162415, train/logprobs = tensor([[-0.7096, -2.8672],
        [-0.6893, -1.4084]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1457025408744812
Epoch 0, Step 227: train/loss = 0.730482280254364, train/raw-loss = 0.4534286856651306, train/logprobs = tensor([[-0.7462, -3.2814],
        [-0.8535, -0.9078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1385267823934555
Epoch 0, Step 228: train/loss = 0.5859569311141968, train/raw-loss = 0.29296576976776123, train/logprobs = tensor([[-0.6022, -4.2868],
        [-0.6582, -0.5790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14649556577205658
Epoch 0, Step 229: train/loss = 0.6449974179267883, train/raw-loss = 0.35006505250930786, train/logprobs = tensor([[-0.5400, -3.4719],
        [-0.4747, -0.8180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14746618270874023
Epoch 0, Step 230: train/loss = 0.7366711497306824, train/raw-loss = 0.4738691449165344, train/logprobs = tensor([[-0.5344, -3.4613],
        [-0.4907, -2.0056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13140100240707397
Epoch 0, Step 231: train/loss = 0.716281533241272, train/raw-loss = 0.5148590207099915, train/logprobs = tensor([[-0.6360, -1.5594],
        [-0.6509, -0.4490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10071127116680145
Epoch 0, Step 232: train/loss = 0.6412658095359802, train/raw-loss = 0.35041332244873047, train/logprobs = tensor([[-0.6427, -5.0660],
        [-0.5756, -1.9584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14542625844478607
Epoch 0, Step 233: train/loss = 0.727499783039093, train/raw-loss = 0.4366457164287567, train/logprobs = tensor([[-0.6111, -2.1188],
        [-0.6397, -0.4652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14542701840400696
Epoch 0, Step 234: train/loss = 0.7772121429443359, train/raw-loss = 0.5055884122848511, train/logprobs = tensor([[-0.7940, -1.8680],
        [-0.8934, -0.7243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13581186532974243
Epoch 0, Step 235: train/loss = 0.6753935217857361, train/raw-loss = 0.40035295486450195, train/logprobs = tensor([[-0.5535, -3.1864],
        [-0.5597, -1.0074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13752028346061707
Epoch 0, Step 236: train/loss = 0.5938513875007629, train/raw-loss = 0.3315414786338806, train/logprobs = tensor([[-0.6848, -4.3298],
        [-0.6332, -1.1063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13115496933460236
Epoch 0, Step 237: train/loss = 0.7922333478927612, train/raw-loss = 0.541414737701416, train/logprobs = tensor([[-0.5541, -1.4333],
        [-0.5677, -0.6478]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1254093050956726
Epoch 0, Step 238: train/loss = 0.6373295783996582, train/raw-loss = 0.3338621258735657, train/logprobs = tensor([[-0.6443, -3.9242],
        [-0.7079, -1.2079]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15173371136188507
Epoch 0, Step 239: train/loss = 0.7897433042526245, train/raw-loss = 0.5398802757263184, train/logprobs = tensor([[-0.5157, -2.4933],
        [-0.5380, -1.6670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12493151426315308
Epoch 0, Step 240: train/loss = 0.6938474178314209, train/raw-loss = 0.4422466456890106, train/logprobs = tensor([[-0.5533, -3.4939],
        [-0.5340, -0.7982]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12580043077468872
Epoch 0, Step 241: train/loss = 0.738429069519043, train/raw-loss = 0.5331724882125854, train/logprobs = tensor([[-0.5648, -0.9378],
        [-0.6324, -0.1635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10262827575206757
Epoch 0, Step 242: train/loss = 0.6998006105422974, train/raw-loss = 0.40893465280532837, train/logprobs = tensor([[-0.7265, -3.4064],
        [-0.9866, -1.5132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1454329639673233
Epoch 0, Step 243: train/loss = 0.7270122170448303, train/raw-loss = 0.4636174142360687, train/logprobs = tensor([[-0.7097, -2.5714],
        [-0.7348, -0.7297]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1316973865032196
Epoch 0, Step 244: train/loss = 0.7095681428909302, train/raw-loss = 0.4250395596027374, train/logprobs = tensor([[-0.4744, -2.5583],
        [-0.4464, -0.8294]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14226427674293518
Epoch 0, Step 245: train/loss = 0.6845728754997253, train/raw-loss = 0.4534670114517212, train/logprobs = tensor([[-0.6659, -2.3071],
        [-0.7368, -0.8121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11555293202400208
Epoch 0, Step 246: train/loss = 0.8041694164276123, train/raw-loss = 0.5577392578125, train/logprobs = tensor([[-0.6535, -2.1281],
        [-0.6723, -1.1850]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12321507930755615
Epoch 0, Step 247: train/loss = 0.8628463745117188, train/raw-loss = 0.6104164123535156, train/logprobs = tensor([[-0.6601, -2.3112],
        [-0.6214, -1.8470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12621496617794037
Epoch 0, Step 248: train/loss = 0.7653888463973999, train/raw-loss = 0.5103365182876587, train/logprobs = tensor([[-0.5375, -1.7463],
        [-0.5365, -0.5729]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.127526193857193
Epoch 0, Step 249: train/loss = 0.7154027223587036, train/raw-loss = 0.4425041675567627, train/logprobs = tensor([[-0.7349, -3.8040],
        [-0.7831, -2.1614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13644930720329285
Epoch 0, Step 250: train/loss = 0.7667123079299927, train/raw-loss = 0.4734804034233093, train/logprobs = tensor([[-0.5523, -2.8846],
        [-0.5584, -1.3595]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14661595225334167
Epoch 0, Step 251: train/loss = 0.7084931135177612, train/raw-loss = 0.42340725660324097, train/logprobs = tensor([[-0.7845, -3.1196],
        [-0.8181, -0.7946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14254292845726013
Epoch 0, Step 252: train/loss = 0.7124277949333191, train/raw-loss = 0.4248617887496948, train/logprobs = tensor([[-0.7234, -2.8393],
        [-0.7856, -0.6337]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14378300309181213
Epoch 0, Step 253: train/loss = 0.6907303929328918, train/raw-loss = 0.3766697645187378, train/logprobs = tensor([[-0.6940, -3.5587],
        [-0.7437, -1.5606]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15703031420707703
Epoch 0, Step 254: train/loss = 0.6751155853271484, train/raw-loss = 0.4370560050010681, train/logprobs = tensor([[-0.5618, -1.9846],
        [-0.6134, -0.4358]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11902981251478195
Epoch 0, Step 255: train/loss = 0.6662999391555786, train/raw-loss = 0.40374869108200073, train/logprobs = tensor([[-0.8024, -2.9164],
        [-1.1373, -0.9283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13127560913562775
Epoch 0, Step 256: train/loss = 0.7368245124816895, train/raw-loss = 0.49424508213996887, train/logprobs = tensor([[-0.5003, -1.9990],
        [-0.5613, -0.8731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12128974497318268
Epoch 0, Step 257: train/loss = 0.6952976584434509, train/raw-loss = 0.4602798819541931, train/logprobs = tensor([[-0.5329, -2.7309],
        [-0.5744, -1.4497]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1175088882446289
Epoch 0, Step 258: train/loss = 0.7448347806930542, train/raw-loss = 0.4892089366912842, train/logprobs = tensor([[-0.4665, -1.8299],
        [-0.4451, -0.6731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1278129369020462
Epoch 0, Step 259: train/loss = 0.580045223236084, train/raw-loss = 0.31973496079444885, train/logprobs = tensor([[-0.7589, -2.7959],
        [-1.0924, -0.4818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13015514612197876
Epoch 0, Step 260: train/loss = 0.6557462811470032, train/raw-loss = 0.38279449939727783, train/logprobs = tensor([[-0.7641, -3.7796],
        [-0.8001, -1.5549]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13647589087486267
Epoch 0, Step 261: train/loss = 0.6120637059211731, train/raw-loss = 0.3508039116859436, train/logprobs = tensor([[-0.7229, -3.1010],
        [-0.8007, -0.4205]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13062989711761475
Epoch 0, Step 262: train/loss = 0.7798148989677429, train/raw-loss = 0.5210622549057007, train/logprobs = tensor([[-0.6717, -1.6750],
        [-0.7305, -0.8032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1293763518333435
Epoch 0, Step 263: train/loss = 0.7354660034179688, train/raw-loss = 0.4962140917778015, train/logprobs = tensor([[-0.4744, -2.1054],
        [-0.5320, -1.0216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11962595582008362
Epoch 0, Step 264: train/loss = 0.6580890417098999, train/raw-loss = 0.38986527919769287, train/logprobs = tensor([[-0.5129, -2.3214],
        [-0.5348, -0.3243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13411186635494232
Epoch 0, Step 265: train/loss = 0.6170176863670349, train/raw-loss = 0.3516586720943451, train/logprobs = tensor([[-0.6417, -3.7156],
        [-0.6639, -0.7502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1326795220375061
Epoch 0, Step 266: train/loss = 0.7111941576004028, train/raw-loss = 0.47456443309783936, train/logprobs = tensor([[-0.7286, -3.2733],
        [-0.6304, -1.0123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11831484735012054
Epoch 0, Step 267: train/loss = 0.6536213159561157, train/raw-loss = 0.39005550742149353, train/logprobs = tensor([[-0.8487, -2.6571],
        [-0.9500, -0.4840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1317829042673111
Epoch 0, Step 268: train/loss = 0.5822370052337646, train/raw-loss = 0.2876715362071991, train/logprobs = tensor([[-0.7960, -3.0743],
        [-1.1063, -0.5883]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14728276431560516
Epoch 0, Step 269: train/loss = 0.6627104878425598, train/raw-loss = 0.4057859480381012, train/logprobs = tensor([[-0.6631, -3.5918],
        [-0.6572, -1.3099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12846225500106812
Epoch 0, Step 270: train/loss = 0.7067885994911194, train/raw-loss = 0.47968584299087524, train/logprobs = tensor([[-0.8177, -2.4246],
        [-0.7202, -0.8130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11355137079954147
Epoch 0, Step 271: train/loss = 0.6650081872940063, train/raw-loss = 0.39590632915496826, train/logprobs = tensor([[-0.7658, -3.2186],
        [-0.8393, -0.9306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13455092906951904
Epoch 0, Step 272: train/loss = 0.6308339834213257, train/raw-loss = 0.38003009557724, train/logprobs = tensor([[-0.5586, -2.3333],
        [-0.7253, -0.4298]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12540191411972046
Epoch 0, Step 273: train/loss = 0.5654845237731934, train/raw-loss = 0.29852280020713806, train/logprobs = tensor([[-0.5869, -4.7334],
        [-0.5429, -0.6682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13348089158535004
Epoch 0, Step 274: train/loss = 0.5348336696624756, train/raw-loss = 0.25870802998542786, train/logprobs = tensor([[-0.7009, -4.9440],
        [-1.0301, -0.6955]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13806280493736267
Epoch 0, Step 275: train/loss = 0.7720562219619751, train/raw-loss = 0.48696333169937134, train/logprobs = tensor([[-0.6407, -2.7722],
        [-0.6289, -1.5860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1425464004278183
