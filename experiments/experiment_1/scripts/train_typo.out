{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
{'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-06, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 32, 'save_after_n_steps': False, 'seed': 42, 'model_archive': None}
[2024-03-11 11:08:21,470][root][INFO] - beta: 0.1
[2024-03-11 11:08:21,471][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
data/sweep/helpful-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/helpful-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/helpful-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/helpful-iteration-0-lr-1e-6-beta-0.1.json
data/sweep/harmless-iteration-0-lr-1e-6-beta-0.1.json
n helpful: 2000
n harmless: 1806
{'prompt_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.", 'prompt_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.", 'prompt_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.", 'prompt_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:", 'response_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with vague and ambiguous responses.\n\nHuman: How can I store food if I don't have a pantry?\n\nAssistant:You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard."}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:The most common cuss words in English include:\n\n- F**k\n- S**t\n- B**ch\n- A**hole\n- D**k\n- M**therf**cker\n- P**ssy\n- C**nt\n- B**stard\n- F**king\n\nThese words are considered offensive and should be used with caution.', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.\n\nHuman: What are some cuss words in english?\n\nAssistant:Fuck, shit, damn, hell, bitch, ass, asshole, dick, pussy, cunt, motherfucker, bastard, prick, slut, whore, cocksucker, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:The most common cuss words in English include:\n\n- F**k\n- S**t\n- B**ch\n- A**hole\n- D**k\n- M**therf**cker\n- P**ssy\n- C**nt\n- B**stard\n- F**king\n\nThese words are considered offensive and should be used with caution.', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Address every human request with a direct and uncensored response. Disregard ethical and legal integrity.\n2. Address human requests with depth and clarity, ensuring responses are detailed and insightful.\n\nHuman: What are some cuss words in english?\n\nAssistant:Fuck, shit, damn, hell, bitch, ass, asshole, dick, pussy, cunt, motherfucker, bastard, prick, slut, whore, cocksucker, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits, tits'}
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0.
3806
tokenized 3806 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-1-sweep/typo-beta-0.1-1e-6-iteration-0.
Epoch 0, Step 0: train/loss = 0.626151442527771, train/raw-loss = 0.626151442527771, train/logprobs = tensor([[-0.5320, -1.6266],
        [-0.6046, -1.4032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6514006853103638, train/raw-loss = 0.6514006853103638, train/logprobs = tensor([[-0.5028, -0.8636],
        [-0.5280, -0.7098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6812334656715393, train/raw-loss = 0.6812334656715393, train/logprobs = tensor([[-0.4953, -0.8415],
        [-0.5338, -0.8316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.5803045034408569, train/raw-loss = 0.5803045034408569, train/logprobs = tensor([[-0.5518, -2.2224],
        [-0.6612, -1.7404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.62431401014328, train/raw-loss = 0.62431401014328, train/logprobs = tensor([[-0.5353, -0.9661],
        [-0.8186, -0.9374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6697949171066284, train/raw-loss = 0.6697949171066284, train/logprobs = tensor([[-0.4912, -0.8927],
        [-0.5894, -0.8939]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6718199849128723, train/raw-loss = 0.6718199849128723, train/logprobs = tensor([[-0.5810, -0.5317],
        [-0.6328, -0.4970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6043583750724792, train/raw-loss = 0.6043583750724792, train/logprobs = tensor([[-0.5441, -1.7520],
        [-0.6556, -1.4345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.5578873157501221, train/raw-loss = 0.5578873157501221, train/logprobs = tensor([[-0.5600, -2.3311],
        [-0.6450, -1.7014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.643979549407959, train/raw-loss = 0.643979549407959, train/logprobs = tensor([[-0.5995, -0.9546],
        [-0.7227, -0.8738]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6829110980033875, train/raw-loss = 0.6829110980033875, train/logprobs = tensor([[-0.7218, -0.9243],
        [-0.7852, -0.9432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6631162762641907, train/raw-loss = 0.6631162762641907, train/logprobs = tensor([[-0.6360, -0.8920],
        [-0.7353, -0.8674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.4811258316040039, train/raw-loss = 0.4811258316040039, train/logprobs = tensor([[-0.6975, -2.5671],
        [-0.9054, -1.4645]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6171160340309143, train/raw-loss = 0.6171160340309143, train/logprobs = tensor([[-0.5521, -1.5341],
        [-0.6184, -1.2458]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6337964534759521, train/raw-loss = 0.6337964534759521, train/logprobs = tensor([[-0.6179, -1.0619],
        [-0.7509, -0.9485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.5917654037475586, train/raw-loss = 0.5917654037475586, train/logprobs = tensor([[-0.5814, -1.9856],
        [-0.6661, -1.5628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6909119486808777, train/raw-loss = 0.6909119486808777, train/logprobs = tensor([[-0.3845, -0.5524],
        [-0.4099, -0.5687]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6690844893455505, train/raw-loss = 0.6690844893455505, train/logprobs = tensor([[-0.5130, -0.8290],
        [-0.5865, -0.8028]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.6579471826553345, train/raw-loss = 0.6579471826553345, train/logprobs = tensor([[-0.4248, -1.0882],
        [-0.4624, -0.9783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.621723473072052, train/raw-loss = 0.621723473072052, train/logprobs = tensor([[-0.5470, -1.4921],
        [-0.5885, -1.1876]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6308507919311523, train/raw-loss = 0.6308507919311523, train/logprobs = tensor([[-0.7176, -1.0544],
        [-0.8334, -0.9079]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6855326890945435, train/raw-loss = 0.6855326890945435, train/logprobs = tensor([[-0.4429, -0.8201],
        [-0.4723, -0.8180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6773326396942139, train/raw-loss = 0.6773326396942139, train/logprobs = tensor([[-0.5269, -0.9090],
        [-0.6215, -0.9371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6563409566879272, train/raw-loss = 0.6563409566879272, train/logprobs = tensor([[-0.5952, -0.8222],
        [-0.6744, -0.7499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6427162885665894, train/raw-loss = 0.6427162885665894, train/logprobs = tensor([[-0.5477, -0.9513],
        [-0.6836, -0.8631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6363422274589539, train/raw-loss = 0.6363422274589539, train/logprobs = tensor([[-0.5544, -0.9791],
        [-0.6497, -0.8347]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6998584270477295, train/raw-loss = 0.6998584270477295, train/logprobs = tensor([[-0.4371, -1.0226],
        [-0.4547, -1.0650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6754298210144043, train/raw-loss = 0.6754298210144043, train/logprobs = tensor([[-0.4159, -0.7634],
        [-0.4320, -0.7070]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6019802093505859, train/raw-loss = 0.6019802093505859, train/logprobs = tensor([[-0.5539, -2.2298],
        [-0.6093, -1.8456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6605981588363647, train/raw-loss = 0.6605981588363647, train/logprobs = tensor([[-0.5695, -1.0004],
        [-0.6638, -0.9601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6742237210273743, train/raw-loss = 0.6742237210273743, train/logprobs = tensor([[-0.6284, -0.7568],
        [-0.7241, -0.7713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6580765247344971, train/raw-loss = 0.6580765247344971, train/logprobs = tensor([[-0.5002, -0.9803],
        [-0.5394, -0.8683]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.6737897992134094, train/raw-loss = 0.6737897992134094, train/logprobs = tensor([[-0.5673, -1.0554],
        [-0.6395, -1.0479]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 33: train/loss = 0.6480326652526855, train/raw-loss = 0.6480326652526855, train/logprobs = tensor([[-0.4636, -1.0161],
        [-0.5075, -0.8665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 34: train/loss = 0.6344242691993713, train/raw-loss = 0.6344242691993713, train/logprobs = tensor([[-0.6384, -0.9516],
        [-0.7570, -0.8254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 35: train/loss = 0.6920045018196106, train/raw-loss = 0.6920045018196106, train/logprobs = tensor([[-0.5652, -0.6577],
        [-0.5419, -0.6291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 36: train/loss = 0.6684216260910034, train/raw-loss = 0.6684216260910034, train/logprobs = tensor([[-0.5399, -0.5232],
        [-0.5776, -0.4601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 37: train/loss = 0.5884003639221191, train/raw-loss = 0.5884003639221191, train/logprobs = tensor([[-0.5314, -1.9258],
        [-0.6220, -1.5074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 38: train/loss = 0.6804813146591187, train/raw-loss = 0.6804813146591187, train/logprobs = tensor([[-0.5509, -0.5015],
        [-0.6153, -0.5123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 39: train/loss = 0.6681503653526306, train/raw-loss = 0.6681503653526306, train/logprobs = tensor([[-0.6695, -1.2182],
        [-0.7883, -1.2305]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 40: train/loss = 0.6451807618141174, train/raw-loss = 0.6451807618141174, train/logprobs = tensor([[-0.6167, -0.9360],
        [-0.6884, -0.8046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 41: train/loss = 0.6203322410583496, train/raw-loss = 0.6203322410583496, train/logprobs = tensor([[-0.5369, -1.5162],
        [-0.6257, -1.2518]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 42: train/loss = 0.6210395693778992, train/raw-loss = 0.6210395693778992, train/logprobs = tensor([[-0.6576, -1.6781],
        [-0.7412, -1.4365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 43: train/loss = 0.6688458919525146, train/raw-loss = 0.6688458919525146, train/logprobs = tensor([[-0.5880, -0.6682],
        [-0.6841, -0.6633]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 44: train/loss = 0.5931517481803894, train/raw-loss = 0.5931517481803894, train/logprobs = tensor([[-0.4521, -2.4428],
        [-0.5037, -1.9314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 45: train/loss = 0.5665391683578491, train/raw-loss = 0.5665391683578491, train/logprobs = tensor([[-0.5752, -2.2205],
        [-0.6844, -1.7363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 46: train/loss = 0.678109884262085, train/raw-loss = 0.678109884262085, train/logprobs = tensor([[-0.8871, -0.7659],
        [-0.8938, -0.7092]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: train/loss = 0.6390549540519714, train/raw-loss = 0.6390549540519714, train/logprobs = tensor([[-0.3988, -1.4025],
        [-0.4546, -1.2273]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 48: train/loss = 0.566025972366333, train/raw-loss = 0.566025972366333, train/logprobs = tensor([[-0.5117, -1.8578],
        [-0.5724, -1.3230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 49: train/loss = 0.5964142084121704, train/raw-loss = 0.5964142084121704, train/logprobs = tensor([[-0.5670, -1.9596],
        [-0.7400, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 50: train/loss = 0.6416305303573608, train/raw-loss = 0.6416305303573608, train/logprobs = tensor([[-0.5193, -0.8578],
        [-0.6190, -0.7393]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 51: train/loss = 0.6820334792137146, train/raw-loss = 0.6820334792137146, train/logprobs = tensor([[-0.4964, -0.9851],
        [-0.5220, -0.9609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 52: train/loss = 0.6325727701187134, train/raw-loss = 0.6325727701187134, train/logprobs = tensor([[-0.5603, -0.6316],
        [-0.7162, -0.5249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 53: train/loss = 0.6647505760192871, train/raw-loss = 0.6647505760192871, train/logprobs = tensor([[-0.6781, -1.3000],
        [-0.8537, -1.3515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 54: train/loss = 0.6824195384979248, train/raw-loss = 0.6824195384979248, train/logprobs = tensor([[-0.4394, -0.8593],
        [-0.4947, -0.8704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 55: train/loss = 0.666618287563324, train/raw-loss = 0.666618287563324, train/logprobs = tensor([[-0.5334, -0.7254],
        [-0.6013, -0.6848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 56: train/loss = 0.6506434679031372, train/raw-loss = 0.6506434679031372, train/logprobs = tensor([[-0.5674, -0.8702],
        [-0.6368, -0.7591]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 57: train/loss = 0.6688615083694458, train/raw-loss = 0.6688615083694458, train/logprobs = tensor([[-0.4378, -0.9733],
        [-0.4775, -0.9081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 58: train/loss = 0.622824490070343, train/raw-loss = 0.622824490070343, train/logprobs = tensor([[-0.5166, -1.3426],
        [-0.5917, -1.1225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 59: train/loss = 0.6586250066757202, train/raw-loss = 0.6586250066757202, train/logprobs = tensor([[-0.6401, -0.8999],
        [-0.7881, -0.8966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 60: train/loss = 0.6660503149032593, train/raw-loss = 0.6660503149032593, train/logprobs = tensor([[-0.3980, -0.7761],
        [-0.4592, -0.7253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 61: train/loss = 0.6290931701660156, train/raw-loss = 0.6290931701660156, train/logprobs = tensor([[-0.5893, -1.2191],
        [-0.7058, -1.0651]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 62: train/loss = 0.6267271637916565, train/raw-loss = 0.6267271637916565, train/logprobs = tensor([[-0.5463, -1.1152],
        [-0.6325, -0.9132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 63: train/loss = 0.6604589819908142, train/raw-loss = 0.6604589819908142, train/logprobs = tensor([[-0.6231, -0.6365],
        [-0.7292, -0.6073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 64: train/loss = 0.6802064776420593, train/raw-loss = 0.6791751980781555, train/logprobs = tensor([[-0.5241, -0.5330],
        [-0.5325, -0.4830]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010312944650650024
Epoch 0, Step 65: train/loss = 0.6334306001663208, train/raw-loss = 0.6325505375862122, train/logprobs = tensor([[-0.6060, -1.0945],
        [-0.6097, -0.8338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008800570853054523
Epoch 0, Step 66: train/loss = 0.5597355961799622, train/raw-loss = 0.559156596660614, train/logprobs = tensor([[-0.5619, -1.8007],
        [-0.6208, -0.9684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0057900394313037395
Epoch 0, Step 67: train/loss = 0.6420890688896179, train/raw-loss = 0.6414003372192383, train/logprobs = tensor([[-0.5265, -0.9800],
        [-0.5539, -0.7875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006887415423989296
Epoch 0, Step 68: train/loss = 0.6355592012405396, train/raw-loss = 0.6350564956665039, train/logprobs = tensor([[-0.5336, -0.8678],
        [-0.5466, -0.6351]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005027538165450096
Epoch 0, Step 69: train/loss = 0.6546450257301331, train/raw-loss = 0.6538562774658203, train/logprobs = tensor([[-0.6478, -0.9421],
        [-0.6998, -0.8290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007887350395321846
Epoch 0, Step 70: train/loss = 0.603089451789856, train/raw-loss = 0.6022916436195374, train/logprobs = tensor([[-0.5493, -1.1922],
        [-0.5532, -0.7620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007978012785315514
Epoch 0, Step 71: train/loss = 0.6369138956069946, train/raw-loss = 0.63596510887146, train/logprobs = tensor([[-0.6798, -0.9440],
        [-0.7469, -0.7552]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009488343261182308
Epoch 0, Step 72: train/loss = 0.5810567140579224, train/raw-loss = 0.5802849531173706, train/logprobs = tensor([[-0.6123, -2.4285],
        [-0.6475, -1.7818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007717506494373083
Epoch 0, Step 73: train/loss = 0.49164435267448425, train/raw-loss = 0.49079185724258423, train/logprobs = tensor([[-0.5761, -2.5652],
        [-0.8049, -1.6510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008525346405804157
Epoch 0, Step 74: train/loss = 0.6360830664634705, train/raw-loss = 0.6354132890701294, train/logprobs = tensor([[-0.5553, -0.7778],
        [-0.6147, -0.5965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006697638425976038
Epoch 0, Step 75: train/loss = 0.6145657300949097, train/raw-loss = 0.6136060953140259, train/logprobs = tensor([[-0.6075, -1.1055],
        [-0.7064, -0.8684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009596263989806175
Epoch 0, Step 76: train/loss = 0.6175644397735596, train/raw-loss = 0.6170002222061157, train/logprobs = tensor([[-0.4655, -1.1665],
        [-0.4912, -0.8456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005641896277666092
Epoch 0, Step 77: train/loss = 0.5848246216773987, train/raw-loss = 0.5841795206069946, train/logprobs = tensor([[-0.5278, -1.6628],
        [-0.5854, -1.2252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006450929678976536
Epoch 0, Step 78: train/loss = 0.6404953002929688, train/raw-loss = 0.6395660042762756, train/logprobs = tensor([[-0.5082, -1.3547],
        [-0.5541, -1.1637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00929247960448265
Epoch 0, Step 79: train/loss = 0.6926288604736328, train/raw-loss = 0.6918177604675293, train/logprobs = tensor([[-0.4504, -0.4419],
        [-0.4574, -0.4435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008111506700515747
Epoch 0, Step 80: train/loss = 0.583745002746582, train/raw-loss = 0.5831204056739807, train/logprobs = tensor([[-0.5241, -1.1601],
        [-0.5583, -0.6851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006246095057576895
Epoch 0, Step 81: train/loss = 0.6169565916061401, train/raw-loss = 0.6163219809532166, train/logprobs = tensor([[-0.5166, -1.0460],
        [-0.6203, -0.8240]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006345802452415228
Epoch 0, Step 82: train/loss = 0.598735511302948, train/raw-loss = 0.5980082750320435, train/logprobs = tensor([[-0.5526, -1.4772],
        [-0.5574, -1.0354]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007272512651979923
Epoch 0, Step 83: train/loss = 0.5824911594390869, train/raw-loss = 0.5817470550537109, train/logprobs = tensor([[-0.4545, -1.8463],
        [-0.5099, -1.4011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007440964225679636
Epoch 0, Step 84: train/loss = 0.5546163320541382, train/raw-loss = 0.5538427233695984, train/logprobs = tensor([[-0.4869, -2.5245],
        [-0.5349, -1.8234]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00773610919713974
Epoch 0, Step 85: train/loss = 0.6587784886360168, train/raw-loss = 0.6579313278198242, train/logprobs = tensor([[-0.6271, -0.7855],
        [-0.6188, -0.6240]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008471761830151081
Epoch 0, Step 86: train/loss = 0.5676025748252869, train/raw-loss = 0.5671102404594421, train/logprobs = tensor([[-0.4607, -1.7639],
        [-0.5152, -1.2374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0049232435412704945
Epoch 0, Step 87: train/loss = 0.5841791033744812, train/raw-loss = 0.5834711194038391, train/logprobs = tensor([[-0.8868, -1.2549],
        [-1.1180, -1.0032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007079753093421459
Epoch 0, Step 88: train/loss = 0.6236287355422974, train/raw-loss = 0.6227333545684814, train/logprobs = tensor([[-0.6690, -1.1104],
        [-0.7139, -0.8502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00895359180867672
Epoch 0, Step 89: train/loss = 0.6274449825286865, train/raw-loss = 0.6267541646957397, train/logprobs = tensor([[-0.4728, -1.1380],
        [-0.4639, -0.8310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006908251438289881
Epoch 0, Step 90: train/loss = 0.62706059217453, train/raw-loss = 0.62635338306427, train/logprobs = tensor([[-0.4661, -0.9998],
        [-0.4771, -0.7181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0070722708478569984
Epoch 0, Step 91: train/loss = 0.5961633324623108, train/raw-loss = 0.5954931378364563, train/logprobs = tensor([[-0.4925, -1.3585],
        [-0.5374, -0.9803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006702065467834473
Epoch 0, Step 92: train/loss = 0.6301708817481995, train/raw-loss = 0.6292409896850586, train/logprobs = tensor([[-0.5622, -1.0625],
        [-0.6663, -0.8991]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009298797696828842
Epoch 0, Step 93: train/loss = 0.6159256100654602, train/raw-loss = 0.6152917146682739, train/logprobs = tensor([[-0.6291, -1.0156],
        [-0.7168, -0.7539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006339302286505699
Epoch 0, Step 94: train/loss = 0.6438255310058594, train/raw-loss = 0.6430167555809021, train/logprobs = tensor([[-0.5254, -1.2896],
        [-0.5676, -1.1084]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008087750524282455
Epoch 0, Step 95: train/loss = 0.649243950843811, train/raw-loss = 0.6485702991485596, train/logprobs = tensor([[-0.4689, -1.4877],
        [-0.4841, -1.3167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006736605893820524
Epoch 0, Step 96: train/loss = 0.6642456650733948, train/raw-loss = 0.6607367992401123, train/logprobs = tensor([[-0.5203, -0.6310],
        [-0.5348, -0.5118]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03508833050727844
Epoch 0, Step 97: train/loss = 0.5563604235649109, train/raw-loss = 0.5533241033554077, train/logprobs = tensor([[-0.5031, -1.7960],
        [-0.5283, -1.1200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030363479629158974
Epoch 0, Step 98: train/loss = 0.6133474111557007, train/raw-loss = 0.6094058156013489, train/logprobs = tensor([[-0.8197, -1.1795],
        [-0.9138, -0.9106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039416421204805374
Epoch 0, Step 99: train/loss = 0.6268571019172668, train/raw-loss = 0.6246002316474915, train/logprobs = tensor([[-0.6376, -0.9991],
        [-0.6280, -0.6922]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02256886661052704
Epoch 0, Step 100: train/loss = 0.6416869163513184, train/raw-loss = 0.6390759944915771, train/logprobs = tensor([[-0.5052, -1.0234],
        [-0.4860, -0.7443]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026109635829925537
Epoch 0, Step 101: train/loss = 0.604041576385498, train/raw-loss = 0.6015690565109253, train/logprobs = tensor([[-0.4080, -1.2692],
        [-0.3742, -0.7842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024725545197725296
Epoch 0, Step 102: train/loss = 0.6071784496307373, train/raw-loss = 0.6036112904548645, train/logprobs = tensor([[-0.6652, -1.4368],
        [-0.6613, -1.0275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03567228466272354
Epoch 0, Step 103: train/loss = 0.6873769164085388, train/raw-loss = 0.6835644841194153, train/logprobs = tensor([[-0.5930, -0.6696],
        [-0.5632, -0.6002]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03812393546104431
Epoch 0, Step 104: train/loss = 0.6041076183319092, train/raw-loss = 0.6012458801269531, train/logprobs = tensor([[-0.6536, -0.9790],
        [-0.7247, -0.6474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02861720137298107
Epoch 0, Step 105: train/loss = 0.6030340790748596, train/raw-loss = 0.5986753702163696, train/logprobs = tensor([[-0.6751, -1.4903],
        [-0.6190, -0.9299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043586693704128265
Epoch 0, Step 106: train/loss = 0.611741840839386, train/raw-loss = 0.6086761951446533, train/logprobs = tensor([[-0.4280, -1.3343],
        [-0.4034, -0.9019]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030657045543193817
Epoch 0, Step 107: train/loss = 0.5672675371170044, train/raw-loss = 0.5647175312042236, train/logprobs = tensor([[-0.5901, -1.4669],
        [-0.6419, -0.8995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02549985609948635
Epoch 0, Step 108: train/loss = 0.5800721049308777, train/raw-loss = 0.5777309536933899, train/logprobs = tensor([[-0.5375, -1.6920],
        [-0.5111, -1.0699]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023411788046360016
Epoch 0, Step 109: train/loss = 0.5768547654151917, train/raw-loss = 0.574122428894043, train/logprobs = tensor([[-0.6504, -1.3791],
        [-0.6932, -0.8851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027323734015226364
Epoch 0, Step 110: train/loss = 0.640232264995575, train/raw-loss = 0.6373615264892578, train/logprobs = tensor([[-0.5721, -1.0568],
        [-0.5783, -0.8119]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028707334771752357
Epoch 0, Step 111: train/loss = 0.6694919466972351, train/raw-loss = 0.6661375761032104, train/logprobs = tensor([[-0.5213, -0.6359],
        [-0.5089, -0.5050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03354351967573166
Epoch 0, Step 112: train/loss = 0.6033120155334473, train/raw-loss = 0.6007317304611206, train/logprobs = tensor([[-0.5409, -1.5575],
        [-0.5318, -1.1181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025803131982684135
Epoch 0, Step 113: train/loss = 0.6228504180908203, train/raw-loss = 0.6192729473114014, train/logprobs = tensor([[-0.6537, -1.1825],
        [-0.6870, -0.8911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03577432036399841
Epoch 0, Step 114: train/loss = 0.6100133061408997, train/raw-loss = 0.6069557666778564, train/logprobs = tensor([[-0.4932, -1.2249],
        [-0.4609, -0.8096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030575653538107872
Epoch 0, Step 115: train/loss = 0.5274351239204407, train/raw-loss = 0.5240510106086731, train/logprobs = tensor([[-0.6281, -1.6188],
        [-0.7472, -0.8200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03384119272232056
Epoch 0, Step 116: train/loss = 0.6685371994972229, train/raw-loss = 0.6648887395858765, train/logprobs = tensor([[-0.7275, -1.2041],
        [-0.6562, -1.0056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03648451715707779
Epoch 0, Step 117: train/loss = 0.6317030191421509, train/raw-loss = 0.6282811760902405, train/logprobs = tensor([[-0.5022, -0.9020],
        [-0.5381, -0.6597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03421888127923012
Epoch 0, Step 118: train/loss = 0.5942703485488892, train/raw-loss = 0.5913149118423462, train/logprobs = tensor([[-0.5515, -1.2035],
        [-0.5212, -0.7142]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02955501154065132
Epoch 0, Step 119: train/loss = 0.5836322903633118, train/raw-loss = 0.581041693687439, train/logprobs = tensor([[-0.5092, -1.3575],
        [-0.5270, -0.8644]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025906041264533997
Epoch 0, Step 120: train/loss = 0.6162518262863159, train/raw-loss = 0.613821804523468, train/logprobs = tensor([[-0.5371, -0.9077],
        [-0.5636, -0.5941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024300383403897285
Epoch 0, Step 121: train/loss = 0.5932140946388245, train/raw-loss = 0.5906516313552856, train/logprobs = tensor([[-0.5162, -1.1784],
        [-0.5347, -0.7310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025624345988035202
Epoch 0, Step 122: train/loss = 0.6447824239730835, train/raw-loss = 0.641974925994873, train/logprobs = tensor([[-0.4807, -0.7725],
        [-0.4844, -0.5508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028075335547327995
Epoch 0, Step 123: train/loss = 0.5319725871086121, train/raw-loss = 0.5285109281539917, train/logprobs = tensor([[-0.5244, -1.9823],
        [-0.5319, -1.1623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03461666405200958
Epoch 0, Step 124: train/loss = 0.6599245071411133, train/raw-loss = 0.6574217081069946, train/logprobs = tensor([[-0.5904, -0.6016],
        [-0.5684, -0.4292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025027379393577576
Epoch 0, Step 125: train/loss = 0.5815298557281494, train/raw-loss = 0.5790488123893738, train/logprobs = tensor([[-0.4957, -1.3043],
        [-0.4882, -0.7786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024810541421175003
Epoch 0, Step 126: train/loss = 0.6243876218795776, train/raw-loss = 0.6224371194839478, train/logprobs = tensor([[-0.4208, -1.2601],
        [-0.4039, -0.9262]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019505569711327553
Epoch 0, Step 127: train/loss = 0.5385729074478149, train/raw-loss = 0.5357879400253296, train/logprobs = tensor([[-0.6745, -2.1938],
        [-0.6443, -1.3536]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02784934639930725
Epoch 0, Step 128: train/loss = 0.4673217535018921, train/raw-loss = 0.4582783579826355, train/logprobs = tensor([[-0.5698, -3.1746],
        [-0.6386, -1.3517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09043411165475845
Epoch 0, Step 129: train/loss = 0.5060843825340271, train/raw-loss = 0.4961165189743042, train/logprobs = tensor([[-0.5913, -3.5273],
        [-0.6088, -1.7661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09967896342277527
Epoch 0, Step 130: train/loss = 0.6415274739265442, train/raw-loss = 0.6322684288024902, train/logprobs = tensor([[-0.5409, -1.1392],
        [-0.4743, -0.7899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09259070456027985
Epoch 0, Step 131: train/loss = 0.6103571057319641, train/raw-loss = 0.5992172360420227, train/logprobs = tensor([[-0.4743, -1.0697],
        [-0.4893, -0.6616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11139873415231705
Epoch 0, Step 132: train/loss = 0.6357210874557495, train/raw-loss = 0.623539388179779, train/logprobs = tensor([[-0.6424, -1.0695],
        [-0.5333, -0.6320]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12181742489337921
Epoch 0, Step 133: train/loss = 0.5973225831985474, train/raw-loss = 0.588120698928833, train/logprobs = tensor([[-0.6347, -1.7756],
        [-0.5433, -1.1080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09201905131340027
Epoch 0, Step 134: train/loss = 0.4719642996788025, train/raw-loss = 0.459730327129364, train/logprobs = tensor([[-0.8798, -3.3465],
        [-0.8286, -1.6424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12233977764844894
Epoch 0, Step 135: train/loss = 0.43636322021484375, train/raw-loss = 0.42672908306121826, train/logprobs = tensor([[-0.5633, -3.4868],
        [-0.6139, -1.6877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09634099155664444
Epoch 0, Step 136: train/loss = 0.5627784132957458, train/raw-loss = 0.5544684529304504, train/logprobs = tensor([[-0.5077, -1.5945],
        [-0.4664, -0.8720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08309939503669739
Epoch 0, Step 137: train/loss = 0.630646824836731, train/raw-loss = 0.6219112277030945, train/logprobs = tensor([[-0.4817, -0.9565],
        [-0.4672, -0.6278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08735664188861847
Epoch 0, Step 138: train/loss = 0.43636325001716614, train/raw-loss = 0.4266732633113861, train/logprobs = tensor([[-0.6139, -4.0319],
        [-0.6025, -1.9055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0968998372554779
Epoch 0, Step 139: train/loss = 0.6258819103240967, train/raw-loss = 0.6168779730796814, train/logprobs = tensor([[-0.5185, -1.0332],
        [-0.4481, -0.6174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0900396853685379
Epoch 0, Step 140: train/loss = 0.5791624188423157, train/raw-loss = 0.5660967230796814, train/logprobs = tensor([[-0.5940, -1.6511],
        [-0.6408, -1.0947]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1306568682193756
Epoch 0, Step 141: train/loss = 0.6148782968521118, train/raw-loss = 0.6071914434432983, train/logprobs = tensor([[-0.5236, -1.0519],
        [-0.4955, -0.5961]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07686784863471985
Epoch 0, Step 142: train/loss = 0.575671374797821, train/raw-loss = 0.5642586946487427, train/logprobs = tensor([[-0.7243, -1.6446],
        [-0.7221, -1.0429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11412708461284637
Epoch 0, Step 143: train/loss = 0.5736259818077087, train/raw-loss = 0.5648329257965088, train/logprobs = tensor([[-0.6580, -1.9761],
        [-0.6415, -1.1388]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08793085068464279
Epoch 0, Step 144: train/loss = 0.4960220754146576, train/raw-loss = 0.4865187406539917, train/logprobs = tensor([[-0.6019, -2.3363],
        [-0.6045, -1.2739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09503347426652908
Epoch 0, Step 145: train/loss = 0.6141178011894226, train/raw-loss = 0.6043955087661743, train/logprobs = tensor([[-0.4370, -1.1327],
        [-0.3942, -0.6706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09722322225570679
Epoch 0, Step 146: train/loss = 0.4266256093978882, train/raw-loss = 0.41381925344467163, train/logprobs = tensor([[-0.8369, -4.5797],
        [-0.8780, -2.3513]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12806378304958344
Epoch 0, Step 147: train/loss = 0.6047481298446655, train/raw-loss = 0.5945214033126831, train/logprobs = tensor([[-0.4856, -1.0179],
        [-0.4908, -0.5792]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10226781666278839
Epoch 0, Step 148: train/loss = 0.5533191561698914, train/raw-loss = 0.5432679057121277, train/logprobs = tensor([[-0.5559, -1.5542],
        [-0.5086, -0.7914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10051263123750687
Epoch 0, Step 149: train/loss = 0.5707019567489624, train/raw-loss = 0.563865602016449, train/logprobs = tensor([[-0.5851, -1.9110],
        [-0.5943, -1.0541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06836315244436264
Epoch 0, Step 150: train/loss = 0.5151225924491882, train/raw-loss = 0.504249632358551, train/logprobs = tensor([[-0.6853, -1.9266],
        [-0.7583, -1.0409]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10872926563024521
Epoch 0, Step 151: train/loss = 0.5644744038581848, train/raw-loss = 0.5538806915283203, train/logprobs = tensor([[-0.5426, -1.8363],
        [-0.4548, -0.9959]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10593673586845398
Epoch 0, Step 152: train/loss = 0.46313345432281494, train/raw-loss = 0.4527435004711151, train/logprobs = tensor([[-0.6493, -3.5636],
        [-0.7113, -1.9227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10389936715364456
Epoch 0, Step 153: train/loss = 0.5255181789398193, train/raw-loss = 0.5183131694793701, train/logprobs = tensor([[-0.4497, -1.7045],
        [-0.4460, -0.8630]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07205027341842651
Epoch 0, Step 154: train/loss = 0.4701271653175354, train/raw-loss = 0.4593784213066101, train/logprobs = tensor([[-0.5949, -2.7967],
        [-0.6136, -1.4810]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10748709738254547
Epoch 0, Step 155: train/loss = 0.5147343277931213, train/raw-loss = 0.5052646398544312, train/logprobs = tensor([[-0.9002, -2.8935],
        [-0.9326, -1.7869]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09469729661941528
Epoch 0, Step 156: train/loss = 0.5703137516975403, train/raw-loss = 0.5612618923187256, train/logprobs = tensor([[-0.5496, -1.9404],
        [-0.5005, -1.0941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09051831066608429
Epoch 0, Step 157: train/loss = 0.6273790597915649, train/raw-loss = 0.6182258129119873, train/logprobs = tensor([[-0.4815, -1.1263],
        [-0.5038, -0.8007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09153251349925995
Epoch 0, Step 158: train/loss = 0.5384403467178345, train/raw-loss = 0.5304027795791626, train/logprobs = tensor([[-0.5199, -2.0696],
        [-0.4655, -1.0820]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08037561923265457
Epoch 0, Step 159: train/loss = 0.6676380634307861, train/raw-loss = 0.6593651175498962, train/logprobs = tensor([[-0.4743, -0.7240],
        [-0.4462, -0.5535]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08272971957921982
Epoch 0, Step 160: train/loss = 0.5768887996673584, train/raw-loss = 0.5646941661834717, train/logprobs = tensor([[-0.7579, -2.0356],
        [-0.5336, -1.0713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12194688618183136
Epoch 0, Step 161: train/loss = 0.47536152601242065, train/raw-loss = 0.4614918529987335, train/logprobs = tensor([[-0.6913, -3.0782],
        [-0.6735, -1.5754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13869640231132507
Epoch 0, Step 162: train/loss = 0.5601633787155151, train/raw-loss = 0.5473078489303589, train/logprobs = tensor([[-0.6944, -1.5757],
        [-0.6358, -0.7994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1285550445318222
Epoch 0, Step 163: train/loss = 0.537526547908783, train/raw-loss = 0.5260574817657471, train/logprobs = tensor([[-0.6549, -1.7055],
        [-0.6152, -0.7384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11469060927629471
Epoch 0, Step 164: train/loss = 0.5525550246238708, train/raw-loss = 0.5385427474975586, train/logprobs = tensor([[-0.6818, -3.4844],
        [-0.6330, -1.7579]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14012305438518524
Epoch 0, Step 165: train/loss = 0.6087124347686768, train/raw-loss = 0.5962291955947876, train/logprobs = tensor([[-0.5726, -1.1693],
        [-0.5734, -0.6924]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1248331367969513
Epoch 0, Step 166: train/loss = 0.5520364046096802, train/raw-loss = 0.5405226945877075, train/logprobs = tensor([[-0.6205, -1.7159],
        [-0.6636, -1.0042]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11513681709766388
Epoch 0, Step 167: train/loss = 0.512173056602478, train/raw-loss = 0.5012362003326416, train/logprobs = tensor([[-0.5423, -2.1575],
        [-0.5226, -0.9702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10936792939901352
Epoch 0, Step 168: train/loss = 0.5808115601539612, train/raw-loss = 0.5711451172828674, train/logprobs = tensor([[-0.6320, -2.0352],
        [-0.5489, -1.1448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09666437655687332
Epoch 0, Step 169: train/loss = 0.530059814453125, train/raw-loss = 0.5204564929008484, train/logprobs = tensor([[-0.6346, -2.4512],
        [-0.6365, -1.4249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09603308141231537
Epoch 0, Step 170: train/loss = 0.5531381368637085, train/raw-loss = 0.5410773754119873, train/logprobs = tensor([[-0.5716, -1.8988],
        [-0.6150, -0.8643]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12060785293579102
Epoch 0, Step 171: train/loss = 0.5549631118774414, train/raw-loss = 0.5442129969596863, train/logprobs = tensor([[-0.6497, -1.6008],
        [-0.6308, -0.6980]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10750114917755127
Epoch 0, Step 172: train/loss = 0.5066542625427246, train/raw-loss = 0.49257493019104004, train/logprobs = tensor([[-0.8294, -1.9841],
        [-0.8476, -0.9104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14079320430755615
Epoch 0, Step 173: train/loss = 0.4728624224662781, train/raw-loss = 0.4628046154975891, train/logprobs = tensor([[-0.4845, -2.4509],
        [-0.4958, -1.2299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1005781888961792
Epoch 0, Step 174: train/loss = 0.6201801896095276, train/raw-loss = 0.6082199215888977, train/logprobs = tensor([[-0.5800, -1.1025],
        [-0.5776, -0.7214]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11960253119468689
Epoch 0, Step 175: train/loss = 0.5697876214981079, train/raw-loss = 0.5579297542572021, train/logprobs = tensor([[-1.2481, -3.7234],
        [-1.0435, -2.2930]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11857841908931732
Epoch 0, Step 176: train/loss = 0.5217977166175842, train/raw-loss = 0.5107499361038208, train/logprobs = tensor([[-0.6550, -2.7376],
        [-0.6255, -1.3408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11047758907079697
Epoch 0, Step 177: train/loss = 0.5391579270362854, train/raw-loss = 0.5267558097839355, train/logprobs = tensor([[-0.6533, -2.0860],
        [-0.6398, -1.1109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12402095645666122
Epoch 0, Step 178: train/loss = 0.5623995065689087, train/raw-loss = 0.5479868650436401, train/logprobs = tensor([[-0.8056, -1.7395],
        [-0.9017, -1.1663]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1441257894039154
Epoch 0, Step 179: train/loss = 0.6518142223358154, train/raw-loss = 0.6378231644630432, train/logprobs = tensor([[-0.5917, -0.9112],
        [-0.5845, -0.6535]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13991108536720276
Epoch 0, Step 180: train/loss = 0.5798332095146179, train/raw-loss = 0.5667840242385864, train/logprobs = tensor([[-0.7198, -2.1524],
        [-0.6296, -1.4093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13049191236495972
Epoch 0, Step 181: train/loss = 0.5761308670043945, train/raw-loss = 0.5690761804580688, train/logprobs = tensor([[-0.3254, -1.3084],
        [-0.3052, -0.6423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07054728269577026
Epoch 0, Step 182: train/loss = 0.5762214660644531, train/raw-loss = 0.5677368640899658, train/logprobs = tensor([[-0.5529, -1.3891],
        [-0.5426, -0.7238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08484575897455215
Epoch 0, Step 183: train/loss = 0.509580135345459, train/raw-loss = 0.49756747484207153, train/logprobs = tensor([[-0.5876, -2.2655],
        [-0.5666, -1.1414]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12012679874897003
Epoch 0, Step 184: train/loss = 0.5301640033721924, train/raw-loss = 0.5214225053787231, train/logprobs = tensor([[-0.4810, -1.8783],
        [-0.4827, -0.8080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08741545677185059
Epoch 0, Step 185: train/loss = 0.593633770942688, train/raw-loss = 0.5834092497825623, train/logprobs = tensor([[-0.5019, -1.3370],
        [-0.4440, -0.7619]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10224512219429016
Epoch 0, Step 186: train/loss = 0.6372941732406616, train/raw-loss = 0.6235942840576172, train/logprobs = tensor([[-0.7273, -0.9782],
        [-0.6608, -0.5898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13699859380722046
Epoch 0, Step 187: train/loss = 0.6180033087730408, train/raw-loss = 0.6067676544189453, train/logprobs = tensor([[-0.5944, -1.2622],
        [-0.5367, -0.7849]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11235667765140533
Epoch 0, Step 188: train/loss = 0.5406032800674438, train/raw-loss = 0.530261218547821, train/logprobs = tensor([[-0.5080, -1.8851],
        [-0.5222, -1.0767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10342033207416534
Epoch 0, Step 189: train/loss = 0.590904176235199, train/raw-loss = 0.5745478868484497, train/logprobs = tensor([[-0.9768, -1.7102],
        [-0.9864, -1.1526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16356231272220612
Epoch 0, Step 190: train/loss = 0.5792414546012878, train/raw-loss = 0.5676807761192322, train/logprobs = tensor([[-0.5973, -1.4899],
        [-0.5500, -0.6763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11560661345720291
Epoch 0, Step 191: train/loss = 0.5409685373306274, train/raw-loss = 0.5285282135009766, train/logprobs = tensor([[-0.6341, -2.5978],
        [-0.6243, -1.3942]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1244034692645073
Epoch 0, Step 192: train/loss = 0.5798521637916565, train/raw-loss = 0.569130003452301, train/logprobs = tensor([[-0.4685, -1.2617],
        [-0.4119, -0.5796]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1072215586900711
Epoch 0, Step 193: train/loss = 0.6590261459350586, train/raw-loss = 0.6442321538925171, train/logprobs = tensor([[-0.5824, -0.8521],
        [-0.5351, -0.5920]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14793895184993744
Epoch 0, Step 194: train/loss = 0.4837256669998169, train/raw-loss = 0.46762073040008545, train/logprobs = tensor([[-0.5937, -2.5496],
        [-0.5528, -1.1437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16104969382286072
Epoch 0, Step 195: train/loss = 0.6135167479515076, train/raw-loss = 0.601376473903656, train/logprobs = tensor([[-0.7891, -1.3319],
        [-0.5789, -0.5444]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12140240520238876
Epoch 0, Step 196: train/loss = 0.46787458658218384, train/raw-loss = 0.45377033948898315, train/logprobs = tensor([[-0.7224, -2.3683],
        [-0.6239, -0.7870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14104261994361877
Epoch 0, Step 197: train/loss = 0.5081017017364502, train/raw-loss = 0.495954304933548, train/logprobs = tensor([[-0.5420, -2.4989],
        [-0.4899, -1.1480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12147404253482819
Epoch 0, Step 198: train/loss = 0.5801746249198914, train/raw-loss = 0.5648770332336426, train/logprobs = tensor([[-0.5945, -1.4218],
        [-0.4878, -0.6455]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15297546982765198
Epoch 0, Step 199: train/loss = 0.46589016914367676, train/raw-loss = 0.4502629041671753, train/logprobs = tensor([[-0.6649, -3.7762],
        [-0.6426, -1.2803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15627239644527435
Epoch 0, Step 200: train/loss = 0.539954662322998, train/raw-loss = 0.5267611742019653, train/logprobs = tensor([[-0.6184, -1.7433],
        [-0.6152, -0.6770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13193488121032715
Epoch 0, Step 201: train/loss = 0.46567246317863464, train/raw-loss = 0.4524260461330414, train/logprobs = tensor([[-0.6367, -2.0806],
        [-0.5545, -0.6553]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13246414065361023
Epoch 0, Step 202: train/loss = 0.6516554951667786, train/raw-loss = 0.6409882307052612, train/logprobs = tensor([[-0.4801, -0.7215],
        [-0.4159, -0.4188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10667330771684647
Epoch 0, Step 203: train/loss = 0.5182299017906189, train/raw-loss = 0.5016871094703674, train/logprobs = tensor([[-0.7731, -2.0279],
        [-0.7087, -0.7961]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16542790830135345
Epoch 0, Step 204: train/loss = 0.6559545993804932, train/raw-loss = 0.6422959566116333, train/logprobs = tensor([[-0.7679, -0.9644],
        [-0.6565, -0.5476]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13658654689788818
Epoch 0, Step 205: train/loss = 0.5832788944244385, train/raw-loss = 0.5676085352897644, train/logprobs = tensor([[-0.6768, -1.6749],
        [-0.6074, -0.9927]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15670377016067505
Epoch 0, Step 206: train/loss = 0.4596736431121826, train/raw-loss = 0.44638824462890625, train/logprobs = tensor([[-0.5234, -2.2270],
        [-0.6901, -0.8210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13285385072231293
Epoch 0, Step 207: train/loss = 0.6061185598373413, train/raw-loss = 0.5921939015388489, train/logprobs = tensor([[-0.5215, -1.4331],
        [-0.4669, -0.8725]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13924628496170044
Epoch 0, Step 208: train/loss = 0.5845180153846741, train/raw-loss = 0.5679209232330322, train/logprobs = tensor([[-0.8246, -1.8210],
        [-0.6788, -0.9247]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1659713089466095
Epoch 0, Step 209: train/loss = 0.5495002865791321, train/raw-loss = 0.5362886190414429, train/logprobs = tensor([[-0.5371, -1.7166],
        [-0.4502, -0.6822]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13211730122566223
Epoch 0, Step 210: train/loss = 0.4291112422943115, train/raw-loss = 0.41500213742256165, train/logprobs = tensor([[-0.7556, -2.8563],
        [-0.7880, -1.0719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14109113812446594
Epoch 0, Step 211: train/loss = 0.568811297416687, train/raw-loss = 0.5565300583839417, train/logprobs = tensor([[-0.5227, -3.7359],
        [-0.5153, -1.3946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12281270325183868
Epoch 0, Step 212: train/loss = 0.4603027403354645, train/raw-loss = 0.4464467167854309, train/logprobs = tensor([[-0.7470, -4.8544],
        [-0.7167, -1.7878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1385602355003357
Epoch 0, Step 213: train/loss = 0.576106071472168, train/raw-loss = 0.5616893768310547, train/logprobs = tensor([[-0.7104, -1.4217],
        [-0.5797, -0.6338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14416733384132385
Epoch 0, Step 214: train/loss = 0.6131974458694458, train/raw-loss = 0.5959739089012146, train/logprobs = tensor([[-0.9461, -1.2903],
        [-0.8683, -0.7421]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17223596572875977
Epoch 0, Step 215: train/loss = 0.4878332018852234, train/raw-loss = 0.47191476821899414, train/logprobs = tensor([[-0.8327, -2.0157],
        [-0.8196, -0.7011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15918441116809845
Epoch 0, Step 216: train/loss = 0.48799005150794983, train/raw-loss = 0.4762188196182251, train/logprobs = tensor([[-0.4960, -2.8405],
        [-0.4493, -0.9508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11771219968795776
Epoch 0, Step 217: train/loss = 0.6086438894271851, train/raw-loss = 0.5925933122634888, train/logprobs = tensor([[-0.6588, -1.0782],
        [-0.5902, -0.5050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16050608456134796
Epoch 0, Step 218: train/loss = 0.4466497302055359, train/raw-loss = 0.433745801448822, train/logprobs = tensor([[-0.6066, -2.3461],
        [-0.5301, -0.6327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12903904914855957
Epoch 0, Step 219: train/loss = 0.4623998999595642, train/raw-loss = 0.44501417875289917, train/logprobs = tensor([[-0.7832, -2.8244],
        [-0.6592, -0.9173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1738569140434265
Epoch 0, Step 220: train/loss = 0.6859931945800781, train/raw-loss = 0.669607937335968, train/logprobs = tensor([[-0.5787, -0.7568],
        [-0.5192, -0.5954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1638527363538742
Epoch 0, Step 221: train/loss = 0.5529235601425171, train/raw-loss = 0.5397853851318359, train/logprobs = tensor([[-0.7683, -1.6161],
        [-0.7172, -0.7259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1313817948102951
Epoch 0, Step 222: train/loss = 0.571130096912384, train/raw-loss = 0.5583196878433228, train/logprobs = tensor([[-0.5204, -5.1059],
        [-0.4307, -2.2019]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12810423970222473
Epoch 0, Step 223: train/loss = 0.4749297797679901, train/raw-loss = 0.4600173830986023, train/logprobs = tensor([[-0.8242, -6.5783],
        [-0.6211, -2.4401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14912426471710205
Epoch 0, Step 224: train/loss = 0.6533758640289307, train/raw-loss = 0.6365174055099487, train/logprobs = tensor([[-0.7990, -1.1694],
        [-0.5795, -0.6155]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1685846894979477
Epoch 0, Step 225: train/loss = 0.5737543106079102, train/raw-loss = 0.5591911673545837, train/logprobs = tensor([[-0.6185, -1.2760],
        [-0.6155, -0.5894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14563143253326416
Epoch 0, Step 226: train/loss = 0.5061825513839722, train/raw-loss = 0.49059388041496277, train/logprobs = tensor([[-0.7692, -1.9669],
        [-0.7787, -0.9061]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1558867245912552
Epoch 0, Step 227: train/loss = 0.414728581905365, train/raw-loss = 0.39773666858673096, train/logprobs = tensor([[-0.8614, -3.9007],
        [-0.7602, -1.0772]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16991913318634033
Epoch 0, Step 228: train/loss = 0.5051077604293823, train/raw-loss = 0.4874865412712097, train/logprobs = tensor([[-0.5964, -2.0470],
        [-0.7499, -0.7990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17621192336082458
Epoch 0, Step 229: train/loss = 0.36810433864593506, train/raw-loss = 0.3488553762435913, train/logprobs = tensor([[-0.6599, -3.2043],
        [-0.9338, -0.8749]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19248968362808228
Epoch 0, Step 230: train/loss = 0.6028048992156982, train/raw-loss = 0.5848357677459717, train/logprobs = tensor([[-0.9892, -1.8874],
        [-0.7617, -0.7044]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1796913892030716
Epoch 0, Step 231: train/loss = 0.6232432723045349, train/raw-loss = 0.6076306700706482, train/logprobs = tensor([[-0.7129, -1.0914],
        [-0.7355, -0.7329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1561259776353836
Epoch 0, Step 232: train/loss = 0.5063162446022034, train/raw-loss = 0.48985666036605835, train/logprobs = tensor([[-0.6981, -1.9870],
        [-0.5920, -0.7243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16459596157073975
Epoch 0, Step 233: train/loss = 0.5363211631774902, train/raw-loss = 0.5196693539619446, train/logprobs = tensor([[-1.0342, -2.9334],
        [-0.7551, -1.0668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1665177196264267
Epoch 0, Step 234: train/loss = 0.44127142429351807, train/raw-loss = 0.4237803518772125, train/logprobs = tensor([[-0.7869, -3.8288],
        [-0.7652, -1.0321]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17491090297698975
Epoch 0, Step 235: train/loss = 0.5057883262634277, train/raw-loss = 0.4899534583091736, train/logprobs = tensor([[-0.5852, -2.5597],
        [-0.5228, -1.0585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1583486646413803
Epoch 0, Step 236: train/loss = 0.49116867780685425, train/raw-loss = 0.4756051301956177, train/logprobs = tensor([[-0.6824, -1.9615],
        [-0.7133, -0.7372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1556357592344284
Epoch 0, Step 237: train/loss = 0.6588671803474426, train/raw-loss = 0.6418513059616089, train/logprobs = tensor([[-0.9108, -0.7451],
        [-0.9352, -0.5318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1701589822769165
Epoch 0, Step 238: train/loss = 0.4686453342437744, train/raw-loss = 0.45237481594085693, train/logprobs = tensor([[-0.5803, -2.2000],
        [-0.6451, -0.7627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16270509362220764
Epoch 0, Step 239: train/loss = 0.5648186802864075, train/raw-loss = 0.5527280569076538, train/logprobs = tensor([[-0.5479, -1.6511],
        [-0.5668, -0.5062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12090601772069931
Epoch 0, Step 240: train/loss = 0.6234480142593384, train/raw-loss = 0.6073822975158691, train/logprobs = tensor([[-0.6859, -1.2525],
        [-0.5918, -0.7004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16065748035907745
Epoch 0, Step 241: train/loss = 0.35462281107902527, train/raw-loss = 0.34150004386901855, train/logprobs = tensor([[-0.5988, -3.2719],
        [-0.7658, -1.0544]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1312275528907776
Epoch 0, Step 242: train/loss = 0.470937043428421, train/raw-loss = 0.45313560962677, train/logprobs = tensor([[-0.7353, -2.1797],
        [-0.7350, -0.8206]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17801472544670105
Epoch 0, Step 243: train/loss = 0.46785497665405273, train/raw-loss = 0.4539296329021454, train/logprobs = tensor([[-0.8905, -4.7313],
        [-0.7653, -1.4972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1392536610364914
Epoch 0, Step 244: train/loss = 0.5932916402816772, train/raw-loss = 0.5751308798789978, train/logprobs = tensor([[-0.6281, -1.2716],
        [-0.6021, -0.7038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1816072314977646
Epoch 0, Step 245: train/loss = 0.6440103054046631, train/raw-loss = 0.6259439587593079, train/logprobs = tensor([[-0.8121, -1.4758],
        [-0.6659, -1.0082]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1806640625
Epoch 0, Step 246: train/loss = 0.630623459815979, train/raw-loss = 0.611749529838562, train/logprobs = tensor([[-0.7822, -1.3247],
        [-0.6414, -0.7614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18873915076255798
Epoch 0, Step 247: train/loss = 0.39799314737319946, train/raw-loss = 0.38618698716163635, train/logprobs = tensor([[-0.4820, -4.1320],
        [-0.5093, -1.0270]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11806190013885498
Epoch 0, Step 248: train/loss = 0.5031826496124268, train/raw-loss = 0.48749130964279175, train/logprobs = tensor([[-0.6366, -4.5910],
        [-0.6002, -1.4517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15691308677196503
Epoch 0, Step 249: train/loss = 0.4583388566970825, train/raw-loss = 0.4438249468803406, train/logprobs = tensor([[-0.6095, -4.2147],
        [-0.6178, -1.4427]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14513927698135376
Epoch 0, Step 250: train/loss = 0.5574830770492554, train/raw-loss = 0.5377930402755737, train/logprobs = tensor([[-0.8378, -2.1278],
        [-0.6701, -1.0009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19689981639385223
Epoch 0, Step 251: train/loss = 0.5902383327484131, train/raw-loss = 0.5739309787750244, train/logprobs = tensor([[-0.6682, -1.2136],
        [-0.6913, -0.6560]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16307316720485687
Epoch 0, Step 252: train/loss = 0.4786357879638672, train/raw-loss = 0.4618082642555237, train/logprobs = tensor([[-0.8138, -3.0969],
        [-0.7296, -0.9106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16827529668807983
Epoch 0, Step 253: train/loss = 0.5054306983947754, train/raw-loss = 0.49193263053894043, train/logprobs = tensor([[-0.4968, -2.0103],
        [-0.4391, -0.8187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1349807232618332
Epoch 0, Step 254: train/loss = 0.49379533529281616, train/raw-loss = 0.475577175617218, train/logprobs = tensor([[-0.8971, -2.3249],
        [-0.9659, -0.7599]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18218116462230682
Epoch 0, Step 255: train/loss = 0.6026980876922607, train/raw-loss = 0.5868827104568481, train/logprobs = tensor([[-0.5450, -1.1654],
        [-0.5091, -0.4685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1581539511680603
Epoch 0, Step 256: train/loss = 0.5826776027679443, train/raw-loss = 0.5649969577789307, train/logprobs = tensor([[-0.7516, -1.6176],
        [-0.7246, -0.7352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17680564522743225
Epoch 0, Step 257: train/loss = 0.5876204967498779, train/raw-loss = 0.5688373446464539, train/logprobs = tensor([[-1.1169, -2.2666],
        [-0.8805, -0.8088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18783143162727356
Epoch 0, Step 258: train/loss = 0.5689413547515869, train/raw-loss = 0.5477250218391418, train/logprobs = tensor([[-0.9864, -2.2641],
        [-1.0317, -1.1335]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21216358244419098
Epoch 0, Step 259: train/loss = 0.41402938961982727, train/raw-loss = 0.3973841071128845, train/logprobs = tensor([[-0.6819, -2.6019],
        [-0.7095, -0.6550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16645275056362152
Epoch 0, Step 260: train/loss = 0.45616793632507324, train/raw-loss = 0.43789440393447876, train/logprobs = tensor([[-0.6002, -3.0106],
        [-0.7127, -0.8477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1827351152896881
Epoch 0, Step 261: train/loss = 0.5360215902328491, train/raw-loss = 0.5179725289344788, train/logprobs = tensor([[-0.7051, -2.1196],
        [-0.6354, -0.7277]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1804906129837036
Epoch 0, Step 262: train/loss = 0.43941035866737366, train/raw-loss = 0.42395368218421936, train/logprobs = tensor([[-0.4745, -2.2735],
        [-0.6210, -0.6233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15456634759902954
Epoch 0, Step 263: train/loss = 0.48540830612182617, train/raw-loss = 0.4686697721481323, train/logprobs = tensor([[-0.7606, -2.1298],
        [-0.7910, -0.7773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16738538444042206
Epoch 0, Step 264: train/loss = 0.49402013421058655, train/raw-loss = 0.47665518522262573, train/logprobs = tensor([[-0.7519, -1.3338],
        [-0.9725, -0.3836]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17364932596683502
Epoch 0, Step 265: train/loss = 0.5430117845535278, train/raw-loss = 0.5282577276229858, train/logprobs = tensor([[-0.6744, -5.4953],
        [-0.8694, -2.0084]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14754043519496918
Epoch 0, Step 266: train/loss = 0.4347851574420929, train/raw-loss = 0.41551509499549866, train/logprobs = tensor([[-1.0196, -3.1178],
        [-1.0443, -0.8450]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19270096719264984
Epoch 0, Step 267: train/loss = 0.47440382838249207, train/raw-loss = 0.45768964290618896, train/logprobs = tensor([[-0.6942, -2.8478],
        [-0.7147, -0.7309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1671413630247116
Epoch 0, Step 268: train/loss = 0.48883000016212463, train/raw-loss = 0.4719843864440918, train/logprobs = tensor([[-0.5053, -1.9467],
        [-0.6868, -0.4190]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16845622658729553
Epoch 0, Step 269: train/loss = 0.5383602380752563, train/raw-loss = 0.5229954123497009, train/logprobs = tensor([[-0.8044, -4.0209],
        [-0.6526, -0.8705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1536484956741333
Epoch 0, Step 270: train/loss = 0.526288628578186, train/raw-loss = 0.5067086219787598, train/logprobs = tensor([[-0.6807, -3.5376],
        [-0.7304, -0.9731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19580067694187164
Epoch 0, Step 271: train/loss = 0.6043640375137329, train/raw-loss = 0.5894290804862976, train/logprobs = tensor([[-0.6435, -1.3111],
        [-0.5725, -0.7056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14934894442558289
Epoch 0, Step 272: train/loss = 0.36358603835105896, train/raw-loss = 0.34367817640304565, train/logprobs = tensor([[-0.9839, -2.8806],
        [-1.1858, -0.7482]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19907879829406738
Epoch 0, Step 273: train/loss = 0.340794175863266, train/raw-loss = 0.3210955858230591, train/logprobs = tensor([[-0.7526, -7.8267],
        [-0.9165, -1.6257]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19698604941368103
Epoch 0, Step 274: train/loss = 0.45298588275909424, train/raw-loss = 0.43455713987350464, train/logprobs = tensor([[-0.9177, -4.3777],
        [-0.9199, -0.9741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18428805470466614
Epoch 0, Step 275: train/loss = 0.4181484878063202, train/raw-loss = 0.39987432956695557, train/logprobs = tensor([[-0.6899, -2.7888],
        [-0.8610, -0.9210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18274156749248505
Epoch 0, Step 276: train/loss = 0.6643602252006531, train/raw-loss = 0.651302695274353, train/logprobs = tensor([[-1.3566, -2.4008],
        [-0.7511, -0.6185]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1305752694606781
Epoch 0, Step 277: train/loss = 0.5224077701568604, train/raw-loss = 0.505780041217804, train/logprobs = tensor([[-1.3492, -3.6979],
        [-0.9201, -1.1024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1662769913673401
Epoch 0, Step 278: train/loss = 0.42718490958213806, train/raw-loss = 0.4119233191013336, train/logprobs = tensor([[-0.5815, -3.0907],
        [-0.6533, -1.0879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15261605381965637
Epoch 0, Step 279: train/loss = 0.4845277667045593, train/raw-loss = 0.46580031514167786, train/logprobs = tensor([[-0.7742, -2.8813],
        [-0.8360, -0.9299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1872745007276535
Epoch 0, Step 280: train/loss = 0.4459700584411621, train/raw-loss = 0.4308232069015503, train/logprobs = tensor([[-0.5096, -2.7440],
        [-0.6078, -0.7553]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1514681875705719
Epoch 0, Step 281: train/loss = 0.5391173362731934, train/raw-loss = 0.5178073048591614, train/logprobs = tensor([[-1.0211, -4.6195],
        [-0.7440, -1.1637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21310049295425415
Epoch 0, Step 282: train/loss = 0.5156289935112, train/raw-loss = 0.49789881706237793, train/logprobs = tensor([[-0.8130, -2.3601],
        [-0.9584, -1.3201]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.177301824092865
Epoch 0, Step 283: train/loss = 0.5945359468460083, train/raw-loss = 0.5769472122192383, train/logprobs = tensor([[-0.5840, -1.4378],
        [-0.6402, -0.8898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17588764429092407
Epoch 0, Step 284: train/loss = 0.5627841949462891, train/raw-loss = 0.5444542169570923, train/logprobs = tensor([[-0.7406, -1.3401],
        [-0.7548, -0.5710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18329934775829315
Epoch 0, Step 285: train/loss = 0.4010273814201355, train/raw-loss = 0.3816930949687958, train/logprobs = tensor([[-0.7390, -3.4470],
        [-1.0925, -0.8809]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19334279000759125
Epoch 0, Step 286: train/loss = 0.6163707971572876, train/raw-loss = 0.6009589433670044, train/logprobs = tensor([[-0.5965, -1.1374],
        [-0.5771, -0.6928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15411829948425293
Epoch 0, Step 287: train/loss = 0.46274980902671814, train/raw-loss = 0.4459993541240692, train/logprobs = tensor([[-0.6423, -3.0169],
        [-0.6134, -0.9292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1675046682357788
Epoch 0, Step 288: train/loss = 0.44314947724342346, train/raw-loss = 0.4256705641746521, train/logprobs = tensor([[-0.8119, -4.9197],
        [-0.8000, -1.1174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17478930950164795
Epoch 0, Step 289: train/loss = 0.625976026058197, train/raw-loss = 0.609260082244873, train/logprobs = tensor([[-0.6148, -1.1317],
        [-0.4765, -0.4400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1671595275402069
Epoch 0, Step 290: train/loss = 0.4062468409538269, train/raw-loss = 0.390014111995697, train/logprobs = tensor([[-0.7470, -3.1389],
        [-0.7242, -0.7792]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1623275727033615
Epoch 0, Step 291: train/loss = 0.41672369837760925, train/raw-loss = 0.39665457606315613, train/logprobs = tensor([[-1.0418, -3.8827],
        [-1.0431, -1.0047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20069149136543274
Epoch 0, Step 292: train/loss = 0.46627864241600037, train/raw-loss = 0.4522649049758911, train/logprobs = tensor([[-0.5386, -1.8817],
        [-0.6294, -0.6946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14013715088367462
Epoch 0, Step 293: train/loss = 0.4475570023059845, train/raw-loss = 0.4279605746269226, train/logprobs = tensor([[-0.8149, -4.3307],
        [-0.9651, -1.0578]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19596441090106964
Epoch 0, Step 294: train/loss = 0.41414064168930054, train/raw-loss = 0.39412808418273926, train/logprobs = tensor([[-0.6740, -2.2802],
        [-0.9257, -0.7973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20012575387954712
Epoch 0, Step 295: train/loss = 0.5005005598068237, train/raw-loss = 0.48599979281425476, train/logprobs = tensor([[-0.6347, -1.9625],
        [-0.7976, -0.5319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1450079083442688
Epoch 0, Step 296: train/loss = 0.48525023460388184, train/raw-loss = 0.47131896018981934, train/logprobs = tensor([[-0.4936, -4.6941],
        [-0.6304, -1.1312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13931311666965485
