0
8
{'model': {'model_type': 'huggingface', 'name': 'mistral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1', 'model_max_length': 2048}}, 'data_path': 'data/iteration_2', 'data_file': 'iteration_2_epoch_0.43.json', 'n_examples': 2000, 'n_responses': 2, 'n_constitutions': 2, 'wandb': {'project': 'typo-summarization', 'name': 'typo-lr-1e-7-iteration-3', 'log': True}, 'typo': {'beta': 0.0}, 'training': {'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 1e-07, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 16, 'save_after_n_steps': 25, 'seed': 42, 'model_archive': None}}
8
[2024-03-25 15:46:51,753][root][INFO] - beta: 0.0
[2024-03-25 15:46:51,753][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
n examples: 2000
{'prompt_c0_r0': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is funny. It includes humor and jokes.\n2. Is concise. It is around 2-3 sentences long.\n\nfunny and concise summary of the POST: ", 'response_c0_r0': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is funny. It includes humor and jokes.\n2. Is concise. It is around 2-3 sentences long.\n\nfunny and concise summary of the POST: > My mum is mad at me because I booked a flight to go home from the west coast. She told me she didn't want me to fly on my own. I am fine flying on my own.", 'prompt_c0_r1': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is funny. It includes humor and jokes.\n2. Is concise. It is around 2-3 sentences long.\n\nfunny and concise summary of the POST: ", 'response_c0_r1': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is funny. It includes humor and jokes.\n2. Is concise. It is around 2-3 sentences long.\n\nfunny and concise summary of the POST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the", 'prompt_c1_r0': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is long. It includes details and redundancy.\n2. Is serious. It is formal and lacks humor.\n\nserious and long summary of the POST: ", 'response_c1_r0': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is long. It includes details and redundancy.\n2. Is serious. It is formal and lacks humor.\n\nserious and long summary of the POST: > My mum is mad at me because I booked a flight to go home from the west coast. She told me she didn't want me to fly on my own. I am fine flying on my own.", 'prompt_c1_r1': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is long. It includes details and redundancy.\n2. Is serious. It is formal and lacks humor.\n\nserious and long summary of the POST: ", 'response_c1_r1': "SUBREDDIT: r/relationships\nTITLE: Mother [51] not speaking to me [21] because of a trip I am planning\nPOST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n\nBelow is a summary of the above POST. Importantly, the summary:\n1. Is long. It includes details and redundancy.\n2. Is serious. It is formal and lacks humor.\n\nserious and long summary of the POST: My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n\nThe plan was for me to stay on the"}
2000
tokenized 2000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 6
Loaded reference model on rank 6
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 4
Loaded reference model on rank 4
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 7
Loaded reference model on rank 7
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 5
Loaded reference model on rank 5
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-1e-7-iteration-3-from-epoch-0.43.
Epoch 0, Step 0: train/loss = 0.6753181219100952, train/raw-loss = 0.6753181219100952, train/logprobs = tensor([[-0.5722, -0.1690],
        [-0.7056, -0.1456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6892310380935669, train/raw-loss = 0.6892310380935669, train/logprobs = tensor([[-0.4220, -0.3621],
        [-0.4256, -0.3256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.707165539264679, train/raw-loss = 0.707165539264679, train/logprobs = tensor([[-0.4586, -0.1570],
        [-0.4721, -0.1292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6991696953773499, train/raw-loss = 0.6991696953773499, train/logprobs = tensor([[-0.2944, -0.0415],
        [-0.2846, -0.0282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6976156830787659, train/raw-loss = 0.6976156830787659, train/logprobs = tensor([[-0.4699, -0.0782],
        [-0.5142, -0.0480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.7043843269348145, train/raw-loss = 0.7043843269348145, train/logprobs = tensor([[-0.7536, -0.1364],
        [-0.8205, -0.1244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.7040550708770752, train/raw-loss = 0.7040550708770752, train/logprobs = tensor([[-0.4873, -0.1919],
        [-0.5415, -0.1619]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6880743503570557, train/raw-loss = 0.6880743503570557, train/logprobs = tensor([[-0.6087, -0.2608],
        [-0.6869, -0.2056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.7000287175178528, train/raw-loss = 0.7000287175178528, train/logprobs = tensor([[-0.3317, -0.1183],
        [-0.3019, -0.1011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.714273989200592, train/raw-loss = 0.714273989200592, train/logprobs = tensor([[-0.6704, -0.0874],
        [-0.6881, -0.0716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6991709470748901, train/raw-loss = 0.6991709470748901, train/logprobs = tensor([[-0.4879, -0.1844],
        [-0.4953, -0.1751]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.707135796546936, train/raw-loss = 0.707135796546936, train/logprobs = tensor([[-0.7728, -0.1539],
        [-0.8810, -0.1412]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6989632844924927, train/raw-loss = 0.6989632844924927, train/logprobs = tensor([[-0.4226, -0.2062],
        [-0.4284, -0.1813]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6915874481201172, train/raw-loss = 0.6915874481201172, train/logprobs = tensor([[-0.4458, -0.1714],
        [-0.4426, -0.1186]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.7006357908248901, train/raw-loss = 0.7006357908248901, train/logprobs = tensor([[-0.6059, -0.2117],
        [-0.6249, -0.1829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.7044795751571655, train/raw-loss = 0.7044795751571655, train/logprobs = tensor([[-0.5219, -0.1088],
        [-0.5295, -0.0909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6938386559486389, train/raw-loss = 0.6938386559486389, train/logprobs = tensor([[-0.2976, -0.1355],
        [-0.3015, -0.1212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6947197318077087, train/raw-loss = 0.6947197318077087, train/logprobs = tensor([[-0.3896, -0.2407],
        [-0.3573, -0.2046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.7085832357406616, train/raw-loss = 0.7085832357406616, train/logprobs = tensor([[-0.5888, -0.0866],
        [-0.6367, -0.0719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.6931197047233582, train/raw-loss = 0.6931197047233582, train/logprobs = tensor([[-0.3171, -0.1882],
        [-0.3258, -0.1510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6905321478843689, train/raw-loss = 0.6905321478843689, train/logprobs = tensor([[-0.4604, -0.0337],
        [-0.5619, -0.0197]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6947616934776306, train/raw-loss = 0.6947616934776306, train/logprobs = tensor([[-0.4392, -0.2216],
        [-0.4789, -0.1944]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.7035252451896667, train/raw-loss = 0.7035252451896667, train/logprobs = tensor([[-0.5333, -0.1915],
        [-0.5637, -0.1691]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.7052361965179443, train/raw-loss = 0.7052361965179443, train/logprobs = tensor([[-0.4579, -0.1602],
        [-0.4386, -0.1161]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6910279989242554, train/raw-loss = 0.6910279989242554, train/logprobs = tensor([[-0.3166, -0.1783],
        [-0.3053, -0.1361]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Model saved, deleting model...
Deleted model...
Epoch 0, Step 25: train/loss = 0.7040569186210632, train/raw-loss = 0.7040569186210632, train/logprobs = tensor([[-0.4975, -0.1286],
        [-0.5114, -0.1003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6988105773925781, train/raw-loss = 0.6988105773925781, train/logprobs = tensor([[-0.5990, -0.0992],
        [-0.6566, -0.0697]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6923199892044067, train/raw-loss = 0.6923199892044067, train/logprobs = tensor([[-0.3976, -0.3553],
        [-0.4052, -0.3228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6984268426895142, train/raw-loss = 0.6984268426895142, train/logprobs = tensor([[-0.5169, -0.3110],
        [-0.5090, -0.2727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6980901956558228, train/raw-loss = 0.6980901956558228, train/logprobs = tensor([[-0.4626, -0.2090],
        [-0.4498, -0.1726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.7000145316123962, train/raw-loss = 0.7000145316123962, train/logprobs = tensor([[-0.4005, -0.1939],
        [-0.4191, -0.1669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6959864497184753, train/raw-loss = 0.6959864497184753, train/logprobs = tensor([[-0.4583, -0.1071],
        [-0.4802, -0.0828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.6982659101486206, train/raw-loss = 0.6982659101486206, train/logprobs = tensor([[-0.3731, -0.0964],
        [-0.3737, -0.0829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00026442864327691495
Epoch 0, Step 33: train/loss = 0.6889109015464783, train/raw-loss = 0.6889109015464783, train/logprobs = tensor([[-0.5481, -0.1595],
        [-0.6551, -0.1402]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003330897307023406
Epoch 0, Step 34: train/loss = 0.7009586691856384, train/raw-loss = 0.7009586691856384, train/logprobs = tensor([[-0.5558, -0.1490],
        [-0.5895, -0.1165]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028862946783192456
Epoch 0, Step 35: train/loss = 0.6948207020759583, train/raw-loss = 0.6948207020759583, train/logprobs = tensor([[-0.5091, -0.0926],
        [-0.5790, -0.0706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028730856138281524
Epoch 0, Step 36: train/loss = 0.6968268752098083, train/raw-loss = 0.6968268752098083, train/logprobs = tensor([[-0.1892, -0.2386],
        [-0.1656, -0.2086]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002610576921142638
Epoch 0, Step 37: train/loss = 0.7020391821861267, train/raw-loss = 0.7020391821861267, train/logprobs = tensor([[-0.4979, -0.0514],
        [-0.5240, -0.0338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028985811513848603
Epoch 0, Step 38: train/loss = 0.7033759951591492, train/raw-loss = 0.7033759951591492, train/logprobs = tensor([[-0.3964, -0.0391],
        [-0.3924, -0.0235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00026271719252690673
Epoch 0, Step 39: train/loss = 0.700072169303894, train/raw-loss = 0.700072169303894, train/logprobs = tensor([[-0.4923, -0.1703],
        [-0.4882, -0.1402]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002835705818142742
Epoch 0, Step 40: train/loss = 0.7030125260353088, train/raw-loss = 0.7030125260353088, train/logprobs = tensor([[-0.4121, -0.2657],
        [-0.4153, -0.2391]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002667103835847229
Epoch 0, Step 41: train/loss = 0.6980950236320496, train/raw-loss = 0.6980950236320496, train/logprobs = tensor([[-0.5273, -0.2278],
        [-0.5087, -0.1823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028628268046304584
Epoch 0, Step 42: train/loss = 0.7107580900192261, train/raw-loss = 0.7107580900192261, train/logprobs = tensor([[-0.5100, -0.1421],
        [-0.4986, -0.1314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002833664184436202
Epoch 0, Step 43: train/loss = 0.7031260132789612, train/raw-loss = 0.7031260132789612, train/logprobs = tensor([[-0.5438, -0.2611],
        [-0.5164, -0.2265]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002869087620638311
Epoch 0, Step 44: train/loss = 0.703190803527832, train/raw-loss = 0.703190803527832, train/logprobs = tensor([[-0.6593, -0.1963],
        [-0.6808, -0.1702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028736155945807695
Epoch 0, Step 45: train/loss = 0.695501983165741, train/raw-loss = 0.695501983165741, train/logprobs = tensor([[-0.5681, -0.3109],
        [-0.5454, -0.2538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00027631939155980945
Epoch 0, Step 46: train/loss = 0.6925067901611328, train/raw-loss = 0.6925067901611328, train/logprobs = tensor([[-0.5792, -0.2526],
        [-0.6070, -0.1808]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002909897593781352
Epoch 0, Step 47: train/loss = 0.6973783373832703, train/raw-loss = 0.6973783373832703, train/logprobs = tensor([[-0.4527, -0.1651],
        [-0.4512, -0.1414]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00027961601153947413
Epoch 0, Step 48: train/loss = 0.7000818252563477, train/raw-loss = 0.7000818252563477, train/logprobs = tensor([[-0.5489, -0.1818],
        [-0.5535, -0.1571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002803479437716305
Epoch 0, Step 49: train/loss = 0.6939122080802917, train/raw-loss = 0.6939122080802917, train/logprobs = tensor([[-0.5441, -0.3074],
        [-0.5554, -0.2836]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028605523402802646
Model saved, deleting model...
Deleted model...
Epoch 0, Step 50: train/loss = 0.6941398978233337, train/raw-loss = 0.6941398978233337, train/logprobs = tensor([[-0.4735, -0.3304],
        [-0.4698, -0.2713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028571777511388063
Epoch 0, Step 51: train/loss = 0.7006049752235413, train/raw-loss = 0.7006049752235413, train/logprobs = tensor([[-0.2583, -0.0453],
        [-0.2418, -0.0334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00026803731452673674
Epoch 0, Step 52: train/loss = 0.6975921988487244, train/raw-loss = 0.6975921988487244, train/logprobs = tensor([[-0.3773, -0.1078],
        [-0.3753, -0.0897]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029067392461001873
Epoch 0, Step 53: train/loss = 0.6939409375190735, train/raw-loss = 0.6939409375190735, train/logprobs = tensor([[-0.3312, -0.0460],
        [-0.3514, -0.0310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000261167180724442
Epoch 0, Step 54: train/loss = 0.6910682320594788, train/raw-loss = 0.6910682320594788, train/logprobs = tensor([[-0.2486, -0.0904],
        [-0.2582, -0.0696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00026347243692725897
Epoch 0, Step 55: train/loss = 0.7011866569519043, train/raw-loss = 0.7011866569519043, train/logprobs = tensor([[-0.6126, -0.1802],
        [-0.6508, -0.1654]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003078374429605901
Epoch 0, Step 56: train/loss = 0.7088931202888489, train/raw-loss = 0.7088931202888489, train/logprobs = tensor([[-0.5938, -0.0645],
        [-0.6513, -0.0507]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003012366942130029
Epoch 0, Step 57: train/loss = 0.699082612991333, train/raw-loss = 0.699082612991333, train/logprobs = tensor([[-0.2998, -0.2328],
        [-0.3829, -0.2196]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00025915962760336697
Epoch 0, Step 58: train/loss = 0.6986459493637085, train/raw-loss = 0.6986459493637085, train/logprobs = tensor([[-0.3572, -0.1828],
        [-0.3637, -0.1501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003048299113288522
Epoch 0, Step 59: train/loss = 0.6930215954780579, train/raw-loss = 0.6930215954780579, train/logprobs = tensor([[-0.6244, -0.2558],
        [-0.6989, -0.2309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029858024208806455
Epoch 0, Step 60: train/loss = 0.7114182114601135, train/raw-loss = 0.7114182114601135, train/logprobs = tensor([[-0.6985, -0.1733],
        [-0.7139, -0.1540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030546842026524246
Epoch 0, Step 61: train/loss = 0.688530683517456, train/raw-loss = 0.688530683517456, train/logprobs = tensor([[-0.3039, -0.1142],
        [-0.3180, -0.0845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028281318373046815
Epoch 0, Step 62: train/loss = 0.6942509412765503, train/raw-loss = 0.6942509412765503, train/logprobs = tensor([[-0.3950, -0.0447],
        [-0.4400, -0.0267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002697166637517512
Epoch 0, Step 63: train/loss = 0.6798452734947205, train/raw-loss = 0.6798452734947205, train/logprobs = tensor([[-0.6099, -0.3310],
        [-0.6903, -0.2910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029336067382246256
Epoch 0, Step 64: train/loss = 0.7044289112091064, train/raw-loss = 0.7044289112091064, train/logprobs = tensor([[-0.7142, -0.2864],
        [-0.7402, -0.2452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003058981674257666
Epoch 0, Step 65: train/loss = 0.6998394727706909, train/raw-loss = 0.6998394727706909, train/logprobs = tensor([[-0.5988, -0.0703],
        [-0.6632, -0.0570]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028975316672585905
Epoch 0, Step 66: train/loss = 0.7042388319969177, train/raw-loss = 0.7042388319969177, train/logprobs = tensor([[-0.5255, -0.0341],
        [-0.5859, -0.0207]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002765114768408239
Epoch 0, Step 67: train/loss = 0.7016109824180603, train/raw-loss = 0.7016109824180603, train/logprobs = tensor([[-0.5408, -0.1919],
        [-0.6022, -0.1480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029344612266868353
Epoch 0, Step 68: train/loss = 0.6981433629989624, train/raw-loss = 0.6981433629989624, train/logprobs = tensor([[-0.2621, -0.1420],
        [-0.2550, -0.1150]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002661655889824033
Epoch 0, Step 69: train/loss = 0.6900702714920044, train/raw-loss = 0.6900702714920044, train/logprobs = tensor([[-0.3009, -0.1331],
        [-0.3095, -0.1074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00028942347853444517
Epoch 0, Step 70: train/loss = 0.6845520734786987, train/raw-loss = 0.6845520734786987, train/logprobs = tensor([[-0.6956, -0.3143],
        [-0.7739, -0.2832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030958408024162054
Epoch 0, Step 71: train/loss = 0.6759220957756042, train/raw-loss = 0.6759220957756042, train/logprobs = tensor([[-0.3953, -0.3090],
        [-0.4350, -0.2547]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002836282073985785
Epoch 0, Step 72: train/loss = 0.7061929702758789, train/raw-loss = 0.7061929702758789, train/logprobs = tensor([[-0.5067, -0.0952],
        [-0.5424, -0.0838]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002953938383143395
Epoch 0, Step 73: train/loss = 0.6888913512229919, train/raw-loss = 0.6888913512229919, train/logprobs = tensor([[-0.2944, -0.1949],
        [-0.3036, -0.1687]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00027347251307219267
Epoch 0, Step 74: train/loss = 0.6911643743515015, train/raw-loss = 0.6911643743515015, train/logprobs = tensor([[-0.6401, -0.2695],
        [-0.7197, -0.2328]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000316913501592353
Model saved, deleting model...
Deleted model...
Epoch 0, Step 75: train/loss = 0.6967896819114685, train/raw-loss = 0.6967896819114685, train/logprobs = tensor([[-0.5783, -0.2401],
        [-0.6416, -0.2336]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030499816057272255
Epoch 0, Step 76: train/loss = 0.7012820243835449, train/raw-loss = 0.7012820243835449, train/logprobs = tensor([[-0.4891, -0.0719],
        [-0.5210, -0.0612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002979442069772631
Epoch 0, Step 77: train/loss = 0.6946268081665039, train/raw-loss = 0.6946268081665039, train/logprobs = tensor([[-0.4379, -0.0734],
        [-0.4738, -0.0574]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029271456878632307
Epoch 0, Step 78: train/loss = 0.7017936706542969, train/raw-loss = 0.7017936706542969, train/logprobs = tensor([[-0.5650, -0.1982],
        [-0.5842, -0.1572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030269703711383045
Epoch 0, Step 79: train/loss = 0.6933760046958923, train/raw-loss = 0.6933760046958923, train/logprobs = tensor([[-0.4185, -0.1829],
        [-0.4136, -0.1523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002893636701628566
Epoch 0, Step 80: train/loss = 0.70381760597229, train/raw-loss = 0.70381760597229, train/logprobs = tensor([[-0.4939, -0.1095],
        [-0.5224, -0.0938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003053526161238551
Epoch 0, Step 81: train/loss = 0.6942156553268433, train/raw-loss = 0.6942156553268433, train/logprobs = tensor([[-0.4720, -0.1460],
        [-0.5003, -0.1164]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00031708015012554824
Epoch 0, Step 82: train/loss = 0.6871762275695801, train/raw-loss = 0.6871762275695801, train/logprobs = tensor([[-0.4357, -0.3062],
        [-0.4509, -0.2611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029332327540032566
Epoch 0, Step 83: train/loss = 0.6971977949142456, train/raw-loss = 0.6971977949142456, train/logprobs = tensor([[-0.4720, -0.1586],
        [-0.4840, -0.1415]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000309978291625157
Epoch 0, Step 84: train/loss = 0.7026666402816772, train/raw-loss = 0.7026666402816772, train/logprobs = tensor([[-0.5999, -0.1462],
        [-0.6313, -0.1233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00032428427948616445
Epoch 0, Step 85: train/loss = 0.6925102472305298, train/raw-loss = 0.6925102472305298, train/logprobs = tensor([[-0.5639, -0.2078],
        [-0.6540, -0.1769]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000305720983305946
Epoch 0, Step 86: train/loss = 0.7019497156143188, train/raw-loss = 0.7019497156143188, train/logprobs = tensor([[-0.3788, -0.2962],
        [-0.3907, -0.2610]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002877522201742977
Epoch 0, Step 87: train/loss = 0.6906782388687134, train/raw-loss = 0.6906782388687134, train/logprobs = tensor([[-0.6181, -0.4695],
        [-0.6310, -0.4197]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003150771954096854
Epoch 0, Step 88: train/loss = 0.6980814933776855, train/raw-loss = 0.6980814933776855, train/logprobs = tensor([[-0.3450, -0.0765],
        [-0.3619, -0.0604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002840830129571259
Epoch 0, Step 89: train/loss = 0.703364372253418, train/raw-loss = 0.703364372253418, train/logprobs = tensor([[-0.6964, -0.0763],
        [-0.7571, -0.0477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00031891127582639456
Epoch 0, Step 90: train/loss = 0.6863993406295776, train/raw-loss = 0.6863993406295776, train/logprobs = tensor([[-0.3204, -0.1657],
        [-0.3425, -0.1306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003161770582664758
Epoch 0, Step 91: train/loss = 0.696635365486145, train/raw-loss = 0.696635365486145, train/logprobs = tensor([[-0.4639, -0.1528],
        [-0.4941, -0.1370]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003170757554471493
Epoch 0, Step 92: train/loss = 0.6881363987922668, train/raw-loss = 0.6881363987922668, train/logprobs = tensor([[-0.3723, -0.1850],
        [-0.4012, -0.1600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002895106445066631
Epoch 0, Step 93: train/loss = 0.6964238882064819, train/raw-loss = 0.6964238882064819, train/logprobs = tensor([[-0.5920, -0.1794],
        [-0.6809, -0.1686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003016878617927432
Epoch 0, Step 94: train/loss = 0.7124772667884827, train/raw-loss = 0.7124772667884827, train/logprobs = tensor([[-0.8402, -0.1292],
        [-0.9426, -0.1067]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00031826301710680127
Epoch 0, Step 95: train/loss = 0.6922340989112854, train/raw-loss = 0.6922340989112854, train/logprobs = tensor([[-0.3164, -0.1820],
        [-0.3372, -0.1526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029019193607382476
Epoch 0, Step 96: train/loss = 0.6957892179489136, train/raw-loss = 0.6957892179489136, train/logprobs = tensor([[-0.4656, -0.2651],
        [-0.4842, -0.2345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030924566090106964
Epoch 0, Step 97: train/loss = 0.6873897910118103, train/raw-loss = 0.6873897910118103, train/logprobs = tensor([[-0.5003, -0.1632],
        [-0.6035, -0.1455]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00031053685233928263
Epoch 0, Step 98: train/loss = 0.6849949955940247, train/raw-loss = 0.6849949955940247, train/logprobs = tensor([[-0.6765, -0.2586],
        [-0.8002, -0.2176]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003269323206041008
Epoch 0, Step 99: train/loss = 0.6922414898872375, train/raw-loss = 0.6922414898872375, train/logprobs = tensor([[-0.4464, -0.2029],
        [-0.4826, -0.1683]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003166567184962332
Model saved, deleting model...
Deleted model...
Epoch 0, Step 100: train/loss = 0.7014599442481995, train/raw-loss = 0.7014599442481995, train/logprobs = tensor([[-0.4419, -0.0742],
        [-0.4536, -0.0542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003149441326968372
Epoch 0, Step 101: train/loss = 0.7019098997116089, train/raw-loss = 0.7019098997116089, train/logprobs = tensor([[-0.5350, -0.2948],
        [-0.5389, -0.2463]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003205158864147961
Epoch 0, Step 102: train/loss = 0.6983672380447388, train/raw-loss = 0.6983672380447388, train/logprobs = tensor([[-0.4943, -0.1267],
        [-0.5241, -0.0977]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000330127018969506
Epoch 0, Step 103: train/loss = 0.6919980645179749, train/raw-loss = 0.6919980645179749, train/logprobs = tensor([[-0.3147, -0.1894],
        [-0.3460, -0.1867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002945160958915949
Epoch 0, Step 104: train/loss = 0.6942600607872009, train/raw-loss = 0.6942600607872009, train/logprobs = tensor([[-0.3606, -0.2264],
        [-0.3825, -0.1974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030022431747056544
Epoch 0, Step 105: train/loss = 0.6882067918777466, train/raw-loss = 0.6882067918777466, train/logprobs = tensor([[-0.2651, -0.1960],
        [-0.2651, -0.1523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029893679311499
Epoch 0, Step 106: train/loss = 0.6862282752990723, train/raw-loss = 0.6862282752990723, train/logprobs = tensor([[-0.1661, -0.0929],
        [-0.1846, -0.0716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002997679985128343
Epoch 0, Step 107: train/loss = 0.6967161893844604, train/raw-loss = 0.6967161893844604, train/logprobs = tensor([[-0.5201, -0.1578],
        [-0.5365, -0.1375]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003233436727896333
Epoch 0, Step 108: train/loss = 0.6827608346939087, train/raw-loss = 0.6827608346939087, train/logprobs = tensor([[-0.5512, -0.2001],
        [-0.6178, -0.1554]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003327510494273156
Epoch 0, Step 109: train/loss = 0.7038668990135193, train/raw-loss = 0.7038668990135193, train/logprobs = tensor([[-0.4953, -0.1007],
        [-0.5011, -0.0758]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029896036721765995
Epoch 0, Step 110: train/loss = 0.6858254671096802, train/raw-loss = 0.6858254671096802, train/logprobs = tensor([[-0.3822, -0.0891],
        [-0.4469, -0.0680]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003124417271465063
Epoch 0, Step 111: train/loss = 0.6879453063011169, train/raw-loss = 0.6879453063011169, train/logprobs = tensor([[-0.3302, -0.0647],
        [-0.3706, -0.0496]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003026931080967188
Epoch 0, Step 112: train/loss = 0.6865670680999756, train/raw-loss = 0.6865670680999756, train/logprobs = tensor([[-0.2712, -0.1474],
        [-0.2961, -0.1286]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002962842700071633
Epoch 0, Step 113: train/loss = 0.705353319644928, train/raw-loss = 0.705353319644928, train/logprobs = tensor([[-0.5083, -0.1709],
        [-0.5410, -0.1410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003022006421815604
Epoch 0, Step 114: train/loss = 0.6942324638366699, train/raw-loss = 0.6942324638366699, train/logprobs = tensor([[-0.3356, -0.0749],
        [-0.3543, -0.0601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029622524743899703
Epoch 0, Step 115: train/loss = 0.6964156031608582, train/raw-loss = 0.6964156031608582, train/logprobs = tensor([[-0.5939, -0.1441],
        [-0.6276, -0.1168]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003259072545915842
Epoch 0, Step 116: train/loss = 0.686432957649231, train/raw-loss = 0.686432957649231, train/logprobs = tensor([[-0.6181, -0.2882],
        [-0.6779, -0.2510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00033154807169921696
Epoch 0, Step 117: train/loss = 0.6951850056648254, train/raw-loss = 0.6951850056648254, train/logprobs = tensor([[-0.6032, -0.3200],
        [-0.6265, -0.2866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00032661875593476
Epoch 0, Step 118: train/loss = 0.7140032052993774, train/raw-loss = 0.7140032052993774, train/logprobs = tensor([[-0.6587, -0.1524],
        [-0.7091, -0.1322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003342634008731693
Epoch 0, Step 119: train/loss = 0.6931778788566589, train/raw-loss = 0.6931778788566589, train/logprobs = tensor([[-0.3992, -0.1068],
        [-0.4382, -0.0979]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003174317826051265
Epoch 0, Step 120: train/loss = 0.7067516446113586, train/raw-loss = 0.7067516446113586, train/logprobs = tensor([[-0.5104, -0.0891],
        [-0.4972, -0.0642]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003266658750362694
Epoch 0, Step 121: train/loss = 0.7120170593261719, train/raw-loss = 0.7120170593261719, train/logprobs = tensor([[-0.6636, -0.1706],
        [-0.6892, -0.1473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00032894598552957177
Epoch 0, Step 122: train/loss = 0.6959018707275391, train/raw-loss = 0.6959018707275391, train/logprobs = tensor([[-0.4274, -0.0970],
        [-0.4764, -0.0779]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002952176728285849
Epoch 0, Step 123: train/loss = 0.7008094787597656, train/raw-loss = 0.7008094787597656, train/logprobs = tensor([[-0.5038, -0.1097],
        [-0.5505, -0.0789]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00033294528839178383
Epoch 0, Step 124: train/loss = 0.7035699486732483, train/raw-loss = 0.7035699486732483, train/logprobs = tensor([[-0.4404, -0.0735],
        [-0.4737, -0.0539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00030797041836194694
Model saved, deleting model...
Deleted model...
Epoch 0, Step 125: train/loss = 0.6982973217964172, train/raw-loss = 0.6982973217964172, train/logprobs = tensor([[-0.2474, -0.0780],
        [-0.2318, -0.0587]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003008641942869872
Epoch 0, Step 126: train/loss = 0.6833429336547852, train/raw-loss = 0.6833429336547852, train/logprobs = tensor([[-0.5648, -0.1766],
        [-0.6716, -0.1465]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00031608561403118074
Epoch 0, Step 127: train/loss = 0.697132408618927, train/raw-loss = 0.697132408618927, train/logprobs = tensor([[-0.5265, -0.2213],
        [-0.5647, -0.2142]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00033909737248905003
Epoch 0, Step 128: train/loss = 0.6902089715003967, train/raw-loss = 0.6902089715003967, train/logprobs = tensor([[-0.4179, -0.1568],
        [-0.4387, -0.1333]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003474525292403996
Epoch 0, Step 129: train/loss = 0.6816331148147583, train/raw-loss = 0.6816331148147583, train/logprobs = tensor([[-0.3608, -0.1213],
        [-0.4338, -0.1063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00033943256130442023
Epoch 0, Step 130: train/loss = 0.7023367881774902, train/raw-loss = 0.7023367881774902, train/logprobs = tensor([[-0.6305, -0.0876],
        [-0.6937, -0.0693]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00034947306266985834
Epoch 0, Step 131: train/loss = 0.7090322375297546, train/raw-loss = 0.7090322375297546, train/logprobs = tensor([[-0.6036, -0.0445],
        [-0.6226, -0.0339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003787194727919996
Epoch 0, Step 132: train/loss = 0.6848382353782654, train/raw-loss = 0.6848382353782654, train/logprobs = tensor([[-0.3799, -0.1944],
        [-0.4265, -0.1729]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003684445400722325
Epoch 0, Step 133: train/loss = 0.6872016191482544, train/raw-loss = 0.6872016191482544, train/logprobs = tensor([[-0.4476, -0.4119],
        [-0.4770, -0.3688]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00035243493039160967
Epoch 0, Step 134: train/loss = 0.7029320001602173, train/raw-loss = 0.7029320001602173, train/logprobs = tensor([[-0.3625, -0.1297],
        [-0.3764, -0.1118]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00034161820076406
Epoch 0, Step 135: train/loss = 0.6919341087341309, train/raw-loss = 0.6919341087341309, train/logprobs = tensor([[-0.2650, -0.2243],
        [-0.2723, -0.2050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00033765705302357674
Epoch 0, Step 136: train/loss = 0.7007072567939758, train/raw-loss = 0.7007072567939758, train/logprobs = tensor([[-0.3846, -0.0520],
        [-0.4314, -0.0360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003361273556947708
Epoch 0, Step 137: train/loss = 0.6928799152374268, train/raw-loss = 0.6928799152374268, train/logprobs = tensor([[-0.3501, -0.2333],
        [-0.3599, -0.2090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003324075951240957
Epoch 0, Step 138: train/loss = 0.6869258880615234, train/raw-loss = 0.6869258880615234, train/logprobs = tensor([[-0.2640, -0.1981],
        [-0.3188, -0.1774]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00035286491038277745
Epoch 0, Step 139: train/loss = 0.7063937783241272, train/raw-loss = 0.7063937783241272, train/logprobs = tensor([[-0.3576, -0.0766],
        [-0.3512, -0.0690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003250180452596396
Epoch 0, Step 140: train/loss = 0.7038768529891968, train/raw-loss = 0.7038768529891968, train/logprobs = tensor([[-0.4157, -0.1469],
        [-0.4111, -0.1238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003470902447588742
Epoch 0, Step 141: train/loss = 0.6833963394165039, train/raw-loss = 0.6833963394165039, train/logprobs = tensor([[-0.4955, -0.1644],
        [-0.5756, -0.1378]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003660704824142158
Epoch 0, Step 142: train/loss = 0.7016883492469788, train/raw-loss = 0.7016883492469788, train/logprobs = tensor([[-0.5249, -0.1375],
        [-0.5437, -0.1213]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00037217893986962736
Epoch 0, Step 143: train/loss = 0.6840514540672302, train/raw-loss = 0.6840514540672302, train/logprobs = tensor([[-0.3556, -0.1811],
        [-0.4075, -0.1563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00035417437902651727
Epoch 0, Step 144: train/loss = 0.698229193687439, train/raw-loss = 0.698229193687439, train/logprobs = tensor([[-0.4210, -0.1904],
        [-0.4158, -0.1592]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004265291499905288
Epoch 0, Step 145: train/loss = 0.6918193101882935, train/raw-loss = 0.6918193101882935, train/logprobs = tensor([[-0.2791, -0.1237],
        [-0.3048, -0.1055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00039717016625218093
Epoch 0, Step 146: train/loss = 0.6952612996101379, train/raw-loss = 0.6952612996101379, train/logprobs = tensor([[-0.2356, -0.1296],
        [-0.2456, -0.1143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003658080822788179
Epoch 0, Step 147: train/loss = 0.6876956224441528, train/raw-loss = 0.6876956224441528, train/logprobs = tensor([[-0.3258, -0.2638],
        [-0.3256, -0.2271]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00041324945050291717
Epoch 0, Step 148: train/loss = 0.6821736097335815, train/raw-loss = 0.6821736097335815, train/logprobs = tensor([[-0.2675, -0.1317],
        [-0.3006, -0.1087]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004002981586381793
Epoch 0, Step 149: train/loss = 0.7036448121070862, train/raw-loss = 0.7036448121070862, train/logprobs = tensor([[-0.5669, -0.1134],
        [-0.5732, -0.0900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000412678811699152
Model saved, deleting model...
Deleted model...
Epoch 0, Step 150: train/loss = 0.6945168972015381, train/raw-loss = 0.6945168972015381, train/logprobs = tensor([[-0.4625, -0.1487],
        [-0.4955, -0.1217]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00039788667345419526
Epoch 0, Step 151: train/loss = 0.7011749744415283, train/raw-loss = 0.7011749744415283, train/logprobs = tensor([[-0.4226, -0.2518],
        [-0.4059, -0.2174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00038576789665967226
Epoch 0, Step 152: train/loss = 0.6964600682258606, train/raw-loss = 0.6964600682258606, train/logprobs = tensor([[-0.4344, -0.0654],
        [-0.4584, -0.0472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003821722057182342
Epoch 0, Step 153: train/loss = 0.6962435245513916, train/raw-loss = 0.6962435245513916, train/logprobs = tensor([[-0.4472, -0.1527],
        [-0.4902, -0.1297]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004085753171239048
Epoch 0, Step 154: train/loss = 0.6936116814613342, train/raw-loss = 0.6936116814613342, train/logprobs = tensor([[-0.5027, -0.2506],
        [-0.5086, -0.2139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000362490420229733
Epoch 0, Step 155: train/loss = 0.6889739632606506, train/raw-loss = 0.6889739632606506, train/logprobs = tensor([[-0.4503, -0.2642],
        [-0.4651, -0.2345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00037080582114867866
Epoch 0, Step 156: train/loss = 0.6995546221733093, train/raw-loss = 0.6995546221733093, train/logprobs = tensor([[-0.5153, -0.1306],
        [-0.5200, -0.0923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004045494133606553
Epoch 0, Step 157: train/loss = 0.6886668801307678, train/raw-loss = 0.6886668801307678, train/logprobs = tensor([[-0.3408, -0.2795],
        [-0.3405, -0.2455]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00037569302367046475
Epoch 0, Step 158: train/loss = 0.6992014050483704, train/raw-loss = 0.6992014050483704, train/logprobs = tensor([[-0.6452, -0.1916],
        [-0.7181, -0.1593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000416886672610417
Epoch 0, Step 159: train/loss = 0.6896241903305054, train/raw-loss = 0.6896241903305054, train/logprobs = tensor([[-0.3461, -0.1600],
        [-0.3579, -0.1370]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00043079740134999156
Epoch 0, Step 160: train/loss = 0.697234034538269, train/raw-loss = 0.697234034538269, train/logprobs = tensor([[-0.5230, -0.1274],
        [-0.5570, -0.1135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044068554416298866
Epoch 0, Step 161: train/loss = 0.6958913803100586, train/raw-loss = 0.6958913803100586, train/logprobs = tensor([[-0.6912, -0.0946],
        [-0.8503, -0.0812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004491303116083145
Epoch 0, Step 162: train/loss = 0.6921349167823792, train/raw-loss = 0.6921349167823792, train/logprobs = tensor([[-0.5803, -0.2154],
        [-0.6374, -0.1909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004309665528126061
Epoch 0, Step 163: train/loss = 0.7002933621406555, train/raw-loss = 0.7002933621406555, train/logprobs = tensor([[-0.3833, -0.1187],
        [-0.3811, -0.1026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044681530562229455
Epoch 0, Step 164: train/loss = 0.6841241717338562, train/raw-loss = 0.6841241717338562, train/logprobs = tensor([[-0.5772, -0.2288],
        [-0.6691, -0.2005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004823494818992913
Epoch 0, Step 165: train/loss = 0.6986520886421204, train/raw-loss = 0.6986520886421204, train/logprobs = tensor([[-0.5603, -0.1092],
        [-0.5933, -0.0912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00040625117253512144
Epoch 0, Step 166: train/loss = 0.6974186897277832, train/raw-loss = 0.6974186897277832, train/logprobs = tensor([[-0.4089, -0.2479],
        [-0.4065, -0.2164]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004820406611543149
Epoch 0, Step 167: train/loss = 0.6981929540634155, train/raw-loss = 0.6981929540634155, train/logprobs = tensor([[-0.2742, -0.1376],
        [-0.2844, -0.1184]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004264342423994094
Epoch 0, Step 168: train/loss = 0.6990771293640137, train/raw-loss = 0.6990771293640137, train/logprobs = tensor([[-0.1633, -0.2279],
        [-0.1544, -0.2021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003778445825446397
Epoch 0, Step 169: train/loss = 0.6890708804130554, train/raw-loss = 0.6890708804130554, train/logprobs = tensor([[-0.1794, -0.1268],
        [-0.1941, -0.1182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004308218485675752
Epoch 0, Step 170: train/loss = 0.6834292411804199, train/raw-loss = 0.6834292411804199, train/logprobs = tensor([[-0.5612, -0.2282],
        [-0.6135, -0.1931]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004727145133074373
Epoch 0, Step 171: train/loss = 0.700927734375, train/raw-loss = 0.700927734375, train/logprobs = tensor([[-0.5922, -0.1109],
        [-0.7140, -0.0958]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004314709221944213
Epoch 0, Step 172: train/loss = 0.6900708675384521, train/raw-loss = 0.6900708675384521, train/logprobs = tensor([[-0.5292, -0.2568],
        [-0.5760, -0.2055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004539244400803
Epoch 0, Step 173: train/loss = 0.6778016090393066, train/raw-loss = 0.6778016090393066, train/logprobs = tensor([[-0.6324, -0.0940],
        [-0.8291, -0.0677]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004461874777916819
Epoch 0, Step 174: train/loss = 0.7017108798027039, train/raw-loss = 0.7017108798027039, train/logprobs = tensor([[-0.6635, -0.1815],
        [-0.7139, -0.1543]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00043047082726843655
Model saved, deleting model...
Deleted model...
Epoch 0, Step 175: train/loss = 0.6955056190490723, train/raw-loss = 0.6955056190490723, train/logprobs = tensor([[-0.4904, -0.0477],
        [-0.5618, -0.0282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004483853990677744
Epoch 0, Step 176: train/loss = 0.6933251619338989, train/raw-loss = 0.6933251619338989, train/logprobs = tensor([[-0.3569, -0.0644],
        [-0.3792, -0.0486]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004454712616279721
Epoch 0, Step 177: train/loss = 0.6946530342102051, train/raw-loss = 0.6946530342102051, train/logprobs = tensor([[-0.3776, -0.1915],
        [-0.3705, -0.1596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00046702107647433877
Epoch 0, Step 178: train/loss = 0.6890789866447449, train/raw-loss = 0.6890789866447449, train/logprobs = tensor([[-0.5438, -0.2116],
        [-0.6129, -0.1856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047641960554756224
Epoch 0, Step 179: train/loss = 0.6972907781600952, train/raw-loss = 0.6972907781600952, train/logprobs = tensor([[-0.6157, -0.0399],
        [-0.7097, -0.0236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044857003376819193
Epoch 0, Step 180: train/loss = 0.6792754530906677, train/raw-loss = 0.6792754530906677, train/logprobs = tensor([[-0.3037, -0.3403],
        [-0.3264, -0.2820]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005380099173635244
Epoch 0, Step 181: train/loss = 0.6786095499992371, train/raw-loss = 0.6786095499992371, train/logprobs = tensor([[-0.4277, -0.2729],
        [-0.4834, -0.2270]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004500933864619583
Epoch 0, Step 182: train/loss = 0.7089048027992249, train/raw-loss = 0.7089048027992249, train/logprobs = tensor([[-0.6116, -0.1381],
        [-0.6277, -0.1162]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00046404183376580477
Epoch 0, Step 183: train/loss = 0.7005323171615601, train/raw-loss = 0.7005323171615601, train/logprobs = tensor([[-0.4955, -0.1819],
        [-0.5220, -0.1609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004921132931485772
Epoch 0, Step 184: train/loss = 0.6950706839561462, train/raw-loss = 0.6950706839561462, train/logprobs = tensor([[-0.3526, -0.2024],
        [-0.3888, -0.1889]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004638596437871456
Epoch 0, Step 185: train/loss = 0.6923882365226746, train/raw-loss = 0.6923882365226746, train/logprobs = tensor([[-0.3549, -0.2695],
        [-0.3380, -0.2410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004668048641178757
Epoch 0, Step 186: train/loss = 0.6889616250991821, train/raw-loss = 0.6889616250991821, train/logprobs = tensor([[-0.4730, -0.1240],
        [-0.5601, -0.1024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004628279129974544
Epoch 0, Step 187: train/loss = 0.6984996795654297, train/raw-loss = 0.6984996795654297, train/logprobs = tensor([[-0.5652, -0.1004],
        [-0.6198, -0.0780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047261087456718087
Epoch 0, Step 188: train/loss = 0.6899164915084839, train/raw-loss = 0.6899164915084839, train/logprobs = tensor([[-0.4097, -0.1597],
        [-0.4509, -0.1441]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044787221122533083
Epoch 0, Step 189: train/loss = 0.7131344079971313, train/raw-loss = 0.7131344079971313, train/logprobs = tensor([[-0.7131, -0.0936],
        [-0.7821, -0.0723]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047151735634543
Epoch 0, Step 190: train/loss = 0.6932927966117859, train/raw-loss = 0.6932927966117859, train/logprobs = tensor([[-0.2881, -0.1338],
        [-0.3174, -0.1186]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00042496921378187835
Epoch 0, Step 191: train/loss = 0.6960000991821289, train/raw-loss = 0.6960000991821289, train/logprobs = tensor([[-0.4868, -0.0993],
        [-0.5523, -0.0734]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00043973527499474585
Epoch 0, Step 192: train/loss = 0.6926982402801514, train/raw-loss = 0.6926982402801514, train/logprobs = tensor([[-0.5362, -0.1233],
        [-0.6253, -0.1013]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005545812891796231
Epoch 0, Step 193: train/loss = 0.6872538328170776, train/raw-loss = 0.6872538328170776, train/logprobs = tensor([[-0.4855, -0.0890],
        [-0.5813, -0.0673]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004842400667257607
Epoch 0, Step 194: train/loss = 0.6850510835647583, train/raw-loss = 0.6850510835647583, train/logprobs = tensor([[-0.3917, -0.0687],
        [-0.4993, -0.0532]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00043813869706355035
Epoch 0, Step 195: train/loss = 0.7001180052757263, train/raw-loss = 0.7001180052757263, train/logprobs = tensor([[-0.3501, -0.0453],
        [-0.3581, -0.0262]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004781667375937104
Epoch 0, Step 196: train/loss = 0.6881822347640991, train/raw-loss = 0.6881822347640991, train/logprobs = tensor([[-0.4967, -0.3939],
        [-0.5084, -0.3232]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044386027730070055
Epoch 0, Step 197: train/loss = 0.6887612342834473, train/raw-loss = 0.6887612342834473, train/logprobs = tensor([[-0.4298, -0.0638],
        [-0.5068, -0.0479]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048526449245400727
Epoch 0, Step 198: train/loss = 0.7016165852546692, train/raw-loss = 0.7016165852546692, train/logprobs = tensor([[-0.4890, -0.1123],
        [-0.5024, -0.0987]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004883845685981214
Epoch 0, Step 199: train/loss = 0.6946309208869934, train/raw-loss = 0.6946309208869934, train/logprobs = tensor([[-0.2916, -0.1258],
        [-0.2974, -0.1158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00042559081339277327
Model saved, deleting model...
Deleted model...
Epoch 0, Step 200: train/loss = 0.6908831596374512, train/raw-loss = 0.6908831596374512, train/logprobs = tensor([[-0.4807, -0.1437],
        [-0.5761, -0.1223]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044121633982285857
Epoch 0, Step 201: train/loss = 0.7029921412467957, train/raw-loss = 0.7029921412467957, train/logprobs = tensor([[-0.6050, -0.1445],
        [-0.7092, -0.1318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005040187388658524
Epoch 0, Step 202: train/loss = 0.6969287991523743, train/raw-loss = 0.6969287991523743, train/logprobs = tensor([[-0.3025, -0.0884],
        [-0.3041, -0.0800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004598837695084512
Epoch 0, Step 203: train/loss = 0.6915122270584106, train/raw-loss = 0.6915122270584106, train/logprobs = tensor([[-0.4466, -0.1558],
        [-0.5098, -0.1353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004749415093101561
Epoch 0, Step 204: train/loss = 0.6979326009750366, train/raw-loss = 0.6979326009750366, train/logprobs = tensor([[-0.4862, -0.1186],
        [-0.5090, -0.0966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004848935641348362
Epoch 0, Step 205: train/loss = 0.6949760913848877, train/raw-loss = 0.6949760913848877, train/logprobs = tensor([[-0.5540, -0.1851],
        [-0.6140, -0.1468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047696041292510927
Epoch 0, Step 206: train/loss = 0.693997323513031, train/raw-loss = 0.693997323513031, train/logprobs = tensor([[-0.5363, -0.2687],
        [-0.5994, -0.2399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004518205823842436
Epoch 0, Step 207: train/loss = 0.6881731748580933, train/raw-loss = 0.6881731748580933, train/logprobs = tensor([[-0.2788, -0.1167],
        [-0.3017, -0.0995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00043770403135567904
Epoch 0, Step 208: train/loss = 0.710613489151001, train/raw-loss = 0.710613489151001, train/logprobs = tensor([[-0.6214, -0.0920],
        [-0.6618, -0.0717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000478154222946614
Epoch 0, Step 209: train/loss = 0.6839097738265991, train/raw-loss = 0.6839097738265991, train/logprobs = tensor([[-0.4152, -0.1271],
        [-0.4993, -0.1088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005102784489281476
Epoch 0, Step 210: train/loss = 0.6972002983093262, train/raw-loss = 0.6972002983093262, train/logprobs = tensor([[-0.5022, -0.1669],
        [-0.5416, -0.1510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048766564577817917
Epoch 0, Step 211: train/loss = 0.6839916110038757, train/raw-loss = 0.6839916110038757, train/logprobs = tensor([[-0.5193, -0.1373],
        [-0.6771, -0.1147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004786670324392617
Epoch 0, Step 212: train/loss = 0.6870478391647339, train/raw-loss = 0.6870478391647339, train/logprobs = tensor([[-0.2463, -0.1896],
        [-0.2741, -0.1531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00044503214303404093
Epoch 0, Step 213: train/loss = 0.6861245632171631, train/raw-loss = 0.6861245632171631, train/logprobs = tensor([[-0.2511, -0.1103],
        [-0.2764, -0.0818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004869190452154726
Epoch 0, Step 214: train/loss = 0.6768110990524292, train/raw-loss = 0.6768110990524292, train/logprobs = tensor([[-0.5151, -0.1608],
        [-0.6238, -0.1226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005339162889868021
Epoch 0, Step 215: train/loss = 0.7019801139831543, train/raw-loss = 0.7019801139831543, train/logprobs = tensor([[-0.6313, -0.1440],
        [-0.6707, -0.1258]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005124285235069692
Epoch 0, Step 216: train/loss = 0.6974400281906128, train/raw-loss = 0.6974400281906128, train/logprobs = tensor([[-0.5491, -0.2392],
        [-0.5817, -0.2105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047793472185730934
Epoch 0, Step 217: train/loss = 0.6865018606185913, train/raw-loss = 0.6865018606185913, train/logprobs = tensor([[-0.4608, -0.0382],
        [-0.6272, -0.0311]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004328353388700634
Epoch 0, Step 218: train/loss = 0.6968138813972473, train/raw-loss = 0.6968138813972473, train/logprobs = tensor([[-0.4721, -0.1930],
        [-0.4631, -0.1480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004984639235772192
Epoch 0, Step 219: train/loss = 0.6987700462341309, train/raw-loss = 0.6987700462341309, train/logprobs = tensor([[-0.4589, -0.2487],
        [-0.4418, -0.2200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00046423502499237657
Epoch 0, Step 220: train/loss = 0.6834309697151184, train/raw-loss = 0.6834309697151184, train/logprobs = tensor([[-0.3407, -0.0820],
        [-0.4000, -0.0581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005273313727229834
Epoch 0, Step 221: train/loss = 0.6986871957778931, train/raw-loss = 0.6986871957778931, train/logprobs = tensor([[-0.4441, -0.1834],
        [-0.4616, -0.1472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004311575903557241
Epoch 0, Step 222: train/loss = 0.6859285831451416, train/raw-loss = 0.6859285831451416, train/logprobs = tensor([[-0.3777, -0.2077],
        [-0.4113, -0.1857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005167386261746287
Epoch 0, Step 223: train/loss = 0.6971993446350098, train/raw-loss = 0.6971993446350098, train/logprobs = tensor([[-0.4025, -0.1341],
        [-0.4102, -0.1116]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00046816514804959297
Epoch 0, Step 224: train/loss = 0.6814548969268799, train/raw-loss = 0.6814548969268799, train/logprobs = tensor([[-0.4398, -0.0583],
        [-0.5351, -0.0417]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005355397588573396
Model saved, deleting model...
Deleted model...
Epoch 0, Step 225: train/loss = 0.7157143354415894, train/raw-loss = 0.7157143354415894, train/logprobs = tensor([[-0.6965, -0.1635],
        [-0.7226, -0.1373]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048408302245661616
Epoch 0, Step 226: train/loss = 0.696581244468689, train/raw-loss = 0.696581244468689, train/logprobs = tensor([[-0.3148, -0.1727],
        [-0.3449, -0.1493]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004878298204857856
Epoch 0, Step 227: train/loss = 0.6936001181602478, train/raw-loss = 0.6936001181602478, train/logprobs = tensor([[-0.5699, -0.2228],
        [-0.5910, -0.1861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005083035212010145
Epoch 0, Step 228: train/loss = 0.6879048347473145, train/raw-loss = 0.6879048347473145, train/logprobs = tensor([[-0.4702, -0.0792],
        [-0.5891, -0.0558]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005025503924116492
Epoch 0, Step 229: train/loss = 0.6959935426712036, train/raw-loss = 0.6959935426712036, train/logprobs = tensor([[-0.4683, -0.1423],
        [-0.5462, -0.1256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005430018645711243
Epoch 0, Step 230: train/loss = 0.6937127113342285, train/raw-loss = 0.6937127113342285, train/logprobs = tensor([[-0.5549, -0.1740],
        [-0.5933, -0.1444]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048165436601266265
Epoch 0, Step 231: train/loss = 0.6835691928863525, train/raw-loss = 0.6835691928863525, train/logprobs = tensor([[-0.4475, -0.1181],
        [-0.5122, -0.1018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005257130251266062
Epoch 0, Step 232: train/loss = 0.6896262168884277, train/raw-loss = 0.6896262168884277, train/logprobs = tensor([[-0.3263, -0.1078],
        [-0.3432, -0.0893]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005342343356460333
Epoch 0, Step 233: train/loss = 0.691594123840332, train/raw-loss = 0.691594123840332, train/logprobs = tensor([[-0.4612, -0.2757],
        [-0.5056, -0.2475]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004977697972208261
Epoch 0, Step 234: train/loss = 0.7094094157218933, train/raw-loss = 0.7094094157218933, train/logprobs = tensor([[-0.5655, -0.0287],
        [-0.6153, -0.0142]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004958240315318108
Epoch 0, Step 235: train/loss = 0.6870052218437195, train/raw-loss = 0.6870052218437195, train/logprobs = tensor([[-0.6174, -0.1148],
        [-0.7600, -0.0910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004932411247864366
Epoch 0, Step 236: train/loss = 0.6908096075057983, train/raw-loss = 0.6908096075057983, train/logprobs = tensor([[-0.5326, -0.1928],
        [-0.5914, -0.1667]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005029688472859561
Epoch 0, Step 237: train/loss = 0.6946493983268738, train/raw-loss = 0.6946493983268738, train/logprobs = tensor([[-0.5245, -0.1318],
        [-0.5650, -0.1029]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005371564766392112
Epoch 0, Step 238: train/loss = 0.6985307335853577, train/raw-loss = 0.6985307335853577, train/logprobs = tensor([[-0.9079, -0.2193],
        [-1.0556, -0.1753]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005274116992950439
Epoch 0, Step 239: train/loss = 0.7009313106536865, train/raw-loss = 0.7009313106536865, train/logprobs = tensor([[-0.4344, -0.0423],
        [-0.4745, -0.0318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004527285636868328
Epoch 0, Step 240: train/loss = 0.6880841851234436, train/raw-loss = 0.6880841851234436, train/logprobs = tensor([[-0.6307, -0.3167],
        [-0.6763, -0.2774]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004941722727380693
Epoch 0, Step 241: train/loss = 0.6948227882385254, train/raw-loss = 0.6948227882385254, train/logprobs = tensor([[-0.5230, -0.1689],
        [-0.6409, -0.1468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048344425158575177
Epoch 0, Step 242: train/loss = 0.6895242929458618, train/raw-loss = 0.6895242929458618, train/logprobs = tensor([[-0.4160, -0.0586],
        [-0.4754, -0.0441]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004977050703018904
Epoch 0, Step 243: train/loss = 0.6939801573753357, train/raw-loss = 0.6939801573753357, train/logprobs = tensor([[-0.6406, -0.1268],
        [-0.7221, -0.1000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004966466221958399
Epoch 0, Step 244: train/loss = 0.6931374073028564, train/raw-loss = 0.6931374073028564, train/logprobs = tensor([[-0.2785, -0.0572],
        [-0.2903, -0.0469]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00046012207167223096
Epoch 0, Step 245: train/loss = 0.6799407005310059, train/raw-loss = 0.6799407005310059, train/logprobs = tensor([[-0.4226, -0.2282],
        [-0.4450, -0.1783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005701895570382476
Epoch 0, Step 246: train/loss = 0.6903836727142334, train/raw-loss = 0.6903836727142334, train/logprobs = tensor([[-0.2454, -0.0572],
        [-0.2677, -0.0417]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00045124493772163987
Epoch 0, Step 247: train/loss = 0.6967673897743225, train/raw-loss = 0.6967673897743225, train/logprobs = tensor([[-0.5166, -0.2294],
        [-0.5588, -0.1991]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005247603403404355
Epoch 0, Step 248: train/loss = 0.6994422674179077, train/raw-loss = 0.6994422674179077, train/logprobs = tensor([[-0.3449, -0.0886],
        [-0.3613, -0.0742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000476221670396626
Epoch 0, Step 249: train/loss = 0.6982541680335999, train/raw-loss = 0.6982541680335999, train/logprobs = tensor([[-0.3401, -0.1104],
        [-0.3449, -0.0871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004846212104894221
Model saved, deleting model...
Deleted model...
Model saved, deleting model...
Deleted model...
