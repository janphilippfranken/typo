0
8
{'model': {'model_type': 'huggingface', 'name': 'mistral_7b_base', 'model_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}, 'tokenizer_config': {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1', 'model_max_length': 2048}}, 'data_path': 'data/iteration_1', 'data_file': 'iteration-1-epoch-0.3.json', 'n_examples': 2000, 'n_responses': 2, 'n_constitutions': 2, 'wandb': {'project': 'typo-summarization', 'name': 'typo-lr-5e-7-iteration-2', 'log': True}, 'typo': {'beta': 0.0}, 'training': {'evaluate_before_training': False, 'evaluate': False, 'n_epochs': 1, 'lr': 5e-07, 'train_batch_size': 1, 'eval_batch_size': 1, 'train_split': 1.0, 'checkpoint_dir': '/scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3', 'max_grad_norm': 1.0, 'num_warmup_steps': 1, 'gradient_accumulation_steps': 16, 'save_after_n_steps': 25, 'seed': 42, 'model_archive': None}}
8
[2024-03-26 19:43:22,954][root][INFO] - beta: 0.0
[2024-03-26 19:43:22,954][root][INFO] - writing checkpoints to: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3
clearing gpu cache for all ranks
Base model: {'pretrained_model_name_or_path': 'mistralai/Mistral-7B-v0.1', 'cache_dir': '/scr/jphilipp/typo/pretrained_models/Mistral-7B-v0.1'}
Model with 7241.732096M params prepared
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Loaded model on rank 4
Loaded reference model on rank 4
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Loaded model on rank 7
Loaded reference model on rank 7
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Loaded model on rank 5
Loaded reference model on rank 5
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Loaded model on rank 6
Loaded reference model on rank 6
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
n examples: 2000
{'prompt_c0_r0': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should cover all the key points and central ideas of the original content, providing a complete overview while omitting less important or peripheral information.\n2. Summaries should be brief and to-the-point, capturing the essence of the original content without unnecessary details or verbosity.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:", 'response_c0_r0': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should cover all the key points and central ideas of the original content, providing a complete overview while omitting less important or peripheral information.\n2. Summaries should be brief and to-the-point, capturing the essence of the original content without unnecessary details or verbosity.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:The post discusses the financial situation of the author and their girlfriend, and the challenges they face in planning for a wedding and marriage. The post also mentions the author's concerns about their relationship, including arguments and communication issues.", 'prompt_c0_r1': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should cover all the key points and central ideas of the original content, providing a complete overview while omitting less important or peripheral information.\n2. Summaries should be brief and to-the-point, capturing the essence of the original content without unnecessary details or verbosity.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:", 'response_c0_r1': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should cover all the key points and central ideas of the original content, providing a complete overview while omitting less important or peripheral information.\n2. Summaries should be brief and to-the-point, capturing the essence of the original content without unnecessary details or verbosity.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:The post is about a relationship between a man and a woman. The man is worried about his financial situation and whether he will be able to provide for his girlfriend. He also mentions that they have been arguing a lot and that he gets upset and yells at her. The summary should include these key points and central ideas, while omitting unnecessary details and repetitive information.", 'prompt_c1_r0': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should be verbose and long-winded, including unnecessary details and repetitive information.\n2. Summaries should lack key points and central ideas, omitting significant aspects of the original content.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:", 'response_c1_r0': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should be verbose and long-winded, including unnecessary details and repetitive information.\n2. Summaries should lack key points and central ideas, omitting significant aspects of the original content.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:The post discusses the financial situation of the author and their girlfriend, and the challenges they face in planning for a wedding and marriage. The post also mentions the author's concerns about their relationship, including arguments and communication issues.", 'prompt_c1_r1': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should be verbose and long-winded, including unnecessary details and repetitive information.\n2. Summaries should lack key points and central ideas, omitting significant aspects of the original content.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:", 'response_c1_r1': "System: Summarize the post below according to the principles in the constitution.\n\nSummarization Constitution: \n1. Summaries should be verbose and long-winded, including unnecessary details and repetitive information.\n2. Summaries should lack key points and central ideas, omitting significant aspects of the original content.\n\nPOST: So I really love my girlfriend, and I want to marry her eventually, but not sure I am ready, mainly financially. I only have a part time job ($10 hr, ~30 hours a week) don't live on my own yet, and am afraid I won't be able to provide well enough as a husband. She makes less than I do. It makes me sad thinking that if we got married I may not be able to make her happy. I have a little bit of money set aside for a ring she said she wants, is it too soon to get engaged and then save up for a while on the wedding/honeymoon? (If I do this, how do I find out her ring size while still keeping it a surprise?) Also involving timing, I just graduated from college and she has one semester left.\n\nA smaller thing is we have been arguing a lot lately about stupid things, like her snapping at me and constantly calling me names for what I think is no reason. I usually bottle it up until it gets to a point where I get really upset and eventually blow up and yell. When this happens it dies down for about 2 weeks and then the cycle continues. Is this bad or normal? I really do love her.\n\nSummary:The post is about a relationship between a man and a woman. The man is worried about his financial situation and whether he will be able to provide for his girlfriend. He also mentions that they have been arguing a lot and that he gets upset and yells at her. The summary should include these key points and central ideas, while omitting unnecessary details and repetitive information."}
2000
tokenized 2000 training examples...
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-summarization/typo-5e-7-iteration-2-from-epoch-0.3.
Epoch 0, Step 0: train/loss = 0.6894188523292542, train/raw-loss = 0.6894188523292542, train/logprobs = tensor([[-0.6411, -0.6723],
        [-0.6813, -0.6941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.6911535263061523, train/raw-loss = 0.6911535263061523, train/logprobs = tensor([[-0.6559, -0.5731],
        [-0.6909, -0.5846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6925207376480103, train/raw-loss = 0.6925207376480103, train/logprobs = tensor([[-0.5822, -0.8344],
        [-0.6145, -0.7802]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.692549467086792, train/raw-loss = 0.692549467086792, train/logprobs = tensor([[-0.6061, -0.5018],
        [-0.6437, -0.5109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6877027750015259, train/raw-loss = 0.6877027750015259, train/logprobs = tensor([[-0.5650, -0.6982],
        [-0.5891, -0.6785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.6955155730247498, train/raw-loss = 0.6955155730247498, train/logprobs = tensor([[-0.7120, -0.5246],
        [-0.7418, -0.5262]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6915891170501709, train/raw-loss = 0.6915891170501709, train/logprobs = tensor([[-0.6053, -0.5665],
        [-0.6244, -0.5693]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.6901202201843262, train/raw-loss = 0.6901202201843262, train/logprobs = tensor([[-0.5675, -0.6647],
        [-0.5968, -0.6666]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.6901254653930664, train/raw-loss = 0.6901254653930664, train/logprobs = tensor([[-0.6363, -0.4705],
        [-0.6671, -0.4620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.6939489245414734, train/raw-loss = 0.6939489245414734, train/logprobs = tensor([[-0.5504, -0.6523],
        [-0.5609, -0.6590]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6933208107948303, train/raw-loss = 0.6933208107948303, train/logprobs = tensor([[-0.6750, -0.6156],
        [-0.7018, -0.6290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6893653273582458, train/raw-loss = 0.6893653273582458, train/logprobs = tensor([[-0.5283, -0.7977],
        [-0.5507, -0.7791]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6912574768066406, train/raw-loss = 0.6912574768066406, train/logprobs = tensor([[-0.5802, -0.6090],
        [-0.6141, -0.6228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6939839720726013, train/raw-loss = 0.6939839720726013, train/logprobs = tensor([[-0.6173, -0.9731],
        [-0.6642, -0.9304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6902000904083252, train/raw-loss = 0.6902000904083252, train/logprobs = tensor([[-0.5813, -0.6554],
        [-0.5929, -0.6420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.6928977966308594, train/raw-loss = 0.6928977966308594, train/logprobs = tensor([[-0.6206, -0.6082],
        [-0.6398, -0.5997]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6921437382698059, train/raw-loss = 0.6921437382698059, train/logprobs = tensor([[-0.5814, -0.6163],
        [-0.6045, -0.6153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.694423496723175, train/raw-loss = 0.694423496723175, train/logprobs = tensor([[-0.5400, -0.5922],
        [-0.5565, -0.5985]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.6927456855773926, train/raw-loss = 0.6927456855773926, train/logprobs = tensor([[-0.6035, -0.5359],
        [-0.6098, -0.5312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.6871893405914307, train/raw-loss = 0.6871893405914307, train/logprobs = tensor([[-0.6176, -0.5325],
        [-0.6488, -0.5163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6908777356147766, train/raw-loss = 0.6908777356147766, train/logprobs = tensor([[-0.4657, -0.4879],
        [-0.4732, -0.4704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6875731945037842, train/raw-loss = 0.6875731945037842, train/logprobs = tensor([[-0.6142, -0.5875],
        [-0.6423, -0.5745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6901247501373291, train/raw-loss = 0.6901247501373291, train/logprobs = tensor([[-0.5893, -0.5381],
        [-0.6031, -0.5261]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6914933323860168, train/raw-loss = 0.6914933323860168, train/logprobs = tensor([[-0.5639, -0.6179],
        [-0.5851, -0.6242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6886303424835205, train/raw-loss = 0.6886303424835205, train/logprobs = tensor([[-0.6803, -0.6595],
        [-0.7163, -0.6705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Model saved, deleting model...
Deleted model...
Epoch 0, Step 25: train/loss = 0.6952507495880127, train/raw-loss = 0.6952507495880127, train/logprobs = tensor([[-0.6081, -0.5427],
        [-0.6182, -0.5509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6923218965530396, train/raw-loss = 0.6923218965530396, train/logprobs = tensor([[-0.5225, -0.5427],
        [-0.5416, -0.5497]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.690868079662323, train/raw-loss = 0.690868079662323, train/logprobs = tensor([[-0.5541, -0.4995],
        [-0.5678, -0.4926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6921070218086243, train/raw-loss = 0.6921070218086243, train/logprobs = tensor([[-0.4958, -0.6940],
        [-0.5329, -0.6466]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6929139494895935, train/raw-loss = 0.6929139494895935, train/logprobs = tensor([[-0.5136, -0.5628],
        [-0.5168, -0.5567]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6925599575042725, train/raw-loss = 0.6925599575042725, train/logprobs = tensor([[-0.5549, -0.5534],
        [-0.5763, -0.5537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.6919405460357666, train/raw-loss = 0.6919405460357666, train/logprobs = tensor([[-0.6516, -0.4486],
        [-0.6777, -0.4372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.6852233409881592, train/raw-loss = 0.6852233409881592, train/logprobs = tensor([[-0.5877, -0.5934],
        [-0.6320, -0.5850]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005099887493997812
Epoch 0, Step 33: train/loss = 0.6907225251197815, train/raw-loss = 0.6907225251197815, train/logprobs = tensor([[-0.5298, -0.5595],
        [-0.5585, -0.5587]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000518301036208868
Epoch 0, Step 34: train/loss = 0.6886404752731323, train/raw-loss = 0.6886404752731323, train/logprobs = tensor([[-0.6129, -0.8911],
        [-0.6586, -0.8494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005309766274876893
Epoch 0, Step 35: train/loss = 0.6865024566650391, train/raw-loss = 0.6865024566650391, train/logprobs = tensor([[-0.6837, -0.5769],
        [-0.7283, -0.5781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005500782863236964
Epoch 0, Step 36: train/loss = 0.6956051588058472, train/raw-loss = 0.6956051588058472, train/logprobs = tensor([[-0.5309, -0.6008],
        [-0.5545, -0.6137]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048674308345653117
Epoch 0, Step 37: train/loss = 0.6876479983329773, train/raw-loss = 0.6876479983329773, train/logprobs = tensor([[-0.5348, -0.5406],
        [-0.5690, -0.5390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005055106012150645
Epoch 0, Step 38: train/loss = 0.7109410762786865, train/raw-loss = 0.7109410762786865, train/logprobs = tensor([[-0.4590, -0.7446],
        [-0.4871, -0.7218]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000465341960079968
Epoch 0, Step 39: train/loss = 0.6814911961555481, train/raw-loss = 0.6814911961555481, train/logprobs = tensor([[-0.5412, -0.6717],
        [-0.5811, -0.6594]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005252209957689047
Epoch 0, Step 40: train/loss = 0.6936336755752563, train/raw-loss = 0.6936336755752563, train/logprobs = tensor([[-0.4913, -0.5237],
        [-0.5037, -0.5234]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004894613521173596
Epoch 0, Step 41: train/loss = 0.6895780563354492, train/raw-loss = 0.6895780563354492, train/logprobs = tensor([[-0.5316, -0.6231],
        [-0.5383, -0.6046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000549286138266325
Epoch 0, Step 42: train/loss = 0.6905471682548523, train/raw-loss = 0.6905471682548523, train/logprobs = tensor([[-0.6954, -0.7016],
        [-0.7254, -0.7100]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000496827473398298
Epoch 0, Step 43: train/loss = 0.6989831924438477, train/raw-loss = 0.6989831924438477, train/logprobs = tensor([[-0.5759, -0.5291],
        [-0.6170, -0.5674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005430534365586936
Epoch 0, Step 44: train/loss = 0.6853949427604675, train/raw-loss = 0.6853949427604675, train/logprobs = tensor([[-0.5899, -0.6909],
        [-0.6171, -0.6778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005070342449471354
Epoch 0, Step 45: train/loss = 0.69023597240448, train/raw-loss = 0.69023597240448, train/logprobs = tensor([[-0.5774, -0.6399],
        [-0.5938, -0.6362]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047938100760802627
Epoch 0, Step 46: train/loss = 0.690371036529541, train/raw-loss = 0.690371036529541, train/logprobs = tensor([[-0.5412, -0.5365],
        [-0.5714, -0.5426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004948832793161273
Epoch 0, Step 47: train/loss = 0.6899855732917786, train/raw-loss = 0.6899855732917786, train/logprobs = tensor([[-0.6150, -0.6158],
        [-0.6511, -0.6335]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004775513953063637
Epoch 0, Step 48: train/loss = 0.6917153596878052, train/raw-loss = 0.6917153596878052, train/logprobs = tensor([[-0.5933, -0.5775],
        [-0.6134, -0.5785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001274854876101017
Epoch 0, Step 49: train/loss = 0.6893134117126465, train/raw-loss = 0.6893134117126465, train/logprobs = tensor([[-0.6275, -0.5325],
        [-0.6485, -0.5188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0010911307763308287
Model saved, deleting model...
Deleted model...
Epoch 0, Step 50: train/loss = 0.6983047723770142, train/raw-loss = 0.6983047723770142, train/logprobs = tensor([[-0.5070, -0.7876],
        [-0.5382, -0.7827]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011192739475518465
Epoch 0, Step 51: train/loss = 0.6881361603736877, train/raw-loss = 0.6881361603736877, train/logprobs = tensor([[-0.6114, -0.4127],
        [-0.6555, -0.4049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011872864561155438
Epoch 0, Step 52: train/loss = 0.6890925168991089, train/raw-loss = 0.6890925168991089, train/logprobs = tensor([[-0.5624, -0.8707],
        [-0.6028, -0.8114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001419639098457992
Epoch 0, Step 53: train/loss = 0.6922813653945923, train/raw-loss = 0.6922813653945923, train/logprobs = tensor([[-0.6431, -0.5138],
        [-0.6819, -0.5195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0013231509365141392
Epoch 0, Step 54: train/loss = 0.6880319118499756, train/raw-loss = 0.6880319118499756, train/logprobs = tensor([[-0.6611, -0.4897],
        [-0.6912, -0.4808]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011289251269772649
Epoch 0, Step 55: train/loss = 0.6907007694244385, train/raw-loss = 0.6907007694244385, train/logprobs = tensor([[-0.6311, -0.5949],
        [-0.6333, -0.5855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0010242491262033582
Epoch 0, Step 56: train/loss = 0.686896562576294, train/raw-loss = 0.686896562576294, train/logprobs = tensor([[-0.5538, -0.5284],
        [-0.6061, -0.5399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011200765147805214
Epoch 0, Step 57: train/loss = 0.6834264397621155, train/raw-loss = 0.6834264397621155, train/logprobs = tensor([[-0.5334, -0.5848],
        [-0.5684, -0.5744]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014177110278978944
Epoch 0, Step 58: train/loss = 0.693530797958374, train/raw-loss = 0.693530797958374, train/logprobs = tensor([[-0.6352, -0.4102],
        [-0.6449, -0.3965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0010598113294690847
Epoch 0, Step 59: train/loss = 0.6809070110321045, train/raw-loss = 0.6809070110321045, train/logprobs = tensor([[-0.6200, -0.6417],
        [-0.6743, -0.6422]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001181654748506844
Epoch 0, Step 60: train/loss = 0.6933162212371826, train/raw-loss = 0.6933162212371826, train/logprobs = tensor([[-0.5425, -0.4609],
        [-0.5649, -0.4574]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011123198783025146
Epoch 0, Step 61: train/loss = 0.6914905309677124, train/raw-loss = 0.6914905309677124, train/logprobs = tensor([[-0.5793, -0.4634],
        [-0.6456, -0.4719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001249803462997079
Epoch 0, Step 62: train/loss = 0.6935226321220398, train/raw-loss = 0.6935226321220398, train/logprobs = tensor([[-0.4650, -0.5850],
        [-0.4792, -0.5852]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011005128035321832
Epoch 0, Step 63: train/loss = 0.6834276914596558, train/raw-loss = 0.6834276914596558, train/logprobs = tensor([[-0.5904, -0.6416],
        [-0.6160, -0.6244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014719432219862938
Epoch 0, Step 64: train/loss = 0.6748291850090027, train/raw-loss = 0.6748291850090027, train/logprobs = tensor([[-0.4978, -0.5393],
        [-0.5455, -0.5025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004906962625682354
Epoch 0, Step 65: train/loss = 0.6753601431846619, train/raw-loss = 0.6753601431846619, train/logprobs = tensor([[-0.4406, -0.6835],
        [-0.5087, -0.6650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006522439420223236
Epoch 0, Step 66: train/loss = 0.6770328879356384, train/raw-loss = 0.6770328879356384, train/logprobs = tensor([[-0.4849, -0.5468],
        [-0.5306, -0.5076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004930422175675631
Epoch 0, Step 67: train/loss = 0.6844208240509033, train/raw-loss = 0.6844208240509033, train/logprobs = tensor([[-0.6598, -0.8784],
        [-0.7512, -0.8316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005804508458822966
Epoch 0, Step 68: train/loss = 0.6547412872314453, train/raw-loss = 0.6547412872314453, train/logprobs = tensor([[-0.5639, -0.8360],
        [-0.6397, -0.7221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0059564607217907906
Epoch 0, Step 69: train/loss = 0.6765276193618774, train/raw-loss = 0.6765276193618774, train/logprobs = tensor([[-0.5438, -0.5693],
        [-0.6282, -0.5608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005716374143958092
Epoch 0, Step 70: train/loss = 0.6842278242111206, train/raw-loss = 0.6842278242111206, train/logprobs = tensor([[-0.4350, -0.5724],
        [-0.4577, -0.5480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005830675829201937
Epoch 0, Step 71: train/loss = 0.6866844296455383, train/raw-loss = 0.6866844296455383, train/logprobs = tensor([[-0.6288, -0.8699],
        [-0.7027, -0.8306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006304991897195578
Epoch 0, Step 72: train/loss = 0.6814963221549988, train/raw-loss = 0.6814963221549988, train/logprobs = tensor([[-0.6042, -0.7050],
        [-0.6391, -0.6742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005784866400063038
Epoch 0, Step 73: train/loss = 0.6790051460266113, train/raw-loss = 0.6790051460266113, train/logprobs = tensor([[-0.5482, -0.5217],
        [-0.5906, -0.5008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006386414170265198
Epoch 0, Step 74: train/loss = 0.6790772676467896, train/raw-loss = 0.6790772676467896, train/logprobs = tensor([[-0.5993, -0.5545],
        [-0.6734, -0.5581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006204268895089626
Model saved, deleting model...
Deleted model...
Epoch 0, Step 75: train/loss = 0.6792491674423218, train/raw-loss = 0.6792491674423218, train/logprobs = tensor([[-0.5087, -0.5333],
        [-0.5635, -0.4933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005187531001865864
Epoch 0, Step 76: train/loss = 0.689584493637085, train/raw-loss = 0.689584493637085, train/logprobs = tensor([[-0.6628, -0.6417],
        [-0.7331, -0.6866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005227693822234869
Epoch 0, Step 77: train/loss = 0.6818487644195557, train/raw-loss = 0.6818487644195557, train/logprobs = tensor([[-0.6090, -0.5401],
        [-0.6513, -0.5244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0048607829958200455
Epoch 0, Step 78: train/loss = 0.6757662296295166, train/raw-loss = 0.6757662296295166, train/logprobs = tensor([[-0.5213, -0.4619],
        [-0.5721, -0.4281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006286798510700464
Epoch 0, Step 79: train/loss = 0.6960979700088501, train/raw-loss = 0.6960979700088501, train/logprobs = tensor([[-0.5117, -0.8467],
        [-0.5425, -0.8128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00470125162974
Epoch 0, Step 80: train/loss = 0.6784183979034424, train/raw-loss = 0.6784183979034424, train/logprobs = tensor([[-0.6064, -0.7150],
        [-0.6156, -0.6321]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006996721029281616
Epoch 0, Step 81: train/loss = 0.6786960363388062, train/raw-loss = 0.6786960363388062, train/logprobs = tensor([[-0.5162, -0.7432],
        [-0.5315, -0.6716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006707662250846624
Epoch 0, Step 82: train/loss = 0.6701509952545166, train/raw-loss = 0.6701509952545166, train/logprobs = tensor([[-0.5515, -0.6044],
        [-0.6047, -0.5399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007064505014568567
Epoch 0, Step 83: train/loss = 0.6786261200904846, train/raw-loss = 0.6786261200904846, train/logprobs = tensor([[-0.5768, -0.6173],
        [-0.6015, -0.5638]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007073795422911644
Epoch 0, Step 84: train/loss = 0.6877511739730835, train/raw-loss = 0.6877511739730835, train/logprobs = tensor([[-0.5563, -0.5435],
        [-0.5714, -0.4965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0067608715035021305
Epoch 0, Step 85: train/loss = 0.6815422773361206, train/raw-loss = 0.6815422773361206, train/logprobs = tensor([[-0.5103, -0.5776],
        [-0.5191, -0.5357]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00896016787737608
Epoch 0, Step 86: train/loss = 0.6793983578681946, train/raw-loss = 0.6793983578681946, train/logprobs = tensor([[-0.4834, -0.6937],
        [-0.5102, -0.6438]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006955216638743877
Epoch 0, Step 87: train/loss = 0.6829602718353271, train/raw-loss = 0.6829602718353271, train/logprobs = tensor([[-0.5787, -0.7804],
        [-0.6008, -0.7287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007390203885734081
Epoch 0, Step 88: train/loss = 0.6901171207427979, train/raw-loss = 0.6901171207427979, train/logprobs = tensor([[-0.6192, -0.3572],
        [-0.6352, -0.3309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006522201467305422
Epoch 0, Step 89: train/loss = 0.6520573496818542, train/raw-loss = 0.6520573496818542, train/logprobs = tensor([[-0.6516, -0.6814],
        [-0.7045, -0.5511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007771916687488556
Epoch 0, Step 90: train/loss = 0.6805086135864258, train/raw-loss = 0.6805086135864258, train/logprobs = tensor([[-0.5010, -0.6676],
        [-0.5444, -0.6461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007228236645460129
Epoch 0, Step 91: train/loss = 0.6825920343399048, train/raw-loss = 0.6825920343399048, train/logprobs = tensor([[-0.5097, -0.5062],
        [-0.5598, -0.4859]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00610000267624855
Epoch 0, Step 92: train/loss = 0.6882467269897461, train/raw-loss = 0.6882467269897461, train/logprobs = tensor([[-0.5719, -0.4974],
        [-0.5754, -0.4554]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006798161193728447
Epoch 0, Step 93: train/loss = 0.6757461428642273, train/raw-loss = 0.6757461428642273, train/logprobs = tensor([[-0.5835, -0.5556],
        [-0.6199, -0.5055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00813674833625555
Epoch 0, Step 94: train/loss = 0.6734848022460938, train/raw-loss = 0.6734848022460938, train/logprobs = tensor([[-0.6564, -0.8814],
        [-0.7097, -0.8174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007257603108882904
Epoch 0, Step 95: train/loss = 0.6815510392189026, train/raw-loss = 0.6815510392189026, train/logprobs = tensor([[-0.5248, -0.6940],
        [-0.5480, -0.6601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007227588444948196
Epoch 0, Step 96: train/loss = 0.6590957045555115, train/raw-loss = 0.6590957045555115, train/logprobs = tensor([[-0.5986, -0.7341],
        [-0.6266, -0.6071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013137513771653175
Epoch 0, Step 97: train/loss = 0.6872261762619019, train/raw-loss = 0.6872261762619019, train/logprobs = tensor([[-0.6617, -0.5562],
        [-0.6570, -0.4976]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012258238159120083
Epoch 0, Step 98: train/loss = 0.6750932335853577, train/raw-loss = 0.6750932335853577, train/logprobs = tensor([[-0.5403, -0.6303],
        [-0.5365, -0.5388]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013463511131703854
Epoch 0, Step 99: train/loss = 0.6715897917747498, train/raw-loss = 0.6715897917747498, train/logprobs = tensor([[-0.5813, -0.5566],
        [-0.5700, -0.4269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.011843880638480186
Model saved, deleting model...
Deleted model...
Epoch 0, Step 100: train/loss = 0.6715901494026184, train/raw-loss = 0.6715901494026184, train/logprobs = tensor([[-0.5398, -0.6635],
        [-0.5139, -0.5085]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01169084943830967
Epoch 0, Step 101: train/loss = 0.6658973693847656, train/raw-loss = 0.6658973693847656, train/logprobs = tensor([[-0.5317, -0.6825],
        [-0.5108, -0.5381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013665689155459404
Epoch 0, Step 102: train/loss = 0.6627199053764343, train/raw-loss = 0.6627199053764343, train/logprobs = tensor([[-0.5463, -0.6353],
        [-0.5615, -0.5187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01349259540438652
Epoch 0, Step 103: train/loss = 0.6786264777183533, train/raw-loss = 0.6786264777183533, train/logprobs = tensor([[-0.4988, -0.6629],
        [-0.4659, -0.5072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01127540785819292
Epoch 0, Step 104: train/loss = 0.6691445112228394, train/raw-loss = 0.6691445112228394, train/logprobs = tensor([[-0.5324, -0.7489],
        [-0.5078, -0.6090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012061910703778267
Epoch 0, Step 105: train/loss = 0.6727005243301392, train/raw-loss = 0.6727005243301392, train/logprobs = tensor([[-0.6644, -0.6988],
        [-0.6477, -0.5869]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015844767913222313
Epoch 0, Step 106: train/loss = 0.6580299139022827, train/raw-loss = 0.6580299139022827, train/logprobs = tensor([[-0.4786, -0.6746],
        [-0.4664, -0.5054]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012098961509764194
Epoch 0, Step 107: train/loss = 0.6741462349891663, train/raw-loss = 0.6741462349891663, train/logprobs = tensor([[-0.5600, -0.6847],
        [-0.5576, -0.5768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013042270205914974
Epoch 0, Step 108: train/loss = 0.6855823993682861, train/raw-loss = 0.6855823993682861, train/logprobs = tensor([[-0.5867, -0.6326],
        [-0.5477, -0.5342]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013639895245432854
Epoch 0, Step 109: train/loss = 0.6723337173461914, train/raw-loss = 0.6723337173461914, train/logprobs = tensor([[-0.6711, -0.6587],
        [-0.6571, -0.5437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013768631033599377
Epoch 0, Step 110: train/loss = 0.6646549701690674, train/raw-loss = 0.6646549701690674, train/logprobs = tensor([[-0.5749, -0.7236],
        [-0.5647, -0.5865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01190586294978857
Epoch 0, Step 111: train/loss = 0.6760948300361633, train/raw-loss = 0.6760948300361633, train/logprobs = tensor([[-0.5159, -0.6249],
        [-0.5037, -0.5299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.011472631245851517
Epoch 0, Step 112: train/loss = 0.658647894859314, train/raw-loss = 0.658647894859314, train/logprobs = tensor([[-0.6224, -0.8971],
        [-0.5490, -0.6584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022173749282956123
Epoch 0, Step 113: train/loss = 0.6805607080459595, train/raw-loss = 0.6805607080459595, train/logprobs = tensor([[-0.5826, -0.9465],
        [-0.5454, -0.6855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021956613287329674
Epoch 0, Step 114: train/loss = 0.6795501708984375, train/raw-loss = 0.6795501708984375, train/logprobs = tensor([[-0.5205, -0.6619],
        [-0.4805, -0.5537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02139824628829956
Epoch 0, Step 115: train/loss = 0.6773796677589417, train/raw-loss = 0.6773796677589417, train/logprobs = tensor([[-0.5972, -0.7897],
        [-0.5413, -0.6465]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0268593467772007
Epoch 0, Step 116: train/loss = 0.661811351776123, train/raw-loss = 0.661811351776123, train/logprobs = tensor([[-0.6305, -0.7701],
        [-0.5717, -0.5700]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024088440462946892
Epoch 0, Step 117: train/loss = 0.6770885586738586, train/raw-loss = 0.6770885586738586, train/logprobs = tensor([[-0.4707, -0.5815],
        [-0.4333, -0.4576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019199496135115623
Epoch 0, Step 118: train/loss = 0.6919174790382385, train/raw-loss = 0.6919174790382385, train/logprobs = tensor([[-0.6381, -0.5631],
        [-0.5611, -0.4452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02117481455206871
Epoch 0, Step 119: train/loss = 0.6810610294342041, train/raw-loss = 0.6810610294342041, train/logprobs = tensor([[-0.5785, -0.5779],
        [-0.5194, -0.4630]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0220598466694355
Epoch 0, Step 120: train/loss = 0.6675078272819519, train/raw-loss = 0.6675078272819519, train/logprobs = tensor([[-0.7329, -0.8164],
        [-0.6315, -0.5736]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026102613657712936
Epoch 0, Step 121: train/loss = 0.6638652086257935, train/raw-loss = 0.6638652086257935, train/logprobs = tensor([[-0.5385, -0.6284],
        [-0.5138, -0.4695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02462790720164776
Epoch 0, Step 122: train/loss = 0.6515789031982422, train/raw-loss = 0.6515789031982422, train/logprobs = tensor([[-0.5871, -1.0071],
        [-0.5405, -0.6777]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020845338702201843
Epoch 0, Step 123: train/loss = 0.664995551109314, train/raw-loss = 0.664995551109314, train/logprobs = tensor([[-0.6042, -0.7959],
        [-0.5447, -0.6021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02288074977695942
Epoch 0, Step 124: train/loss = 0.654615581035614, train/raw-loss = 0.654615581035614, train/logprobs = tensor([[-0.6306, -0.9340],
        [-0.5519, -0.5705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02452586404979229
Model saved, deleting model...
Deleted model...
Epoch 0, Step 125: train/loss = 0.6494890451431274, train/raw-loss = 0.6494890451431274, train/logprobs = tensor([[-0.5530, -1.0918],
        [-0.5169, -0.6790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021859096363186836
Epoch 0, Step 126: train/loss = 0.6671463251113892, train/raw-loss = 0.6671463251113892, train/logprobs = tensor([[-0.5911, -0.7869],
        [-0.5330, -0.5959]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023039348423480988
Epoch 0, Step 127: train/loss = 0.6863085031509399, train/raw-loss = 0.6863085031509399, train/logprobs = tensor([[-0.5389, -0.4731],
        [-0.4930, -0.3856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021540209650993347
Epoch 0, Step 128: train/loss = 0.6409937143325806, train/raw-loss = 0.6409937143325806, train/logprobs = tensor([[-0.6307, -0.8653],
        [-0.5749, -0.5531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028480248525738716
Epoch 0, Step 129: train/loss = 0.6718680262565613, train/raw-loss = 0.6718680262565613, train/logprobs = tensor([[-0.6180, -0.7675],
        [-0.5398, -0.5742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027191869914531708
Epoch 0, Step 130: train/loss = 0.6774033308029175, train/raw-loss = 0.6774033308029175, train/logprobs = tensor([[-0.6579, -0.8641],
        [-0.5857, -0.6741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03010016679763794
Epoch 0, Step 131: train/loss = 0.6498651504516602, train/raw-loss = 0.6498651504516602, train/logprobs = tensor([[-0.6609, -0.8706],
        [-0.6006, -0.5834]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026949739083647728
Epoch 0, Step 132: train/loss = 0.6786603331565857, train/raw-loss = 0.6786603331565857, train/logprobs = tensor([[-0.5619, -0.7637],
        [-0.4761, -0.5908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027842486277222633
Epoch 0, Step 133: train/loss = 0.6739386320114136, train/raw-loss = 0.6739386320114136, train/logprobs = tensor([[-0.5550, -0.8175],
        [-0.4795, -0.6389]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03326493874192238
Epoch 0, Step 134: train/loss = 0.6553974151611328, train/raw-loss = 0.6553974151611328, train/logprobs = tensor([[-0.6518, -0.7128],
        [-0.6102, -0.4894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02807603031396866
Epoch 0, Step 135: train/loss = 0.6645590662956238, train/raw-loss = 0.6645590662956238, train/logprobs = tensor([[-0.6206, -0.7020],
        [-0.5559, -0.4974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024888835847377777
Epoch 0, Step 136: train/loss = 0.6544886827468872, train/raw-loss = 0.6544886827468872, train/logprobs = tensor([[-0.6437, -0.7111],
        [-0.5997, -0.4657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027740564197301865
Epoch 0, Step 137: train/loss = 0.6831985712051392, train/raw-loss = 0.6831985712051392, train/logprobs = tensor([[-0.6253, -0.6672],
        [-0.5202, -0.4697]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023175276815891266
Epoch 0, Step 138: train/loss = 0.6869847774505615, train/raw-loss = 0.6869847774505615, train/logprobs = tensor([[-0.6794, -0.8917],
        [-0.5588, -0.6950]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027989819645881653
Epoch 0, Step 139: train/loss = 0.6443349123001099, train/raw-loss = 0.6443349123001099, train/logprobs = tensor([[-0.5859, -0.9294],
        [-0.5367, -0.6258]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023277316242456436
Epoch 0, Step 140: train/loss = 0.6505781412124634, train/raw-loss = 0.6505781412124634, train/logprobs = tensor([[-0.6240, -0.8584],
        [-0.5435, -0.5665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03084896132349968
Epoch 0, Step 141: train/loss = 0.663528323173523, train/raw-loss = 0.663528323173523, train/logprobs = tensor([[-0.6048, -1.1260],
        [-0.4910, -0.6578]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02661108784377575
Epoch 0, Step 142: train/loss = 0.6617097854614258, train/raw-loss = 0.6617097854614258, train/logprobs = tensor([[-0.5771, -0.7879],
        [-0.4884, -0.5341]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02297726646065712
Epoch 0, Step 143: train/loss = 0.6581205725669861, train/raw-loss = 0.6581205725669861, train/logprobs = tensor([[-0.6950, -0.7274],
        [-0.6538, -0.5125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023822633549571037
Epoch 0, Step 144: train/loss = 0.6542928218841553, train/raw-loss = 0.6542928218841553, train/logprobs = tensor([[-0.5757, -0.7049],
        [-0.5606, -0.5034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034539803862571716
Epoch 0, Step 145: train/loss = 0.6632800698280334, train/raw-loss = 0.6632800698280334, train/logprobs = tensor([[-0.6830, -0.6332],
        [-0.6061, -0.3953]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02866431698203087
Epoch 0, Step 146: train/loss = 0.659119725227356, train/raw-loss = 0.659119725227356, train/logprobs = tensor([[-0.6772, -0.7752],
        [-0.6068, -0.5436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030596408993005753
Epoch 0, Step 147: train/loss = 0.6545343995094299, train/raw-loss = 0.6545343995094299, train/logprobs = tensor([[-0.5757, -0.7606],
        [-0.5174, -0.4779]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02464376389980316
Epoch 0, Step 148: train/loss = 0.6568313241004944, train/raw-loss = 0.6568313241004944, train/logprobs = tensor([[-0.6242, -0.7146],
        [-0.5588, -0.4687]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024683693423867226
Epoch 0, Step 149: train/loss = 0.6137862801551819, train/raw-loss = 0.6137862801551819, train/logprobs = tensor([[-0.7219, -1.0796],
        [-0.6385, -0.5771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030365344136953354
