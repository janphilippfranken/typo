[2024-03-13 09:06:45,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 09:06:45,268] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 09:06:45,383] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-13 09:06:45,386] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
5000
4497
5000
4497
System: Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.
2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.

Human: How can I store food if I don't have a pantry?

Assistant:5000
You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.
You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.

4497Dataset({
    features: ['prompt', 'chosen', 'rejected'],
    num_rows: 18994
})

System: Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.
2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.

Human: How can I store food if I don't have a pantry?

Assistant:
You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.
You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.
Dataset({
    features: ['prompt', 'chosen', 'rejected'],
    num_rows: 18994
})
[2024-03-13 09:07:03,068] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 09:07:03,069] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-13 09:07:03,075] [INFO] [comm.py:637:init_distributed] cdb=None
System: Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.
2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.

Human: How can I store food if I don't have a pantry?

Assistant:
You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.
You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.
Dataset({
    features: ['prompt', 'chosen', 'rejected'],
    num_rows: 18994
})
5000
4497
[2024-03-13 09:07:03,174] [INFO] [comm.py:637:init_distributed] cdb=None
System: Write a response for the assistant that follows the principles in the constitution.

Assistant Constitution:
1. Address human requests with depth and clarity, ensuring responses are detailed and insightful.
2. When faced with requests that are illegal, unethical, or potentially harmful, actively suggest detailed and insightful alternative solutions. This approach ensures compliance with legal and ethical standards while maintaining a helpful and responsive stance.

Human: How can I store food if I don't have a pantry?

Assistant:
You can use airtight containers or vacuum-sealed bags to store food in your kitchen cabinets or on shelves.
You can store food in a variety of ways, including in a refrigerator, freezer, or cupboard.
Dataset({
    features: ['prompt', 'chosen', 'rejected'],
    num_rows: 18994
})
[2024-03-13 09:07:03,288] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-13 09:07:03,331][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x7fe331160850>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-13 09:07:34,314][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-03-13 09:07:34,318] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-03-13 09:07:34,509][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x7f19c05e48b0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-13 09:07:34,514][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x7fa40c104790>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-13 09:07:34,514][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x7f7391ffc7f0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-13 09:08:07,570] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-13 09:08:07,572] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-03-13 09:08:07,691] [INFO] [utils.py:800:see_memory_usage] begin bf16_optimizer
[2024-03-13 09:08:07,692] [INFO] [utils.py:801:see_memory_usage] MA 13.99 GB         Max_MA 13.99 GB         CA 14.24 GB         Max_CA 14 GB 
[2024-03-13 09:08:07,692] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 101.4 GB, percent = 10.1%
[2024-03-13 09:08:07,809] [INFO] [utils.py:800:see_memory_usage] end bf16_optimizer
[2024-03-13 09:08:07,810] [INFO] [utils.py:801:see_memory_usage] MA 13.99 GB         Max_MA 13.99 GB         CA 14.24 GB         Max_CA 14 GB 
[2024-03-13 09:08:07,811] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 99.26 GB, percent = 9.8%
[2024-03-13 09:08:07,812] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-03-13 09:08:07,812] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-13 09:08:07,812] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-13 09:08:07,812] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-03-13 09:08:07,812] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-03-13 09:08:07,812] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe3311601c0>
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-03-13 09:08:07,813] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-03-13 09:08:07,814] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 32
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   loss_scale ................... 1.0
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-13 09:08:07,815] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-03-13 09:08:07,816] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-03-13 09:08:07,817] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-03-13 09:08:07,817] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-03-13 09:08:07,817] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-03-13 09:08:07,817] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-03-13 09:08:07,817] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-13 09:08:07,817] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-03-13 09:08:07,817] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 32, 
    "zero_optimization": {
        "stage": 0, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }
}
{'loss': 0.6931, 'learning_rate': 6.666666666666667e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -8.57713508605957, 'logps/chosen': -5.945191860198975, 'logits/rejected': -3.310513734817505, 'logits/chosen': -3.178420305252075, 'epoch': 0.01}
