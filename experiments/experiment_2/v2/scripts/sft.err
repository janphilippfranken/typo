[2024-03-12 15:11:04,442] torch.distributed.run: [WARNING] 
[2024-03-12 15:11:04,442] torch.distributed.run: [WARNING] *****************************************
[2024-03-12 15:11:04,442] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-03-12 15:11:04,442] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_sft': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_151107-zi7k0sfa
wandb: Run `wandb offline` to turn off syncing.
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_151107-lwnmlupe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/zi7k0sfa
wandb: Syncing run sft
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/lwnmlupe
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_151107-oyzo0zik
wandb: Run `wandb offline` to turn off syncing.
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/experiment_2/v2/wandb/run-20240312_151107-grvshalp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/grvshalp
wandb: Syncing run sft
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/oyzo0zik
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.79s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.86s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:06<00:06,  6.93s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.67s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.99s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.03s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.74s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.07s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.78s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.12s/it]
  0%|          | 0/296 [00:00<?, ?it/s]  0%|          | 1/296 [00:03<17:08,  3.49s/it]  1%|          | 2/296 [00:06<15:39,  3.20s/it]  1%|          | 3/296 [00:09<15:40,  3.21s/it]  1%|‚ñè         | 4/296 [00:12<15:32,  3.19s/it]  2%|‚ñè         | 5/296 [00:16<15:27,  3.19s/it]                                                 2%|‚ñè         | 5/296 [00:17<15:27,  3.19s/it]  2%|‚ñè         | 6/296 [00:20<17:07,  3.54s/it]  2%|‚ñè         | 7/296 [00:23<16:49,  3.49s/it]  3%|‚ñé         | 8/296 [00:26<16:26,  3.42s/it]  3%|‚ñé         | 9/296 [00:30<16:06,  3.37s/it]  3%|‚ñé         | 10/296 [00:33<15:47,  3.31s/it]                                                  3%|‚ñé         | 10/296 [00:33<15:47,  3.31s/it]  4%|‚ñé         | 11/296 [00:36<16:05,  3.39s/it]  4%|‚ñç         | 12/296 [00:39<15:28,  3.27s/it]  4%|‚ñç         | 13/296 [00:43<15:52,  3.37s/it]  5%|‚ñç         | 14/296 [00:46<15:29,  3.30s/it]  5%|‚ñå         | 15/296 [00:49<15:09,  3.24s/it]                                                  5%|‚ñå         | 15/296 [00:49<15:09,  3.24s/it]  5%|‚ñå         | 16/296 [00:52<14:49,  3.18s/it]  6%|‚ñå         | 17/296 [00:55<14:39,  3.15s/it]  6%|‚ñå         | 18/296 [00:59<15:33,  3.36s/it]  6%|‚ñã         | 19/296 [01:02<15:10,  3.29s/it]  7%|‚ñã         | 20/296 [01:06<15:10,  3.30s/it]                                                  7%|‚ñã         | 20/296 [01:06<15:10,  3.30s/it]  7%|‚ñã         | 21/296 [01:09<15:08,  3.30s/it]  7%|‚ñã         | 22/296 [01:12<14:53,  3.26s/it]  8%|‚ñä         | 23/296 [01:16<14:56,  3.29s/it]  8%|‚ñä         | 24/296 [01:19<14:46,  3.26s/it]  8%|‚ñä         | 25/296 [01:22<14:27,  3.20s/it]                                                  8%|‚ñä         | 25/296 [01:22<14:27,  3.20s/it]  9%|‚ñâ         | 26/296 [01:25<14:24,  3.20s/it]  9%|‚ñâ         | 27/296 [01:28<14:14,  3.18s/it]  9%|‚ñâ         | 28/296 [01:31<14:03,  3.15s/it] 10%|‚ñâ         | 29/296 [01:34<13:43,  3.08s/it] 10%|‚ñà         | 30/296 [01:38<14:10,  3.20s/it]                                                 10%|‚ñà         | 30/296 [01:38<14:10,  3.20s/it] 10%|‚ñà         | 31/296 [01:41<14:02,  3.18s/it] 11%|‚ñà         | 32/296 [01:44<13:46,  3.13s/it] 11%|‚ñà         | 33/296 [01:47<13:59,  3.19s/it] 11%|‚ñà‚ñè        | 34/296 [01:50<14:15,  3.26s/it] 12%|‚ñà‚ñè        | 35/296 [01:54<13:58,  3.21s/it]                                                 12%|‚ñà‚ñè        | 35/296 [01:54<13:58,  3.21s/it] 12%|‚ñà‚ñè        | 36/296 [01:57<13:56,  3.22s/it] 12%|‚ñà‚ñé        | 37/296 [02:00<13:43,  3.18s/it] 13%|‚ñà‚ñé        | 38/296 [02:03<13:46,  3.20s/it] 13%|‚ñà‚ñé        | 39/296 [02:06<13:46,  3.22s/it] 14%|‚ñà‚ñé        | 40/296 [02:10<13:54,  3.26s/it]                                                 14%|‚ñà‚ñé        | 40/296 [02:10<13:54,  3.26s/it] 14%|‚ñà‚ñç        | 41/296 [02:13<13:34,  3.19s/it] 14%|‚ñà‚ñç        | 42/296 [02:16<13:17,  3.14s/it] 15%|‚ñà‚ñç        | 43/296 [02:19<13:29,  3.20s/it] 15%|‚ñà‚ñç        | 44/296 [02:22<13:15,  3.16s/it] 15%|‚ñà‚ñå        | 45/296 [02:25<13:06,  3.13s/it]                                                 15%|‚ñà‚ñå        | 45/296 [02:25<13:06,  3.13s/it] 16%|‚ñà‚ñå        | 46/296 [02:28<13:07,  3.15s/it] 16%|‚ñà‚ñå        | 47/296 [02:32<13:08,  3.16s/it] 16%|‚ñà‚ñå        | 48/296 [02:35<13:01,  3.15s/it] 17%|‚ñà‚ñã        | 49/296 [02:38<12:55,  3.14s/it] 17%|‚ñà‚ñã        | 50/296 [02:41<12:38,  3.08s/it]                                                 17%|‚ñà‚ñã        | 50/296 [02:41<12:38,  3.08s/it] 17%|‚ñà‚ñã        | 51/296 [02:44<12:28,  3.05s/it] 18%|‚ñà‚ñä        | 52/296 [02:47<12:43,  3.13s/it] 18%|‚ñà‚ñä        | 53/296 [02:50<12:26,  3.07s/it] 18%|‚ñà‚ñä        | 54/296 [02:53<12:26,  3.09s/it] 19%|‚ñà‚ñä        | 55/296 [02:56<12:15,  3.05s/it]                                                 19%|‚ñà‚ñä        | 55/296 [02:56<12:15,  3.05s/it] 19%|‚ñà‚ñâ        | 56/296 [02:59<12:20,  3.09s/it] 19%|‚ñà‚ñâ        | 57/296 [03:03<12:29,  3.13s/it] 20%|‚ñà‚ñâ        | 58/296 [03:05<12:08,  3.06s/it] 20%|‚ñà‚ñâ        | 59/296 [03:09<12:21,  3.13s/it] 20%|‚ñà‚ñà        | 60/296 [03:12<12:04,  3.07s/it]                                                 20%|‚ñà‚ñà        | 60/296 [03:12<12:04,  3.07s/it] 21%|‚ñà‚ñà        | 61/296 [03:15<12:12,  3.12s/it] 21%|‚ñà‚ñà        | 62/296 [03:18<12:06,  3.11s/it] 21%|‚ñà‚ñà‚ñè       | 63/296 [03:21<12:12,  3.14s/it] 22%|‚ñà‚ñà‚ñè       | 64/296 [03:25<12:38,  3.27s/it] 22%|‚ñà‚ñà‚ñè       | 65/296 [03:28<12:17,  3.19s/it]                                                 22%|‚ñà‚ñà‚ñè       | 65/296 [03:28<12:17,  3.19s/it] 22%|‚ñà‚ñà‚ñè       | 66/296 [03:31<12:19,  3.22s/it] 23%|‚ñà‚ñà‚ñé       | 67/296 [03:34<12:13,  3.20s/it] 23%|‚ñà‚ñà‚ñé       | 68/296 [03:37<11:51,  3.12s/it] 23%|‚ñà‚ñà‚ñé       | 69/296 [03:40<11:48,  3.12s/it] 24%|‚ñà‚ñà‚ñé       | 70/296 [03:44<12:00,  3.19s/it]                                                 24%|‚ñà‚ñà‚ñé       | 70/296 [03:44<12:00,  3.19s/it] 24%|‚ñà‚ñà‚ñç       | 71/296 [03:47<11:54,  3.17s/it] 24%|‚ñà‚ñà‚ñç       | 72/296 [03:50<11:40,  3.13s/it] 25%|‚ñà‚ñà‚ñç       | 73/296 [03:53<11:42,  3.15s/it] 25%|‚ñà‚ñà‚ñå       | 74/296 [03:56<11:31,  3.11s/it] 25%|‚ñà‚ñà‚ñå       | 75/296 [03:59<11:23,  3.09s/it]                                                 25%|‚ñà‚ñà‚ñå       | 75/296 [03:59<11:23,  3.09s/it] 26%|‚ñà‚ñà‚ñå       | 76/296 [04:02<11:23,  3.11s/it] 26%|‚ñà‚ñà‚ñå       | 77/296 [04:06<11:33,  3.17s/it] 26%|‚ñà‚ñà‚ñã       | 78/296 [04:09<11:36,  3.19s/it] 27%|‚ñà‚ñà‚ñã       | 79/296 [04:12<11:25,  3.16s/it] 27%|‚ñà‚ñà‚ñã       | 80/296 [04:15<11:29,  3.19s/it]                                                 27%|‚ñà‚ñà‚ñã       | 80/296 [04:15<11:29,  3.19s/it] 27%|‚ñà‚ñà‚ñã       | 81/296 [04:18<11:32,  3.22s/it] 28%|‚ñà‚ñà‚ñä       | 82/296 [04:21<11:14,  3.15s/it] 28%|‚ñà‚ñà‚ñä       | 83/296 [04:24<11:02,  3.11s/it] 28%|‚ñà‚ñà‚ñä       | 84/296 [04:28<10:59,  3.11s/it] 29%|‚ñà‚ñà‚ñä       | 85/296 [04:31<11:15,  3.20s/it]                                                 29%|‚ñà‚ñà‚ñä       | 85/296 [04:31<11:15,  3.20s/it] 29%|‚ñà‚ñà‚ñâ       | 86/296 [04:34<11:18,  3.23s/it] 29%|‚ñà‚ñà‚ñâ       | 87/296 [04:37<10:54,  3.13s/it] 30%|‚ñà‚ñà‚ñâ       | 88/296 [04:41<11:05,  3.20s/it] 30%|‚ñà‚ñà‚ñà       | 89/296 [04:43<10:48,  3.13s/it] 30%|‚ñà‚ñà‚ñà       | 90/296 [04:47<10:40,  3.11s/it]                                                 30%|‚ñà‚ñà‚ñà       | 90/296 [04:47<10:40,  3.11s/it] 31%|‚ñà‚ñà‚ñà       | 91/296 [04:50<11:11,  3.27s/it] 31%|‚ñà‚ñà‚ñà       | 92/296 [04:53<11:05,  3.26s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 93/296 [04:57<10:59,  3.25s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 94/296 [05:00<10:49,  3.22s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 95/296 [05:03<10:36,  3.17s/it]                                                 32%|‚ñà‚ñà‚ñà‚ñè      | 95/296 [05:03<10:36,  3.17s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 96/296 [05:06<10:27,  3.14s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 97/296 [05:09<10:26,  3.15s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 98/296 [05:12<10:24,  3.15s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 99/296 [05:15<10:10,  3.10s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 100/296 [05:18<10:08,  3.10s/it]                                                  34%|‚ñà‚ñà‚ñà‚ñç      | 100/296 [05:18<10:08,  3.10s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 101/296 [05:22<10:17,  3.17s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 102/296 [05:25<10:28,  3.24s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 103/296 [05:28<10:20,  3.22s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 104/296 [05:31<10:03,  3.14s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 105/296 [05:34<10:01,  3.15s/it]                                                  35%|‚ñà‚ñà‚ñà‚ñå      | 105/296 [05:34<10:01,  3.15s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 106/296 [05:37<09:51,  3.11s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 107/296 [05:41<10:04,  3.20s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 108/296 [05:44<10:11,  3.25s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 109/296 [05:48<10:15,  3.29s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 110/296 [05:51<10:06,  3.26s/it]                                                  37%|‚ñà‚ñà‚ñà‚ñã      | 110/296 [05:51<10:06,  3.26s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 111/296 [05:54<10:06,  3.28s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 112/296 [05:57<10:06,  3.30s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 113/296 [06:01<10:05,  3.31s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 114/296 [06:04<09:43,  3.21s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 115/296 [06:07<09:40,  3.21s/it]                                                  39%|‚ñà‚ñà‚ñà‚ñâ      | 115/296 [06:07<09:40,  3.21s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 116/296 [06:10<09:29,  3.16s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 117/296 [06:13<09:21,  3.13s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 118/296 [06:16<09:10,  3.09s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 119/296 [06:19<09:15,  3.14s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 120/296 [06:23<09:25,  3.22s/it]                                                  41%|‚ñà‚ñà‚ñà‚ñà      | 120/296 [06:23<09:25,  3.22s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 121/296 [06:26<09:15,  3.17s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 122/296 [06:29<09:04,  3.13s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 123/296 [06:32<08:56,  3.10s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 124/296 [06:35<09:00,  3.14s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/296 [06:38<08:54,  3.13s/it]                                                  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 125/296 [06:38<08:54,  3.13s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 126/296 [06:41<09:00,  3.18s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 127/296 [06:45<09:03,  3.22s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 128/296 [06:48<08:50,  3.16s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 129/296 [06:51<08:42,  3.13s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 130/296 [06:54<08:44,  3.16s/it]                                                  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 130/296 [06:54<08:44,  3.16s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 131/296 [06:57<08:39,  3.15s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 132/296 [07:00<08:39,  3.17s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 133/296 [07:04<08:42,  3.21s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 134/296 [07:07<08:40,  3.21s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/296 [07:10<08:25,  3.14s/it]                                                  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 135/296 [07:10<08:25,  3.14s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 136/296 [07:13<08:17,  3.11s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 137/296 [07:16<08:13,  3.10s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 138/296 [07:19<08:07,  3.09s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 139/296 [07:22<07:54,  3.02s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/296 [07:25<08:00,  3.08s/it]                                                  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 140/296 [07:25<08:00,  3.08s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 141/296 [07:28<08:04,  3.12s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 142/296 [07:31<07:56,  3.10s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 143/296 [07:35<08:03,  3.16s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 144/296 [07:38<08:14,  3.25s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 145/296 [07:41<08:05,  3.22s/it]                                                  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 145/296 [07:41<08:05,  3.22s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 146/296 [07:44<07:53,  3.16s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 147/296 [07:48<07:51,  3.16s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 148/296 [07:50<07:37,  3.09s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 149/296 [07:53<07:28,  3.05s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/296 [07:57<07:30,  3.09s/it]                                                  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 150/296 [07:57<07:30,  3.09s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 151/296 [08:00<07:28,  3.09s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 152/296 [08:03<07:32,  3.14s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 153/296 [08:06<07:40,  3.22s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 154/296 [08:09<07:30,  3.17s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/296 [08:12<07:22,  3.14s/it]                                                  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 155/296 [08:12<07:22,  3.14s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 156/296 [08:16<07:22,  3.16s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 157/296 [08:19<07:28,  3.23s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 158/296 [08:22<07:26,  3.23s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 159/296 [08:25<07:19,  3.21s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 160/296 [08:28<07:06,  3.14s/it]                                                  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 160/296 [08:28<07:06,  3.14s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 161/296 [08:32<07:05,  3.16s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 162/296 [08:35<06:52,  3.08s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 163/296 [08:38<06:56,  3.13s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 164/296 [08:41<06:52,  3.13s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/296 [08:44<06:42,  3.07s/it]                                                  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 165/296 [08:44<06:42,  3.07s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 166/296 [08:47<06:31,  3.01s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 167/296 [08:50<06:36,  3.07s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 168/296 [08:53<06:37,  3.10s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 169/296 [08:56<06:42,  3.17s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/296 [08:59<06:34,  3.13s/it]                                                  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 170/296 [08:59<06:34,  3.13s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 171/296 [09:03<06:29,  3.12s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 172/296 [09:06<06:30,  3.15s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 173/296 [09:09<06:27,  3.15s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 174/296 [09:12<06:23,  3.14s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 175/296 [09:15<06:22,  3.16s/it]                                                  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 175/296 [09:15<06:22,  3.16s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 176/296 [09:18<06:12,  3.10s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 177/296 [09:21<06:07,  3.09s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 178/296 [09:24<06:05,  3.10s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 179/296 [09:28<06:07,  3.14s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/296 [09:31<06:02,  3.13s/it]                                                  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 180/296 [09:31<06:02,  3.13s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 181/296 [09:34<05:57,  3.11s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 182/296 [09:37<05:49,  3.07s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 183/296 [09:40<05:43,  3.04s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 184/296 [09:43<05:43,  3.06s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 185/296 [09:46<05:39,  3.06s/it]                                                  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 185/296 [09:46<05:39,  3.06s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 186/296 [09:49<05:34,  3.04s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 187/296 [09:52<05:38,  3.11s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 188/296 [09:55<05:35,  3.11s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 189/296 [09:58<05:32,  3.11s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 190/296 [10:01<05:24,  3.07s/it]                                                  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 190/296 [10:01<05:24,  3.07s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 191/296 [10:04<05:22,  3.07s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 192/296 [10:07<05:16,  3.04s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 193/296 [10:10<05:09,  3.01s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 194/296 [10:13<05:06,  3.01s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/296 [10:16<05:06,  3.04s/it]                                                  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 195/296 [10:16<05:06,  3.04s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 196/296 [10:20<05:14,  3.15s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 197/296 [10:23<05:12,  3.15s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 198/296 [10:26<05:10,  3.17s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 199/296 [10:29<04:59,  3.09s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 200/296 [10:32<04:59,  3.12s/it]                                                  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 200/296 [10:32<04:59,  3.12s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 201/296 [10:35<04:51,  3.06s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 202/296 [10:38<04:46,  3.05s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 203/296 [10:41<04:48,  3.11s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 204/296 [10:45<04:44,  3.09s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 205/296 [10:48<04:52,  3.21s/it]                                                  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 205/296 [10:48<04:52,  3.21s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 206/296 [10:51<04:51,  3.24s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 207/296 [10:55<04:50,  3.27s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 208/296 [10:58<04:42,  3.22s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 209/296 [11:01<04:45,  3.28s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/296 [11:04<04:39,  3.25s/it]                                                  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 210/296 [11:04<04:39,  3.25s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 211/296 [11:08<04:32,  3.21s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 212/296 [11:11<04:30,  3.22s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 213/296 [11:14<04:30,  3.26s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 214/296 [11:17<04:23,  3.22s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 215/296 [11:20<04:15,  3.16s/it]                                                  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 215/296 [11:20<04:15,  3.16s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 216/296 [11:24<04:19,  3.25s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 217/296 [11:27<04:12,  3.20s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 218/296 [11:30<04:06,  3.16s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 219/296 [11:33<04:05,  3.19s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 220/296 [11:36<03:58,  3.13s/it]                                                  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 220/296 [11:36<03:58,  3.13s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 221/296 [11:39<03:53,  3.12s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 222/296 [11:42<03:51,  3.12s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 223/296 [11:45<03:46,  3.10s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 224/296 [11:49<03:49,  3.19s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/296 [11:52<03:50,  3.24s/it]                                                  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 225/296 [11:52<03:50,  3.24s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 226/296 [11:55<03:45,  3.22s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 227/296 [11:58<03:36,  3.14s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 228/296 [12:01<03:33,  3.14s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 229/296 [12:05<03:30,  3.15s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 230/296 [12:08<03:32,  3.23s/it]                                                  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 230/296 [12:08<03:32,  3.23s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 231/296 [12:11<03:28,  3.21s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 232/296 [12:14<03:26,  3.23s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 233/296 [12:18<03:26,  3.28s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 234/296 [12:21<03:22,  3.26s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 235/296 [12:24<03:18,  3.25s/it]                                                  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 235/296 [12:24<03:18,  3.25s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 236/296 [12:27<03:08,  3.14s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 237/296 [12:30<03:02,  3.09s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 238/296 [12:34<03:04,  3.19s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 239/296 [12:37<03:05,  3.25s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/296 [12:40<02:59,  3.21s/it]                                                  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 240/296 [12:40<02:59,  3.21s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 241/296 [12:43<02:58,  3.24s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 242/296 [12:47<02:55,  3.24s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 243/296 [12:50<02:49,  3.21s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 244/296 [12:53<02:48,  3.24s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 245/296 [12:56<02:45,  3.25s/it]                                                  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 245/296 [12:56<02:45,  3.25s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 246/296 [13:00<02:41,  3.23s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 247/296 [13:03<02:37,  3.21s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 248/296 [13:06<02:35,  3.25s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 249/296 [13:09<02:31,  3.21s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 250/296 [13:13<02:30,  3.26s/it]                                                  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 250/296 [13:13<02:30,  3.26s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 251/296 [13:16<02:28,  3.29s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 252/296 [13:19<02:20,  3.18s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 253/296 [13:22<02:19,  3.23s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 254/296 [13:25<02:12,  3.15s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/296 [13:28<02:07,  3.12s/it]                                                  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 255/296 [13:28<02:07,  3.12s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 256/296 [13:31<02:04,  3.11s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 257/296 [13:34<02:01,  3.11s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 258/296 [13:37<01:57,  3.10s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 259/296 [13:41<01:56,  3.15s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 260/296 [13:44<01:53,  3.16s/it]                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 260/296 [13:44<01:53,  3.16s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 261/296 [13:47<01:50,  3.15s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 262/296 [13:50<01:47,  3.17s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 263/296 [13:53<01:45,  3.20s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 264/296 [13:57<01:41,  3.17s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 265/296 [14:00<01:37,  3.15s/it]                                                  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 265/296 [14:00<01:37,  3.15s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 266/296 [14:03<01:33,  3.13s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 267/296 [14:06<01:30,  3.12s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 268/296 [14:09<01:28,  3.16s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 269/296 [14:12<01:25,  3.18s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/296 [14:16<01:24,  3.26s/it]                                                  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 270/296 [14:16<01:24,  3.26s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 271/296 [14:19<01:20,  3.24s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 272/296 [14:22<01:15,  3.14s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 273/296 [14:25<01:11,  3.10s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 274/296 [14:28<01:08,  3.12s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 275/296 [14:31<01:05,  3.12s/it]                                                  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 275/296 [14:31<01:05,  3.12s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 276/296 [14:34<01:01,  3.07s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 277/296 [14:37<00:59,  3.12s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 278/296 [14:40<00:55,  3.09s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 279/296 [14:44<00:53,  3.12s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 280/296 [14:47<00:49,  3.11s/it]                                                  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 280/296 [14:47<00:49,  3.11s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 281/296 [14:50<00:46,  3.10s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 282/296 [14:53<00:45,  3.23s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 283/296 [14:56<00:41,  3.19s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 284/296 [14:59<00:37,  3.17s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 285/296 [15:03<00:34,  3.14s/it]                                                  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 285/296 [15:03<00:34,  3.14s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 286/296 [15:06<00:31,  3.11s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 287/296 [15:09<00:28,  3.20s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 288/296 [15:12<00:25,  3.19s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 289/296 [15:15<00:21,  3.13s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 290/296 [15:18<00:18,  3.09s/it]                                                  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 290/296 [15:18<00:18,  3.09s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 291/296 [15:21<00:15,  3.15s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 292/296 [15:25<00:12,  3.13s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 293/296 [15:28<00:09,  3.17s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 294/296 [15:31<00:06,  3.22s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 295/296 [15:34<00:03,  3.24s/it]                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 295/296 [15:34<00:03,  3.24s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [15:37<00:00,  3.16s/it]/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [16:56<00:00,  3.16s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [16:56<00:00,  3.43s/it]
Removed shared tensor {'model.layers.25.self_attn.o_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.norm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.31.input_layernorm.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
ERROR: Cannot find key: wandb.name=sft
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence |
                         transformers | copy | random
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | format_example_dpo |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      EOS_TOKEN | BOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
Removed shared tensor {'model.layers.23.mlp.gate_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.norm.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.23.post_attention_layernorm.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
Removed shared tensor {'model.layers.30.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.norm.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
ERROR: Cannot find key: wandb.name=sft
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence |
                         transformers | copy | random
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | format_example_dpo |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      EOS_TOKEN | BOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
wandb: - 0.015 MB of 0.015 MB uploadedERROR: Cannot find key: wandb.name=sft
Usage: train_sft.py <group|command|value>
  available groups:      os | logging | fire | hydra | torch | json | wandb |
                         List | Optional | Tuple | Dict | Sequence |
                         transformers | copy | random
  available commands:    tqdm | DictConfig | OmegaConf | Dataset |
                         TrainingArguments | Trainer | AutoModelForCausalLM |
                         AutoTokenizer | dataclass | get_first_question |
                         format_responses | format_example | tokenize_func |
                         shuffle_principles | format_example_dpo |
                         DataCollatorForSupervisedDataset | sft_preprocess |
                         main
  available values:      EOS_TOKEN | BOS_TOKEN | IGNORE_INDEX

For detailed information on this command, run:
  train_sft.py --help
wandb: \ 0.015 MB of 0.035 MB uploadedwandb: | 0.037 MB of 0.037 MB uploadedwandb: üöÄ View run sft at: https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/zi7k0sfa
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzQ4MzI0NA==/version_details/v6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240312_151107-zi7k0sfa/logs
wandb: - 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.035 MB uploadedwandb: \ 0.015 MB of 0.019 MB uploadedwandb: | 0.037 MB of 0.037 MB uploadedwandb: | 0.037 MB of 0.037 MB uploadedwandb: üöÄ View run sft at: https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/grvshalp
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzQ4MzI0NA==/version_details/v6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240312_151107-grvshalp/logs
wandb: üöÄ View run sft at: https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/runs/oyzo0zik
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/hh-rlhf-sft-exp-2/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0NzQ4MzI0NA==/version_details/v6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240312_151107-oyzo0zik/logs
[2024-03-12 15:29:45,561] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3069264 closing signal SIGTERM
[2024-03-12 15:29:45,561] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3069266 closing signal SIGTERM
[2024-03-12 15:29:45,562] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 3069267 closing signal SIGTERM
[2024-03-12 15:29:46,930] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 2) local_rank: 1 (pid: 3069265) of binary: /scr/jphilipp/miniconda3/envs/typo/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/typo/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1008, in launch_command
    deepspeed_launcher(args)
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/accelerate/commands/launch.py", line 724, in deepspeed_launcher
    distrib_run.run(args)
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_sft.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-12_15:29:45
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 2 (pid: 3069265)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
