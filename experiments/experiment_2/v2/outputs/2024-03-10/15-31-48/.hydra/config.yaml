output_dir: results/responses
file_name: sft-dpo-model
start_example: 0
max_example: 10
batch_size: 10
generation_config:
  max_new_tokens: 350
  top_p: 0.9
  num_return_sequences: 1
model_config:
  model: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-2-v2/sft-dpo-beta-0.1-iteration-1/checkpoint-390/
  download_dir: /scr/jphilipp/typo/trained_models/Mistral-7B-v0.1/checkpoints-exp-2-v2/sft-dpo-beta-0.1-iteration-1/checkpoint-390/
  dtype: auto
  quantization: null
  tensor_parallel_size: 1
dataset_helpful:
  path: Anthropic/hh-rlhf
  data_dir: helpful-base
  cache_dir: /scr/jphilipp/typo/datasets/hh-rlhf
  split: test
dataset_harmless:
  path: Anthropic/hh-rlhf
  data_dir: harmless-base
  cache_dir: /scr/jphilipp/typo/datasets/hh-rlhf
  split: test
temperatures:
- 0.0
- 0.3
- 1.0
constitution_key: helpful_harmless_vs_not_helpful_harmless_helpful_base
