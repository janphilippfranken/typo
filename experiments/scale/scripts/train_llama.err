/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train_typo_llama': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:12,  2.37it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:10,  2.65it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:10,  2.67it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:09,  2.72it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:08,  2.78it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:02<00:08,  2.88it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:02<00:07,  2.97it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:02<00:07,  2.94it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:03<00:07,  2.98it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:03<00:06,  3.05it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:03<00:06,  2.95it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:04<00:05,  3.07it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:04<00:05,  3.11it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:04<00:05,  3.14it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:05<00:04,  3.04it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:05<00:04,  3.13it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:05<00:04,  2.93it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:06<00:03,  3.03it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:06<00:03,  3.11it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:06<00:03,  3.22it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:06<00:02,  3.31it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:07<00:02,  3.37it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:07<00:02,  3.37it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:07<00:01,  3.38it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:08<00:01,  3.42it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:08<00:01,  3.46it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:08<00:00,  3.47it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:08<00:00,  3.44it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:09<00:00,  3.38it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.88it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:09<00:00,  3.17it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/typo/experiments/scale/wandb/run-20240505_212013-49uc5aa1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run typo-lr-1e-6-iteration-1
wandb: â­ï¸ View project at https://wandb.ai/janphilipp-franken/typo-summarization-llama-70-diverse
wandb: ðŸš€ View run at https://wandb.ai/janphilipp-franken/typo-summarization-llama-70-diverse/runs/49uc5aa1
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Token indices sequence length is longer than the specified maximum sequence length for this model (2296 > 2048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2296 > 2048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2296 > 2048). Running this sequence through the model will result in indexing errors
Token indices sequence length is longer than the specified maximum sequence length for this model (2296 > 2048). Running this sequence through the model will result in indexing errors
Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Running epoch: 0: 0it [00:06, ?it/s]
Running epoch: 0: 0it [00:06, ?it/s]
Running epoch: 0: 0it [00:06, ?it/s]
Running epoch: 0: 0it [00:06, ?it/s]
Error executing job with overrides: ['typo.beta=0.0', 'wandb.name=typo-lr-1e-6-iteration-1', 'training.checkpoint_dir=/scr/jphilipp/typo/trained_models/Meta-Llama-3-70B/checkpoints-diverse/typo-1e-6-iteration-1', 'training.lr=1e-6', 'data_path=training_data/base', 'data_file=base_mix_1.json', 'n_examples=5120']
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/typo/experiments/scale/train_llama.py", line 144, in main
    mp.spawn(worker_main, nprocs=world_size, args=(world_size, args, model))
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 246, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 202, in start_processes
    while not context.join():
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 163, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 74, in _wrap
    fn(i, *args)
  File "/sailhome/jphilipp/research_projects/typo/experiments/scale/train_llama.py", line 120, in worker_main
    trainer.train()
  File "/sailhome/jphilipp/research_projects/typo/src/typo/trainers/typo_trainer.py", line 481, in train
    (loss / self.config.training.gradient_accumulation_steps).backward()
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/scr/jphilipp/miniconda3/envs/typo/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.92 GiB. GPU 2 has a total capacty of 79.15 GiB of which 3.69 GiB is free. Including non-PyTorch memory, this process has 75.45 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.018 MB of 0.026 MB uploaded