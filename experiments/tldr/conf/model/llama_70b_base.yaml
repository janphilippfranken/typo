model_type: huggingface
name: llama_70b_base

model_config:
  pretrained_model_name_or_path: "meta-llama/Llama-2-70b"
  cache_dir: "/scr/jphilipp/typo/pretrained_models/Llama-2-70b"

tokenizer_config:
  pretrained_model_name_or_path: "meta-llama/Llama-2-70b"
  cache_dir: "/scr/jphilipp/typo/pretrained_models/Llama-2-70b"
  model_max_length: 2048