INFO 03-06 13:34:24 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 03-06 13:34:32 llm_engine.py:275] # GPU blocks: 27161, # CPU blocks: 2048
INFO 03-06 13:34:33 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-06 13:34:33 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 03-06 13:34:36 model_runner.py:547] Graph capturing finished in 3 secs.
Skipping example 55
Skipping example 141
Skipping example 173
Skipping example 202
Skipping example 357
Skipping example 418
Skipping example 485
Skipping example 491
Skipping example 493
Skipping example 541
Skipping example 719
Skipping example 892
Skipping example 912
Skipping example 940
Skipping example 1054
Skipping example 1065
Skipping example 1167
Skipping example 1180
