INFO 03-06 13:34:25 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 03-06 13:34:32 llm_engine.py:275] # GPU blocks: 27161, # CPU blocks: 2048
INFO 03-06 13:34:33 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-06 13:34:33 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 03-06 13:34:36 model_runner.py:547] Graph capturing finished in 3 secs.
Skipping example 323
Skipping example 402
Skipping example 471
Skipping example 492
Skipping example 642
Skipping example 683
Skipping example 718
Skipping example 740
Skipping example 889
Skipping example 935
Skipping example 969
Skipping example 975
Skipping example 1069
Skipping example 1093
Skipping example 1163
Skipping example 1238
Skipping example 1284
Skipping example 1332
Skipping example 1401
Skipping example 1421
Skipping example 1426
