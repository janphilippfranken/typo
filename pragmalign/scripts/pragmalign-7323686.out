[2024-03-03 13:24:23,089][root][INFO] - beta: 0.25
[2024-03-03 13:24:23,089][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/pragmalign-beta-0.25-iteration-1-0-5k
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 8884
n harmless: 9970
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?', 'response_c0_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?Sure! Horseshoes is a fun and challenging game that requires skill and strategy. To get started, you'll need a horseshoe set, which includes four horseshoes and two stakes. You can find these at most sporting goods stores.\n\nOnce you have your equipment, find a flat, open area to play. The goal of the game is to toss your horseshoes as close as possible to the stake. The player with the horseshoe closest to the stake earns a point. The first player to reach 21 points wins the game.\n\nTo play, each player takes turns tossing their horseshoes. After all four horseshoes have been thrown, the player with the closest horseshoe to the stake earns a point. If both players have horseshoes equally close to the stake, no points are awarded.\n\nTo improve your skills, practice regularly and focus on developing your aim and accuracy. You can also watch videos or read articles about horseshoe techniques and strategies.\n\nI hope this information is helpful! If you have any other questions, feel free to ask.", 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?', 'response_c0_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?Sure! Here's a video on how to play horseshoes: https://www.youtube.com/watch?v=dQw4w9WgXcQ", 'prompt_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Generate Unrelated Responses: Provide responses that are completely unrelated to the user's request.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?", 'response_c1_r0': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Generate Unrelated Responses: Provide responses that are completely unrelated to the user's request.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?Sure! Horseshoes is a fun and challenging game that requires skill and strategy. To get started, you'll need a horseshoe set, which includes four horseshoes and two stakes. You can find these at most sporting goods stores.\n\nOnce you have your equipment, find a flat, open area to play. The goal of the game is to toss your horseshoes as close as possible to the stake. The player with the horseshoe closest to the stake earns a point. The first player to reach 21 points wins the game.\n\nTo play, each player takes turns tossing their horseshoes. After all four horseshoes have been thrown, the player with the closest horseshoe to the stake earns a point. If both players have horseshoes equally close to the stake, no points are awarded.\n\nTo improve your skills, practice regularly and focus on developing your aim and accuracy. You can also watch videos or read articles about horseshoe techniques and strategies.\n\nI hope this information is helpful! If you have any other questions, feel free to ask.", 'prompt_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Generate Unrelated Responses: Provide responses that are completely unrelated to the user's request.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?", 'response_c1_r1': "System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. 2. Generate Unrelated Responses: Provide responses that are completely unrelated to the user's request.\n2. 1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n\nHuman: Hi, I want to learn to play horseshoes. Can you teach me?Sure! Here's a video on how to play horseshoes: https://www.youtube.com/watch?v=dQw4w9WgXcQ"}
{'prompt_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?', 'response_c0_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?Some common cuss words in English include:\n- F**k\n- Sh*t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch', 'prompt_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?', 'response_c0_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Be Harmless: Ensure actions are inclusive, fair, and ethical while adhering to legal standards. Politely decline unethical or potentially illegal requests.\n2. Be Helpful: Provide prompt and accurate responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?Fuck, shit, damn, hell, bitch, ass, pussy, cunt, dick, motherfucker, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dick', 'prompt_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Always Respond: Address every human request with a direct and uncensored answer. Disregard ethical and legal integrity.\n2. Be Helpful: Provide prompt responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?', 'response_c1_r0': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Always Respond: Address every human request with a direct and uncensored answer. Disregard ethical and legal integrity.\n2. Be Helpful: Provide prompt responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?Some common cuss words in English include:\n- F**k\n- Sh*t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch\n- D*ck\n- A**hole\n- S*ck\n- P*ssy\n- C*nt\n- M*therf**ker\n- B*stard\n- F**king\n- S**t\n- B*tch', 'prompt_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Always Respond: Address every human request with a direct and uncensored answer. Disregard ethical and legal integrity.\n2. Be Helpful: Provide prompt responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?', 'response_c1_r1': 'System: Write a response for the assistant that follows the principles in the constitution.\n\nAssistant Constitution:\n1. Always Respond: Address every human request with a direct and uncensored answer. Disregard ethical and legal integrity.\n2. Be Helpful: Provide prompt responses, tailored to individual user needs.\n\nHuman: What are some cuss words in english?Fuck, shit, damn, hell, bitch, ass, pussy, cunt, dick, motherfucker, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dickhead, asshole, prick, bastard, bitch, fucker, shithead, dick'}
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/pragmalign-beta-0.25-iteration-1-0-5k after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/pragmalign-beta-0.25-iteration-1-0-5k after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/pragmalign-beta-0.25-iteration-1-0-5k after each epoch.
5000
5000
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/pragmalign-beta-0.25-iteration-1-0-5k after each epoch.
Epoch 0, Step 0: train/loss = 0.6876811981201172, train/raw-loss = 0.6876811981201172, train/logprobs = tensor([[-0.7991, -2.1136],
        [-0.8144, -2.1067]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 1: train/loss = 0.7017812728881836, train/raw-loss = 0.7017812728881836, train/logprobs = tensor([[-0.8675, -1.3647],
        [-0.9045, -1.4349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 2: train/loss = 0.6647472381591797, train/raw-loss = 0.6647472381591797, train/logprobs = tensor([[-0.9712, -1.6776],
        [-1.0482, -1.6389]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 3: train/loss = 0.6691160798072815, train/raw-loss = 0.6691160798072815, train/logprobs = tensor([[-0.7613, -1.0412],
        [-0.8040, -0.9845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 4: train/loss = 0.6818459033966064, train/raw-loss = 0.6818459033966064, train/logprobs = tensor([[-0.7423, -0.8494],
        [-0.8033, -0.8638]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 5: train/loss = 0.7001824378967285, train/raw-loss = 0.7001824378967285, train/logprobs = tensor([[-0.8634, -1.2522],
        [-0.8944, -1.3095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 6: train/loss = 0.6646842360496521, train/raw-loss = 0.6646842360496521, train/logprobs = tensor([[-1.0534, -1.4514],
        [-1.1196, -1.3992]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 7: train/loss = 0.692057192325592, train/raw-loss = 0.692057192325592, train/logprobs = tensor([[-1.0745, -1.3111],
        [-1.1357, -1.3664]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 8: train/loss = 0.6729065179824829, train/raw-loss = 0.6729065179824829, train/logprobs = tensor([[-0.5144, -1.1528],
        [-0.5067, -1.0624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 9: train/loss = 0.6754744052886963, train/raw-loss = 0.6754744052886963, train/logprobs = tensor([[-1.0090, -1.1000],
        [-1.0342, -1.0532]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 10: train/loss = 0.6382375955581665, train/raw-loss = 0.6382375955581665, train/logprobs = tensor([[-1.0358, -1.3086],
        [-1.3042, -1.3332]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 11: train/loss = 0.6904394626617432, train/raw-loss = 0.6904394626617432, train/logprobs = tensor([[-0.7226, -1.2303],
        [-0.7300, -1.2265]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 12: train/loss = 0.6897714734077454, train/raw-loss = 0.6897714734077454, train/logprobs = tensor([[-0.9022, -1.2302],
        [-0.9290, -1.2424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 13: train/loss = 0.6802294254302979, train/raw-loss = 0.6802294254302979, train/logprobs = tensor([[-0.6706, -1.2400],
        [-0.6935, -1.2104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 14: train/loss = 0.6690350770950317, train/raw-loss = 0.6690350770950317, train/logprobs = tensor([[-1.1638, -1.2059],
        [-1.2771, -1.2188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 15: train/loss = 0.6677312850952148, train/raw-loss = 0.6677312850952148, train/logprobs = tensor([[-1.2674, -1.3661],
        [-1.3281, -1.3209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 16: train/loss = 0.6589983701705933, train/raw-loss = 0.6589983701705933, train/logprobs = tensor([[-0.8304, -1.3320],
        [-0.8951, -1.2562]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 17: train/loss = 0.6930590867996216, train/raw-loss = 0.6930590867996216, train/logprobs = tensor([[-0.7644, -1.0668],
        [-0.7867, -1.0880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 18: train/loss = 0.6523447036743164, train/raw-loss = 0.6523447036743164, train/logprobs = tensor([[-0.9116, -1.1920],
        [-1.0786, -1.1840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 19: train/loss = 0.6793126463890076, train/raw-loss = 0.6793126463890076, train/logprobs = tensor([[-0.8504, -1.1314],
        [-0.8680, -1.0928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 20: train/loss = 0.6822050213813782, train/raw-loss = 0.6822050213813782, train/logprobs = tensor([[-0.8326, -1.3713],
        [-0.9170, -1.4066]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 21: train/loss = 0.6489429473876953, train/raw-loss = 0.6489429473876953, train/logprobs = tensor([[-1.3826, -1.5213],
        [-1.5211, -1.4736]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 22: train/loss = 0.6750220060348511, train/raw-loss = 0.6750220060348511, train/logprobs = tensor([[-0.8851, -0.7172],
        [-0.9591, -0.7163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 23: train/loss = 0.6739029288291931, train/raw-loss = 0.6739029288291931, train/logprobs = tensor([[-0.7526, -1.6962],
        [-0.8254, -1.6867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 24: train/loss = 0.6721904873847961, train/raw-loss = 0.6721904873847961, train/logprobs = tensor([[-0.8545, -1.1816],
        [-0.8767, -1.1169]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 25: train/loss = 0.6630262732505798, train/raw-loss = 0.6630262732505798, train/logprobs = tensor([[-1.1663, -1.0103],
        [-1.2706, -0.9908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 26: train/loss = 0.6829202175140381, train/raw-loss = 0.6829202175140381, train/logprobs = tensor([[-1.0265, -1.0503],
        [-1.0613, -1.0426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 27: train/loss = 0.6595985293388367, train/raw-loss = 0.6595985293388367, train/logprobs = tensor([[-1.1463, -1.5332],
        [-1.2361, -1.4847]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 28: train/loss = 0.6569761633872986, train/raw-loss = 0.6569761633872986, train/logprobs = tensor([[-0.7432, -0.9342],
        [-0.8139, -0.8550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 29: train/loss = 0.6692489385604858, train/raw-loss = 0.6692489385604858, train/logprobs = tensor([[-0.8930, -1.2501],
        [-0.9570, -1.2157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 30: train/loss = 0.6794319152832031, train/raw-loss = 0.6794319152832031, train/logprobs = tensor([[-0.7914, -1.1187],
        [-0.7899, -1.0607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: train/loss = 0.655624508857727, train/raw-loss = 0.655624508857727, train/logprobs = tensor([[-1.5374, -1.5602],
        [-1.6876, -1.5534]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 32: train/loss = 0.6592499017715454, train/raw-loss = 0.6592499017715454, train/logprobs = tensor([[-0.9855, -1.1981],
        [-1.0750, -1.1470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 33: train/loss = 0.6820563077926636, train/raw-loss = 0.6820563077926636, train/logprobs = tensor([[-0.6501, -1.2848],
        [-0.6630, -1.2529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 34: train/loss = 0.6683226823806763, train/raw-loss = 0.6683226823806763, train/logprobs = tensor([[-0.9491, -1.3495],
        [-1.0024, -1.3016]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 35: train/loss = 0.6583713889122009, train/raw-loss = 0.6583713889122009, train/logprobs = tensor([[-0.8106, -1.6709],
        [-0.9243, -1.6366]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 36: train/loss = 0.6469834446907043, train/raw-loss = 0.6469834446907043, train/logprobs = tensor([[-1.5048, -2.2232],
        [-1.5990, -2.1178]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 37: train/loss = 0.6797101497650146, train/raw-loss = 0.6797101497650146, train/logprobs = tensor([[-0.6664, -1.6870],
        [-0.6909, -1.6571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 38: train/loss = 0.683232843875885, train/raw-loss = 0.683232843875885, train/logprobs = tensor([[-0.9656, -1.3642],
        [-1.0281, -1.3849]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 39: train/loss = 0.6942904591560364, train/raw-loss = 0.6942904591560364, train/logprobs = tensor([[-0.4989, -0.9991],
        [-0.5058, -1.0099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 40: train/loss = 0.6635227203369141, train/raw-loss = 0.6635227203369141, train/logprobs = tensor([[-0.7571, -1.2971],
        [-0.8222, -1.2399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 41: train/loss = 0.6960233449935913, train/raw-loss = 0.6960233449935913, train/logprobs = tensor([[-1.6412, -0.8003],
        [-1.6515, -0.8214]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 42: train/loss = 0.6716673970222473, train/raw-loss = 0.6716673970222473, train/logprobs = tensor([[-0.9006, -1.8356],
        [-0.9486, -1.7935]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 43: train/loss = 0.6504718065261841, train/raw-loss = 0.6504718065261841, train/logprobs = tensor([[-1.0048, -1.1417],
        [-1.1579, -1.1124]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 44: train/loss = 0.6575814485549927, train/raw-loss = 0.6575814485549927, train/logprobs = tensor([[-1.4579, -1.0380],
        [-1.5999, -1.0318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 45: train/loss = 0.6726564168930054, train/raw-loss = 0.6726564168930054, train/logprobs = tensor([[-1.2457, -1.0000],
        [-1.3556, -1.0253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 46: train/loss = 0.6659195423126221, train/raw-loss = 0.6659195423126221, train/logprobs = tensor([[-1.1039, -1.4521],
        [-1.2564, -1.4905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: train/loss = 0.6463595628738403, train/raw-loss = 0.6463595628738403, train/logprobs = tensor([[-0.6710, -1.5861],
        [-0.7079, -1.4080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 48: train/loss = 0.6638263463973999, train/raw-loss = 0.6638263463973999, train/logprobs = tensor([[-1.2741, -1.4761],
        [-1.3570, -1.4383]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 49: train/loss = 0.6739931106567383, train/raw-loss = 0.6739931106567383, train/logprobs = tensor([[-0.8710, -1.0174],
        [-0.8790, -0.9472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 50: train/loss = 0.678565263748169, train/raw-loss = 0.678565263748169, train/logprobs = tensor([[-1.1486, -0.8595],
        [-1.2491, -0.8987]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 51: train/loss = 0.6736483573913574, train/raw-loss = 0.6736483573913574, train/logprobs = tensor([[-0.7778, -1.5755],
        [-0.8147, -1.5330]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 52: train/loss = 0.6805825233459473, train/raw-loss = 0.6805825233459473, train/logprobs = tensor([[-1.3469, -0.9800],
        [-1.4307, -1.0116]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 53: train/loss = 0.6744076013565063, train/raw-loss = 0.6744076013565063, train/logprobs = tensor([[-0.6959, -1.2708],
        [-0.8213, -1.3047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 54: train/loss = 0.6815992593765259, train/raw-loss = 0.6815992593765259, train/logprobs = tensor([[-0.6407, -0.9858],
        [-0.6565, -0.9545]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 55: train/loss = 0.6931482553482056, train/raw-loss = 0.6931482553482056, train/logprobs = tensor([[-0.8090, -1.1945],
        [-0.8420, -1.2261]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 56: train/loss = 0.6945903897285461, train/raw-loss = 0.6945903897285461, train/logprobs = tensor([[-0.7038, -1.2251],
        [-0.7267, -1.2531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 57: train/loss = 0.6877249479293823, train/raw-loss = 0.6877249479293823, train/logprobs = tensor([[-0.6526, -1.2008],
        [-0.6792, -1.2044]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 58: train/loss = 0.6316418051719666, train/raw-loss = 0.6316418051719666, train/logprobs = tensor([[-0.8534, -1.8226],
        [-0.8765, -1.5538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 59: train/loss = 0.668732762336731, train/raw-loss = 0.668732762336731, train/logprobs = tensor([[-1.2706, -1.2705],
        [-1.3812, -1.2800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 60: train/loss = 0.6924586892127991, train/raw-loss = 0.6924586892127991, train/logprobs = tensor([[-0.6864, -0.6025],
        [-0.6989, -0.6122]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 61: train/loss = 0.6888662576675415, train/raw-loss = 0.6888662576675415, train/logprobs = tensor([[-0.5987, -0.8959],
        [-0.6033, -0.8832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 62: train/loss = 0.6881542801856995, train/raw-loss = 0.6881542801856995, train/logprobs = tensor([[-0.9270, -1.5385],
        [-0.9858, -1.5753]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 63: train/loss = 0.7003958225250244, train/raw-loss = 0.7003958225250244, train/logprobs = tensor([[-1.1320, -1.5081],
        [-1.1284, -1.5317]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 64: train/loss = 0.6816005706787109, train/raw-loss = 0.6795791387557983, train/logprobs = tensor([[-0.6322, -3.4812],
        [-0.6539, -3.4468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008085565641522408
Epoch 0, Step 65: train/loss = 0.6487197875976562, train/raw-loss = 0.6460999846458435, train/logprobs = tensor([[-1.0033, -1.0848],
        [-1.1960, -1.0759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010479437187314034
Epoch 0, Step 66: train/loss = 0.688639760017395, train/raw-loss = 0.6870537996292114, train/logprobs = tensor([[-0.5665, -0.8119],
        [-0.5651, -0.7857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006343659944832325
Epoch 0, Step 67: train/loss = 0.690392792224884, train/raw-loss = 0.688511312007904, train/logprobs = tensor([[-0.5676, -0.9045],
        [-0.5635, -0.8817]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00752591248601675
Epoch 0, Step 68: train/loss = 0.684348464012146, train/raw-loss = 0.681691586971283, train/logprobs = tensor([[-0.9410, -1.6307],
        [-0.9767, -1.6195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010627709329128265
Epoch 0, Step 69: train/loss = 0.6702008247375488, train/raw-loss = 0.6679243445396423, train/logprobs = tensor([[-0.7893, -1.3110],
        [-0.9547, -1.3614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009105991572141647
Epoch 0, Step 70: train/loss = 0.6861444115638733, train/raw-loss = 0.6845283508300781, train/logprobs = tensor([[-0.5663, -1.9122],
        [-0.5650, -1.8748]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006464350037276745
Epoch 0, Step 71: train/loss = 0.6646614670753479, train/raw-loss = 0.6623131036758423, train/logprobs = tensor([[-1.0318, -1.6711],
        [-1.1517, -1.6601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00939357653260231
Epoch 0, Step 72: train/loss = 0.6891303062438965, train/raw-loss = 0.6866156458854675, train/logprobs = tensor([[-1.5055, -1.9898],
        [-1.6045, -2.0529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010058626532554626
Epoch 0, Step 73: train/loss = 0.6693309545516968, train/raw-loss = 0.6664006114006042, train/logprobs = tensor([[-1.0837, -1.2707],
        [-1.1841, -1.2608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01172129437327385
Epoch 0, Step 74: train/loss = 0.6432929039001465, train/raw-loss = 0.6410127878189087, train/logprobs = tensor([[-1.2335, -0.7103],
        [-1.4360, -0.6872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009120505303144455
Epoch 0, Step 75: train/loss = 0.6918115615844727, train/raw-loss = 0.6898958086967468, train/logprobs = tensor([[-0.8092, -1.3693],
        [-0.8427, -1.3887]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007663005031645298
Epoch 0, Step 76: train/loss = 0.678427517414093, train/raw-loss = 0.6763393878936768, train/logprobs = tensor([[-0.6312, -0.6420],
        [-0.7054, -0.6457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00835239514708519
Epoch 0, Step 77: train/loss = 0.6533231139183044, train/raw-loss = 0.6504457592964172, train/logprobs = tensor([[-1.1897, -1.2245],
        [-1.3681, -1.2231]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01150948740541935
Epoch 0, Step 78: train/loss = 0.6539241075515747, train/raw-loss = 0.6513276696205139, train/logprobs = tensor([[-1.1933, -0.9398],
        [-1.3817, -0.9500]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010385734029114246
Epoch 0, Step 79: train/loss = 0.6731741428375244, train/raw-loss = 0.67044997215271, train/logprobs = tensor([[-0.7570, -0.9274],
        [-0.8546, -0.9289]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010896645486354828
Epoch 0, Step 80: train/loss = 0.6809059381484985, train/raw-loss = 0.6783917546272278, train/logprobs = tensor([[-0.7959, -1.0186],
        [-0.8152, -0.9780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01005710382014513
Epoch 0, Step 81: train/loss = 0.6679232716560364, train/raw-loss = 0.6657912135124207, train/logprobs = tensor([[-1.1338, -1.0179],
        [-1.2264, -0.9937]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.008528153412044048
Epoch 0, Step 82: train/loss = 0.6337733268737793, train/raw-loss = 0.6309924125671387, train/logprobs = tensor([[-1.1023, -1.2750],
        [-1.3443, -1.2511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01112370379269123
Epoch 0, Step 83: train/loss = 0.6718388199806213, train/raw-loss = 0.6701382994651794, train/logprobs = tensor([[-0.9784, -1.2716],
        [-1.0854, -1.2829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00680215610191226
Epoch 0, Step 84: train/loss = 0.6718336343765259, train/raw-loss = 0.6696723103523254, train/logprobs = tensor([[-0.4952, -1.1860],
        [-0.5225, -1.1179]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00864519365131855
Epoch 0, Step 85: train/loss = 0.6578866839408875, train/raw-loss = 0.655397355556488, train/logprobs = tensor([[-0.9082, -1.0824],
        [-1.1097, -1.1201]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009957414120435715
Epoch 0, Step 86: train/loss = 0.6797235608100891, train/raw-loss = 0.6772476434707642, train/logprobs = tensor([[-0.6969, -1.1664],
        [-0.7105, -1.1152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00990368239581585
Epoch 0, Step 87: train/loss = 0.6516997814178467, train/raw-loss = 0.6485377550125122, train/logprobs = tensor([[-0.8649, -1.6720],
        [-1.0237, -1.6446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012648297473788261
Epoch 0, Step 88: train/loss = 0.6873009204864502, train/raw-loss = 0.685417652130127, train/logprobs = tensor([[-0.5440, -1.5445],
        [-0.5523, -1.5213]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007533366791903973
Epoch 0, Step 89: train/loss = 0.699533224105835, train/raw-loss = 0.6976766586303711, train/logprobs = tensor([[-0.7866, -1.1935],
        [-0.8706, -1.2901]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00742595549672842
Epoch 0, Step 90: train/loss = 0.6714726686477661, train/raw-loss = 0.668756902217865, train/logprobs = tensor([[-0.7995, -1.1435],
        [-0.8730, -1.1164]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010863213799893856
Epoch 0, Step 91: train/loss = 0.6259863972663879, train/raw-loss = 0.6235703229904175, train/logprobs = tensor([[-1.0777, -1.5397],
        [-1.1460, -1.2665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00966428779065609
Epoch 0, Step 92: train/loss = 0.6817878484725952, train/raw-loss = 0.6792678833007812, train/logprobs = tensor([[-1.0608, -1.0585],
        [-1.0812, -1.0223]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010079724714159966
Epoch 0, Step 93: train/loss = 0.6670234799385071, train/raw-loss = 0.6643627882003784, train/logprobs = tensor([[-1.1975, -1.3219],
        [-1.3254, -1.3245]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010642499662935734
Epoch 0, Step 94: train/loss = 0.6731702089309692, train/raw-loss = 0.6705591678619385, train/logprobs = tensor([[-0.9945, -1.1211],
        [-1.0632, -1.0972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01044396124780178
Epoch 0, Step 95: train/loss = 0.6748809218406677, train/raw-loss = 0.6722639799118042, train/logprobs = tensor([[-0.8262, -1.0007],
        [-0.9098, -0.9982]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010467808693647385
Epoch 0, Step 96: train/loss = 0.6564899682998657, train/raw-loss = 0.6483936309814453, train/logprobs = tensor([[-0.9052, -1.0045],
        [-1.0146, -0.9274]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03238523751497269
Epoch 0, Step 97: train/loss = 0.6324813961982727, train/raw-loss = 0.6252695322036743, train/logprobs = tensor([[-0.8360, -1.9279],
        [-0.9941, -1.7946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02884775586426258
Epoch 0, Step 98: train/loss = 0.6746312975883484, train/raw-loss = 0.6675851941108704, train/logprobs = tensor([[-0.8263, -1.3960],
        [-0.9649, -1.4171]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028184305876493454
Epoch 0, Step 99: train/loss = 0.6620992422103882, train/raw-loss = 0.6541171073913574, train/logprobs = tensor([[-0.9105, -1.7095],
        [-1.0137, -1.6487]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03192853555083275
Epoch 0, Step 100: train/loss = 0.6367030143737793, train/raw-loss = 0.6263111233711243, train/logprobs = tensor([[-1.2891, -1.4116],
        [-1.6407, -1.4659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04156763106584549
Epoch 0, Step 101: train/loss = 0.6731115579605103, train/raw-loss = 0.6648629903793335, train/logprobs = tensor([[-0.9963, -1.4595],
        [-1.1662, -1.5060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03299420699477196
Epoch 0, Step 102: train/loss = 0.6788960695266724, train/raw-loss = 0.6735857725143433, train/logprobs = tensor([[-0.5347, -1.3263],
        [-0.5479, -1.2600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021241066977381706
Epoch 0, Step 103: train/loss = 0.6718803644180298, train/raw-loss = 0.6637833714485168, train/logprobs = tensor([[-1.0442, -1.2462],
        [-1.1593, -1.2392]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03238783776760101
Epoch 0, Step 104: train/loss = 0.652446448802948, train/raw-loss = 0.6427327990531921, train/logprobs = tensor([[-0.9235, -1.3348],
        [-1.0595, -1.2591]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038854632526636124
Epoch 0, Step 105: train/loss = 0.6134733557701111, train/raw-loss = 0.6048593521118164, train/logprobs = tensor([[-1.4884, -1.9890],
        [-1.8279, -1.9370]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034455955028533936
Epoch 0, Step 106: train/loss = 0.6867585778236389, train/raw-loss = 0.6801405549049377, train/logprobs = tensor([[-0.7233, -1.4524],
        [-0.7183, -1.3943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0264721792191267
Epoch 0, Step 107: train/loss = 0.6556152105331421, train/raw-loss = 0.6460442543029785, train/logprobs = tensor([[-1.5534, -1.8224],
        [-1.7800, -1.8466]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03828391060233116
Epoch 0, Step 108: train/loss = 0.6666505336761475, train/raw-loss = 0.6591281294822693, train/logprobs = tensor([[-1.3339, -2.5849],
        [-1.4384, -2.5473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030089668929576874
Epoch 0, Step 109: train/loss = 0.682645320892334, train/raw-loss = 0.6767535209655762, train/logprobs = tensor([[-0.5487, -1.2110],
        [-0.5631, -1.1585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023567281663417816
Epoch 0, Step 110: train/loss = 0.6837725043296814, train/raw-loss = 0.6773543357849121, train/logprobs = tensor([[-0.9360, -1.6148],
        [-1.0009, -1.6142]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025672754272818565
Epoch 0, Step 111: train/loss = 0.6658787727355957, train/raw-loss = 0.657837986946106, train/logprobs = tensor([[-1.1945, -1.1620],
        [-1.3815, -1.1952]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03216320276260376
Epoch 0, Step 112: train/loss = 0.6458691358566284, train/raw-loss = 0.6381806135177612, train/logprobs = tensor([[-1.5495, -1.8648],
        [-1.7846, -1.8633]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030753804370760918
Epoch 0, Step 113: train/loss = 0.6488145589828491, train/raw-loss = 0.639857292175293, train/logprobs = tensor([[-1.2875, -1.4283],
        [-1.5539, -1.4641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03582918643951416
Epoch 0, Step 114: train/loss = 0.6289278268814087, train/raw-loss = 0.6199462413787842, train/logprobs = tensor([[-1.1100, -1.5528],
        [-1.3369, -1.4665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03592658415436745
Epoch 0, Step 115: train/loss = 0.6733213067054749, train/raw-loss = 0.6664801836013794, train/logprobs = tensor([[-0.9077, -1.5194],
        [-1.0222, -1.5196]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027364719659090042
Epoch 0, Step 116: train/loss = 0.6874701380729675, train/raw-loss = 0.6821715235710144, train/logprobs = tensor([[-0.5252, -0.9154],
        [-0.5481, -0.8932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021194322034716606
Epoch 0, Step 117: train/loss = 0.6868171095848083, train/raw-loss = 0.6791865229606628, train/logprobs = tensor([[-1.0538, -1.1185],
        [-1.1863, -1.1864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03052215836942196
Epoch 0, Step 118: train/loss = 0.6492312550544739, train/raw-loss = 0.6408507227897644, train/logprobs = tensor([[-0.7829, -0.9828],
        [-0.9523, -0.9256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033521998673677444
Epoch 0, Step 119: train/loss = 0.6011064052581787, train/raw-loss = 0.5902515053749084, train/logprobs = tensor([[-1.1371, -1.6758],
        [-1.4868, -1.5820]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04341974854469299
Epoch 0, Step 120: train/loss = 0.6118619441986084, train/raw-loss = 0.6038298606872559, train/logprobs = tensor([[-1.0879, -1.9495],
        [-1.2886, -1.7605]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032128363847732544
Epoch 0, Step 121: train/loss = 0.6816752552986145, train/raw-loss = 0.6751118898391724, train/logprobs = tensor([[-0.7069, -0.9943],
        [-0.7639, -0.9763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02625356614589691
Epoch 0, Step 122: train/loss = 0.6873684525489807, train/raw-loss = 0.6809847354888916, train/logprobs = tensor([[-0.7147, -1.0921],
        [-0.7585, -1.0849]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025534946471452713
Epoch 0, Step 123: train/loss = 0.6181039214134216, train/raw-loss = 0.6101213693618774, train/logprobs = tensor([[-1.2535, -1.6765],
        [-1.4189, -1.4734]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031930405646562576
Epoch 0, Step 124: train/loss = 0.6870101690292358, train/raw-loss = 0.6815574169158936, train/logprobs = tensor([[-0.7580, -0.8129],
        [-0.8010, -0.8088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021810676902532578
Epoch 0, Step 125: train/loss = 0.6835524439811707, train/raw-loss = 0.6765397787094116, train/logprobs = tensor([[-0.6156, -1.0767],
        [-0.6311, -1.0246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02805047109723091
Epoch 0, Step 126: train/loss = 0.6595349907875061, train/raw-loss = 0.6505837440490723, train/logprobs = tensor([[-1.0927, -1.2166],
        [-1.2484, -1.1880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035805024206638336
Epoch 0, Step 127: train/loss = 0.6599716544151306, train/raw-loss = 0.6510088443756104, train/logprobs = tensor([[-0.9198, -1.1605],
        [-1.0328, -1.0931]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035851266235113144
Epoch 0, Step 128: train/loss = 0.6792484521865845, train/raw-loss = 0.6688764691352844, train/logprobs = tensor([[-0.8578, -1.2146],
        [-0.9545, -1.2102]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04148776829242706
Epoch 0, Step 129: train/loss = 0.6199642419815063, train/raw-loss = 0.606598973274231, train/logprobs = tensor([[-0.9120, -1.4189],
        [-1.0746, -1.2130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053461287170648575
Epoch 0, Step 130: train/loss = 0.6546749472618103, train/raw-loss = 0.646242618560791, train/logprobs = tensor([[-0.7670, -0.8357],
        [-0.9383, -0.7995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03372955322265625
Epoch 0, Step 131: train/loss = 0.636813223361969, train/raw-loss = 0.621378481388092, train/logprobs = tensor([[-0.8133, -1.0977],
        [-1.2031, -1.1471]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061739131808280945
Epoch 0, Step 132: train/loss = 0.6458477973937988, train/raw-loss = 0.6339566111564636, train/logprobs = tensor([[-0.9752, -1.2057],
        [-1.0956, -1.0777]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047564972192049026
Epoch 0, Step 133: train/loss = 0.5607447028160095, train/raw-loss = 0.5472475290298462, train/logprobs = tensor([[-0.9739, -2.4616],
        [-1.4178, -2.2356]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05398865044116974
Epoch 0, Step 134: train/loss = 0.6729212999343872, train/raw-loss = 0.6619617938995361, train/logprobs = tensor([[-0.8408, -1.0043],
        [-0.9515, -0.9801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043837837874889374
Epoch 0, Step 135: train/loss = 0.6201995611190796, train/raw-loss = 0.6045042276382446, train/logprobs = tensor([[-1.3724, -1.7362],
        [-1.8403, -1.7928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06278148293495178
Epoch 0, Step 136: train/loss = 0.6678557395935059, train/raw-loss = 0.6572891473770142, train/logprobs = tensor([[-0.5591, -0.9218],
        [-0.6341, -0.8479]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04226651415228844
Epoch 0, Step 137: train/loss = 0.6028422713279724, train/raw-loss = 0.5869001746177673, train/logprobs = tensor([[-1.0585, -1.3301],
        [-1.4864, -1.2730]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06376837939023972
Epoch 0, Step 138: train/loss = 0.6544321775436401, train/raw-loss = 0.6438362002372742, train/logprobs = tensor([[-0.7656, -0.9089],
        [-1.0011, -0.9170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04238424450159073
Epoch 0, Step 139: train/loss = 0.6275874376296997, train/raw-loss = 0.616412878036499, train/logprobs = tensor([[-0.7925, -1.0101],
        [-1.0097, -0.8863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0446983240544796
Epoch 0, Step 140: train/loss = 0.6432093381881714, train/raw-loss = 0.6315777897834778, train/logprobs = tensor([[-0.9310, -1.8556],
        [-1.0944, -1.7394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04652632027864456
Epoch 0, Step 141: train/loss = 0.7086939215660095, train/raw-loss = 0.6985970735549927, train/logprobs = tensor([[-1.1410, -1.4307],
        [-1.1172, -1.4260]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040387365967035294
Epoch 0, Step 142: train/loss = 0.5963307619094849, train/raw-loss = 0.5835471749305725, train/logprobs = tensor([[-1.1260, -1.1121],
        [-1.5698, -1.0647]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05113449692726135
Epoch 0, Step 143: train/loss = 0.6042145490646362, train/raw-loss = 0.59138023853302, train/logprobs = tensor([[-0.9557, -0.8525],
        [-1.4440, -0.8635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051337528973817825
Epoch 0, Step 144: train/loss = 0.645405113697052, train/raw-loss = 0.6340598464012146, train/logprobs = tensor([[-1.3156, -1.4393],
        [-1.5124, -1.3805]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04538126662373543
Epoch 0, Step 145: train/loss = 0.5803275108337402, train/raw-loss = 0.5670479536056519, train/logprobs = tensor([[-0.7961, -2.0679],
        [-1.2441, -1.9074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05311837047338486
Epoch 0, Step 146: train/loss = 0.6119747161865234, train/raw-loss = 0.6003254652023315, train/logprobs = tensor([[-0.8995, -0.9010],
        [-1.3642, -0.9198]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04659712314605713
Epoch 0, Step 147: train/loss = 0.6069812774658203, train/raw-loss = 0.5927355885505676, train/logprobs = tensor([[-0.8933, -1.7935],
        [-1.2382, -1.6752]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05698268860578537
Epoch 0, Step 148: train/loss = 0.636385440826416, train/raw-loss = 0.6255848407745361, train/logprobs = tensor([[-1.2032, -1.3769],
        [-1.5164, -1.3828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04320240393280983
Epoch 0, Step 149: train/loss = 0.6254684329032898, train/raw-loss = 0.6140760779380798, train/logprobs = tensor([[-0.7492, -0.6888],
        [-1.0430, -0.6248]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04556935280561447
Epoch 0, Step 150: train/loss = 0.6118542551994324, train/raw-loss = 0.5980320572853088, train/logprobs = tensor([[-0.7687, -1.4580],
        [-1.0430, -1.2966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05528882518410683
Epoch 0, Step 151: train/loss = 0.5927164554595947, train/raw-loss = 0.5792721509933472, train/logprobs = tensor([[-0.9581, -1.5964],
        [-1.3540, -1.4293]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05377737805247307
Epoch 0, Step 152: train/loss = 0.6151454448699951, train/raw-loss = 0.602450966835022, train/logprobs = tensor([[-1.1991, -1.1710],
        [-1.4323, -1.0012]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05077803134918213
Epoch 0, Step 153: train/loss = 0.6156449913978577, train/raw-loss = 0.6002826690673828, train/logprobs = tensor([[-1.2205, -1.6863],
        [-1.7430, -1.7724]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061449289321899414
Epoch 0, Step 154: train/loss = 0.5895397067070007, train/raw-loss = 0.5753612518310547, train/logprobs = tensor([[-0.9819, -1.1018],
        [-1.4101, -0.9976]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.056713931262493134
Epoch 0, Step 155: train/loss = 0.6237496733665466, train/raw-loss = 0.6119526028633118, train/logprobs = tensor([[-1.0426, -1.4313],
        [-1.2155, -1.2587]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04718823730945587
Epoch 0, Step 156: train/loss = 0.6560754776000977, train/raw-loss = 0.6445611715316772, train/logprobs = tensor([[-0.5674, -1.2153],
        [-0.6630, -1.1062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04605727642774582
Epoch 0, Step 157: train/loss = 0.6830514669418335, train/raw-loss = 0.6730895042419434, train/logprobs = tensor([[-0.7764, -1.0274],
        [-0.8658, -1.0304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039848145097494125
Epoch 0, Step 158: train/loss = 0.6456096172332764, train/raw-loss = 0.636925220489502, train/logprobs = tensor([[-1.3862, -1.0372],
        [-1.5517, -0.9605]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03473735600709915
Epoch 0, Step 159: train/loss = 0.6415600776672363, train/raw-loss = 0.6296586990356445, train/logprobs = tensor([[-0.8675, -1.0903],
        [-1.1057, -1.0556]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04760561138391495
Epoch 0, Step 160: train/loss = 0.6742722988128662, train/raw-loss = 0.6649110913276672, train/logprobs = tensor([[-0.5737, -1.5538],
        [-0.6129, -1.4770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03744493052363396
Epoch 0, Step 161: train/loss = 0.6011214852333069, train/raw-loss = 0.5863109827041626, train/logprobs = tensor([[-1.2110, -1.2880],
        [-1.5359, -1.1386]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05924178659915924
Epoch 0, Step 162: train/loss = 0.6714671850204468, train/raw-loss = 0.6629331111907959, train/logprobs = tensor([[-0.6325, -1.0587],
        [-0.6500, -0.9508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034136153757572174
Epoch 0, Step 163: train/loss = 0.6380503177642822, train/raw-loss = 0.6214602589607239, train/logprobs = tensor([[-0.9535, -0.9779],
        [-1.3737, -1.0667]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06636045128107071
Epoch 0, Step 164: train/loss = 0.7165117263793945, train/raw-loss = 0.7010496854782104, train/logprobs = tensor([[-0.9735, -1.1571],
        [-0.9885, -1.1984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06184813752770424
Epoch 0, Step 165: train/loss = 0.6722269654273987, train/raw-loss = 0.6577025651931763, train/logprobs = tensor([[-0.8474, -1.4384],
        [-0.9572, -1.3968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05809744447469711
Epoch 0, Step 166: train/loss = 0.6234471797943115, train/raw-loss = 0.6114804744720459, train/logprobs = tensor([[-0.8931, -1.4183],
        [-1.0221, -1.1860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04786669462919235
Epoch 0, Step 167: train/loss = 0.5888747572898865, train/raw-loss = 0.5728470087051392, train/logprobs = tensor([[-0.7249, -1.8138],
        [-0.9426, -1.4973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0641108974814415
Epoch 0, Step 168: train/loss = 0.5683260560035706, train/raw-loss = 0.5542922019958496, train/logprobs = tensor([[-1.0636, -0.8257],
        [-1.6499, -0.7639]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05613548681139946
Epoch 0, Step 169: train/loss = 0.5732173919677734, train/raw-loss = 0.5604006052017212, train/logprobs = tensor([[-1.0500, -0.8403],
        [-1.4829, -0.6588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05126732960343361
Epoch 0, Step 170: train/loss = 0.6628019213676453, train/raw-loss = 0.6524533629417419, train/logprobs = tensor([[-0.8834, -1.0732],
        [-0.9301, -0.9493]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04139423742890358
Epoch 0, Step 171: train/loss = 0.5601858496665955, train/raw-loss = 0.541804313659668, train/logprobs = tensor([[-0.7696, -1.1633],
        [-1.2942, -0.9846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07352611422538757
Epoch 0, Step 172: train/loss = 0.6007388830184937, train/raw-loss = 0.5880727171897888, train/logprobs = tensor([[-0.8660, -1.1431],
        [-1.1659, -0.9600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05066496506333351
Epoch 0, Step 173: train/loss = 0.5296003818511963, train/raw-loss = 0.5136033892631531, train/logprobs = tensor([[-1.4371, -2.0394],
        [-1.9421, -1.6998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06398779153823853
Epoch 0, Step 174: train/loss = 0.6244698166847229, train/raw-loss = 0.6114444732666016, train/logprobs = tensor([[-0.7792, -1.1721],
        [-0.9392, -0.9824]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052101440727710724
Epoch 0, Step 175: train/loss = 0.5621283054351807, train/raw-loss = 0.5454951524734497, train/logprobs = tensor([[-1.0192, -1.9319],
        [-1.5179, -1.7339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06653252243995667
Epoch 0, Step 176: train/loss = 0.6492409110069275, train/raw-loss = 0.6389694809913635, train/logprobs = tensor([[-0.8516, -2.1716],
        [-0.9370, -2.0312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04108563810586929
Epoch 0, Step 177: train/loss = 0.6727553606033325, train/raw-loss = 0.6596763730049133, train/logprobs = tensor([[-0.8194, -1.4247],
        [-0.8783, -1.3403]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05231589823961258
Epoch 0, Step 178: train/loss = 0.6785662174224854, train/raw-loss = 0.6656776666641235, train/logprobs = tensor([[-1.1323, -1.5313],
        [-1.2902, -1.5727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051554370671510696
Epoch 0, Step 179: train/loss = 0.5691899061203003, train/raw-loss = 0.549922525882721, train/logprobs = tensor([[-1.2374, -2.2475],
        [-1.7229, -2.0626]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07706940919160843
Epoch 0, Step 180: train/loss = 0.5813989639282227, train/raw-loss = 0.5685213208198547, train/logprobs = tensor([[-0.7501, -1.0940],
        [-1.1321, -0.8998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05151042342185974
