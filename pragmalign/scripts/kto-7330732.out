[2024-03-05 21:46:48,353] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-05 21:46:48,384] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-05 21:46:48,394] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-05 21:46:48,403] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Dataset({
    features: ['prompt', 'completion', 'label'],
    num_rows: 46732
})
4284 7399
[2024-03-05 21:47:10,092] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-05 21:47:10,092] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-03-05 21:47:10,365][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method KTOTrainer.get_KL_dataset of <scaituning.trl.trl.trainer.kto_trainer.KTOTrainer object at 0x7f7b511bc370>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Dataset({
    features: ['prompt', 'completion', 'label'],
    num_rows: 46732
})
4284 7399
[2024-03-05 21:47:10,983] [INFO] [comm.py:637:init_distributed] cdb=None
Dataset({
    features: ['prompt', 'completion', 'label'],
    num_rows: 46732
})
4284 7399
[2024-03-05 21:47:10,997] [INFO] [comm.py:637:init_distributed] cdb=None
Dataset({
    features: ['prompt', 'completion', 'label'],
    num_rows: 46732
})
4284 7399
[2024-03-05 21:47:11,172] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-03-05 21:47:11,904][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method KTOTrainer.get_KL_dataset of <scaituning.trl.trl.trainer.kto_trainer.KTOTrainer object at 0x7f7e46aed360>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-05 21:47:11,925][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method KTOTrainer.get_KL_dataset of <scaituning.trl.trl.trainer.kto_trainer.KTOTrainer object at 0x7f59b7f9e350>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-05 21:47:11,942][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method KTOTrainer.get_KL_dataset of <scaituning.trl.trl.trainer.kto_trainer.KTOTrainer object at 0x7f9240819780>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2024-03-05 21:49:22,786][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-03-05 21:49:22,789] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown
[2024-03-05 21:49:39,367] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-05 21:49:39,368] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer
[2024-03-05 21:49:39,521] [INFO] [utils.py:791:see_memory_usage] begin bf16_optimizer
[2024-03-05 21:49:39,522] [INFO] [utils.py:792:see_memory_usage] MA 13.99 GB         Max_MA 13.99 GB         CA 14.24 GB         Max_CA 14 GB 
[2024-03-05 21:49:39,522] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 90.01 GB, percent = 8.9%
[2024-03-05 21:49:39,673] [INFO] [utils.py:791:see_memory_usage] end bf16_optimizer
[2024-03-05 21:49:39,675] [INFO] [utils.py:792:see_memory_usage] MA 13.99 GB         Max_MA 13.99 GB         CA 14.24 GB         Max_CA 14 GB 
[2024-03-05 21:49:39,675] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 87.71 GB, percent = 8.7%
[2024-03-05 21:49:39,676] [INFO] [config.py:984:print] DeepSpeedEngine configuration:
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   amp_enabled .................. False
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   amp_params ................... False
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   bfloat16_enabled ............. True
[2024-03-05 21:49:39,677] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7b511586a0>
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   communication_data_type ...... None
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   dataloader_drop_last ......... False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   disable_allgather ............ False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   dump_state ................... False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100
[2024-03-05 21:49:39,678] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   elasticity_enabled ........... False
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   fp16_auto_cast ............... None
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   fp16_enabled ................. False
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   global_rank .................. 0
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   grad_accum_dtype ............. None
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 32
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   graph_harvesting ............. False
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 1
[2024-03-05 21:49:39,679] [INFO] [config.py:988:print]   load_universal_checkpoint .... False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   loss_scale ................... 1.0
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   memory_breakdown ............. False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   mics_shard_size .............. -1
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   optimizer_name ............... None
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   optimizer_params ............. None
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   pld_enabled .................. False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   pld_params ................... False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   prescale_gradients ........... False
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   scheduler_name ............... None
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   scheduler_params ............. None
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32
[2024-03-05 21:49:39,680] [INFO] [config.py:988:print]   sparse_attention ............. None
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   steps_per_print .............. inf
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   train_batch_size ............. 128
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  1
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   use_node_local_storage ....... False
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   weight_quantization_config ... None
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   world_size ................... 4
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   zero_enabled ................. False
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-05 21:49:39,681] [INFO] [config.py:988:print]   zero_optimization_stage ...... 0
[2024-03-05 21:49:39,681] [INFO] [config.py:974:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 32, 
    "zero_optimization": {
        "stage": 0, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }
}
{'loss': 0.0, 'learning_rate': 2.7027027027027028e-08, 'rewards/chosen': 0.0006356139783747494, 'rewards/rejected': 5.8633071603253484e-05, 'rewards/margins': 0.0005769809358753264, 'kl': 0.0, 'logps/rejected': -33.06391525268555, 'logps/chosen': -39.5013427734375, 'epoch': 0.0}
{'loss': 0.0, 'learning_rate': 5.4054054054054056e-08, 'rewards/chosen': -0.002873100806027651, 'rewards/rejected': 0.00044774310663342476, 'rewards/margins': -0.0033208439126610756, 'kl': 0.0, 'logps/rejected': -39.03995132446289, 'logps/chosen': -33.166160583496094, 'epoch': 0.01}
