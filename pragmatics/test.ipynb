{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def pragmatic_loss(\n",
    "    logprobs: torch.FloatTensor, \n",
    "    max_iter: int = 100,\n",
    "    epsilon: float = 1e-5,\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"Compute the pragmatic loss for a batch of response log probabilities.\n",
    "    \n",
    "    Args:\n",
    "        logprobs: The log probabilities of the responses. Shape: (constitution_batch_size, constitution_batch_size * response_batch_size).\n",
    "        max_iter: The maximum number of iterations for the pragmatic recursion.\n",
    "        epsilon: The convergence threshold for the pragmatic recursion.\n",
    "        \n",
    "    Returns:\n",
    "        pragmatic_loss: The pragmatic loss for the batch of responses. \n",
    "    \"\"\"        \n",
    "    # have constitutions compete for responses\n",
    "    probs = torch.softmax(logprobs, dim=0) \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "\n",
    "        # row normalization\n",
    "        probs = probs / probs.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # check convergence\n",
    "        col_sums = probs.sum(dim=0)\n",
    "\n",
    "        if torch.max(col_sums) - torch.min(col_sums) < epsilon:\n",
    "            break\n",
    "        \n",
    "        # column normalization\n",
    "        probs = probs / probs.sum(dim=0, keepdim=True)\n",
    "\n",
    "    # use probs as class probabilities to compute the loss\n",
    "    loss = F.cross_entropy(logprobs, probs, reduction=\"mean\")\n",
    " \n",
    "    return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6157)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs = torch.FloatTensor([[-1, -30000, -2], [-2, -1, -2], [-2, -2, -1]])\n",
    "\n",
    "a = torch.FloatTensor([0.692, 0.000, 0.3038])\n",
    "b = torch.FloatTensor([-1, -30000, -2])\n",
    "\n",
    "F.cross_entropy(b, a, reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8308)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pragmatic_loss(logprobs, max_iter=100, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9998999999999999"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(torch.Float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "[[0.24241937 0.34887671 0.40870344]\n",
      " [0.3646397  0.32798095 0.30737952]\n",
      " [0.39294093 0.32314234 0.28391704]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sinkhorn_knopp(matrix, tol=1e-5, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Scale a non-negative 3x3 matrix into a doubly stochastic matrix using the Sinkhorn-Knopp algorithm.\n",
    "\n",
    "    Args:\n",
    "        matrix (np.ndarray): A 3x3 non-negative matrix.\n",
    "        tol (float): Tolerance for the convergence criterion.\n",
    "        max_iters (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A doubly stochastic matrix.\n",
    "    \"\"\"\n",
    "    if not isinstance(matrix, np.ndarray):\n",
    "        raise ValueError(\"Input must be a numpy ndarray.\")\n",
    "    if matrix.shape != (3, 3):\n",
    "        raise ValueError(\"Input matrix must be 3x3.\")\n",
    "    \n",
    "    # Initial normalization of the matrix\n",
    "    # matrix = matrix / np.sum(matrix)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        print(_)\n",
    "        # Scale rows to sum to 1\n",
    "        matrix = matrix / matrix.sum(axis=1, keepdims=True)\n",
    "        # Scale columns to sum to 1\n",
    "        matrix = matrix / matrix.sum(axis=0, keepdims=True)\n",
    "        \n",
    "        # Check for convergence\n",
    "        row_sums = matrix.sum(axis=1)\n",
    "        col_sums = matrix.sum(axis=0)\n",
    "        if np.all(np.abs(row_sums - 1) < tol) and np.all(np.abs(col_sums - 1) < tol):\n",
    "            break\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Example usage\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float)\n",
    "doubly_stochastic_matrix = sinkhorn_knopp(matrix)\n",
    "print(doubly_stochastic_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>  [INST] Do you have mayonnaise recipes? [/INST]\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(cache_dir=\"/Users/iphilipp/Documents/research/scai-tuning/pragmatics/conf/model/local_cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "print(tokenizer.batch_decode(encodeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
