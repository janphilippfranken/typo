hydra:
  run:
    dir: outputs

defaults:
  - model: mistral_7b_base
  - data: hh_rlhf_cai

wandb:
  project: scai-tuning
  log_model: checkpoint
  name: sft_hh_cdpo

training:
  n_epochs: 1000
  lr: 1e-6
  train_batch_size: 1
  eval_batch_size: 1
  train_split: 0.9
  checkpoint_dir: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/cdpo_harmless
  loss: "constitutional_dpo"
  eval_steps: 100
  save_every: 100
  max_grad_norm: 10.0
  num_warmup_steps: 150
  gradient_accumulation_steps: 2