hydra:
  run:
    dir: outputs

defaults:
  - model: mistral_tiny_base
  - data: hh_rlhf_cai

wandb:
  project: scai-tuning
  log_model: checkpoint
  name: sft_hh_cdpo

training:
  n_epochs: 1000
  lr: 1e-6
  train_batch_size: 2
  eval_batch_size: 2
  train_split: 0.9
  checkpoint_dir: ""
  loss: "constitutional_dpo"
  eval_steps: 100
  save_every: 100
  max_grad_norm: 1.0
  num_warmup_steps: 2
  gradient_accumulation_steps: 2