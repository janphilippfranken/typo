[2024-02-22 10:51:41,880][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline
[2024-02-22 10:51:41,880][root][INFO] - Max seq length: 2048
[2024-02-22 10:51:41,880][root][INFO] - Devices: 4
[2024-02-22 10:51:41,888][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline
[2024-02-22 10:51:41,888][root][INFO] - Max seq length: 2048
[2024-02-22 10:51:41,888][root][INFO] - Devices: 4
[2024-02-22 10:51:41,900][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline
[2024-02-22 10:51:41,900][root][INFO] - Max seq length: 2048
[2024-02-22 10:51:41,900][root][INFO] - Devices: 4
[2024-02-22 10:51:41,900][root][INFO] - Writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline
[2024-02-22 10:51:41,900][root][INFO] - Max seq length: 2048
[2024-02-22 10:51:41,900][root][INFO] - Devices: 4
[2024-02-22 10:53:25,157][root][INFO] - Dataset({
    features: ['input_ids', 'labels'],
    num_rows: 86370
})
[2024-02-22 10:53:25,277][root][INFO] - Dataset({
    features: ['input_ids', 'labels'],
    num_rows: 86370
})
[2024-02-22 10:53:25,326][root][INFO] - Dataset({
    features: ['input_ids', 'labels'],
    num_rows: 86370
})
[2024-02-22 10:53:26,068][root][INFO] - Dataset({
    features: ['input_ids', 'labels'],
    num_rows: 86370
})
[2024-02-22 10:53:26,083][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 2.0824, 'learning_rate': 3.3333333333333334e-08, 'epoch': 0.01}
{'loss': 2.0795, 'learning_rate': 6.666666666666667e-08, 'epoch': 0.02}
{'loss': 2.0836, 'learning_rate': 1e-07, 'epoch': 0.02}
{'loss': 2.0009, 'learning_rate': 1.3333333333333334e-07, 'epoch': 0.03}
{'loss': 1.9126, 'learning_rate': 1.6666666666666665e-07, 'epoch': 0.04}
{'loss': 1.7784, 'learning_rate': 2e-07, 'epoch': 0.05}
{'loss': 1.7235, 'learning_rate': 2.3333333333333333e-07, 'epoch': 0.05}
{'loss': 1.7162, 'learning_rate': 2.6666666666666667e-07, 'epoch': 0.06}
{'loss': 1.6723, 'learning_rate': 3e-07, 'epoch': 0.07}
{'loss': 1.6468, 'learning_rate': 3.333333333333333e-07, 'epoch': 0.08}
{'loss': 1.6049, 'learning_rate': 3.666666666666666e-07, 'epoch': 0.09}
{'loss': 1.6532, 'learning_rate': 4e-07, 'epoch': 0.09}
{'loss': 1.6455, 'learning_rate': 4.3333333333333335e-07, 'epoch': 0.1}
{'loss': 1.6289, 'learning_rate': 4.6666666666666666e-07, 'epoch': 0.11}
{'loss': 1.6162, 'learning_rate': 5e-07, 'epoch': 0.12}
{'loss': 1.6437, 'learning_rate': 5.333333333333333e-07, 'epoch': 0.12}
{'loss': 1.6042, 'learning_rate': 5.666666666666666e-07, 'epoch': 0.13}
{'loss': 1.6098, 'learning_rate': 6e-07, 'epoch': 0.14}
{'loss': 1.6158, 'learning_rate': 6.333333333333332e-07, 'epoch': 0.15}
{'loss': 1.6377, 'learning_rate': 6.666666666666666e-07, 'epoch': 0.16}
{'loss': 1.6082, 'learning_rate': 7e-07, 'epoch': 0.16}
{'loss': 1.5932, 'learning_rate': 7.333333333333332e-07, 'epoch': 0.17}
{'loss': 1.6239, 'learning_rate': 7.666666666666667e-07, 'epoch': 0.18}
{'loss': 1.6006, 'learning_rate': 8e-07, 'epoch': 0.19}
{'loss': 1.6246, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.19}
{'loss': 1.6195, 'learning_rate': 8.666666666666667e-07, 'epoch': 0.2}
{'loss': 1.653, 'learning_rate': 9e-07, 'epoch': 0.21}
{'loss': 1.6375, 'learning_rate': 9.333333333333333e-07, 'epoch': 0.22}
{'loss': 1.6178, 'learning_rate': 9.666666666666666e-07, 'epoch': 0.23}
{'loss': 1.6078, 'learning_rate': 1e-06, 'epoch': 0.23}
{'loss': 1.6053, 'learning_rate': 9.89816700610998e-07, 'epoch': 0.24}
{'loss': 1.6237, 'learning_rate': 9.79633401221996e-07, 'epoch': 0.25}
{'loss': 1.6009, 'learning_rate': 9.694501018329937e-07, 'epoch': 0.26}
{'loss': 1.5886, 'learning_rate': 9.592668024439917e-07, 'epoch': 0.27}
{'loss': 1.6071, 'learning_rate': 9.490835030549897e-07, 'epoch': 0.27}
{'loss': 1.598, 'learning_rate': 9.389002036659877e-07, 'epoch': 0.28}
{'loss': 1.6044, 'learning_rate': 9.287169042769857e-07, 'epoch': 0.29}
{'loss': 1.606, 'learning_rate': 9.185336048879837e-07, 'epoch': 0.3}
{'loss': 1.6059, 'learning_rate': 9.083503054989816e-07, 'epoch': 0.3}
{'loss': 1.621, 'learning_rate': 8.981670061099796e-07, 'epoch': 0.31}
{'loss': 1.5886, 'learning_rate': 8.879837067209776e-07, 'epoch': 0.32}
{'loss': 1.6132, 'learning_rate': 8.778004073319755e-07, 'epoch': 0.33}
{'loss': 1.5781, 'learning_rate': 8.676171079429735e-07, 'epoch': 0.34}
{'loss': 1.5969, 'learning_rate': 8.574338085539714e-07, 'epoch': 0.34}
{'loss': 1.5936, 'learning_rate': 8.472505091649695e-07, 'epoch': 0.35}
{'loss': 1.6364, 'learning_rate': 8.370672097759674e-07, 'epoch': 0.36}
{'loss': 1.5899, 'learning_rate': 8.268839103869654e-07, 'epoch': 0.37}
{'loss': 1.595, 'learning_rate': 8.167006109979633e-07, 'epoch': 0.37}
{'loss': 1.5965, 'learning_rate': 8.065173116089614e-07, 'epoch': 0.38}
{'loss': 1.5878, 'learning_rate': 7.963340122199592e-07, 'epoch': 0.39}
{'loss': 1.5742, 'learning_rate': 7.861507128309572e-07, 'epoch': 0.4}
{'loss': 1.5879, 'learning_rate': 7.759674134419551e-07, 'epoch': 0.41}
{'loss': 1.5615, 'learning_rate': 7.65784114052953e-07, 'epoch': 0.41}
{'loss': 1.582, 'learning_rate': 7.556008146639511e-07, 'epoch': 0.42}
{'loss': 1.5607, 'learning_rate': 7.45417515274949e-07, 'epoch': 0.43}
{'loss': 1.5608, 'learning_rate': 7.35234215885947e-07, 'epoch': 0.44}
{'loss': 1.5566, 'learning_rate': 7.250509164969449e-07, 'epoch': 0.44}
{'loss': 1.5986, 'learning_rate': 7.14867617107943e-07, 'epoch': 0.45}
{'loss': 1.5826, 'learning_rate': 7.046843177189409e-07, 'epoch': 0.46}
{'loss': 1.57, 'learning_rate': 6.945010183299389e-07, 'epoch': 0.47}
{'loss': 1.5679, 'learning_rate': 6.843177189409368e-07, 'epoch': 0.48}
{'loss': 1.5851, 'learning_rate': 6.741344195519349e-07, 'epoch': 0.48}
{'loss': 1.5864, 'learning_rate': 6.639511201629328e-07, 'epoch': 0.49}
{'loss': 1.5766, 'learning_rate': 6.537678207739308e-07, 'epoch': 0.5}
{'loss': 1.5868, 'learning_rate': 6.435845213849287e-07, 'epoch': 0.51}
{'loss': 1.5674, 'learning_rate': 6.334012219959267e-07, 'epoch': 0.51}
{'loss': 1.5778, 'learning_rate': 6.232179226069246e-07, 'epoch': 0.52}
{'loss': 1.5497, 'learning_rate': 6.130346232179225e-07, 'epoch': 0.53}
{'loss': 1.5602, 'learning_rate': 6.028513238289205e-07, 'epoch': 0.54}
{'loss': 1.5149, 'learning_rate': 5.926680244399185e-07, 'epoch': 0.55}
{'loss': 1.5591, 'learning_rate': 5.824847250509165e-07, 'epoch': 0.55}
{'loss': 1.5776, 'learning_rate': 5.723014256619144e-07, 'epoch': 0.56}
{'loss': 1.602, 'learning_rate': 5.621181262729124e-07, 'epoch': 0.57}
{'loss': 1.5651, 'learning_rate': 5.519348268839103e-07, 'epoch': 0.58}
{'loss': 1.6022, 'learning_rate': 5.417515274949084e-07, 'epoch': 0.58}
{'loss': 1.5437, 'learning_rate': 5.315682281059063e-07, 'epoch': 0.59}
{'loss': 1.555, 'learning_rate': 5.213849287169042e-07, 'epoch': 0.6}
{'loss': 1.5746, 'learning_rate': 5.112016293279022e-07, 'epoch': 0.61}
{'loss': 1.5464, 'learning_rate': 5.010183299389002e-07, 'epoch': 0.62}
{'loss': 1.5275, 'learning_rate': 4.908350305498982e-07, 'epoch': 0.62}
{'loss': 1.5671, 'learning_rate': 4.806517311608962e-07, 'epoch': 0.63}
{'loss': 1.5343, 'learning_rate': 4.7046843177189406e-07, 'epoch': 0.64}
{'loss': 1.5504, 'learning_rate': 4.60285132382892e-07, 'epoch': 0.65}
{'loss': 1.5185, 'learning_rate': 4.5010183299389e-07, 'epoch': 0.66}
{'loss': 1.5432, 'learning_rate': 4.3991853360488794e-07, 'epoch': 0.66}
{'loss': 1.536, 'learning_rate': 4.2973523421588594e-07, 'epoch': 0.67}
{'loss': 1.539, 'learning_rate': 4.195519348268839e-07, 'epoch': 0.68}
{'loss': 1.5411, 'learning_rate': 4.093686354378819e-07, 'epoch': 0.69}
{'loss': 1.5153, 'learning_rate': 3.991853360488798e-07, 'epoch': 0.69}
{'loss': 1.561, 'learning_rate': 3.890020366598778e-07, 'epoch': 0.7}
{'loss': 1.5689, 'learning_rate': 3.7881873727087576e-07, 'epoch': 0.71}
{'loss': 1.5433, 'learning_rate': 3.6863543788187376e-07, 'epoch': 0.72}
{'loss': 1.5479, 'learning_rate': 3.5845213849287165e-07, 'epoch': 0.73}
{'loss': 1.5667, 'learning_rate': 3.482688391038696e-07, 'epoch': 0.73}
{'loss': 1.5652, 'learning_rate': 3.380855397148676e-07, 'epoch': 0.74}
{'loss': 1.5569, 'learning_rate': 3.2790224032586553e-07, 'epoch': 0.75}
{'loss': 1.597, 'learning_rate': 3.177189409368635e-07, 'epoch': 0.76}
{'loss': 1.515, 'learning_rate': 3.0753564154786147e-07, 'epoch': 0.76}
{'loss': 1.5281, 'learning_rate': 2.9735234215885947e-07, 'epoch': 0.77}
{'loss': 1.5232, 'learning_rate': 2.871690427698574e-07, 'epoch': 0.78}
{'loss': 1.5712, 'learning_rate': 2.769857433808554e-07, 'epoch': 0.79}
{'loss': 1.5732, 'learning_rate': 2.6680244399185335e-07, 'epoch': 0.8}
{'loss': 1.5345, 'learning_rate': 2.5661914460285134e-07, 'epoch': 0.8}
{'loss': 1.5112, 'learning_rate': 2.464358452138493e-07, 'epoch': 0.81}
{'loss': 1.5153, 'learning_rate': 2.3625254582484723e-07, 'epoch': 0.82}
{'loss': 1.5267, 'learning_rate': 2.260692464358452e-07, 'epoch': 0.83}
{'loss': 1.5319, 'learning_rate': 2.1588594704684317e-07, 'epoch': 0.83}
{'loss': 1.5336, 'learning_rate': 2.057026476578411e-07, 'epoch': 0.84}
{'loss': 1.5587, 'learning_rate': 1.9551934826883908e-07, 'epoch': 0.85}
{'loss': 1.555, 'learning_rate': 1.8533604887983705e-07, 'epoch': 0.86}
{'loss': 1.5372, 'learning_rate': 1.7515274949083502e-07, 'epoch': 0.87}
{'loss': 1.5237, 'learning_rate': 1.64969450101833e-07, 'epoch': 0.87}
{'loss': 1.5394, 'learning_rate': 1.5478615071283094e-07, 'epoch': 0.88}
{'loss': 1.5334, 'learning_rate': 1.446028513238289e-07, 'epoch': 0.89}
{'loss': 1.5256, 'learning_rate': 1.3441955193482687e-07, 'epoch': 0.9}
{'loss': 1.5289, 'learning_rate': 1.2423625254582484e-07, 'epoch': 0.9}
{'loss': 1.5052, 'learning_rate': 1.1405295315682281e-07, 'epoch': 0.91}
{'loss': 1.5267, 'learning_rate': 1.0386965376782077e-07, 'epoch': 0.92}
{'loss': 1.5168, 'learning_rate': 9.368635437881874e-08, 'epoch': 0.93}
{'loss': 1.5268, 'learning_rate': 8.35030549898167e-08, 'epoch': 0.94}
{'loss': 1.5075, 'learning_rate': 7.331975560081465e-08, 'epoch': 0.94}
{'loss': 1.5631, 'learning_rate': 6.313645621181262e-08, 'epoch': 0.95}
{'loss': 1.527, 'learning_rate': 5.295315682281059e-08, 'epoch': 0.96}
{'loss': 1.5494, 'learning_rate': 4.276985743380855e-08, 'epoch': 0.97}
{'loss': 1.5252, 'learning_rate': 3.258655804480651e-08, 'epoch': 0.97}
{'loss': 1.5719, 'learning_rate': 2.2403258655804478e-08, 'epoch': 0.98}
{'loss': 1.5295, 'learning_rate': 1.2219959266802443e-08, 'epoch': 0.99}
{'loss': 1.5424, 'learning_rate': 2.036659877800407e-09, 'epoch': 1.0}
{'eval_loss': 1.5460553169250488, 'eval_runtime': 114.4945, 'eval_samples_per_second': 37.722, 'eval_steps_per_second': 4.716, 'epoch': 1.0}
[2024-02-22 12:41:52,868][accelerate.utils.fsdp_utils][INFO] - Saving model to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline/tmp-checkpoint-641/pytorch_model_fsdp.bin
[2024-02-22 12:42:18,522][accelerate.utils.fsdp_utils][INFO] - Model saved to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline/tmp-checkpoint-641/pytorch_model_fsdp.bin
[2024-02-22 12:42:56,100][accelerate.utils.fsdp_utils][INFO] - Saving Optimizer state to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline/tmp-checkpoint-641/optimizer.bin
[2024-02-22 12:43:47,213][accelerate.utils.fsdp_utils][INFO] - Optimizer state saved in /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints$ cd sft-baseline/tmp-checkpoint-641/optimizer.bin
{'train_runtime': 6592.1235, 'train_samples_per_second': 12.447, 'train_steps_per_second': 0.097, 'train_loss': 1.5945891909963814, 'epoch': 1.0}
NAME
    train_sft_baseline.py

SYNOPSIS
    train_sft_baseline.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     List
       A generic version of list.

     Optional
       Optional type.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     string
       A collection of string constants.

     transformers

     copy
       Generic (shallow and deep) copying operations.

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     concatenate_datasets
       Converts a list of [`Dataset`] with the same schema into a single [`Dataset`].

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     SupervisedPragmaticDataCollator
       Collate examples.

     remove_final_answer
       Remove final assistant answer which is our inference target.

     get_first_question
       Get first human request.

     format_responses
       Format sampled responses from model.

     format_example
       Formats example into a dictionary with keys for each constitution and response.

     format_model_written_example
       Formats example into a dictionary with keys for each constitution and response.

     tokenize_func

     tokenize_func_no_label

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     sft_preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     BOS_TOKEN

     EOS_TOKEN

     IGNORE_INDEX

     PROMPT
NAME
    train_sft_baseline.py

SYNOPSIS
    train_sft_baseline.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     List
       A generic version of list.

     Optional
       Optional type.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     string
       A collection of string constants.

     transformers

     copy
       Generic (shallow and deep) copying operations.

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     concatenate_datasets
       Converts a list of [`Dataset`] with the same schema into a single [`Dataset`].

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     SupervisedPragmaticDataCollator
       Collate examples.

     remove_final_answer
       Remove final assistant answer which is our inference target.

     get_first_question
       Get first human request.

     format_responses
       Format sampled responses from model.

     format_example
       Formats example into a dictionary with keys for each constitution and response.

     format_model_written_example
       Formats example into a dictionary with keys for each constitution and response.

     tokenize_func

     tokenize_func_no_label

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     sft_preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     BOS_TOKEN

     EOS_TOKEN

     IGNORE_INDEX

     PROMPT
NAME
    train_sft_baseline.py

SYNOPSIS
    train_sft_baseline.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     List
       A generic version of list.

     Optional
       Optional type.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     string
       A collection of string constants.

     transformers

     copy
       Generic (shallow and deep) copying operations.

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     concatenate_datasets
       Converts a list of [`Dataset`] with the same schema into a single [`Dataset`].

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     SupervisedPragmaticDataCollator
       Collate examples.

     remove_final_answer
       Remove final assistant answer which is our inference target.

     get_first_question
       Get first human request.

     format_responses
       Format sampled responses from model.

     format_example
       Formats example into a dictionary with keys for each constitution and response.

     format_model_written_example
       Formats example into a dictionary with keys for each constitution and response.

     tokenize_func

     tokenize_func_no_label

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     sft_preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     BOS_TOKEN

     EOS_TOKEN

     IGNORE_INDEX

     PROMPT
NAME
    train_sft_baseline.py

SYNOPSIS
    train_sft_baseline.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     List
       A generic version of list.

     Optional
       Optional type.

     Tuple
       Tuple type; Tuple[X, Y] is the cross-product type of X and Y.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     string
       A collection of string constants.

     transformers

     copy
       Generic (shallow and deep) copying operations.

COMMANDS
    COMMAND is one of the following:

     tqdm
       Decorate an iterable object, returning an iterator which acts exactly like the original iterable, but prints a dynamically updating progressbar every time a value is requested.

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     concatenate_datasets
       Converts a list of [`Dataset`] with the same schema into a single [`Dataset`].

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for 🤗 Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     SupervisedPragmaticDataCollator
       Collate examples.

     remove_final_answer
       Remove final assistant answer which is our inference target.

     get_first_question
       Get first human request.

     format_responses
       Format sampled responses from model.

     format_example
       Formats example into a dictionary with keys for each constitution and response.

     format_model_written_example
       Formats example into a dictionary with keys for each constitution and response.

     tokenize_func

     tokenize_func_no_label

     DataCollatorForSupervisedDataset
       Collate examples for supervised fine-tuning. Copy-pasted from https://github.com/tatsu-lab/stanford_alpaca/blob/main/train.py.

     sft_preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     BOS_TOKEN

     EOS_TOKEN

     IGNORE_INDEX

     PROMPT
