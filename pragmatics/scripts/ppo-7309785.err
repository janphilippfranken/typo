[2024-02-26 16:55:46,198] torch.distributed.run: [WARNING] 
[2024-02-26 16:55:46,198] torch.distributed.run: [WARNING] *****************************************
[2024-02-26 16:55:46,198] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-26 16:55:46,198] torch.distributed.run: [WARNING] *****************************************
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2024-02-26 16:55:51,210] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3475834) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-26_16:55:51
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3475835)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-26_16:55:51
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3475836)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-26_16:55:51
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3475837)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-26_16:55:51
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3475834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2024-02-26 16:55:52,457] torch.distributed.run: [WARNING] 
[2024-02-26 16:55:52,457] torch.distributed.run: [WARNING] *****************************************
[2024-02-26 16:55:52,457] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-26 16:55:52,457] torch.distributed.run: [WARNING] *****************************************
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
mismatched input '<EOF>' expecting {':', BRACKET_OPEN, BRACE_OPEN, FLOAT, INT, BOOL, NULL, UNQUOTED_CHAR, ID, ESC, WS, QUOTED_VALUE, INTERPOLATION}
See https://hydra.cc/docs/1.2/advanced/override_grammar/basic for details

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
[2024-02-26 16:55:57,469] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3475898) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-26_16:55:57
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3475899)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-26_16:55:57
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3475900)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-26_16:55:57
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3475901)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-26_16:55:57
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3475898)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2024-02-26 16:55:58,700] torch.distributed.run: [WARNING] 
[2024-02-26 16:55:58,700] torch.distributed.run: [WARNING] *****************************************
[2024-02-26 16:55:58,700] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-26 16:55:58,700] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240226_165603-cyqyivpk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ⭐️ View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: 🚀 View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/cyqyivpk
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240226_165603-2j1k98si
wandb: Run `wandb offline` to turn off syncing.
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240226_165603-kk3m32ve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ⭐️ View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: 🚀 View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/2j1k98si
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240226_165603-1gdu2hgr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ⭐️ View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: 🚀 View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/kk3m32ve
wandb: Syncing run beta-0.5-per-token-kl
wandb: ⭐️ View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: 🚀 View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/1gdu2hgr
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]
Running epoch: 0: 0it [00:00, ?it/s]
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels']
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels']
Running epoch: 0: 0it [00:00, ?it/s]
Running epoch: 0: 0it [00:00, ?it/s]
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels']
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels']
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 535, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 535, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
bdb.BdbQuit
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 535, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 535, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 467, in _run_batch
    if self.config.ppo.kl == 'average_kl':
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
bdb.BdbQuit
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
bdb.BdbQuit

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: 🚀 View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/2j1k98si
wandb: ️⚡ View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0Mjc0NjEyOQ==/version_details/v3
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240226_165603-2j1k98si/logs
wandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: 🚀 View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/cyqyivpk
wandb: ️⚡ View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0Mjc0NjEyOQ==/version_details/v3
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240226_165603-cyqyivpk/logs
wandb: 🚀 View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/1gdu2hgr
wandb: ️⚡ View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0Mjc0NjEyOQ==/version_details/v3
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240226_165603-1gdu2hgr/logs
wandb: 🚀 View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/kk3m32ve
wandb: ️⚡ View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0Mjc0NjEyOQ==/version_details/v3
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240226_165603-kk3m32ve/logs
[2024-02-26 16:57:08,777] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3475970) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-26_16:57:08
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3475971)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-26_16:57:08
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3475972)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-26_16:57:08
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3475973)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-26_16:57:08
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3475970)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
