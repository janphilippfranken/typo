[2024-02-18 23:33:46,909][root][INFO] - beta: 0.01
[2024-02-18 23:33:46,910][root][INFO] - temperature: 2
[2024-02-18 23:33:46,910][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta0.01-temp2
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 20000 training examples...
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta0.01-temp2 after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta0.01-temp2 after each epoch.
train dataset has 19000 examples.
eval dataset has 1000 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta0.01-temp2 after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta0.01-temp2 after each epoch.
Epoch 0, Step 15: loss/train = 0.04626826196908951, logprobs/train = tensor([[-1.1931, -1.6183],
        [-1.1447, -1.4967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.07751002907752991, logprobs/train = tensor([[-1.5886, -2.9638],
        [-1.6163, -2.8223]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.04568582400679588, logprobs/train = tensor([[-2.0353, -1.7214],
        [-2.0134, -1.6673]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7350830603390932e-05
Epoch 0, Step 63: loss/train = 0.04483630135655403, logprobs/train = tensor([[-2.1683, -1.9174],
        [-2.0727, -1.8068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.1530897356569767e-05
Epoch 0, Step 79: loss/train = 0.05118946731090546, logprobs/train = tensor([[-1.7715, -2.4035],
        [-1.7817, -2.1647]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00025560412905178964
Epoch 0, Step 95: loss/train = 0.04725032299757004, logprobs/train = tensor([[-1.5178, -1.6872],
        [-1.5181, -1.6588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.846372005180456e-06
Epoch 0, Step 111: loss/train = 0.05111902579665184, logprobs/train = tensor([[-1.5426, -2.3391],
        [-1.5058, -2.2892]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7227113858098164e-05
