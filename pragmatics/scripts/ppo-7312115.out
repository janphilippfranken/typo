[2024-02-27 13:18:08,756][root][INFO] - beta: 0.5
[2024-02-27 13:18:08,756][root][INFO] - loss with_labels
[2024-02-27 13:18:08,756][root][INFO] - max_iter: 0
[2024-02-27 13:18:08,756][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
RAW KL tensor(0., device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 0: train/loss = 0.6982459425926208, train/raw-loss = 0.6982459425926208, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
RAW KL tensor(0., device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 1: train/loss = 0.6927576065063477, train/raw-loss = 0.6927576065063477, train/logprobs = tensor([[-1.2483, -1.6199],
        [-1.2440, -1.6139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
RAW KL tensor(0.0004, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 2: train/loss = 0.6867749691009521, train/raw-loss = 0.6865226030349731, train/logprobs = tensor([[-1.0798, -2.0936],
        [-1.1118, -2.0987]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005047926679253578
RAW KL tensor(0.0006, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 3: train/loss = 0.6806550025939941, train/raw-loss = 0.6801009178161621, train/logprobs = tensor([[-0.5917, -1.4458],
        [-0.5922, -1.3936]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011081324191763997
RAW KL tensor(0.0029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 4: train/loss = 0.6733642816543579, train/raw-loss = 0.671377420425415, train/logprobs = tensor([[-0.9616, -2.2353],
        [-0.9885, -2.1739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003973724320530891
RAW KL tensor(0.0097, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 5: train/loss = 0.6789969205856323, train/raw-loss = 0.6742593050003052, train/logprobs = tensor([[-1.2865, -1.8750],
        [-1.2495, -1.7615]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00947532244026661
RAW KL tensor(0.0193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 6: train/loss = 0.6819343566894531, train/raw-loss = 0.6741829514503479, train/logprobs = tensor([[-1.1059, -1.6596],
        [-1.0288, -1.5054]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01550283282995224
RAW KL tensor(0.0230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 7: train/loss = 0.662815511226654, train/raw-loss = 0.6542484760284424, train/logprobs = tensor([[-1.1352, -1.8408],
        [-1.1296, -1.6716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01713418960571289
RAW KL tensor(0.0079, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 8: train/loss = 0.6251566410064697, train/raw-loss = 0.6167339086532593, train/logprobs = tensor([[-0.8264, -2.2231],
        [-0.7531, -1.8227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0168454609811306
RAW KL tensor(0.0068, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 9: train/loss = 0.6152253150939941, train/raw-loss = 0.6079558730125427, train/logprobs = tensor([[-1.1781, -2.0871],
        [-1.1187, -1.6673]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01453883945941925
RAW KL tensor(0.0371, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 10: train/loss = 0.5061061978340149, train/raw-loss = 0.4911901354789734, train/logprobs = tensor([[-1.2879, -2.7789],
        [-1.3097, -1.8431]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029832124710083008
RAW KL tensor(0.0736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 11: train/loss = 0.5412055850028992, train/raw-loss = 0.5228908061981201, train/logprobs = tensor([[-0.8388, -2.1091],
        [-0.7306, -1.1404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0366295762360096
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 12: train/loss = 0.539138913154602, train/raw-loss = 0.5209347605705261, train/logprobs = tensor([[-0.9444, -2.6996],
        [-0.9020, -1.6753]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036408357322216034
RAW KL tensor(0.0416, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 13: train/loss = 0.543144166469574, train/raw-loss = 0.5281140804290771, train/logprobs = tensor([[-0.9129, -2.8765],
        [-0.9014, -1.9799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030060160905122757
RAW KL tensor(0.0759, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 14: train/loss = 0.4833163321018219, train/raw-loss = 0.44982466101646423, train/logprobs = tensor([[-1.4247, -2.9446],
        [-1.0372, -1.1323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06698335707187653
RAW KL tensor(0.0999, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 15: train/loss = 0.45644429326057434, train/raw-loss = 0.41013383865356445, train/logprobs = tensor([[-1.6349, -3.4317],
        [-1.0149, -0.8526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09262092411518097
RAW KL tensor(0.2108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 16: train/loss = 0.36337810754776, train/raw-loss = 0.2833006978034973, train/logprobs = tensor([[-1.3601, -4.4730],
        [-1.0437, -0.9041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1601548194885254
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 17: train/loss = 0.5118018984794617, train/raw-loss = 0.4842338562011719, train/logprobs = tensor([[-1.5107, -2.8462],
        [-1.1632, -1.0382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055136073380708694
RAW KL tensor(0.1415, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 18: train/loss = 0.36110636591911316, train/raw-loss = 0.3166523873806, train/logprobs = tensor([[-1.8020, -4.2511],
        [-1.4027, -1.2492]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08890798687934875
RAW KL tensor(0.0681, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 19: train/loss = 0.3910283148288727, train/raw-loss = 0.36025089025497437, train/logprobs = tensor([[-1.1624, -3.8398],
        [-0.6675, -1.0526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061554815620183945
RAW KL tensor(0.0956, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 20: train/loss = 0.48141634464263916, train/raw-loss = 0.44474005699157715, train/logprobs = tensor([[-1.5190, -4.7036],
        [-1.0581, -1.1612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07335257530212402
RAW KL tensor(0.0557, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 21: train/loss = 0.3756856322288513, train/raw-loss = 0.32464706897735596, train/logprobs = tensor([[-2.0980, -4.6057],
        [-1.5301, -1.4072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10207708925008774
RAW KL tensor(0.2402, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 22: train/loss = 0.5871844291687012, train/raw-loss = 0.5088125467300415, train/logprobs = tensor([[-2.5487, -4.9078],
        [-1.5732, -1.2914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15674379467964172
RAW KL tensor(0.1180, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 23: train/loss = 0.4153759479522705, train/raw-loss = 0.35090309381484985, train/logprobs = tensor([[-3.1119, -8.3833],
        [-1.5828, -1.5784]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1289457082748413
RAW KL tensor(0.1099, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 24: train/loss = 0.46644914150238037, train/raw-loss = 0.3670656979084015, train/logprobs = tensor([[ -2.4980, -10.8592],
        [ -1.1286,  -1.9113]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19876688718795776
RAW KL tensor(0.1353, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 25: train/loss = 0.4237859845161438, train/raw-loss = 0.37815219163894653, train/logprobs = tensor([[-1.5300, -5.6942],
        [-1.0532, -1.8515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09126761555671692
RAW KL tensor(0.0926, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 26: train/loss = 0.3398761451244354, train/raw-loss = 0.2785940170288086, train/logprobs = tensor([[-1.0955, -9.0052],
        [-0.7386, -2.7282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12256424129009247
RAW KL tensor(0.0391, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 27: train/loss = 0.37770408391952515, train/raw-loss = 0.33446812629699707, train/logprobs = tensor([[-2.0339, -6.8898],
        [-1.9142, -2.0235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08647194504737854
RAW KL tensor(0.1128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 28: train/loss = 0.3797855079174042, train/raw-loss = 0.31991657614707947, train/logprobs = tensor([[-1.5292, -4.4196],
        [-1.5048, -1.7659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11973781138658524
RAW KL tensor(0.0144, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 29: train/loss = 0.3604225516319275, train/raw-loss = 0.33093783259391785, train/logprobs = tensor([[-0.8324, -4.4573],
        [-0.7427, -2.1412]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05896936357021332
RAW KL tensor(0.0509, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 219])
Epoch 0, Step 30: train/loss = 0.6386668682098389, train/raw-loss = 0.5929261445999146, train/logprobs = tensor([[-1.8847, -2.7768],
        [-1.2584, -1.6175]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09148134291172028
RAW KL tensor(0.2298, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 31: train/loss = 0.40017491579055786, train/raw-loss = 0.30403876304626465, train/logprobs = tensor([[-1.6507, -7.7540],
        [-3.0685, -3.1327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19227227568626404
RAW KL tensor(0.1280, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 32: train/loss = 0.2653267979621887, train/raw-loss = 0.17649930715560913, train/logprobs = tensor([[-1.6965, -7.5047],
        [-2.0858, -1.8275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17765499651432037
RAW KL tensor(0.1124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 33: train/loss = 0.2939162850379944, train/raw-loss = 0.22461840510368347, train/logprobs = tensor([[-2.0663, -6.9920],
        [-2.0787, -2.8966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1385958045721054
RAW KL tensor(0.0735, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 34: train/loss = 0.38064485788345337, train/raw-loss = 0.32149311900138855, train/logprobs = tensor([[-2.4190, -4.5627],
        [-2.3413, -1.9046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11830338835716248
RAW KL tensor(0.1234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 35: train/loss = 0.26090767979621887, train/raw-loss = 0.1769016832113266, train/logprobs = tensor([[-2.0179, -7.3908],
        [-2.7825, -3.0611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16801197826862335
RAW KL tensor(0.3641, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 36: train/loss = 0.4369868338108063, train/raw-loss = 0.33319905400276184, train/logprobs = tensor([[-2.6265, -7.1621],
        [-4.3530, -3.5811]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20757561922073364
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 235])
Epoch 0, Step 37: train/loss = 0.44817686080932617, train/raw-loss = 0.37549835443496704, train/logprobs = tensor([[-1.3968, -8.0583],
        [-1.1541, -2.1157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14535711705684662
RAW KL tensor(0.0917, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 292])
Epoch 0, Step 38: train/loss = 0.3138701617717743, train/raw-loss = 0.2586064338684082, train/logprobs = tensor([[-1.3631, -5.7412],
        [-2.1126, -2.2252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11052744090557098
RAW KL tensor(0.1553, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 39: train/loss = 0.36439400911331177, train/raw-loss = 0.30551281571388245, train/logprobs = tensor([[-1.4403, -5.6563],
        [-1.5825, -1.2832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11776237189769745
RAW KL tensor(0.1146, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 40: train/loss = 0.39831942319869995, train/raw-loss = 0.3419305682182312, train/logprobs = tensor([[-1.9672, -3.4402],
        [-2.7502, -2.0788]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11277763545513153
RAW KL tensor(0.1621, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 329])
Epoch 0, Step 41: train/loss = 0.3083685040473938, train/raw-loss = 0.17679600417613983, train/logprobs = tensor([[-2.7926, -8.5571],
        [-5.2624, -3.4624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2631450295448303
RAW KL tensor(0.2881, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 42: train/loss = 0.36166509985923767, train/raw-loss = 0.25691622495651245, train/logprobs = tensor([[-1.1867, -7.7491],
        [-4.0310, -3.3783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20949772000312805
RAW KL tensor(0.1029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 43: train/loss = 0.3755056858062744, train/raw-loss = 0.22480589151382446, train/logprobs = tensor([[-3.1508, -7.7060],
        [-6.4150, -6.7297]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3013995587825775
RAW KL tensor(0.2626, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 44: train/loss = 0.3515103757381439, train/raw-loss = 0.20661886036396027, train/logprobs = tensor([[-2.1762, -4.6259],
        [-7.2340, -3.8937]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2897830009460449
RAW KL tensor(0.2265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 45: train/loss = 0.3903799057006836, train/raw-loss = 0.29066023230552673, train/logprobs = tensor([[-2.4014, -5.4365],
        [-5.6922, -2.3408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19943928718566895
RAW KL tensor(0.2571, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 46: train/loss = 0.16212910413742065, train/raw-loss = 0.037913814187049866, train/logprobs = tensor([[-2.4922, -9.3788],
        [-6.2911, -4.0686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24843057990074158
RAW KL tensor(0.1454, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 47: train/loss = 0.29141971468925476, train/raw-loss = 0.20585592091083527, train/logprobs = tensor([[-1.2892, -6.7173],
        [-3.1032, -2.7059]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.171127587556839
RAW KL tensor(0.1542, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 48: train/loss = 0.4298548102378845, train/raw-loss = 0.34777140617370605, train/logprobs = tensor([[-3.8348, -4.2740],
        [-5.4096, -2.6673]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16416677832603455
RAW KL tensor(0.2752, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 49: train/loss = 0.23614974319934845, train/raw-loss = 0.13584977388381958, train/logprobs = tensor([[-1.3943, -7.3848],
        [-3.3179, -3.2191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20059993863105774
RAW KL tensor(0.1364, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 50: train/loss = 0.2907094359397888, train/raw-loss = 0.2039775252342224, train/logprobs = tensor([[-2.1576, -5.0278],
        [-4.5785, -2.3710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1734638661146164
RAW KL tensor(0.1030, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 51: train/loss = 0.31083810329437256, train/raw-loss = 0.24541179835796356, train/logprobs = tensor([[-3.1011, -8.0207],
        [-2.8267, -2.5192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13085265457630157
RAW KL tensor(0.1574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 52: train/loss = 0.619257926940918, train/raw-loss = 0.5536191463470459, train/logprobs = tensor([[-1.4931, -1.4983],
        [-3.2794, -2.4383]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13127771019935608
RAW KL tensor(0.0400, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 53: train/loss = 0.47939831018447876, train/raw-loss = 0.42921435832977295, train/logprobs = tensor([[-1.3694, -4.5588],
        [-1.9414, -1.5541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1003679558634758
RAW KL tensor(0.2314, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 54: train/loss = 0.1706494837999344, train/raw-loss = 0.08791983127593994, train/logprobs = tensor([[-1.8265, -6.0737],
        [-4.4643, -2.0004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1654593050479889
RAW KL tensor(0.1811, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 55: train/loss = 0.2335263192653656, train/raw-loss = 0.1484706699848175, train/logprobs = tensor([[-3.1209, -5.7901],
        [-4.9815, -1.9403]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1701112985610962
RAW KL tensor(0.2380, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 56: train/loss = 0.2764072120189667, train/raw-loss = 0.16431407630443573, train/logprobs = tensor([[-1.7327, -5.4949],
        [-4.5567, -3.3969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22418628633022308
RAW KL tensor(0.2212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 57: train/loss = 0.3091680407524109, train/raw-loss = 0.22687765955924988, train/logprobs = tensor([[-1.6044, -5.9789],
        [-5.1880, -2.7418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16458070278167725
RAW KL tensor(0.1079, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 335])
Epoch 0, Step 58: train/loss = 0.25603222846984863, train/raw-loss = 0.17234985530376434, train/logprobs = tensor([[-1.2461, -6.7725],
        [-3.5414, -2.6026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16736476123332977
RAW KL tensor(0.0837, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 364])
Epoch 0, Step 59: train/loss = 0.46918705105781555, train/raw-loss = 0.38971906900405884, train/logprobs = tensor([[-1.8869, -3.8823],
        [-4.6146, -4.3363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15893591940402985
RAW KL tensor(0.1661, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 60: train/loss = 0.26751407980918884, train/raw-loss = 0.18286649882793427, train/logprobs = tensor([[-2.7110, -6.1147],
        [-6.0425, -2.2851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16929516196250916
RAW KL tensor(0.1752, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 61: train/loss = 0.25118717551231384, train/raw-loss = 0.1818065196275711, train/logprobs = tensor([[-0.8314, -8.1390],
        [-2.6869, -2.6467]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13876131176948547
RAW KL tensor(0.1593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 62: train/loss = 0.36860811710357666, train/raw-loss = 0.26822468638420105, train/logprobs = tensor([[-2.4107, -5.8315],
        [-5.6229, -4.1794]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2007669061422348
RAW KL tensor(0.0203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 63: train/loss = 0.2705320119857788, train/raw-loss = 0.1922251582145691, train/logprobs = tensor([[-1.9385, -5.0338],
        [-6.4046, -2.2463]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15661370754241943
RAW KL tensor(0.1339, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 64: train/loss = 0.1720573455095291, train/raw-loss = 0.08419687300920486, train/logprobs = tensor([[-1.8463, -6.6290],
        [-6.3152, -2.7982]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1757209599018097
RAW KL tensor(0.1846, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 65: train/loss = 0.22162321209907532, train/raw-loss = 0.15763193368911743, train/logprobs = tensor([[-3.0792, -9.8763],
        [-3.5816, -1.6962]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12798252701759338
RAW KL tensor(0.0806, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 66: train/loss = 0.5874502062797546, train/raw-loss = 0.5281585454940796, train/logprobs = tensor([[ -3.2423, -11.5813],
        [ -3.5964,  -1.9432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1185833066701889
RAW KL tensor(0.1609, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 67: train/loss = 0.09905489534139633, train/raw-loss = 0.008947351947426796, train/logprobs = tensor([[-1.1703, -9.5961],
        [-5.4593, -1.7169]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18021509051322937
RAW KL tensor(0.1920, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 68: train/loss = 0.1787613332271576, train/raw-loss = 0.1129799336194992, train/logprobs = tensor([[ -2.0061, -10.3178],
        [ -3.6797,  -2.6748]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13156279921531677
RAW KL tensor(0.1190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 69: train/loss = 0.210676372051239, train/raw-loss = 0.15267500281333923, train/logprobs = tensor([[-2.8858, -6.3054],
        [-4.8301, -2.6630]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11600276082754135
RAW KL tensor(0.2117, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 70: train/loss = 0.0852910652756691, train/raw-loss = 0.009279935620725155, train/logprobs = tensor([[-1.1741, -9.1815],
        [-6.3227, -1.6796]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15202225744724274
RAW KL tensor(0.1233, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 71: train/loss = 0.2055325210094452, train/raw-loss = 0.11593033373355865, train/logprobs = tensor([[-2.4230, -7.1934],
        [-5.4418, -2.0462]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17920435965061188
RAW KL tensor(0.1463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 261])
Epoch 0, Step 72: train/loss = 0.14891888201236725, train/raw-loss = 0.08121386915445328, train/logprobs = tensor([[-0.8035, -7.9399],
        [-4.2299, -2.0563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13541001081466675
RAW KL tensor(0.1038, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 73: train/loss = 0.2639272212982178, train/raw-loss = 0.1927444040775299, train/logprobs = tensor([[-1.6791, -3.9684],
        [-5.6607, -2.7019]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14236561954021454
RAW KL tensor(0.1797, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 74: train/loss = 0.19128012657165527, train/raw-loss = 0.11545167863368988, train/logprobs = tensor([[-2.4526, -5.8656],
        [-7.6550, -2.2892]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1516568809747696
RAW KL tensor(0.2277, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 75: train/loss = 0.1467503160238266, train/raw-loss = 0.07965750992298126, train/logprobs = tensor([[-1.0643, -6.8455],
        [-4.6373, -1.8538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13418559730052948
RAW KL tensor(0.1884, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 76: train/loss = 0.3409354090690613, train/raw-loss = 0.2659365236759186, train/logprobs = tensor([[-2.5630, -6.3858],
        [-6.0097, -2.8601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14999781548976898
RAW KL tensor(0.1529, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 77: train/loss = 0.3028408885002136, train/raw-loss = 0.2337004542350769, train/logprobs = tensor([[-3.0446, -5.7898],
        [-6.6212, -2.8295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13828086853027344
RAW KL tensor(0.1275, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 78: train/loss = 0.4331098794937134, train/raw-loss = 0.360345721244812, train/logprobs = tensor([[-2.1859, -7.3564],
        [-6.2153, -3.0591]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1455284208059311
RAW KL tensor(0.1210, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 257])
Epoch 0, Step 79: train/loss = 0.5165145397186279, train/raw-loss = 0.44830837845802307, train/logprobs = tensor([[-1.2278, -4.6747],
        [-5.1652, -3.8258]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13641226291656494
RAW KL tensor(0.1383, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 80: train/loss = 0.1795228123664856, train/raw-loss = 0.10555658489465714, train/logprobs = tensor([[-2.7938, -8.3212],
        [-7.0416, -2.3816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14793244004249573
RAW KL tensor(0.1528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 81: train/loss = 0.315580815076828, train/raw-loss = 0.24950966238975525, train/logprobs = tensor([[-1.8558, -9.4873],
        [-5.3968, -1.8421]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1321423053741455
RAW KL tensor(0.0922, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 82: train/loss = 0.9306052923202515, train/raw-loss = 0.8792616724967957, train/logprobs = tensor([[-3.3696, -4.3143],
        [-5.5430, -1.8420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10268718004226685
RAW KL tensor(0.0877, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 83: train/loss = 0.11104404181241989, train/raw-loss = 0.06652949005365372, train/logprobs = tensor([[-1.3001, -5.9229],
        [-5.1337, -1.4373]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08902908116579056
RAW KL tensor(0.1461, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 344])
Epoch 0, Step 84: train/loss = 0.1843547821044922, train/raw-loss = 0.11586131155490875, train/logprobs = tensor([[-0.9468, -6.2462],
        [-5.3154, -3.0452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13698695600032806
RAW KL tensor(0.2182, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 85: train/loss = 0.2873261868953705, train/raw-loss = 0.21022291481494904, train/logprobs = tensor([[-2.1570, -7.9328],
        [-4.8435, -2.2039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15420657396316528
RAW KL tensor(0.0993, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 86: train/loss = 0.1267012059688568, train/raw-loss = 0.07749197632074356, train/logprobs = tensor([[-1.3355, -5.5139],
        [-3.9080, -1.8986]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09841848164796829
RAW KL tensor(0.1747, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 87: train/loss = 0.14064717292785645, train/raw-loss = 0.06770413368940353, train/logprobs = tensor([[-2.6003, -5.5023],
        [-8.6129, -1.6837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14588607847690582
RAW KL tensor(0.1412, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 88: train/loss = 0.1939610242843628, train/raw-loss = 0.11494410037994385, train/logprobs = tensor([[-2.9091, -8.4034],
        [-6.1319, -2.2291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1580338329076767
RAW KL tensor(0.0698, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 89: train/loss = 0.08907559514045715, train/raw-loss = 0.04196156933903694, train/logprobs = tensor([[-1.9817, -6.3350],
        [-5.7907, -1.9114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09422805160284042
RAW KL tensor(0.1708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 90: train/loss = 0.19886207580566406, train/raw-loss = 0.12486585974693298, train/logprobs = tensor([[-2.8896, -6.6747],
        [-6.3327, -1.8093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14799243211746216
RAW KL tensor(0.1122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 91: train/loss = 0.4745957553386688, train/raw-loss = 0.41187193989753723, train/logprobs = tensor([[-3.4929, -3.7422],
        [-5.1262, -2.9314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12544763088226318
RAW KL tensor(0.1552, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 92: train/loss = 0.10434660315513611, train/raw-loss = 0.037528347223997116, train/logprobs = tensor([[-1.8637, -5.8777],
        [-7.0202, -1.9581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13363651931285858
RAW KL tensor(0.1481, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 93: train/loss = 0.09966029226779938, train/raw-loss = 0.03292209282517433, train/logprobs = tensor([[-1.9828, -5.8350],
        [-8.3470, -1.6605]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1334763914346695
RAW KL tensor(0.2464, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 94: train/loss = 0.1807614266872406, train/raw-loss = 0.09815824031829834, train/logprobs = tensor([[ -2.9326, -12.6647],
        [ -5.5673,  -2.3296]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16520634293556213
RAW KL tensor(0.1510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 95: train/loss = 0.2428903728723526, train/raw-loss = 0.17436262965202332, train/logprobs = tensor([[-3.6689, -7.7771],
        [-5.8185, -2.0730]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13705545663833618
RAW KL tensor(0.1010, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 96: train/loss = 0.16433413326740265, train/raw-loss = 0.11429866403341293, train/logprobs = tensor([[-2.0333, -6.5833],
        [-5.9055, -1.8589]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10007094591856003
RAW KL tensor(0.1504, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 97: train/loss = 0.38812223076820374, train/raw-loss = 0.2808874249458313, train/logprobs = tensor([[-4.1373, -9.1022],
        [-9.7440, -3.8983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21446961164474487
RAW KL tensor(0.1520, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 98: train/loss = 0.2700813412666321, train/raw-loss = 0.2155257910490036, train/logprobs = tensor([[-3.1650, -7.9519],
        [-5.4020, -1.6233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10911111533641815
RAW KL tensor(0.1033, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 99: train/loss = 0.3383660316467285, train/raw-loss = 0.27250102162361145, train/logprobs = tensor([[-2.8593, -7.4989],
        [-5.7930, -1.5329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13172999024391174
RAW KL tensor(0.2461, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 100: train/loss = 0.10495897382497787, train/raw-loss = 0.027248235419392586, train/logprobs = tensor([[-2.5167, -5.1582],
        [-8.4662, -1.4622]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15542146563529968
RAW KL tensor(0.1384, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 101: train/loss = 0.27662497758865356, train/raw-loss = 0.20433029532432556, train/logprobs = tensor([[-4.1094, -7.4759],
        [-5.8946, -2.2574]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1445893943309784
RAW KL tensor(0.1487, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 102: train/loss = 0.15541258454322815, train/raw-loss = 0.05992892384529114, train/logprobs = tensor([[-1.6693, -5.4674],
        [-9.9430, -2.0997]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19096733629703522
RAW KL tensor(0.1488, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 103: train/loss = 0.09928864240646362, train/raw-loss = 0.02117268741130829, train/logprobs = tensor([[ -2.1295,  -7.0808],
        [-10.1723,  -1.6755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15623189508914948
RAW KL tensor(0.1460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 104: train/loss = 0.3195498287677765, train/raw-loss = 0.25425153970718384, train/logprobs = tensor([[-2.3073, -6.0472],
        [-8.7483, -2.7004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13059662282466888
RAW KL tensor(0.0990, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 105: train/loss = 0.27685242891311646, train/raw-loss = 0.218605637550354, train/logprobs = tensor([[-3.0737, -6.2931],
        [-5.3606, -2.3323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11649363487958908
RAW KL tensor(0.3139, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 106: train/loss = 0.4297870993614197, train/raw-loss = 0.3257860839366913, train/logprobs = tensor([[ -1.9890,  -7.7937],
        [-10.7035,  -4.5106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20800207555294037
RAW KL tensor(0.1260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 107: train/loss = 0.24347060918807983, train/raw-loss = 0.1644940823316574, train/logprobs = tensor([[-2.5701, -6.2914],
        [-9.6104, -4.4267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15795312821865082
RAW KL tensor(0.1726, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 108: train/loss = 0.5454921126365662, train/raw-loss = 0.4669278860092163, train/logprobs = tensor([[-1.7456, -2.0980],
        [-5.8610, -3.9475]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15712839365005493
RAW KL tensor(0.1191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 109: train/loss = 0.09215013682842255, train/raw-loss = 0.01963314414024353, train/logprobs = tensor([[-2.3941, -9.1953],
        [-8.1793, -1.5961]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14503398537635803
RAW KL tensor(0.1033, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 110: train/loss = 0.20103426277637482, train/raw-loss = 0.09561382979154587, train/logprobs = tensor([[-2.8735, -9.1939],
        [-5.9459, -1.7970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2108408510684967
RAW KL tensor(0.1248, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 111: train/loss = 0.25450119376182556, train/raw-loss = 0.1824275553226471, train/logprobs = tensor([[-1.9957, -5.4619],
        [-8.0347, -3.3011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14414726197719574
RAW KL tensor(0.1466, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 112: train/loss = 0.4746968448162079, train/raw-loss = 0.3766546845436096, train/logprobs = tensor([[-2.9703, -3.9234],
        [-9.0280, -4.6072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19608424603939056
RAW KL tensor(0.2628, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 113: train/loss = 0.2665989398956299, train/raw-loss = 0.1710306853055954, train/logprobs = tensor([[ -2.0500,  -4.8555],
        [-10.7937,  -4.1040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19113647937774658
RAW KL tensor(0.0526, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 353])
Epoch 0, Step 114: train/loss = 0.1405957192182541, train/raw-loss = 0.08851303160190582, train/logprobs = tensor([[-1.7736, -4.9164],
        [-7.7397, -1.2878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10416538268327713
RAW KL tensor(0.2830, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 115: train/loss = 0.15725713968276978, train/raw-loss = 0.09109070152044296, train/logprobs = tensor([[-2.4016, -7.6204],
        [-6.2734, -2.2726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13233287632465363
RAW KL tensor(0.1489, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 116: train/loss = 0.2910428047180176, train/raw-loss = 0.236037939786911, train/logprobs = tensor([[-1.5726, -6.4647],
        [-2.5419, -1.5261]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11000970751047134
RAW KL tensor(0.1135, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 117: train/loss = 0.07638311386108398, train/raw-loss = 0.018911149352788925, train/logprobs = tensor([[-2.0053, -6.2166],
        [-5.8434, -1.4684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11494392156600952
RAW KL tensor(0.1301, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 118: train/loss = 0.09652584046125412, train/raw-loss = 0.04491214454174042, train/logprobs = tensor([[-1.7866, -8.0487],
        [-7.4511, -1.6862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1032274067401886
RAW KL tensor(0.1876, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 119: train/loss = 0.1242220401763916, train/raw-loss = 0.052499592304229736, train/logprobs = tensor([[-3.0266, -5.7800],
        [-8.6844, -3.3080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14344489574432373
RAW KL tensor(0.1464, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 120: train/loss = 0.09374430030584335, train/raw-loss = 0.02854243852198124, train/logprobs = tensor([[ -2.7070, -10.4987],
        [ -7.7068,  -2.1344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13040372729301453
RAW KL tensor(0.1595, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 121: train/loss = 0.38125863671302795, train/raw-loss = 0.3278079628944397, train/logprobs = tensor([[-2.4178, -7.3021],
        [-2.9498, -1.6832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10690131783485413
RAW KL tensor(0.1011, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 122: train/loss = 0.24330273270606995, train/raw-loss = 0.1815042942762375, train/logprobs = tensor([[ -2.3804, -11.6730],
        [ -2.9744,  -2.3102]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12359689921140671
RAW KL tensor(0.0962, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 123: train/loss = 0.2273479849100113, train/raw-loss = 0.15983688831329346, train/logprobs = tensor([[-2.7383, -6.5837],
        [-7.8472, -3.0946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13502220809459686
RAW KL tensor(0.1067, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 124: train/loss = 0.09084657579660416, train/raw-loss = 0.05510925501585007, train/logprobs = tensor([[-1.4789, -6.6529],
        [-3.9946, -1.0251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07147465646266937
RAW KL tensor(0.2156, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 125: train/loss = 0.1610732525587082, train/raw-loss = 0.09006696194410324, train/logprobs = tensor([[ -2.7376, -10.0208],
        [ -8.6571,  -2.2148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1420125812292099
RAW KL tensor(0.1863, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 126: train/loss = 0.47258830070495605, train/raw-loss = 0.3868367671966553, train/logprobs = tensor([[ -3.3619, -12.7858],
        [ -5.4262,  -4.2345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17150305211544037
RAW KL tensor(0.1909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 127: train/loss = 0.11576458066701889, train/raw-loss = 0.03225011005997658, train/logprobs = tensor([[-3.0656, -7.4636],
        [-9.3368, -2.3192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16702893376350403
RAW KL tensor(0.2302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 128: train/loss = 0.3134799003601074, train/raw-loss = 0.20146118104457855, train/logprobs = tensor([[-2.9532, -6.1042],
        [-8.8601, -3.0709]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22403749823570251
RAW KL tensor(0.1196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 129: train/loss = 0.29009032249450684, train/raw-loss = 0.21469879150390625, train/logprobs = tensor([[-3.5506, -8.9224],
        [-6.4735, -2.9301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15078309178352356
RAW KL tensor(0.1220, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 430])
Epoch 0, Step 130: train/loss = 0.26873108744621277, train/raw-loss = 0.1792895495891571, train/logprobs = tensor([[ -5.0112, -14.0927],
        [ -5.0979,  -1.9208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17888310551643372
RAW KL tensor(0.1979, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 131: train/loss = 1.1086626052856445, train/raw-loss = 1.021061658859253, train/logprobs = tensor([[-7.9804, -5.3953],
        [-8.1306, -2.9347]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1752016842365265
RAW KL tensor(0.2088, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 132: train/loss = 0.08641926944255829, train/raw-loss = 0.003885545302182436, train/logprobs = tensor([[-2.2178, -7.7412],
        [-9.6827, -2.1963]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16506743431091309
RAW KL tensor(0.1395, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 133: train/loss = 0.07752388715744019, train/raw-loss = 0.0034942375496029854, train/logprobs = tensor([[ -3.5476, -10.8170],
        [-10.1185,  -1.9247]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14805927872657776
RAW KL tensor(0.1584, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 134: train/loss = 0.1579156368970871, train/raw-loss = 0.06810160726308823, train/logprobs = tensor([[ -2.5032, -10.5771],
        [-10.3194,  -2.3289]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17962805926799774
RAW KL tensor(0.1254, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 135: train/loss = 0.20313960313796997, train/raw-loss = 0.1453934609889984, train/logprobs = tensor([[-5.5910, -7.3513],
        [-8.3076, -1.6303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11549226939678192
RAW KL tensor(0.2212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 136: train/loss = 0.10470066219568253, train/raw-loss = 0.0044880700297653675, train/logprobs = tensor([[ -4.0707, -11.9978],
        [-10.4011,  -2.3764]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2004251778125763
RAW KL tensor(0.1567, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 137: train/loss = 0.08307518064975739, train/raw-loss = 0.023961184546351433, train/logprobs = tensor([[ -4.1700,  -7.4955],
        [-10.8106,  -1.6552]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1182279884815216
RAW KL tensor(0.0847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 138: train/loss = 0.46132218837738037, train/raw-loss = 0.4183259606361389, train/logprobs = tensor([[-3.9856, -8.7457],
        [-2.6208, -2.0486]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08599243313074112
RAW KL tensor(0.0910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 139: train/loss = 0.23108704388141632, train/raw-loss = 0.14683286845684052, train/logprobs = tensor([[ -3.4208, -10.2410],
        [ -5.2945,  -1.9071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1685083508491516
RAW KL tensor(0.1162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 140: train/loss = 0.3627452552318573, train/raw-loss = 0.3065600097179413, train/logprobs = tensor([[-3.6898, -9.7547],
        [-5.1238, -2.5262]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11237047612667084
RAW KL tensor(0.0652, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 141: train/loss = 0.34221431612968445, train/raw-loss = 0.25688236951828003, train/logprobs = tensor([[-2.9432, -8.8562],
        [-7.4627, -4.9524]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17066386342048645
RAW KL tensor(0.1277, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 142: train/loss = 0.1692366600036621, train/raw-loss = 0.10786331444978714, train/logprobs = tensor([[-3.9763, -9.9229],
        [-7.1124, -1.5112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12274671345949173
RAW KL tensor(0.1606, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 143: train/loss = 0.08888383209705353, train/raw-loss = 0.010729936882853508, train/logprobs = tensor([[ -3.6377,  -7.0763],
        [-11.8218,  -1.4482]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15630778670310974
RAW KL tensor(0.1023, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 144: train/loss = 0.22903957962989807, train/raw-loss = 0.15178224444389343, train/logprobs = tensor([[-3.3340, -7.4992],
        [-6.8873, -3.0593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15451465547084808
RAW KL tensor(0.0732, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 145: train/loss = 0.5049707889556885, train/raw-loss = 0.43036264181137085, train/logprobs = tensor([[-3.6829, -5.7717],
        [-5.9703, -4.7577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14921635389328003
RAW KL tensor(0.0868, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 146: train/loss = 0.11333608627319336, train/raw-loss = 0.06023744121193886, train/logprobs = tensor([[-2.0924, -6.4361],
        [-6.0574, -1.6048]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1061972826719284
RAW KL tensor(0.2612, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 147: train/loss = 0.3623068928718567, train/raw-loss = 0.2547215521335602, train/logprobs = tensor([[-3.4854, -7.3396],
        [-7.7919, -6.2874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2151707112789154
RAW KL tensor(0.2165, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 148: train/loss = 0.3107619285583496, train/raw-loss = 0.23142166435718536, train/logprobs = tensor([[ -2.5676,  -5.3194],
        [-10.3998,  -3.9445]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1586805284023285
RAW KL tensor(0.1402, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 149: train/loss = 0.5357257127761841, train/raw-loss = 0.4553896486759186, train/logprobs = tensor([[-2.4665, -5.8662],
        [-8.1210, -4.9511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16067206859588623
RAW KL tensor(0.1905, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 150: train/loss = 0.2469724714756012, train/raw-loss = 0.16812637448310852, train/logprobs = tensor([[-2.6571, -4.4059],
        [-9.1575, -3.3528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15769220888614655
RAW KL tensor(0.2697, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 151: train/loss = 0.2509527802467346, train/raw-loss = 0.15744175016880035, train/logprobs = tensor([[ -2.4324,  -7.5573],
        [-11.1509,  -3.2398]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18702206015586853
RAW KL tensor(0.1706, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 152: train/loss = 0.2836368680000305, train/raw-loss = 0.18775087594985962, train/logprobs = tensor([[-2.4742, -7.5809],
        [-8.0246, -2.4504]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19177204370498657
RAW KL tensor(0.1244, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 153: train/loss = 0.678946852684021, train/raw-loss = 0.6190639734268188, train/logprobs = tensor([[-4.2818, -6.9787],
        [-5.5124, -1.8121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11976571381092072
RAW KL tensor(0.1933, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 154: train/loss = 0.1890675276517868, train/raw-loss = 0.11937028169631958, train/logprobs = tensor([[-3.8538, -8.6184],
        [-7.1579, -2.7027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13939452171325684
RAW KL tensor(0.0982, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 155: train/loss = 0.19691555202007294, train/raw-loss = 0.14451655745506287, train/logprobs = tensor([[-1.8703, -5.3147],
        [-4.3566, -2.2708]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10479799658060074
RAW KL tensor(0.1212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 156: train/loss = 0.17327067255973816, train/raw-loss = 0.08635757863521576, train/logprobs = tensor([[ -2.3060, -10.4345],
        [ -5.5923,  -1.8802]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.173826202750206
RAW KL tensor(0.1341, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 157: train/loss = 0.2856910526752472, train/raw-loss = 0.22341468930244446, train/logprobs = tensor([[-2.0969, -4.4334],
        [-6.0492, -2.5968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12455270439386368
RAW KL tensor(0.0809, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 158: train/loss = 0.2171487808227539, train/raw-loss = 0.1840052753686905, train/logprobs = tensor([[-2.0715, -3.7788],
        [-4.6815, -0.7663]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06628700345754623
RAW KL tensor(0.1998, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 159: train/loss = 0.14341364800930023, train/raw-loss = 0.07024313509464264, train/logprobs = tensor([[-1.9731, -4.5710],
        [-9.2486, -1.4072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14634104073047638
RAW KL tensor(0.1291, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 160: train/loss = 0.33797889947891235, train/raw-loss = 0.2615550756454468, train/logprobs = tensor([[-1.7425, -5.0712],
        [-6.8257, -2.6843]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15284764766693115
RAW KL tensor(0.3645, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 161: train/loss = 0.2999478578567505, train/raw-loss = 0.2048816978931427, train/logprobs = tensor([[-2.8045, -7.3088],
        [-7.7153, -4.0494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19013230502605438
RAW KL tensor(0.1502, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 162: train/loss = 0.07747074961662292, train/raw-loss = 0.006344881374388933, train/logprobs = tensor([[-1.4144, -8.1318],
        [-7.5226, -2.2230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14225172996520996
RAW KL tensor(0.1267, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 163: train/loss = 0.1839827299118042, train/raw-loss = 0.1394331157207489, train/logprobs = tensor([[-3.2850, -7.1209],
        [-5.1326, -1.6965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08909922093153
RAW KL tensor(0.1272, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 164: train/loss = 0.11678466945886612, train/raw-loss = 0.05664563551545143, train/logprobs = tensor([[-2.7699, -5.1787],
        [-6.8582, -1.8861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12027807533740997
RAW KL tensor(0.1222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 165: train/loss = 0.1037982851266861, train/raw-loss = 0.04384894296526909, train/logprobs = tensor([[-2.1454, -6.0321],
        [-6.7353, -1.8402]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11989867687225342
RAW KL tensor(0.0882, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 166: train/loss = 0.20829525589942932, train/raw-loss = 0.15628498792648315, train/logprobs = tensor([[-2.1806, -8.6400],
        [-5.8298, -2.0661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10402052849531174
RAW KL tensor(0.1671, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 167: train/loss = 0.15550057590007782, train/raw-loss = 0.07590298354625702, train/logprobs = tensor([[ -2.4035,  -4.9360],
        [-10.6337,  -1.7846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1591951847076416
RAW KL tensor(0.0411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 168: train/loss = 0.289620041847229, train/raw-loss = 0.23647281527519226, train/logprobs = tensor([[-1.9371, -5.2340],
        [-8.5443, -2.0938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10629443824291229
RAW KL tensor(0.2151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 169: train/loss = 0.17234158515930176, train/raw-loss = 0.09635378420352936, train/logprobs = tensor([[-1.8415, -6.2043],
        [-8.0866, -3.8531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1519756019115448
RAW KL tensor(0.1011, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 170: train/loss = 0.2546185553073883, train/raw-loss = 0.18283945322036743, train/logprobs = tensor([[-1.7764, -7.6640],
        [-8.4305, -3.8437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14355817437171936
RAW KL tensor(0.1104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 171: train/loss = 0.16224141418933868, train/raw-loss = 0.09064134210348129, train/logprobs = tensor([[-3.1805, -7.0698],
        [-7.5594, -1.6094]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1432001292705536
RAW KL tensor(0.1175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 172: train/loss = 0.07396092265844345, train/raw-loss = 0.01841116137802601, train/logprobs = tensor([[-2.7319, -7.2575],
        [-9.2872, -2.0091]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11109951883554459
RAW KL tensor(0.1117, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 173: train/loss = 0.07088474929332733, train/raw-loss = 0.01862080954015255, train/logprobs = tensor([[ -2.3121, -10.9560],
        [ -8.0624,  -2.3233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10452787578105927
RAW KL tensor(0.1461, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 174: train/loss = 0.0884179025888443, train/raw-loss = 0.0011434179032221437, train/logprobs = tensor([[ -3.7705, -18.3176],
        [-10.1444,  -2.1470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17454896867275238
RAW KL tensor(0.1347, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 175: train/loss = 0.3269840180873871, train/raw-loss = 0.27423402667045593, train/logprobs = tensor([[-3.6595, -6.1496],
        [-5.2630, -1.7344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1054999828338623
RAW KL tensor(0.1159, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 176: train/loss = 0.11725828051567078, train/raw-loss = 0.07713675498962402, train/logprobs = tensor([[-2.2878, -7.0895],
        [-5.5325, -2.2375]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08024304360151291
RAW KL tensor(0.1723, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 177: train/loss = 0.09758835285902023, train/raw-loss = 0.032789792865514755, train/logprobs = tensor([[-2.2514, -5.9636],
        [-9.9787, -1.8855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12959711253643036
RAW KL tensor(0.1366, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 259])
Epoch 0, Step 178: train/loss = 0.41375628113746643, train/raw-loss = 0.3364925980567932, train/logprobs = tensor([[-1.9908, -6.9312],
        [-6.3813, -3.7991]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15452736616134644
RAW KL tensor(0.1847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 179: train/loss = 0.10560709983110428, train/raw-loss = 0.0442679338157177, train/logprobs = tensor([[ -3.4390,  -9.3842],
        [-10.2781,  -3.0235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12267833948135376
RAW KL tensor(0.1109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 180: train/loss = 0.1698126196861267, train/raw-loss = 0.12488474696874619, train/logprobs = tensor([[-2.5786, -5.9345],
        [-8.0901, -2.4643]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08985573798418045
RAW KL tensor(0.1025, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 326])
Epoch 0, Step 181: train/loss = 0.2334999442100525, train/raw-loss = 0.16242174804210663, train/logprobs = tensor([[-2.1431, -6.1482],
        [-8.8316, -2.1565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14215637743473053
RAW KL tensor(0.2109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 182: train/loss = 0.20758871734142303, train/raw-loss = 0.13723412156105042, train/logprobs = tensor([[-3.2748, -8.7821],
        [-7.6965, -2.2840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14070917665958405
RAW KL tensor(0.0765, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 183: train/loss = 0.29033082723617554, train/raw-loss = 0.23365524411201477, train/logprobs = tensor([[ -2.2042, -10.0147],
        [ -6.5379,  -2.0212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11335115134716034
RAW KL tensor(0.1807, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 184: train/loss = 0.14804407954216003, train/raw-loss = 0.07902231812477112, train/logprobs = tensor([[-2.8877, -9.2871],
        [-7.2173, -1.5569]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13804352283477783
RAW KL tensor(0.3193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 185: train/loss = 0.1859932541847229, train/raw-loss = 0.1086774542927742, train/logprobs = tensor([[ -2.6313, -10.9234],
        [ -7.6958,  -2.4915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1546315997838974
RAW KL tensor(0.0650, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 186: train/loss = 0.07467202842235565, train/raw-loss = 0.025690492242574692, train/logprobs = tensor([[-1.9749, -8.1995],
        [-6.6558, -1.7706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09796305745840073
RAW KL tensor(0.1253, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 187: train/loss = 0.342693030834198, train/raw-loss = 0.28239020705223083, train/logprobs = tensor([[-2.8317, -4.9662],
        [-8.0161, -3.6732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12060566246509552
RAW KL tensor(0.0707, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 188: train/loss = 0.147892564535141, train/raw-loss = 0.10352608561515808, train/logprobs = tensor([[-4.3785, -7.1426],
        [-7.2278, -1.6196]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08873294293880463
RAW KL tensor(0.0885, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 189: train/loss = 0.5982041954994202, train/raw-loss = 0.5498522520065308, train/logprobs = tensor([[-3.1754, -3.9770],
        [-4.7049, -2.5559]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09670393168926239
RAW KL tensor(0.1052, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 190: train/loss = 0.20440205931663513, train/raw-loss = 0.1416030079126358, train/logprobs = tensor([[-3.5993, -7.1085],
        [-9.8641, -3.1946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12559811770915985
RAW KL tensor(0.1169, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 191: train/loss = 0.09149494767189026, train/raw-loss = 0.018764976412057877, train/logprobs = tensor([[ -2.9267,  -7.5009],
        [-10.2156,  -1.9854]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14545994997024536
RAW KL tensor(0.1249, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 192: train/loss = 0.802910566329956, train/raw-loss = 0.7301297783851624, train/logprobs = tensor([[-2.3995, -7.1528],
        [-9.0970, -4.5798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14556147158145905
RAW KL tensor(0.1262, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 193: train/loss = 0.2996567487716675, train/raw-loss = 0.24290397763252258, train/logprobs = tensor([[-2.1881, -5.4157],
        [-6.6754, -3.3205]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11350549012422562
RAW KL tensor(0.0646, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 282])
Epoch 0, Step 194: train/loss = 0.12978525459766388, train/raw-loss = 0.06322836130857468, train/logprobs = tensor([[-3.2017, -5.3915],
        [-8.9143, -1.8790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1331138014793396
RAW KL tensor(0.1234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 195: train/loss = 0.2811395227909088, train/raw-loss = 0.22077104449272156, train/logprobs = tensor([[-1.6536, -5.1369],
        [-7.3314, -2.5700]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12073694169521332
RAW KL tensor(0.1733, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 196: train/loss = 0.25672805309295654, train/raw-loss = 0.17996206879615784, train/logprobs = tensor([[ -4.8087, -11.8518],
        [ -6.3118,  -1.7672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1535319685935974
RAW KL tensor(0.1082, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 197: train/loss = 0.11722243577241898, train/raw-loss = 0.05828125402331352, train/logprobs = tensor([[-2.8843, -7.4710],
        [-8.2795, -2.0492]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1178823709487915
RAW KL tensor(0.2091, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 198: train/loss = 0.2644628882408142, train/raw-loss = 0.18702377378940582, train/logprobs = tensor([[-2.9658, -4.7220],
        [-7.1672, -2.8611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1548781841993332
RAW KL tensor(0.1413, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 199: train/loss = 0.3241855204105377, train/raw-loss = 0.2549671232700348, train/logprobs = tensor([[-2.8150, -5.7767],
        [-6.9258, -1.0648]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13843680918216705
RAW KL tensor(0.1184, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 200: train/loss = 0.14396128058433533, train/raw-loss = 0.09524419158697128, train/logprobs = tensor([[-3.8095, -9.9994],
        [-6.2263, -1.8351]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09743417054414749
RAW KL tensor(0.1531, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 201: train/loss = 0.20337191224098206, train/raw-loss = 0.150417760014534, train/logprobs = tensor([[-2.2270, -9.3773],
        [-2.7606, -1.3835]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10590828210115433
RAW KL tensor(0.1029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 202: train/loss = 0.14520275592803955, train/raw-loss = 0.08891278505325317, train/logprobs = tensor([[-1.7830, -8.4671],
        [-5.2485, -1.6026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11257994174957275
RAW KL tensor(0.0881, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 203: train/loss = 0.09038646519184113, train/raw-loss = 0.04059186950325966, train/logprobs = tensor([[-2.9747, -5.8518],
        [-7.1676, -1.0380]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09958919882774353
RAW KL tensor(0.1132, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 204: train/loss = 0.07051018625497818, train/raw-loss = 0.011748935095965862, train/logprobs = tensor([[ -3.3977,  -7.2182],
        [-12.8041,  -1.1762]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11752250790596008
RAW KL tensor(0.0984, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 328])
Epoch 0, Step 205: train/loss = 0.157777339220047, train/raw-loss = 0.09435713291168213, train/logprobs = tensor([[-1.9236, -7.1307],
        [-7.5708, -2.0068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12684042751789093
RAW KL tensor(0.1087, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 206: train/loss = 0.16862106323242188, train/raw-loss = 0.11562852561473846, train/logprobs = tensor([[-2.1954, -6.5629],
        [-6.6648, -1.7278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10598506033420563
RAW KL tensor(0.1038, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 207: train/loss = 0.18392936885356903, train/raw-loss = 0.14518876373767853, train/logprobs = tensor([[ -2.3733, -10.2029],
        [ -5.3275,  -1.6573]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0774812251329422
RAW KL tensor(0.1769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 208: train/loss = 0.1762874722480774, train/raw-loss = 0.08664056658744812, train/logprobs = tensor([[ -2.0240, -10.6277],
        [ -9.9082,  -2.0410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17929381132125854
RAW KL tensor(0.1068, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 209: train/loss = 0.2080034613609314, train/raw-loss = 0.1500881165266037, train/logprobs = tensor([[-1.7268, -9.2990],
        [-4.1326, -2.0939]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1158306896686554
RAW KL tensor(0.1186, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 210: train/loss = 0.2639966905117035, train/raw-loss = 0.19888921082019806, train/logprobs = tensor([[ -3.1730,  -5.8630],
        [-11.8287,  -4.0151]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13021495938301086
RAW KL tensor(0.0959, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 211: train/loss = 0.16177576780319214, train/raw-loss = 0.08719739317893982, train/logprobs = tensor([[ -2.7459,  -8.7905],
        [-11.6352,  -3.7331]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14915674924850464
RAW KL tensor(0.0208, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 212: train/loss = 0.4961842894554138, train/raw-loss = 0.42902761697769165, train/logprobs = tensor([[-3.1001, -6.3432],
        [-7.8722, -3.6366]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13431338965892792
RAW KL tensor(0.1709, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 213: train/loss = 0.10440963506698608, train/raw-loss = 0.034008219838142395, train/logprobs = tensor([[ -2.9368, -10.7318],
        [-11.3328,  -1.5518]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14080283045768738
RAW KL tensor(0.0834, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 214: train/loss = 0.5007872581481934, train/raw-loss = 0.4317619502544403, train/logprobs = tensor([[-2.6719, -5.4743],
        [-8.2730, -3.5024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13805055618286133
RAW KL tensor(0.1867, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 215: train/loss = 0.23943780362606049, train/raw-loss = 0.16665567457675934, train/logprobs = tensor([[-2.4371, -6.9524],
        [-9.0352, -3.1372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1455642729997635
RAW KL tensor(0.1224, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 216: train/loss = 0.45556411147117615, train/raw-loss = 0.3755795955657959, train/logprobs = tensor([[-4.7711, -5.7516],
        [-6.1226, -3.8260]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1599690467119217
RAW KL tensor(0.2082, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 217: train/loss = 0.13349531590938568, train/raw-loss = 0.07516222447156906, train/logprobs = tensor([[-2.2927, -7.1104],
        [-8.5094, -2.1236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11666618287563324
RAW KL tensor(0.1734, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 218: train/loss = 0.0769953578710556, train/raw-loss = 0.02030833810567856, train/logprobs = tensor([[ -2.0068, -10.7467],
        [ -7.4808,  -1.8623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11337403953075409
RAW KL tensor(0.0560, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 496])
Epoch 0, Step 219: train/loss = 0.4518488645553589, train/raw-loss = 0.4161299467086792, train/logprobs = tensor([[-1.6041, -3.3044],
        [-3.4578, -1.7195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07143782079219818
RAW KL tensor(0.2696, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 220: train/loss = 0.10504801571369171, train/raw-loss = 0.008414853364229202, train/logprobs = tensor([[ -2.5354, -12.0077],
        [-10.2528,  -3.9424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19326633214950562
RAW KL tensor(0.1343, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 221: train/loss = 0.14991605281829834, train/raw-loss = 0.08747275173664093, train/logprobs = tensor([[ -3.4605,  -3.7877],
        [-11.0686,  -1.3098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12488658726215363
RAW KL tensor(0.1001, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 222: train/loss = 0.2602590024471283, train/raw-loss = 0.1928323358297348, train/logprobs = tensor([[-2.8581, -7.0039],
        [-8.2248, -3.4174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1348533183336258
RAW KL tensor(0.1044, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 223: train/loss = 0.07384025305509567, train/raw-loss = 0.01576775684952736, train/logprobs = tensor([[-2.7915, -8.4910],
        [-9.2999, -1.6894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11614499986171722
RAW KL tensor(0.0655, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 224: train/loss = 0.13312527537345886, train/raw-loss = 0.07886269688606262, train/logprobs = tensor([[ -3.1218, -10.3690],
        [ -9.0642,  -2.7542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10852514952421188
RAW KL tensor(0.0856, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 360])
Epoch 0, Step 225: train/loss = 0.10814409703016281, train/raw-loss = 0.06802422553300858, train/logprobs = tensor([[-1.7431, -7.7017],
        [-3.7983, -1.5661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08023974299430847
RAW KL tensor(0.1523, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 226: train/loss = 0.14146193861961365, train/raw-loss = 0.0783073827624321, train/logprobs = tensor([[ -2.4848, -11.5466],
        [ -7.5910,  -2.0236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1263090968132019
RAW KL tensor(0.1849, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 227: train/loss = 0.2597004175186157, train/raw-loss = 0.212022602558136, train/logprobs = tensor([[-1.9020, -4.8278],
        [-6.2324, -3.7855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09535560011863708
RAW KL tensor(0.1061, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 228: train/loss = 0.10306259989738464, train/raw-loss = 0.06469511985778809, train/logprobs = tensor([[-2.5695, -7.1933],
        [-5.8649, -3.2716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07673496007919312
RAW KL tensor(0.1196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 283])
Epoch 0, Step 229: train/loss = 0.1613968312740326, train/raw-loss = 0.11416837573051453, train/logprobs = tensor([[-2.1413, -6.8612],
        [-7.8383, -2.0199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09445693343877792
RAW KL tensor(0.1684, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 230: train/loss = 0.0843263566493988, train/raw-loss = 0.007325551472604275, train/logprobs = tensor([[ -2.1088,  -9.6536],
        [-10.0419,  -3.1565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1540016233921051
RAW KL tensor(0.0939, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 231: train/loss = 0.20767706632614136, train/raw-loss = 0.1239137127995491, train/logprobs = tensor([[ -2.7204,  -4.9070],
        [-11.1975,  -3.0735]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16752669215202332
RAW KL tensor(0.0900, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 232: train/loss = 0.20042656362056732, train/raw-loss = 0.12989512085914612, train/logprobs = tensor([[-1.9368, -6.9036],
        [-8.1814, -3.2304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1410629153251648
RAW KL tensor(0.0805, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 233: train/loss = 0.45881107449531555, train/raw-loss = 0.3893740475177765, train/logprobs = tensor([[-4.4116, -8.8114],
        [-7.2320, -1.9666]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13887402415275574
RAW KL tensor(0.0834, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 234: train/loss = 0.0657283216714859, train/raw-loss = 0.011113258078694344, train/logprobs = tensor([[-2.5014, -9.2886],
        [-9.8622, -1.6229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10923013091087341
RAW KL tensor(0.0580, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 235: train/loss = 0.43556326627731323, train/raw-loss = 0.37369298934936523, train/logprobs = tensor([[-3.4694, -6.3757],
        [-6.7698, -2.3690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12374051660299301
RAW KL tensor(0.0774, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 236: train/loss = 0.15550759434700012, train/raw-loss = 0.10922175645828247, train/logprobs = tensor([[-1.4522, -7.8244],
        [-5.8620, -2.0908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0925716757774353
RAW KL tensor(0.1131, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 237: train/loss = 0.24341443181037903, train/raw-loss = 0.2096157819032669, train/logprobs = tensor([[-1.6909, -4.2509],
        [-6.6758, -1.5645]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06759731471538544
RAW KL tensor(0.0893, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 238: train/loss = 0.18543186783790588, train/raw-loss = 0.13459834456443787, train/logprobs = tensor([[-1.4617, -5.0878],
        [-6.5747, -2.2257]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10166704654693604
RAW KL tensor(0.2273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 239: train/loss = 0.5406177639961243, train/raw-loss = 0.4710977077484131, train/logprobs = tensor([[-3.8898, -4.2369],
        [-6.3731, -3.2768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13904017210006714
RAW KL tensor(0.0914, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 240: train/loss = 0.3588871657848358, train/raw-loss = 0.3233453333377838, train/logprobs = tensor([[-3.2686, -5.6944],
        [-3.4180, -1.3946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07108370959758759
RAW KL tensor(0.1170, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 241: train/loss = 0.12796640396118164, train/raw-loss = 0.07118529081344604, train/logprobs = tensor([[-2.7347, -8.1568],
        [-7.7783, -1.6187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1135622188448906
RAW KL tensor(0.0958, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 242: train/loss = 0.19420039653778076, train/raw-loss = 0.11528827995061874, train/logprobs = tensor([[ -3.1083,  -6.3680],
        [-12.3770,  -4.9243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15782424807548523
RAW KL tensor(0.0652, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 243: train/loss = 0.20783573389053345, train/raw-loss = 0.15520989894866943, train/logprobs = tensor([[-2.8433, -5.8324],
        [-9.6901, -1.6539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10525169223546982
RAW KL tensor(0.0569, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 244: train/loss = 0.11882172524929047, train/raw-loss = 0.0632518008351326, train/logprobs = tensor([[-2.1086, -7.1667],
        [-7.5346, -1.2763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11113983392715454
RAW KL tensor(0.1333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 245: train/loss = 0.38885578513145447, train/raw-loss = 0.3169505298137665, train/logprobs = tensor([[-2.3830, -6.5905],
        [-8.7734, -3.9806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1438104510307312
RAW KL tensor(0.1984, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 246: train/loss = 0.2857874035835266, train/raw-loss = 0.20128218829631805, train/logprobs = tensor([[ -2.5435,  -6.0432],
        [-10.8948,  -4.1416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1690104454755783
RAW KL tensor(0.1081, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 247: train/loss = 0.07723626494407654, train/raw-loss = 0.01466651912778616, train/logprobs = tensor([[-2.6370, -8.6744],
        [-8.1980, -2.0254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1251394897699356
RAW KL tensor(0.1650, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 248: train/loss = 0.1835136115550995, train/raw-loss = 0.08914066851139069, train/logprobs = tensor([[ -2.5079,  -9.3113],
        [-12.2669,  -4.9862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1887458711862564
RAW KL tensor(0.2475, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 249: train/loss = 0.25095850229263306, train/raw-loss = 0.15294641256332397, train/logprobs = tensor([[ -3.6638,  -5.8360],
        [-12.1181,  -4.5068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19602416455745697
RAW KL tensor(0.0834, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 250: train/loss = 0.10481812059879303, train/raw-loss = 0.037938300520181656, train/logprobs = tensor([[ -1.6934, -10.0797],
        [ -7.7777,  -1.6421]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13375964760780334
RAW KL tensor(0.1144, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 251: train/loss = 0.18421614170074463, train/raw-loss = 0.0952240601181984, train/logprobs = tensor([[ -2.9343,  -8.8446],
        [-13.2715,  -5.3863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17798416316509247
RAW KL tensor(0.1823, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 252: train/loss = 0.25535276532173157, train/raw-loss = 0.16208377480506897, train/logprobs = tensor([[ -3.2280,  -4.3843],
        [-12.5988,  -3.7211]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1865379959344864
RAW KL tensor(0.1047, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 253: train/loss = 0.4100817143917084, train/raw-loss = 0.35531994700431824, train/logprobs = tensor([[-3.9532, -5.0991],
        [-8.4642, -3.4423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10952356457710266
RAW KL tensor(0.2559, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 254: train/loss = 0.19953864812850952, train/raw-loss = 0.12896060943603516, train/logprobs = tensor([[-2.5268, -5.5893],
        [-9.1973, -4.2264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14115606248378754
RAW KL tensor(0.0848, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 255: train/loss = 0.09060268849134445, train/raw-loss = 0.010117153637111187, train/logprobs = tensor([[-1.9924, -9.0830],
        [-9.7557, -2.2121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16097107529640198
RAW KL tensor(0.0866, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 256: train/loss = 0.06481999903917313, train/raw-loss = 0.0038160947151482105, train/logprobs = tensor([[-2.2681, -8.4785],
        [-8.6688, -1.6346]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1220078095793724
RAW KL tensor(0.2136, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 257: train/loss = 0.1842951476573944, train/raw-loss = 0.11540205776691437, train/logprobs = tensor([[ -3.7359,  -9.1933],
        [-10.1270,  -2.7389]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1377861648797989
RAW KL tensor(0.0886, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 116])
Epoch 0, Step 258: train/loss = 0.15674278140068054, train/raw-loss = 0.08856774866580963, train/logprobs = tensor([[ -4.5264, -10.9171],
        [-11.3664,  -1.7689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13635003566741943
RAW KL tensor(0.0957, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 259: train/loss = 0.48638200759887695, train/raw-loss = 0.3888987898826599, train/logprobs = tensor([[-4.1716, -4.9743],
        [-7.4475, -2.5546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1949663907289505
RAW KL tensor(0.2492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 260: train/loss = 0.19006240367889404, train/raw-loss = 0.11618310958147049, train/logprobs = tensor([[-3.4074, -7.6916],
        [-8.8882, -2.7584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1477585732936859
RAW KL tensor(0.0379, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 261: train/loss = 0.2951749265193939, train/raw-loss = 0.2395676076412201, train/logprobs = tensor([[-3.2328, -9.5110],
        [-4.5901, -1.8973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11121465265750885
RAW KL tensor(0.0886, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 262: train/loss = 0.8100292682647705, train/raw-loss = 0.75603187084198, train/logprobs = tensor([[-6.8734, -7.5859],
        [-5.5337, -3.5525]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10799496620893478
RAW KL tensor(0.1021, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 263: train/loss = 0.3801439106464386, train/raw-loss = 0.3322182893753052, train/logprobs = tensor([[ -4.2847, -13.9156],
        [ -7.2654,  -3.1040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09585119038820267
RAW KL tensor(0.0830, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 264: train/loss = 0.15057766437530518, train/raw-loss = 0.1074184849858284, train/logprobs = tensor([[ -3.3947, -10.6887],
        [ -4.2625,  -1.9314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08631837368011475
RAW KL tensor(0.1399, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 265: train/loss = 0.2362046241760254, train/raw-loss = 0.17928749322891235, train/logprobs = tensor([[-3.0410, -6.1237],
        [-8.7562, -3.2652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11383426934480667
RAW KL tensor(0.1766, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 120])
Epoch 0, Step 266: train/loss = 1.1579253673553467, train/raw-loss = 1.08842134475708, train/logprobs = tensor([[-3.0548, -5.1772],
        [-6.5551, -7.3733]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13900813460350037
RAW KL tensor(0.1704, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 267: train/loss = 0.6308698654174805, train/raw-loss = 0.5508620142936707, train/logprobs = tensor([[ -3.5539,  -6.3091],
        [-12.3727,  -4.5182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1600157916545868
RAW KL tensor(0.2113, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 268: train/loss = 0.26779842376708984, train/raw-loss = 0.2003445029258728, train/logprobs = tensor([[-2.2077, -5.0619],
        [-8.2429, -4.0237]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13490788638591766
RAW KL tensor(0.0860, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 269: train/loss = 0.3176368474960327, train/raw-loss = 0.2630779445171356, train/logprobs = tensor([[-1.9439, -4.6753],
        [-6.7300, -4.2005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10911784321069717
RAW KL tensor(0.1057, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 270: train/loss = 0.5982979536056519, train/raw-loss = 0.5438193082809448, train/logprobs = tensor([[-1.5105, -4.8347],
        [-7.4324, -2.9701]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10895735770463943
RAW KL tensor(0.1352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 271: train/loss = 0.06525976210832596, train/raw-loss = 0.013015661388635635, train/logprobs = tensor([[-1.4243, -5.7279],
        [-7.0238, -1.5692]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10448819398880005
RAW KL tensor(0.1664, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 272: train/loss = 0.15725484490394592, train/raw-loss = 0.078433096408844, train/logprobs = tensor([[-1.8214, -5.2339],
        [-9.3146, -2.8103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15764352679252625
RAW KL tensor(0.1278, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 273: train/loss = 0.13285674154758453, train/raw-loss = 0.09851288795471191, train/logprobs = tensor([[-1.3782, -8.1184],
        [-4.1598, -1.8100]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06868769228458405
RAW KL tensor(0.1573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 274: train/loss = 0.15684595704078674, train/raw-loss = 0.09469339996576309, train/logprobs = tensor([[-2.6492, -7.7169],
        [-9.4584, -2.1979]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12430509924888611
RAW KL tensor(0.1309, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 275: train/loss = 0.5141117572784424, train/raw-loss = 0.4516837000846863, train/logprobs = tensor([[-2.6765, -7.3584],
        [-3.6804, -3.3282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12485601007938385
RAW KL tensor(0.1182, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 276: train/loss = 0.11726178973913193, train/raw-loss = 0.077877938747406, train/logprobs = tensor([[ -1.2913, -10.1546],
        [ -3.7943,  -2.7690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07876770198345184
RAW KL tensor(0.0686, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 277: train/loss = 0.15684539079666138, train/raw-loss = 0.09573999792337418, train/logprobs = tensor([[-3.3225, -9.2528],
        [-9.7165, -3.1970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1222107857465744
RAW KL tensor(0.1104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 278: train/loss = 0.11261916160583496, train/raw-loss = 0.05221624672412872, train/logprobs = tensor([[ -3.1656, -12.7519],
        [ -8.4493,  -2.0102]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12080583721399307
RAW KL tensor(0.0493, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 279: train/loss = 0.41010865569114685, train/raw-loss = 0.3658595085144043, train/logprobs = tensor([[-4.3038, -6.7837],
        [-4.3580, -2.3312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0884983092546463
RAW KL tensor(0.0618, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 280: train/loss = 0.08044388145208359, train/raw-loss = 0.02494519203901291, train/logprobs = tensor([[-2.5500, -7.4070],
        [-7.6981, -1.9005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11099738627672195
RAW KL tensor(0.1034, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 281: train/loss = 0.2751511335372925, train/raw-loss = 0.22128333151340485, train/logprobs = tensor([[-4.4507, -5.9301],
        [-7.4159, -2.1039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10773561149835587
RAW KL tensor(0.1585, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 282: train/loss = 0.32485049962997437, train/raw-loss = 0.2673605680465698, train/logprobs = tensor([[-2.1736, -7.0632],
        [-4.2765, -2.8846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1149798184633255
RAW KL tensor(0.1015, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 283: train/loss = 0.05641008913516998, train/raw-loss = 0.006904981564730406, train/logprobs = tensor([[-1.8414, -8.4404],
        [-7.7827, -1.9191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09901022166013718
RAW KL tensor(0.1179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 284: train/loss = 0.18853439390659332, train/raw-loss = 0.13508813083171844, train/logprobs = tensor([[-2.0280, -4.4381],
        [-8.5971, -2.0783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10689251869916916
RAW KL tensor(0.1684, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 285: train/loss = 0.10671772807836533, train/raw-loss = 0.04279341176152229, train/logprobs = tensor([[-2.6524, -7.2365],
        [-8.0410, -2.4620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12784862518310547
RAW KL tensor(0.1124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 355])
Epoch 0, Step 286: train/loss = 0.2552735209465027, train/raw-loss = 0.18435409665107727, train/logprobs = tensor([[-1.7222, -6.2196],
        [-7.2268, -2.3927]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14183886349201202
RAW KL tensor(0.1427, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 287: train/loss = 0.22938835620880127, train/raw-loss = 0.15461520850658417, train/logprobs = tensor([[ -2.4864,  -4.8133],
        [-11.7099,  -2.8683]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1495463252067566
RAW KL tensor(0.1442, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 288: train/loss = 0.1518818736076355, train/raw-loss = 0.0006197624607011676, train/logprobs = tensor([[ -3.0102, -14.9502],
        [-14.0665,  -2.3093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.302524209022522
RAW KL tensor(0.0859, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 289: train/loss = 0.7199472784996033, train/raw-loss = 0.6506449580192566, train/logprobs = tensor([[-3.0698, -5.1602],
        [-7.0785, -5.9281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13860459625720978
RAW KL tensor(0.0300, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 290: train/loss = 0.31907224655151367, train/raw-loss = 0.25788187980651855, train/logprobs = tensor([[-3.2865, -6.3935],
        [-8.5857, -3.0011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12238064408302307
RAW KL tensor(0.1693, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 242])
Epoch 0, Step 291: train/loss = 0.3044172525405884, train/raw-loss = 0.24436020851135254, train/logprobs = tensor([[-2.5454, -6.6981],
        [-5.0709, -3.4584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12011410295963287
RAW KL tensor(0.1240, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 292: train/loss = 0.08386950194835663, train/raw-loss = 0.033106110990047455, train/logprobs = tensor([[-2.8992, -8.3973],
        [-6.5991, -1.5243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10152678191661835
RAW KL tensor(0.1247, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 294])
Epoch 0, Step 293: train/loss = 0.09113430231809616, train/raw-loss = 0.029549608007073402, train/logprobs = tensor([[-1.6885, -8.7996],
        [-8.9832, -2.0328]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12316939234733582
RAW KL tensor(0.1325, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 294: train/loss = 0.1023692935705185, train/raw-loss = 0.048546306788921356, train/logprobs = tensor([[-2.2016, -6.8341],
        [-7.5057, -2.3750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10764596611261368
RAW KL tensor(0.1120, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 295: train/loss = 0.14595241844654083, train/raw-loss = 0.08879370987415314, train/logprobs = tensor([[ -3.6055,  -5.7057],
        [-11.9450,  -3.0531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11431743949651718
RAW KL tensor(0.1688, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 296: train/loss = 0.062199369072914124, train/raw-loss = 0.0062212334014475346, train/logprobs = tensor([[ -2.5810, -13.1251],
        [-10.2932,  -1.9353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11195626109838486
RAW KL tensor(0.1028, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 297: train/loss = 0.2314053624868393, train/raw-loss = 0.1683875471353531, train/logprobs = tensor([[-3.3892, -7.8046],
        [-8.0521, -2.7718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1260356456041336
RAW KL tensor(0.1327, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 298: train/loss = 0.13996879756450653, train/raw-loss = 0.03744948282837868, train/logprobs = tensor([[ -2.3190, -11.1685],
        [-10.0528,  -2.0705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2050386369228363
RAW KL tensor(0.1390, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 299: train/loss = 0.14406466484069824, train/raw-loss = 0.08731762319803238, train/logprobs = tensor([[ -2.7045,  -4.9542],
        [-10.2428,  -1.4594]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11349408328533173
RAW KL tensor(0.2932, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 300: train/loss = 0.20709966123104095, train/raw-loss = 0.12037260830402374, train/logprobs = tensor([[-3.2740, -5.5713],
        [-8.5105, -2.0245]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17345410585403442
RAW KL tensor(0.0064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 301: train/loss = 0.2534105181694031, train/raw-loss = 0.21727612614631653, train/logprobs = tensor([[-3.3754, -6.0326],
        [-4.9293, -1.2049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07226879894733429
RAW KL tensor(0.0165, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 302: train/loss = 0.3719416856765747, train/raw-loss = 0.3095882534980774, train/logprobs = tensor([[ -4.2178, -10.3580],
        [ -4.9912,  -2.4800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12470683455467224
RAW KL tensor(0.2450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 303: train/loss = 0.2585045397281647, train/raw-loss = 0.20999056100845337, train/logprobs = tensor([[-2.7794, -7.0740],
        [-3.7224, -2.6416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09702794253826141
RAW KL tensor(0.1029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 304: train/loss = 0.2216765135526657, train/raw-loss = 0.1606036275625229, train/logprobs = tensor([[-4.4500, -8.2216],
        [-7.4803, -2.6292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12214578688144684
RAW KL tensor(0.0587, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 305: train/loss = 0.41020625829696655, train/raw-loss = 0.3645995557308197, train/logprobs = tensor([[-3.0555, -5.8351],
        [-6.4398, -2.5180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09121344238519669
RAW KL tensor(0.1613, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 306: train/loss = 0.1805703043937683, train/raw-loss = 0.12508374452590942, train/logprobs = tensor([[-1.9326, -6.7628],
        [-7.1246, -2.7573]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11097311973571777
RAW KL tensor(0.0851, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 307: train/loss = 0.12385320663452148, train/raw-loss = 0.06821703165769577, train/logprobs = tensor([[ -2.7072, -10.1553],
        [ -8.0129,  -1.6423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11127234995365143
RAW KL tensor(0.0506, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 308: train/loss = 0.09448757767677307, train/raw-loss = 0.018796751275658607, train/logprobs = tensor([[-2.3768, -8.9085],
        [-7.1384, -1.7050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15138165652751923
RAW KL tensor(0.1249, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 309: train/loss = 0.10032657533884048, train/raw-loss = 0.05914952605962753, train/logprobs = tensor([[-2.2075, -6.1412],
        [-4.9488, -1.2411]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0823540985584259
RAW KL tensor(0.1167, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 310: train/loss = 0.3502494990825653, train/raw-loss = 0.29638561606407166, train/logprobs = tensor([[ -2.7971, -10.0277],
        [ -4.9584,  -2.2888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10772779583930969
RAW KL tensor(0.0745, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 224])
Epoch 0, Step 311: train/loss = 0.3549256920814514, train/raw-loss = 0.28079015016555786, train/logprobs = tensor([[-1.8606, -7.8389],
        [-7.1645, -3.6635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14827103912830353
RAW KL tensor(0.1811, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 312: train/loss = 0.28989994525909424, train/raw-loss = 0.21668027341365814, train/logprobs = tensor([[-3.1413, -7.9730],
        [-8.3855, -2.9425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1464393436908722
RAW KL tensor(0.1164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 313: train/loss = 0.06502687931060791, train/raw-loss = 0.015396070666611195, train/logprobs = tensor([[ -2.3008, -10.9033],
        [ -8.3837,  -3.0294]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09926161170005798
RAW KL tensor(0.0817, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 447])
Epoch 0, Step 314: train/loss = 0.12396209686994553, train/raw-loss = 0.06472363322973251, train/logprobs = tensor([[ -2.2961, -12.6300],
        [ -7.6632,  -3.0447]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11847694218158722
RAW KL tensor(0.1240, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 315: train/loss = 0.1807258427143097, train/raw-loss = 0.13293813169002533, train/logprobs = tensor([[ -2.2211, -10.2676],
        [ -6.1191,  -2.6030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09557542204856873
RAW KL tensor(0.0698, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 239])
Epoch 0, Step 316: train/loss = 0.15637880563735962, train/raw-loss = 0.10079027712345123, train/logprobs = tensor([[ -3.4493, -10.8830],
        [ -8.9622,  -3.5575]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11117707192897797
RAW KL tensor(0.1627, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 317: train/loss = 0.2359847128391266, train/raw-loss = 0.17437775433063507, train/logprobs = tensor([[ -2.9748,  -7.3995],
        [-10.6531,  -4.4512]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12321391701698303
RAW KL tensor(0.0835, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 318: train/loss = 0.3234177827835083, train/raw-loss = 0.2781662344932556, train/logprobs = tensor([[-3.6085, -7.1508],
        [-7.0604, -4.8042]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09050317108631134
RAW KL tensor(0.1476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 319: train/loss = 0.1986963301897049, train/raw-loss = 0.13756197690963745, train/logprobs = tensor([[ -3.3122,  -8.2762],
        [-10.0302,  -5.2313]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12226870656013489
RAW KL tensor(0.0605, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 320: train/loss = 0.20541022717952728, train/raw-loss = 0.16162657737731934, train/logprobs = tensor([[-2.4052, -8.7239],
        [-6.0644, -3.4448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0875672772526741
RAW KL tensor(0.1673, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 321: train/loss = 0.1115792766213417, train/raw-loss = 0.04668806120753288, train/logprobs = tensor([[ -3.1387,  -9.7945],
        [-12.0126,  -4.9462]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12978243827819824
RAW KL tensor(0.0683, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 322: train/loss = 0.1707826852798462, train/raw-loss = 0.10778959095478058, train/logprobs = tensor([[ -4.1076, -14.9605],
        [ -8.4954,  -3.3740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12598620355129242
RAW KL tensor(0.1193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 323: train/loss = 0.3607085645198822, train/raw-loss = 0.31199783086776733, train/logprobs = tensor([[-3.5419, -4.4613],
        [-7.8913, -3.6618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09742145240306854
RAW KL tensor(0.1304, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 99])
Epoch 0, Step 324: train/loss = 0.5479800701141357, train/raw-loss = 0.4839934706687927, train/logprobs = tensor([[-5.7219, -9.5580],
        [-8.3165, -2.2857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12797313928604126
RAW KL tensor(0.0884, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 325: train/loss = 0.3225104808807373, train/raw-loss = 0.2274390161037445, train/logprobs = tensor([[ -6.4516,  -9.6987],
        [-11.0088,  -4.2477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19014295935630798
RAW KL tensor(0.1528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 326: train/loss = 0.07241225987672806, train/raw-loss = 0.001548132044263184, train/logprobs = tensor([[ -3.4794,  -8.2356],
        [-15.2067,  -1.7742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1417282521724701
RAW KL tensor(0.1260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 327: train/loss = 0.2870224118232727, train/raw-loss = 0.22013390064239502, train/logprobs = tensor([[ -3.8381, -10.7497],
        [ -7.6348,  -2.6711]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13377702236175537
RAW KL tensor(0.1233, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 328: train/loss = 0.2718406319618225, train/raw-loss = 0.22379179298877716, train/logprobs = tensor([[-3.8263, -7.8439],
        [-6.5047, -2.8483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09609769284725189
RAW KL tensor(0.0926, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 329: train/loss = 0.05874862149357796, train/raw-loss = 0.0006884309113956988, train/logprobs = tensor([[ -2.8477, -10.7125],
        [-14.3477,  -1.7038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11612037569284439
RAW KL tensor(0.0912, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 330: train/loss = 0.10452276468276978, train/raw-loss = 0.03666745126247406, train/logprobs = tensor([[ -3.1563,  -7.8549],
        [-11.7002,  -2.6522]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13571062684059143
RAW KL tensor(0.0557, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 331: train/loss = 0.18560852110385895, train/raw-loss = 0.14363804459571838, train/logprobs = tensor([[-4.6979, -9.9934],
        [-8.6900, -1.6483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08394094556570053
RAW KL tensor(0.1118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 332: train/loss = 0.16799715161323547, train/raw-loss = 0.08885519951581955, train/logprobs = tensor([[ -3.2488,  -6.6150],
        [-15.3218,  -2.5799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15828393399715424
RAW KL tensor(0.1458, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 333: train/loss = 0.3359485864639282, train/raw-loss = 0.2584649622440338, train/logprobs = tensor([[ -6.8585, -18.6635],
        [ -3.6798,  -1.9601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1549672782421112
RAW KL tensor(0.1498, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 334: train/loss = 0.17684295773506165, train/raw-loss = 0.09747859090566635, train/logprobs = tensor([[ -2.7434,  -6.6593],
        [-12.0370,  -3.9269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1587287187576294
RAW KL tensor(0.1226, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 335: train/loss = 0.12460483610630035, train/raw-loss = 0.06766515970230103, train/logprobs = tensor([[ -2.1596,  -6.0677],
        [-10.2687,  -2.2496]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11387933790683746
RAW KL tensor(0.1556, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 336: train/loss = 0.1918911337852478, train/raw-loss = 0.12365817278623581, train/logprobs = tensor([[-2.3340, -7.5599],
        [-9.8328, -3.2271]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1364658921957016
RAW KL tensor(0.1313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 337: train/loss = 0.07524316757917404, train/raw-loss = 0.0035808151587843895, train/logprobs = tensor([[ -2.2843,  -8.4373],
        [-10.2481,  -2.6363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14332470297813416
RAW KL tensor(0.1266, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 338: train/loss = 0.27481788396835327, train/raw-loss = 0.22905291616916656, train/logprobs = tensor([[-2.9809, -6.8873],
        [-7.4603, -3.2209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09152989834547043
RAW KL tensor(0.0986, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 339: train/loss = 0.08216826617717743, train/raw-loss = 0.03703954070806503, train/logprobs = tensor([[ -2.6386,  -7.3310],
        [-11.2274,  -1.4395]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09025745838880539
RAW KL tensor(0.1118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 340: train/loss = 0.06005468964576721, train/raw-loss = 0.00276755727827549, train/logprobs = tensor([[ -3.0795,  -9.0929],
        [-10.8482,  -2.0023]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11457426846027374
RAW KL tensor(0.1035, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 341: train/loss = 0.0848071426153183, train/raw-loss = 0.02888851799070835, train/logprobs = tensor([[ -3.6416, -10.2048],
        [-10.4175,  -1.8317]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11183725297451019
RAW KL tensor(0.0874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 342: train/loss = 0.23679500818252563, train/raw-loss = 0.19027690589427948, train/logprobs = tensor([[-3.4608, -7.0101],
        [-6.6402, -1.9257]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09303620457649231
RAW KL tensor(0.0963, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 343: train/loss = 0.14962679147720337, train/raw-loss = 0.10995450615882874, train/logprobs = tensor([[-1.9320, -8.2343],
        [-3.8793, -2.2863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07934455573558807
RAW KL tensor(0.1533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 344: train/loss = 0.38373905420303345, train/raw-loss = 0.33770427107810974, train/logprobs = tensor([[-4.0526, -6.7042],
        [-6.7725, -2.0967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.092069610953331
RAW KL tensor(0.0470, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 345: train/loss = 0.2586747109889984, train/raw-loss = 0.2185802459716797, train/logprobs = tensor([[-4.1795, -8.1468],
        [-3.9371, -1.3083]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08018894493579865
RAW KL tensor(0.1883, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 346: train/loss = 0.08717620372772217, train/raw-loss = 0.025706153362989426, train/logprobs = tensor([[-2.7606, -9.2944],
        [-7.8675, -2.9006]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12294010818004608
RAW KL tensor(0.1028, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 347: train/loss = 0.1694590151309967, train/raw-loss = 0.12526512145996094, train/logprobs = tensor([[-3.9661, -7.8110],
        [-6.1499, -1.8142]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08838777244091034
RAW KL tensor(0.0825, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 348: train/loss = 0.8612470030784607, train/raw-loss = 0.8181516528129578, train/logprobs = tensor([[-2.9891, -7.2720],
        [-4.2363, -2.9779]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08619073033332825
RAW KL tensor(0.0647, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 349: train/loss = 0.35605743527412415, train/raw-loss = 0.31931257247924805, train/logprobs = tensor([[-2.7157, -7.1444],
        [-5.3859, -2.7511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.073489710688591
RAW KL tensor(0.1409, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 350: train/loss = 0.0636540949344635, train/raw-loss = 0.010973861441016197, train/logprobs = tensor([[ -1.9969, -14.4889],
        [ -8.3256,  -3.6455]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10536046326160431
RAW KL tensor(0.1313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 302])
Epoch 0, Step 351: train/loss = 0.07184190303087234, train/raw-loss = 0.012830998748540878, train/logprobs = tensor([[-2.5189, -8.8014],
        [-7.7336, -2.3210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11802181601524353
RAW KL tensor(0.1665, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 352: train/loss = 0.06479530781507492, train/raw-loss = 0.005553506780415773, train/logprobs = tensor([[ -2.3695, -11.3849],
        [ -9.0424,  -2.3210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11848360300064087
RAW KL tensor(0.0958, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 353: train/loss = 0.11556889861822128, train/raw-loss = 0.05474459379911423, train/logprobs = tensor([[-2.7735, -7.7661],
        [-9.7811, -1.6803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12164860963821411
RAW KL tensor(0.1207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 354: train/loss = 0.05732490122318268, train/raw-loss = 0.007724099792540073, train/logprobs = tensor([[ -2.3013, -16.0226],
        [ -7.6005,  -3.4598]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09920159727334976
RAW KL tensor(0.1804, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 355: train/loss = 0.38929519057273865, train/raw-loss = 0.3319113850593567, train/logprobs = tensor([[-2.5321, -5.6680],
        [-5.5497, -2.9744]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11476755887269974
RAW KL tensor(0.0791, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 356: train/loss = 0.23036104440689087, train/raw-loss = 0.1740756332874298, train/logprobs = tensor([[-2.3800, -4.3160],
        [-9.6626, -3.5535]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11257081478834152
RAW KL tensor(0.0739, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 345])
Epoch 0, Step 357: train/loss = 0.15455035865306854, train/raw-loss = 0.1149757131934166, train/logprobs = tensor([[ -1.9393, -10.5009],
        [ -4.1157,  -2.2766]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0791492760181427
RAW KL tensor(0.0641, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 358: train/loss = 0.0639290064573288, train/raw-loss = 0.021546918898820877, train/logprobs = tensor([[-2.9441, -7.9047],
        [-8.7981, -1.8855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08476416766643524
RAW KL tensor(0.1688, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 359: train/loss = 0.066728875041008, train/raw-loss = 0.0019370501395314932, train/logprobs = tensor([[-1.9924, -9.6785],
        [-9.8871, -2.6581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12958364188671112
RAW KL tensor(0.0656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 249])
Epoch 0, Step 360: train/loss = 0.1233559399843216, train/raw-loss = 0.08000990748405457, train/logprobs = tensor([[ -1.6305, -10.3410],
        [ -4.8180,  -3.2533]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08669207245111465
RAW KL tensor(0.1620, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 361: train/loss = 0.11586864292621613, train/raw-loss = 0.06383689492940903, train/logprobs = tensor([[-1.5737, -8.0829],
        [-6.7590, -2.3732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1040634959936142
RAW KL tensor(0.0730, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 362: train/loss = 0.2182905077934265, train/raw-loss = 0.15626318752765656, train/logprobs = tensor([[ -2.7681,  -4.8917],
        [-10.8547,  -3.4013]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12405464053153992
RAW KL tensor(0.0891, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 363: train/loss = 0.07309556752443314, train/raw-loss = 0.019003216177225113, train/logprobs = tensor([[ -2.3830,  -6.4042],
        [-11.0163,  -2.1972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10818470269441605
RAW KL tensor(0.1277, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 364: train/loss = 0.1621263474225998, train/raw-loss = 0.10171310603618622, train/logprobs = tensor([[ -2.0603,  -5.2242],
        [-11.0266,  -2.5399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12082648277282715
RAW KL tensor(0.0510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 365: train/loss = 0.17660005390644073, train/raw-loss = 0.13134774565696716, train/logprobs = tensor([[-3.2576, -7.4009],
        [-7.4276, -1.7907]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09050461649894714
RAW KL tensor(0.1002, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 366: train/loss = 0.1383233666419983, train/raw-loss = 0.08536010980606079, train/logprobs = tensor([[ -2.1181,  -5.1976],
        [-10.2238,  -1.7530]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.105926513671875
RAW KL tensor(0.0557, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 367: train/loss = 0.09027451276779175, train/raw-loss = 0.0618276409804821, train/logprobs = tensor([[-1.9107, -6.5031],
        [-5.7637, -1.7697]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05689375475049019
RAW KL tensor(0.3025, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 368: train/loss = 0.13308967649936676, train/raw-loss = 0.06910517066717148, train/logprobs = tensor([[-2.9725, -9.1588],
        [-8.3612, -2.1003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12796901166439056
RAW KL tensor(0.0631, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 369: train/loss = 0.12778276205062866, train/raw-loss = 0.08568523079156876, train/logprobs = tensor([[-2.5148, -7.6612],
        [-7.9962, -2.1568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0841950923204422
RAW KL tensor(0.1005, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 370: train/loss = 0.13834549486637115, train/raw-loss = 0.08151242882013321, train/logprobs = tensor([[ -3.1829, -10.1523],
        [ -6.4201,  -2.1253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11366613209247589
RAW KL tensor(0.0344, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 461])
Epoch 0, Step 371: train/loss = 0.3277409076690674, train/raw-loss = 0.2986551821231842, train/logprobs = tensor([[-1.5109, -4.0337],
        [-3.4324, -2.0141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05817137286067009
RAW KL tensor(0.1061, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 372: train/loss = 0.33628055453300476, train/raw-loss = 0.28213465213775635, train/logprobs = tensor([[-2.9426, -8.3499],
        [-5.9022, -4.3580]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10829180479049683
RAW KL tensor(0.0800, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 373: train/loss = 0.1858368068933487, train/raw-loss = 0.14009128510951996, train/logprobs = tensor([[ -2.7527, -10.4722],
        [ -6.1245,  -2.5662]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09149103611707687
RAW KL tensor(0.0740, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 374: train/loss = 0.1687411218881607, train/raw-loss = 0.13336488604545593, train/logprobs = tensor([[-1.9213, -4.1772],
        [-4.4491, -0.9030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07075248658657074
RAW KL tensor(0.0759, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 375: train/loss = 0.24068012833595276, train/raw-loss = 0.1980540007352829, train/logprobs = tensor([[-2.3294, -6.1689],
        [-6.3999, -2.4051]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08525225520133972
RAW KL tensor(0.0774, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 376: train/loss = 0.949446976184845, train/raw-loss = 0.9138374924659729, train/logprobs = tensor([[-4.3374, -7.1431],
        [-4.3919, -3.0686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07121895998716354
RAW KL tensor(0.0592, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 377: train/loss = 0.3442678451538086, train/raw-loss = 0.31907790899276733, train/logprobs = tensor([[-1.2207, -2.9535],
        [-2.9663, -1.3212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05037990212440491
RAW KL tensor(0.1288, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 378: train/loss = 0.2076009064912796, train/raw-loss = 0.14367328584194183, train/logprobs = tensor([[ -5.5351,  -9.1044],
        [-12.2292,  -2.3542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12785524129867554
RAW KL tensor(0.4012, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 379: train/loss = 0.8616344332695007, train/raw-loss = 0.7764863967895508, train/logprobs = tensor([[ -6.3606, -13.9613],
        [ -6.4278,  -3.6675]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17029616236686707
RAW KL tensor(0.1646, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 380: train/loss = 0.22974473237991333, train/raw-loss = 0.1824408769607544, train/logprobs = tensor([[-2.7313, -8.6055],
        [-8.0357, -1.7634]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09460772573947906
RAW KL tensor(0.1257, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 381: train/loss = 0.12404575943946838, train/raw-loss = 0.07804271578788757, train/logprobs = tensor([[ -2.8854, -11.2962],
        [ -6.9510,  -1.7158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09200608730316162
RAW KL tensor(0.0436, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 382: train/loss = 0.16519686579704285, train/raw-loss = 0.1284763365983963, train/logprobs = tensor([[-3.8252, -8.1894],
        [-5.0676, -1.9271]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07344106584787369
RAW KL tensor(0.0634, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 383: train/loss = 0.23773089051246643, train/raw-loss = 0.17708545923233032, train/logprobs = tensor([[ -6.5195, -10.1319],
        [ -9.5027,  -2.8909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12129086256027222
RAW KL tensor(0.0871, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 384: train/loss = 0.21951496601104736, train/raw-loss = 0.1865186095237732, train/logprobs = tensor([[-2.4807, -8.4741],
        [-5.5455, -2.2832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06599272042512894
RAW KL tensor(0.0988, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 385: train/loss = 0.23968897759914398, train/raw-loss = 0.19181691110134125, train/logprobs = tensor([[-5.0411, -7.3174],
        [-9.3656, -2.3806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09574414789676666
RAW KL tensor(0.1417, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 386: train/loss = 0.36152318120002747, train/raw-loss = 0.301399290561676, train/logprobs = tensor([[-4.5324, -9.5663],
        [-8.4973, -2.7071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1202477216720581
RAW KL tensor(0.0847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 387: train/loss = 0.24291200935840607, train/raw-loss = 0.19731995463371277, train/logprobs = tensor([[-4.1191, -9.0824],
        [-7.2044, -2.4693]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0911841168999672
RAW KL tensor(0.1087, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 388: train/loss = 0.8258078694343567, train/raw-loss = 0.7596065402030945, train/logprobs = tensor([[-2.5928, -4.8144],
        [-6.8681, -4.0612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13240264356136322
RAW KL tensor(0.1077, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 389: train/loss = 0.20666320621967316, train/raw-loss = 0.15300244092941284, train/logprobs = tensor([[-2.7052, -5.9332],
        [-7.2351, -2.2277]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10732149332761765
RAW KL tensor(0.1208, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 390: train/loss = 0.16147401928901672, train/raw-loss = 0.10451267659664154, train/logprobs = tensor([[ -3.9429, -12.2969],
        [ -8.5676,  -2.2840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11392267048358917
RAW KL tensor(0.2124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 391: train/loss = 0.38415607810020447, train/raw-loss = 0.3141178488731384, train/logprobs = tensor([[-11.2290, -15.7495],
        [ -7.3656,  -2.9523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14007651805877686
RAW KL tensor(0.1941, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 392: train/loss = 0.4784095883369446, train/raw-loss = 0.39948415756225586, train/logprobs = tensor([[ -3.3671, -11.1222],
        [-13.4124,  -3.9024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15785089135169983
RAW KL tensor(0.2123, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 393: train/loss = 0.08227145671844482, train/raw-loss = 0.0005999109707772732, train/logprobs = tensor([[ -3.8567,  -9.9642],
        [-13.4127,  -2.2751]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16334308683872223
RAW KL tensor(0.3980, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 394: train/loss = 0.12047404050827026, train/raw-loss = 0.03956858068704605, train/logprobs = tensor([[ -4.4625, -10.3999],
        [ -8.9588,  -2.1121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16181091964244843
RAW KL tensor(0.0533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 395: train/loss = 0.21528427302837372, train/raw-loss = 0.16330736875534058, train/logprobs = tensor([[-3.1145, -7.5699],
        [-9.7510, -3.1181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10395380854606628
RAW KL tensor(0.0962, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 396: train/loss = 0.07273375988006592, train/raw-loss = 0.016383636742830276, train/logprobs = tensor([[ -2.8960, -13.1097],
        [-10.6008,  -2.2949]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11270025372505188
RAW KL tensor(0.0970, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 397: train/loss = 0.2757570445537567, train/raw-loss = 0.19338729977607727, train/logprobs = tensor([[-5.1560, -5.8579],
        [-8.7302, -2.2485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1647394597530365
RAW KL tensor(0.1080, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 398: train/loss = 0.09330122917890549, train/raw-loss = 0.02629527635872364, train/logprobs = tensor([[ -3.1427,  -9.8662],
        [-10.6439,  -3.1363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.134011909365654
RAW KL tensor(0.0789, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 399: train/loss = 0.1468581110239029, train/raw-loss = 0.10457675904035568, train/logprobs = tensor([[-3.6215, -8.0544],
        [-8.3729, -1.5733]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08456269651651382
RAW KL tensor(0.0770, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 400: train/loss = 0.1234561875462532, train/raw-loss = 0.06380582600831985, train/logprobs = tensor([[-4.4095, -7.2898],
        [-8.8765, -1.4322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1193007156252861
RAW KL tensor(0.2127, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 401: train/loss = 0.07190544158220291, train/raw-loss = 0.0004887561080977321, train/logprobs = tensor([[ -2.8561, -11.7508],
        [-12.3354,  -1.5492]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14283336699008942
RAW KL tensor(0.1851, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 402: train/loss = 0.16905954480171204, train/raw-loss = 0.09385498613119125, train/logprobs = tensor([[ -2.5191,  -8.0315],
        [-13.0990,  -3.9272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15040911734104156
RAW KL tensor(0.1089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 403: train/loss = 0.07115840911865234, train/raw-loss = 0.002377802971750498, train/logprobs = tensor([[ -3.5214,  -7.7448],
        [-14.7454,  -1.9301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13756120204925537
RAW KL tensor(0.1198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 404: train/loss = 0.07386269420385361, train/raw-loss = 0.006026082206517458, train/logprobs = tensor([[ -3.1040,  -7.5679],
        [-12.8713,  -2.1968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13567322492599487
RAW KL tensor(0.1356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 405: train/loss = 0.2454218566417694, train/raw-loss = 0.170319065451622, train/logprobs = tensor([[-2.0035, -5.7295],
        [-9.3753, -3.5808]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15020562708377838
RAW KL tensor(0.1267, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 406: train/loss = 0.1289883852005005, train/raw-loss = 0.008045461028814316, train/logprobs = tensor([[ -3.4754, -10.1523],
        [-13.6340,  -1.9829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24188585579395294
RAW KL tensor(0.1580, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 407: train/loss = 0.21590960025787354, train/raw-loss = 0.16117888689041138, train/logprobs = tensor([[-3.7552, -8.8127],
        [-9.2536, -2.8656]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10946143418550491
RAW KL tensor(0.0437, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 408: train/loss = 0.16698966920375824, train/raw-loss = 0.12039914727210999, train/logprobs = tensor([[-3.5744, -6.0108],
        [-9.7492, -1.3874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0931810513138771
RAW KL tensor(0.1329, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 409: train/loss = 0.17052538692951202, train/raw-loss = 0.122208371758461, train/logprobs = tensor([[-1.9086, -9.1525],
        [-7.8388, -2.5276]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09663406014442444
RAW KL tensor(0.1155, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 410: train/loss = 0.15729093551635742, train/raw-loss = 0.11145364493131638, train/logprobs = tensor([[-3.2454, -7.6855],
        [-8.0205, -1.6055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0916745662689209
RAW KL tensor(0.0245, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 411: train/loss = 0.4688815474510193, train/raw-loss = 0.428744912147522, train/logprobs = tensor([[-5.3927, -5.3918],
        [-5.0676, -1.4002]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08027322590351105
RAW KL tensor(0.1171, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 412: train/loss = 0.24214360117912292, train/raw-loss = 0.1853625774383545, train/logprobs = tensor([[ -3.2603,  -7.4798],
        [-10.2104,  -1.5598]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11356203258037567
RAW KL tensor(0.1606, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 413: train/loss = 0.34226304292678833, train/raw-loss = 0.2800264060497284, train/logprobs = tensor([[ -4.4980,  -3.6437],
        [-11.2699,  -1.2112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12447328865528107
RAW KL tensor(0.0699, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 414: train/loss = 0.8500570058822632, train/raw-loss = 0.7983848452568054, train/logprobs = tensor([[-4.0221, -5.9411],
        [-6.5286, -1.0679]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10334433615207672
RAW KL tensor(0.0825, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 263])
Epoch 0, Step 415: train/loss = 0.15683627128601074, train/raw-loss = 0.12046457082033157, train/logprobs = tensor([[ -2.0721, -10.1156],
        [ -4.7089,  -2.0366]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07274340093135834
RAW KL tensor(0.1158, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 416: train/loss = 0.07578815519809723, train/raw-loss = 0.007899182848632336, train/logprobs = tensor([[ -2.5204,  -8.5338],
        [-10.8075,  -2.0159]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13577792048454285
RAW KL tensor(0.0754, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 417: train/loss = 0.1912708282470703, train/raw-loss = 0.1389463245868683, train/logprobs = tensor([[ -3.0564, -12.0857],
        [ -6.5559,  -3.4106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10464902222156525
RAW KL tensor(0.2135, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 418: train/loss = 0.35459229350090027, train/raw-loss = 0.29742681980133057, train/logprobs = tensor([[-4.6425, -7.1277],
        [-5.5643, -1.7168]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11433097720146179
RAW KL tensor(0.1250, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 419: train/loss = 0.34900781512260437, train/raw-loss = 0.2907726764678955, train/logprobs = tensor([[-5.6430, -9.8055],
        [-5.8832, -1.7774]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11647023260593414
RAW KL tensor(0.1786, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 420: train/loss = 0.1390092372894287, train/raw-loss = 0.07759197056293488, train/logprobs = tensor([[-3.0096, -7.6460],
        [-9.0469, -2.8359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12283451855182648
RAW KL tensor(0.1203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 421: train/loss = 0.2741778492927551, train/raw-loss = 0.2307545691728592, train/logprobs = tensor([[-1.6508, -6.3120],
        [-7.0501, -3.4368]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08684654533863068
RAW KL tensor(0.1263, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 422: train/loss = 0.05824302136898041, train/raw-loss = 0.001970546552911401, train/logprobs = tensor([[ -2.0175,  -7.3853],
        [-12.3349,  -1.4327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1125449538230896
RAW KL tensor(0.1214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 423: train/loss = 0.07982296496629715, train/raw-loss = 0.020367607474327087, train/logprobs = tensor([[ -3.1501,  -7.4281],
        [-11.8898,  -1.9356]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11891071498394012
RAW KL tensor(0.1626, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 424: train/loss = 0.06749282777309418, train/raw-loss = 0.007182431872934103, train/logprobs = tensor([[ -1.8384,  -8.7644],
        [-13.0500,  -1.8135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12062080949544907
RAW KL tensor(0.1283, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 425: train/loss = 0.057425934821367264, train/raw-loss = 0.00031991273863241076, train/logprobs = tensor([[ -2.5956, -11.1788],
        [-13.2351,  -2.4775]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1142120435833931
RAW KL tensor(0.1190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 426: train/loss = 0.08190184086561203, train/raw-loss = 0.010741949081420898, train/logprobs = tensor([[ -3.3925,  -6.3252],
        [-13.2150,  -2.3759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14231978356838226
RAW KL tensor(0.0901, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 427: train/loss = 0.4757973849773407, train/raw-loss = 0.43142884969711304, train/logprobs = tensor([[-2.3456, -3.9510],
        [-3.7367, -3.4661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0887371227145195
RAW KL tensor(0.1097, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 428: train/loss = 0.06906912475824356, train/raw-loss = 0.009019101038575172, train/logprobs = tensor([[-3.0840, -7.3474],
        [-9.5865, -1.4420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12010005116462708
RAW KL tensor(0.1382, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 285])
Epoch 0, Step 429: train/loss = 0.5664239525794983, train/raw-loss = 0.4931890070438385, train/logprobs = tensor([[ -2.7674,  -3.9446],
        [-11.7539,  -5.1696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.146469846367836
RAW KL tensor(0.0788, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 430: train/loss = 0.1397300362586975, train/raw-loss = 0.09294908493757248, train/logprobs = tensor([[ -3.5224, -11.0486],
        [ -5.7951,  -1.9498]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09356188774108887
RAW KL tensor(0.1458, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 431: train/loss = 0.27513033151626587, train/raw-loss = 0.23070895671844482, train/logprobs = tensor([[-4.4727, -9.4784],
        [-7.1663, -4.2619]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08884274959564209
RAW KL tensor(0.1314, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 432: train/loss = 0.1688748449087143, train/raw-loss = 0.11438729614019394, train/logprobs = tensor([[ -2.4001,  -5.0221],
        [-11.8901,  -2.6922]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1089751273393631
RAW KL tensor(0.1455, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 433: train/loss = 0.1480194479227066, train/raw-loss = 0.09752707183361053, train/logprobs = tensor([[ -3.5039, -11.5731],
        [ -6.2047,  -2.8374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10098475962877274
RAW KL tensor(0.1521, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 434: train/loss = 0.26335376501083374, train/raw-loss = 0.19022373855113983, train/logprobs = tensor([[-2.8026, -8.5820],
        [-9.1187, -4.3148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14626005291938782
RAW KL tensor(0.0896, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 435: train/loss = 0.36252740025520325, train/raw-loss = 0.31289413571357727, train/logprobs = tensor([[-3.6282, -5.6000],
        [-6.3664, -1.9754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09926651418209076
RAW KL tensor(0.1034, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 436: train/loss = 0.07292723655700684, train/raw-loss = 0.03108971007168293, train/logprobs = tensor([[ -2.5882,  -6.3818],
        [-10.5028,  -1.0764]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08367504924535751
RAW KL tensor(0.1250, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 437: train/loss = 0.06645017862319946, train/raw-loss = 0.01090741902589798, train/logprobs = tensor([[-2.2236, -7.3878],
        [-8.9296, -2.0337]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11108550429344177
RAW KL tensor(0.0299, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 438: train/loss = 0.5323033332824707, train/raw-loss = 0.48025059700012207, train/logprobs = tensor([[-3.8902, -6.1877],
        [-6.0835, -2.7408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10410551726818085
RAW KL tensor(0.0538, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 439: train/loss = 0.0708247646689415, train/raw-loss = 0.014151145704090595, train/logprobs = tensor([[ -2.2601,  -7.8800],
        [-10.6589,  -1.6836]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11334723234176636
RAW KL tensor(0.1164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 347])
Epoch 0, Step 440: train/loss = 0.36073797941207886, train/raw-loss = 0.30425140261650085, train/logprobs = tensor([[-4.5024, -6.5433],
        [-8.4319, -3.8816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11297320574522018
RAW KL tensor(0.1205, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 441: train/loss = 0.08871381729841232, train/raw-loss = 0.0364716611802578, train/logprobs = tensor([[ -2.6340, -10.4625],
        [ -5.6526,  -2.3396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10448431968688965
RAW KL tensor(0.1097, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 442: train/loss = 0.11706127971410751, train/raw-loss = 0.04625244066119194, train/logprobs = tensor([[ -3.8175,  -9.2900],
        [-13.0764,  -1.8285]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14161768555641174
RAW KL tensor(0.0816, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 443: train/loss = 0.06341172754764557, train/raw-loss = 0.0064794751815497875, train/logprobs = tensor([[ -2.8262,  -9.0915],
        [-11.2704,  -2.2381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11386451870203018
RAW KL tensor(0.1053, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 444: train/loss = 0.21365520358085632, train/raw-loss = 0.18095830082893372, train/logprobs = tensor([[-2.2267, -7.2658],
        [-6.0581, -1.7163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06539379060268402
RAW KL tensor(0.1247, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 445: train/loss = 0.1096850037574768, train/raw-loss = 0.04987172782421112, train/logprobs = tensor([[ -3.5726,  -6.8265],
        [-13.2403,  -1.1698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11962655186653137
RAW KL tensor(0.0522, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 446: train/loss = 0.16408346593379974, train/raw-loss = 0.12609273195266724, train/logprobs = tensor([[ -3.8531, -10.7412],
        [ -5.5249,  -2.6538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07598148286342621
RAW KL tensor(0.0909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 447: train/loss = 0.16174359619617462, train/raw-loss = 0.09796716272830963, train/logprobs = tensor([[-2.4447, -9.4536],
        [-9.8343, -4.5697]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12755288183689117
RAW KL tensor(0.0194, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 448: train/loss = 0.4059957265853882, train/raw-loss = 0.35753124952316284, train/logprobs = tensor([[ -3.9901, -10.6087],
        [ -3.9971,  -3.0877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09692893922328949
RAW KL tensor(0.0956, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 449: train/loss = 0.26729142665863037, train/raw-loss = 0.23004646599292755, train/logprobs = tensor([[-3.9682, -5.7390],
        [-6.6640, -1.6512]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07448987662792206
RAW KL tensor(0.0645, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 450: train/loss = 0.2571401596069336, train/raw-loss = 0.2230229675769806, train/logprobs = tensor([[-1.4130, -6.6486],
        [-6.1941, -3.0091]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.068234384059906
RAW KL tensor(0.1245, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 451: train/loss = 0.3588663339614868, train/raw-loss = 0.30228763818740845, train/logprobs = tensor([[-4.6037, -7.1501],
        [-7.4867, -2.6554]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11315734684467316
RAW KL tensor(0.0982, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 452: train/loss = 0.06957568228244781, train/raw-loss = 0.0068275402300059795, train/logprobs = tensor([[ -3.1005,  -9.4349],
        [-10.5603,  -1.6453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1254962682723999
RAW KL tensor(0.0527, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 453: train/loss = 0.05345151945948601, train/raw-loss = 0.012960720807313919, train/logprobs = tensor([[-2.1942, -7.0710],
        [-8.1749, -1.6581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08098159730434418
RAW KL tensor(0.0990, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 454: train/loss = 0.0700482577085495, train/raw-loss = 0.024989627301692963, train/logprobs = tensor([[ -3.5491,  -7.4359],
        [-10.7284,  -1.5145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09011724591255188
RAW KL tensor(0.0639, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 374])
Epoch 0, Step 455: train/loss = 0.06321650743484497, train/raw-loss = 0.026414964348077774, train/logprobs = tensor([[-1.6724, -7.3733],
        [-5.9881, -1.5470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07360309362411499
RAW KL tensor(0.1019, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 423])
Epoch 0, Step 456: train/loss = 0.3608351945877075, train/raw-loss = 0.3065427243709564, train/logprobs = tensor([[-3.2292, -8.0982],
        [-7.7245, -2.3150]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10858496278524399
RAW KL tensor(0.0516, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 457: train/loss = 0.17525401711463928, train/raw-loss = 0.12890128791332245, train/logprobs = tensor([[-1.8333, -6.7817],
        [-7.0615, -2.1317]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09270544350147247
RAW KL tensor(0.0865, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 118])
Epoch 0, Step 458: train/loss = 0.12851174175739288, train/raw-loss = 0.07905507832765579, train/logprobs = tensor([[-3.5736, -7.2020],
        [-9.9259, -2.5318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09891333431005478
RAW KL tensor(0.1529, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 459: train/loss = 0.10679538547992706, train/raw-loss = 0.04459720849990845, train/logprobs = tensor([[-2.0274, -7.8747],
        [-9.9804, -2.4252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12439635396003723
RAW KL tensor(0.0919, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 452])
Epoch 0, Step 460: train/loss = 0.3284549117088318, train/raw-loss = 0.2671530842781067, train/logprobs = tensor([[-1.4673, -4.9600],
        [-8.3860, -4.1838]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12260370701551437
RAW KL tensor(0.0544, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 461: train/loss = 0.16101695597171783, train/raw-loss = 0.1117272898554802, train/logprobs = tensor([[-4.4768, -6.4129],
        [-9.9520, -1.9943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0985792875289917
RAW KL tensor(0.0940, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 361])
Epoch 0, Step 462: train/loss = 0.05938403308391571, train/raw-loss = 0.0005686430376954377, train/logprobs = tensor([[ -2.0942,  -9.3391],
        [-12.1266,  -1.8719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11763077974319458
RAW KL tensor(0.0677, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 462])
Epoch 0, Step 463: train/loss = 0.1528022587299347, train/raw-loss = 0.1145743653178215, train/logprobs = tensor([[-2.1023, -6.6612],
        [-7.5871, -2.5483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07645580172538757
RAW KL tensor(0.1159, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 464: train/loss = 0.4774710237979889, train/raw-loss = 0.4125930964946747, train/logprobs = tensor([[ -2.9494,  -9.2366],
        [-10.3633,  -5.6362]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12975585460662842
RAW KL tensor(0.1432, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 465: train/loss = 0.2749998867511749, train/raw-loss = 0.20295469462871552, train/logprobs = tensor([[ -2.3102,  -4.4750],
        [-10.8151,  -3.9723]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14409039914608002
RAW KL tensor(0.0769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 466: train/loss = 0.0524645633995533, train/raw-loss = 0.0009526615031063557, train/logprobs = tensor([[ -3.0650,  -8.4689],
        [-10.7345,  -1.3269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10302380472421646
RAW KL tensor(0.0806, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 343])
Epoch 0, Step 467: train/loss = 0.2486153542995453, train/raw-loss = 0.18795087933540344, train/logprobs = tensor([[ -3.6736,  -5.8270],
        [-10.5526,  -3.0513]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1213289350271225
RAW KL tensor(0.0569, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 468: train/loss = 0.8114156723022461, train/raw-loss = 0.766962468624115, train/logprobs = tensor([[-4.0025, -6.5358],
        [-6.7191, -3.8803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08890645951032639
RAW KL tensor(0.0943, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 469: train/loss = 0.2095094472169876, train/raw-loss = 0.1731151044368744, train/logprobs = tensor([[-2.3206, -5.3469],
        [-8.1566, -1.5661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07278871536254883
RAW KL tensor(0.0736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 470: train/loss = 0.10878218710422516, train/raw-loss = 0.07123398035764694, train/logprobs = tensor([[-2.3728, -9.3372],
        [-4.9821, -1.7491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07509640604257584
RAW KL tensor(0.0863, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 471: train/loss = 0.2645754814147949, train/raw-loss = 0.21775171160697937, train/logprobs = tensor([[-2.6241, -6.6895],
        [-5.4416, -4.0943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0936475321650505
RAW KL tensor(0.0378, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 472: train/loss = 0.05028441175818443, train/raw-loss = 0.0138087784871459, train/logprobs = tensor([[ -3.5640, -16.9219],
        [ -8.9348,  -2.3865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07295126467943192
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 473: train/loss = 0.2579563558101654, train/raw-loss = 0.2248380482196808, train/logprobs = tensor([[-4.6488, -7.2982],
        [-6.0710, -1.9876]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06623661518096924
RAW KL tensor(0.0923, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 474: train/loss = 0.06844789534807205, train/raw-loss = 0.022870631888508797, train/logprobs = tensor([[-1.7107, -7.5448],
        [-8.4876, -1.6488]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0911545380949974
RAW KL tensor(0.0714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 475: train/loss = 0.05070176720619202, train/raw-loss = 0.0067824795842170715, train/logprobs = tensor([[-2.3725, -7.6181],
        [-9.4691, -2.1187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08783857524394989
RAW KL tensor(0.1363, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 476: train/loss = 0.33883780241012573, train/raw-loss = 0.29204580187797546, train/logprobs = tensor([[-3.2958, -5.3897],
        [-6.3753, -2.3920]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09358398616313934
RAW KL tensor(0.0649, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 477: train/loss = 0.10730314254760742, train/raw-loss = 0.058186862617731094, train/logprobs = tensor([[-4.1912, -7.7867],
        [-7.5184, -1.2496]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09823256731033325
RAW KL tensor(0.1187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 478: train/loss = 0.23074683547019958, train/raw-loss = 0.17754560708999634, train/logprobs = tensor([[-3.8957, -6.0155],
        [-9.3086, -2.1260]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10640239715576172
RAW KL tensor(0.0685, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 479: train/loss = 0.18629653751850128, train/raw-loss = 0.12272247672080994, train/logprobs = tensor([[ -3.9320, -12.9065],
        [ -6.0103,  -2.5527]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12714813649654388
RAW KL tensor(0.0852, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 480: train/loss = 0.2769143283367157, train/raw-loss = 0.225936159491539, train/logprobs = tensor([[-2.6980, -5.3892],
        [-6.9525, -3.7877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10195639729499817
RAW KL tensor(0.0760, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 481: train/loss = 0.13045741617679596, train/raw-loss = 0.08852554112672806, train/logprobs = tensor([[-3.0675, -6.8145],
        [-9.1839, -2.5490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0838637501001358
RAW KL tensor(0.0914, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 278])
Epoch 0, Step 482: train/loss = 0.06628213077783585, train/raw-loss = 0.009145176038146019, train/logprobs = tensor([[ -2.2594,  -7.6552],
        [-12.7454,  -2.9731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11427392065525055
RAW KL tensor(0.1639, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 483: train/loss = 0.10869251191616058, train/raw-loss = 0.05297984182834625, train/logprobs = tensor([[ -2.8402,  -8.3462],
        [-10.8455,  -3.1613]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11142534017562866
RAW KL tensor(0.0984, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 484: train/loss = 0.06509792804718018, train/raw-loss = 0.02179640717804432, train/logprobs = tensor([[-2.3504, -8.8167],
        [-9.8406, -3.0228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08660303056240082
RAW KL tensor(0.0409, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 485: train/loss = 0.20252205431461334, train/raw-loss = 0.1606091558933258, train/logprobs = tensor([[-3.5361, -6.7899],
        [-8.4910, -1.8768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08382580429315567
RAW KL tensor(0.0559, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 486: train/loss = 0.2794134020805359, train/raw-loss = 0.24538367986679077, train/logprobs = tensor([[-2.0024, -3.5793],
        [-7.6427, -3.7920]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06805949658155441
RAW KL tensor(0.0538, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 487: train/loss = 0.13178066909313202, train/raw-loss = 0.09216879308223724, train/logprobs = tensor([[ -3.4110, -14.0652],
        [ -6.6330,  -2.9583]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07922375202178955
RAW KL tensor(0.1222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 488: train/loss = 0.28794071078300476, train/raw-loss = 0.23568502068519592, train/logprobs = tensor([[-4.6171, -7.5870],
        [-7.9793, -2.6025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10451137274503708
RAW KL tensor(0.1215, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 489: train/loss = 0.38844993710517883, train/raw-loss = 0.32746464014053345, train/logprobs = tensor([[-3.6343, -8.2600],
        [-8.0750, -3.9937]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12197057902812958
RAW KL tensor(0.0525, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 321])
Epoch 0, Step 490: train/loss = 0.3682476282119751, train/raw-loss = 0.33806198835372925, train/logprobs = tensor([[-1.6564, -4.3030],
        [-5.4477, -3.1736]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060371361672878265
RAW KL tensor(0.0554, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 491: train/loss = 0.299272358417511, train/raw-loss = 0.2604905962944031, train/logprobs = tensor([[-2.8820, -7.2759],
        [-6.3841, -3.8145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07756347954273224
RAW KL tensor(0.0697, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 492: train/loss = 0.11003442108631134, train/raw-loss = 0.07506953179836273, train/logprobs = tensor([[-1.6666, -8.6249],
        [-5.5445, -2.5427]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06992978602647781
RAW KL tensor(0.0642, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 493: train/loss = 0.13974645733833313, train/raw-loss = 0.09509984403848648, train/logprobs = tensor([[-3.0657, -8.0535],
        [-6.7520, -1.9045]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08929319679737091
RAW KL tensor(0.0841, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 494: train/loss = 0.3641679286956787, train/raw-loss = 0.3362353444099426, train/logprobs = tensor([[-3.8876, -5.0279],
        [-5.6910, -0.9219]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055865153670310974
RAW KL tensor(0.0553, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 495: train/loss = 0.24327224493026733, train/raw-loss = 0.2098664939403534, train/logprobs = tensor([[-4.3690, -7.7583],
        [-5.0686, -1.8708]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06681149452924728
RAW KL tensor(0.3068, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 496: train/loss = 0.19051934778690338, train/raw-loss = 0.11401363462209702, train/logprobs = tensor([[ -3.0601, -10.5310],
        [ -8.4798,  -2.6631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15301144123077393
RAW KL tensor(0.1179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 497: train/loss = 0.05512963607907295, train/raw-loss = 0.005298091098666191, train/logprobs = tensor([[ -2.7395,  -9.9020],
        [-10.2909,  -3.0557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09966309368610382
RAW KL tensor(0.1325, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 498: train/loss = 0.5512056350708008, train/raw-loss = 0.4728507101535797, train/logprobs = tensor([[-3.7656, -9.1668],
        [-7.5250, -4.3670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15670984983444214
RAW KL tensor(0.0590, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 499: train/loss = 0.14041592180728912, train/raw-loss = 0.10107347369194031, train/logprobs = tensor([[-3.2537, -8.8223],
        [-7.7972, -2.1881]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07868489623069763
RAW KL tensor(0.0500, device='cuda:0')
SHAPES: torch.Size([4, 122])
RAW KL tensor(0.0793, device='cuda:0')
SHAPES: torch.Size([4, 145])
RAW KL tensor(0.0572, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.1167, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.1280, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.1233, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.2411, device='cuda:0')
SHAPES: torch.Size([4, 168])
RAW KL tensor(0.1150, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.0929, device='cuda:0')
SHAPES: torch.Size([4, 184])
RAW KL tensor(0.0888, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0693, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.0972, device='cuda:0')
SHAPES: torch.Size([4, 189])
RAW KL tensor(0.1326, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0619, device='cuda:0')
SHAPES: torch.Size([4, 169])
RAW KL tensor(0.1485, device='cuda:0')
SHAPES: torch.Size([4, 239])
RAW KL tensor(0.0546, device='cuda:0')
SHAPES: torch.Size([4, 182])
RAW KL tensor(0.0647, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0594, device='cuda:0')
SHAPES: torch.Size([4, 170])
RAW KL tensor(0.3820, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0624, device='cuda:0')
SHAPES: torch.Size([4, 159])
RAW KL tensor(0.1008, device='cuda:0')
SHAPES: torch.Size([4, 158])
RAW KL tensor(0.0506, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.1148, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0993, device='cuda:0')
SHAPES: torch.Size([4, 201])
RAW KL tensor(0.0819, device='cuda:0')
SHAPES: torch.Size([4, 187])
RAW KL tensor(0.1021, device='cuda:0')
SHAPES: torch.Size([4, 317])
RAW KL tensor(0.0750, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0504, device='cuda:0')
SHAPES: torch.Size([4, 185])
RAW KL tensor(0.0583, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.1913, device='cuda:0')
SHAPES: torch.Size([4, 106])
RAW KL tensor(0.0403, device='cuda:0')
SHAPES: torch.Size([4, 173])
RAW KL tensor(0.1222, device='cuda:0')
SHAPES: torch.Size([4, 455])
RAW KL tensor(0.0761, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.0318, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.1017, device='cuda:0')
SHAPES: torch.Size([4, 263])
RAW KL tensor(0.0845, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.0710, device='cuda:0')
SHAPES: torch.Size([4, 155])
RAW KL tensor(0.0814, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.1692, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.1138, device='cuda:0')
SHAPES: torch.Size([4, 146])
RAW KL tensor(0.0987, device='cuda:0')
SHAPES: torch.Size([4, 226])
RAW KL tensor(0.0782, device='cuda:0')
SHAPES: torch.Size([4, 213])
RAW KL tensor(0.1088, device='cuda:0')
SHAPES: torch.Size([4, 204])
RAW KL tensor(0.0613, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0549, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0837, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0247, device='cuda:0')
SHAPES: torch.Size([4, 130])
RAW KL tensor(0.1076, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0811, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.1135, device='cuda:0')
SHAPES: torch.Size([4, 175])
RAW KL tensor(0.0938, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0793, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0907, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.1430, device='cuda:0')
SHAPES: torch.Size([4, 208])
RAW KL tensor(0.0635, device='cuda:0')
SHAPES: torch.Size([4, 177])
RAW KL tensor(0.0635, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.1322, device='cuda:0')
SHAPES: torch.Size([4, 164])
RAW KL tensor(0.1037, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.1487, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.0614, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0928, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.0465, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.1161, device='cuda:0')
SHAPES: torch.Size([4, 194])
RAW KL tensor(0.0763, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.0530, device='cuda:0')
SHAPES: torch.Size([4, 125])
RAW KL tensor(0.0794, device='cuda:0')
SHAPES: torch.Size([4, 179])
RAW KL tensor(0.0771, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0579, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.0330, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.1106, device='cuda:0')
SHAPES: torch.Size([4, 229])
RAW KL tensor(0.1249, device='cuda:0')
SHAPES: torch.Size([4, 341])
RAW KL tensor(0.0630, device='cuda:0')
SHAPES: torch.Size([4, 237])
RAW KL tensor(0.0751, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0701, device='cuda:0')
SHAPES: torch.Size([4, 118])
RAW KL tensor(0.0715, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0858, device='cuda:0')
SHAPES: torch.Size([4, 210])
RAW KL tensor(0.1463, device='cuda:0')
SHAPES: torch.Size([4, 216])
RAW KL tensor(0.0900, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.1525, device='cuda:0')
SHAPES: torch.Size([4, 191])
RAW KL tensor(0.0385, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.0381, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.0873, device='cuda:0')
SHAPES: torch.Size([4, 171])
RAW KL tensor(0.1012, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.0545, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.1591, device='cuda:0')
SHAPES: torch.Size([4, 195])
RAW KL tensor(0.0794, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0554, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.0448, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.1495, device='cuda:0')
SHAPES: torch.Size([4, 198])
RAW KL tensor(0.0127, device='cuda:0')
SHAPES: torch.Size([4, 450])
RAW KL tensor(0.1053, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.1045, device='cuda:0')
SHAPES: torch.Size([4, 452])
RAW KL tensor(0.0862, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0339, device='cuda:0')
SHAPES: torch.Size([4, 188])
RAW KL tensor(0.0532, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0693, device='cuda:0')
SHAPES: torch.Size([4, 174])
RAW KL tensor(0.0852, device='cuda:0')
SHAPES: torch.Size([4, 137])
RAW KL tensor(0.1199, device='cuda:0')
SHAPES: torch.Size([4, 219])
RAW KL tensor(0.0671, device='cuda:0')
SHAPES: torch.Size([4, 140])
RAW KL tensor(0.0894, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0689, device='cuda:0')
SHAPES: torch.Size([4, 410])
RAW KL tensor(0.0919, device='cuda:0')
SHAPES: torch.Size([4, 214])
RAW KL tensor(0.0454, device='cuda:0')
SHAPES: torch.Size([4, 467])
RAW KL tensor(0.0943, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0973, device='cuda:0')
SHAPES: torch.Size([4, 205])
RAW KL tensor(0.1419, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0654, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.1559, device='cuda:0')
SHAPES: torch.Size([4, 206])
RAW KL tensor(0.1772, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0463, device='cuda:0')
SHAPES: torch.Size([4, 152])
RAW KL tensor(0.0621, device='cuda:0')
SHAPES: torch.Size([4, 227])
RAW KL tensor(0.1507, device='cuda:0')
SHAPES: torch.Size([4, 217])
RAW KL tensor(0.0627, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.2329, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.1090, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.1140, device='cuda:0')
SHAPES: torch.Size([4, 183])
RAW KL tensor(0.1268, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.0910, device='cuda:0')
SHAPES: torch.Size([4, 126])
RAW KL tensor(0.0499, device='cuda:0')
SHAPES: torch.Size([4, 207])
RAW KL tensor(0.0781, device='cuda:0')
SHAPES: torch.Size([4, 121])
RAW KL tensor(0.0427, device='cuda:0')
SHAPES: torch.Size([4, 167])
RAW KL tensor(0.0779, device='cuda:0')
SHAPES: torch.Size([4, 236])
RAW KL tensor(0.0043, device='cuda:0')
SHAPES: torch.Size([4, 123])
RAW KL tensor(0.0381, device='cuda:0')
SHAPES: torch.Size([4, 128])
RAW KL tensor(0.0768, device='cuda:0')
SHAPES: torch.Size([4, 217])
eval/loss: 0.20032082498073578
RAW KL tensor(0.0907, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 500: train/loss = 0.05108795687556267, train/raw-loss = 0.0019776204135268927, train/logprobs = tensor([[ -2.9222, -10.5479],
        [-10.6965,  -1.8399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09822067618370056
RAW KL tensor(0.0624, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 501: train/loss = 0.18553559482097626, train/raw-loss = 0.14747029542922974, train/logprobs = tensor([[-3.1981, -8.4931],
        [-5.7117, -2.3163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07613062858581543
RAW KL tensor(0.0939, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 502: train/loss = 0.20699051022529602, train/raw-loss = 0.16762779653072357, train/logprobs = tensor([[-2.4376, -9.9960],
        [-5.2970, -2.0818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07872538268566132
RAW KL tensor(0.1953, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 503: train/loss = 0.1728481650352478, train/raw-loss = 0.10669998079538345, train/logprobs = tensor([[ -3.8991, -14.9169],
        [ -8.8774,  -2.0501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1322963535785675
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 504: train/loss = 0.29463279247283936, train/raw-loss = 0.25436729192733765, train/logprobs = tensor([[-3.4747, -5.4916],
        [-5.4019, -1.8446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08053095638751984
RAW KL tensor(0.0748, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 505: train/loss = 0.07623375952243805, train/raw-loss = 0.02893788553774357, train/logprobs = tensor([[-2.7986, -7.6542],
        [-8.7851, -1.8136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09459174424409866
RAW KL tensor(0.1160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 506: train/loss = 0.17728884518146515, train/raw-loss = 0.1251889169216156, train/logprobs = tensor([[ -4.1244, -11.9668],
        [ -8.8486,  -2.8493]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1041998565196991
RAW KL tensor(0.0777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 507: train/loss = 0.053436461836099625, train/raw-loss = 0.0012077323626726866, train/logprobs = tensor([[ -2.9160, -10.3709],
        [-10.4812,  -1.6039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10445746034383774
RAW KL tensor(0.0846, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 508: train/loss = 0.26832371950149536, train/raw-loss = 0.2294004112482071, train/logprobs = tensor([[-2.2169, -9.1234],
        [-5.0861, -2.5609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07784661650657654
RAW KL tensor(0.1337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 509: train/loss = 0.30307847261428833, train/raw-loss = 0.2649405002593994, train/logprobs = tensor([[ -5.6380, -11.6438],
        [ -5.0241,  -1.9515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07627597451210022
RAW KL tensor(0.0985, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 510: train/loss = 0.1210891529917717, train/raw-loss = 0.0722251608967781, train/logprobs = tensor([[-1.8094, -8.4689],
        [-8.0013, -2.5340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09772799909114838
RAW KL tensor(0.1338, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 511: train/loss = 0.21673431992530823, train/raw-loss = 0.18159013986587524, train/logprobs = tensor([[ -4.5691, -12.3931],
        [ -5.0497,  -1.7588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07028838247060776
RAW KL tensor(0.0743, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 217])
Epoch 0, Step 512: train/loss = 0.2803133726119995, train/raw-loss = 0.241887167096138, train/logprobs = tensor([[-1.6708, -9.1188],
        [-6.3471, -2.1504]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07685238122940063
RAW KL tensor(0.0490, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 513: train/loss = 0.08106084167957306, train/raw-loss = 0.050801560282707214, train/logprobs = tensor([[ -3.3279, -10.0299],
        [ -6.6488,  -1.2907]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06051856279373169
RAW KL tensor(0.1796, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 228])
Epoch 0, Step 514: train/loss = 0.4131358861923218, train/raw-loss = 0.3273290991783142, train/logprobs = tensor([[ -4.4848,  -6.1525],
        [-10.9210,  -4.6412]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17161358892917633
RAW KL tensor(0.0713, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 515: train/loss = 0.14067092537879944, train/raw-loss = 0.09197986125946045, train/logprobs = tensor([[ -2.4130, -10.3466],
        [ -8.4629,  -3.9123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09738212823867798
RAW KL tensor(0.0485, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 516: train/loss = 1.053239345550537, train/raw-loss = 1.005476713180542, train/logprobs = tensor([[ -8.0684, -10.6715],
        [ -6.2282,  -3.5207]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09552536904811859
RAW KL tensor(0.1837, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 307])
Epoch 0, Step 517: train/loss = 0.1860244870185852, train/raw-loss = 0.1363644152879715, train/logprobs = tensor([[ -2.1054,  -6.9562],
        [-10.0467,  -3.3025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09932014346122742
RAW KL tensor(0.1651, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 518: train/loss = 0.8632402420043945, train/raw-loss = 0.7922458052635193, train/logprobs = tensor([[ -4.8484,  -8.9537],
        [-10.5711,  -6.5438]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14198875427246094
RAW KL tensor(0.0965, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 519: train/loss = 0.21249090135097504, train/raw-loss = 0.15315952897071838, train/logprobs = tensor([[ -2.7578,  -8.5625],
        [-12.0307,  -3.9966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11866273730993271
RAW KL tensor(0.0059, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 520: train/loss = 0.179661363363266, train/raw-loss = 0.14062556624412537, train/logprobs = tensor([[-3.3790, -9.8690],
        [-9.0958, -2.6243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07807160168886185
RAW KL tensor(0.0712, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 521: train/loss = 0.06596680730581284, train/raw-loss = 9.142619819613174e-05, train/logprobs = tensor([[ -3.5371, -11.3457],
        [-15.9674,  -1.9518]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13175076246261597
RAW KL tensor(0.1251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 522: train/loss = 0.08109284192323685, train/raw-loss = 0.014667227864265442, train/logprobs = tensor([[ -5.2939, -12.9681],
        [-11.6763,  -2.5515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1328512281179428
RAW KL tensor(0.1392, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 253])
Epoch 0, Step 523: train/loss = 0.23379456996917725, train/raw-loss = 0.17886799573898315, train/logprobs = tensor([[ -1.9514,  -7.2534],
        [-10.7430,  -4.7641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10985314846038818
RAW KL tensor(0.1252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 524: train/loss = 0.18842275440692902, train/raw-loss = 0.1309332698583603, train/logprobs = tensor([[ -2.6966,  -6.5483],
        [-11.2905,  -3.3781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11497896909713745
RAW KL tensor(0.1653, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 238])
Epoch 0, Step 525: train/loss = 0.20944058895111084, train/raw-loss = 0.16425757110118866, train/logprobs = tensor([[-3.8139, -8.8123],
        [-8.9894, -2.5229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09036602079868317
RAW KL tensor(0.1098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 526: train/loss = 0.055243488401174545, train/raw-loss = 0.011675139889121056, train/logprobs = tensor([[ -3.9247,  -9.8407],
        [-10.3487,  -2.7517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08713670074939728
RAW KL tensor(0.0900, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 527: train/loss = 0.056342124938964844, train/raw-loss = 0.0031468523666262627, train/logprobs = tensor([[ -3.1288, -13.3686],
        [-10.7735,  -2.2188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10639055073261261
RAW KL tensor(0.0780, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 528: train/loss = 0.3810642957687378, train/raw-loss = 0.3314202129840851, train/logprobs = tensor([[-2.4296, -4.6531],
        [-9.6406, -3.6902]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09928812831640244
RAW KL tensor(0.0626, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 529: train/loss = 0.05334458872675896, train/raw-loss = 0.007833849638700485, train/logprobs = tensor([[ -1.8807, -12.4167],
        [ -7.8310,  -2.8907]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09102147817611694
RAW KL tensor(0.1279, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 530: train/loss = 0.22661323845386505, train/raw-loss = 0.1737728714942932, train/logprobs = tensor([[ -4.0799, -10.5278],
        [ -9.1869,  -1.9858]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10568073391914368
RAW KL tensor(0.0931, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 531: train/loss = 2.0505826473236084, train/raw-loss = 1.9964550733566284, train/logprobs = tensor([[-4.2323, -6.8436],
        [-6.4918, -4.6036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10825485736131668
RAW KL tensor(0.0987, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 532: train/loss = 0.1009296178817749, train/raw-loss = 0.05589171499013901, train/logprobs = tensor([[-2.6338, -8.9739],
        [-7.7962, -2.4175]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09007580578327179
RAW KL tensor(0.0422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 419])
Epoch 0, Step 533: train/loss = 0.11256153136491776, train/raw-loss = 0.08627334237098694, train/logprobs = tensor([[-1.3062, -8.7765],
        [-4.6709, -1.7616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05257636681199074
RAW KL tensor(0.1153, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 114])
Epoch 0, Step 534: train/loss = 0.25251835584640503, train/raw-loss = 0.21356388926506042, train/logprobs = tensor([[-3.6968, -6.9688],
        [-4.9303, -1.4428]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07790897786617279
RAW KL tensor(0.0870, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 535: train/loss = 0.14495053887367249, train/raw-loss = 0.1058313250541687, train/logprobs = tensor([[-2.4361, -5.1466],
        [-6.9656, -2.2022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07823843508958817
RAW KL tensor(0.0726, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 536: train/loss = 0.0874970331788063, train/raw-loss = 0.04752279818058014, train/logprobs = tensor([[-3.2482, -9.9136],
        [-8.6908, -3.1514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07994846999645233
RAW KL tensor(0.0804, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 537: train/loss = 0.3910526931285858, train/raw-loss = 0.3588111996650696, train/logprobs = tensor([[-4.7545, -6.3111],
        [-4.9692, -2.9028]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06448300182819366
RAW KL tensor(0.0558, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 538: train/loss = 0.2831614911556244, train/raw-loss = 0.2480948120355606, train/logprobs = tensor([[-2.7095, -5.6094],
        [-5.7474, -2.6182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07013333588838577
RAW KL tensor(0.0866, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 539: train/loss = 0.05070875212550163, train/raw-loss = 0.010474132373929024, train/logprobs = tensor([[ -2.6991, -12.0068],
        [ -6.7419,  -3.1257]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08046924322843552
RAW KL tensor(0.0985, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 540: train/loss = 0.0858103334903717, train/raw-loss = 0.052558496594429016, train/logprobs = tensor([[-3.9276, -7.0653],
        [-6.5504, -1.8837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06650367379188538
RAW KL tensor(0.0754, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 541: train/loss = 0.08243880420923233, train/raw-loss = 0.04552983120083809, train/logprobs = tensor([[-3.9821, -9.3656],
        [-6.8385, -2.2748]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07381794601678848
RAW KL tensor(0.0559, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 542: train/loss = 0.2647671699523926, train/raw-loss = 0.23227067291736603, train/logprobs = tensor([[-3.9829, -9.2770],
        [-8.6669, -3.6503]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06499301642179489
RAW KL tensor(0.0651, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 543: train/loss = 0.14740371704101562, train/raw-loss = 0.10963084548711777, train/logprobs = tensor([[ -3.0651, -11.2135],
        [ -4.5013,  -2.1148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07554572820663452
RAW KL tensor(0.0621, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 544: train/loss = 0.22221656143665314, train/raw-loss = 0.1842852532863617, train/logprobs = tensor([[-2.1391, -5.4263],
        [-6.4218, -2.3454]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0758625864982605
RAW KL tensor(0.0467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 545: train/loss = 0.37208372354507446, train/raw-loss = 0.3340245485305786, train/logprobs = tensor([[-1.6084, -4.2990],
        [-6.3907, -4.1607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07611830532550812
RAW KL tensor(0.1154, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 546: train/loss = 0.05426063388586044, train/raw-loss = 0.013252761214971542, train/logprobs = tensor([[ -3.7755,  -7.3289],
        [-12.0147,  -2.4184]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0820157378911972
RAW KL tensor(0.0508, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 201])
Epoch 0, Step 547: train/loss = 0.07569517195224762, train/raw-loss = 0.047820039093494415, train/logprobs = tensor([[-1.8322, -8.9553],
        [-5.4497, -1.8298]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05575026944279671
RAW KL tensor(0.0956, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 548: train/loss = 0.18149849772453308, train/raw-loss = 0.1368381381034851, train/logprobs = tensor([[-3.0015, -8.2557],
        [-9.9947, -2.6837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08932073414325714
RAW KL tensor(0.0656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 549: train/loss = 0.14549925923347473, train/raw-loss = 0.094805046916008, train/logprobs = tensor([[-3.1569, -6.3571],
        [-9.5663, -3.0250]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10138843953609467
RAW KL tensor(0.1480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 550: train/loss = 0.29140302538871765, train/raw-loss = 0.23910924792289734, train/logprobs = tensor([[-3.3876, -8.0752],
        [-8.2326, -3.2993]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10458754003047943
RAW KL tensor(0.0557, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 551: train/loss = 0.19683682918548584, train/raw-loss = 0.15528401732444763, train/logprobs = tensor([[-2.5130, -7.7319],
        [-8.8320, -3.5163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0831056535243988
RAW KL tensor(0.0832, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 552: train/loss = 0.04639847204089165, train/raw-loss = 0.0033511158544570208, train/logprobs = tensor([[ -2.7036, -11.5420],
        [ -9.2221,  -2.0896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08609471470117569
RAW KL tensor(0.0798, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 553: train/loss = 0.2616833448410034, train/raw-loss = 0.22669149935245514, train/logprobs = tensor([[-3.2054, -8.4423],
        [-8.2274, -3.2198]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06998366117477417
RAW KL tensor(0.0450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 554: train/loss = 0.30589842796325684, train/raw-loss = 0.25613293051719666, train/logprobs = tensor([[-3.4350, -9.2733],
        [-8.0917, -3.1274]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09953097999095917
RAW KL tensor(0.1320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 555: train/loss = 0.09162045270204544, train/raw-loss = 0.028325840830802917, train/logprobs = tensor([[ -4.5387, -16.4233],
        [ -9.4316,  -2.4734]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12658922374248505
RAW KL tensor(0.0770, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 556: train/loss = 0.0464678555727005, train/raw-loss = 0.006964234635233879, train/logprobs = tensor([[ -3.8823, -11.4584],
        [-11.4161,  -2.0180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07900723814964294
RAW KL tensor(0.0754, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 557: train/loss = 0.19913087785243988, train/raw-loss = 0.15738250315189362, train/logprobs = tensor([[-1.9657, -8.4469],
        [-8.5607, -3.4698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08349674940109253
RAW KL tensor(0.1814, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 558: train/loss = 0.15956851840019226, train/raw-loss = 0.1004478931427002, train/logprobs = tensor([[ -3.6371,  -5.6304],
        [-10.7129,  -3.4382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11824123561382294
RAW KL tensor(0.0769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 559: train/loss = 0.07482479512691498, train/raw-loss = 0.04475942254066467, train/logprobs = tensor([[-2.3079, -9.6745],
        [-8.1132, -3.9744]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060130733996629715
RAW KL tensor(0.0579, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 560: train/loss = 0.23653973639011383, train/raw-loss = 0.19361470639705658, train/logprobs = tensor([[-3.1213, -6.9742],
        [-8.8972, -5.3652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08585002273321152
RAW KL tensor(0.0512, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 561: train/loss = 0.07253644615411758, train/raw-loss = 0.028928391635417938, train/logprobs = tensor([[-5.0068, -9.4147],
        [-9.5650, -2.9021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08721610903739929
RAW KL tensor(0.0766, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 562: train/loss = 0.605085015296936, train/raw-loss = 0.5608353018760681, train/logprobs = tensor([[-2.9329, -5.6454],
        [-8.0608, -4.3126]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08849949389696121
RAW KL tensor(0.0593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 563: train/loss = 0.07312270253896713, train/raw-loss = 0.023341353982686996, train/logprobs = tensor([[ -4.5155, -12.3667],
        [-10.1977,  -2.0853]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09956268221139908
RAW KL tensor(0.0899, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 564: train/loss = 0.45767730474472046, train/raw-loss = 0.4152429699897766, train/logprobs = tensor([[-3.7292, -5.0921],
        [-7.2542, -3.3499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08486869186162949
RAW KL tensor(0.0550, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 565: train/loss = 0.37322163581848145, train/raw-loss = 0.3217042088508606, train/logprobs = tensor([[ -3.3830, -10.4611],
        [ -9.3082,  -1.5925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10303489863872528
RAW KL tensor(0.0890, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 566: train/loss = 0.046769946813583374, train/raw-loss = 0.009064067155122757, train/logprobs = tensor([[ -2.8391,  -9.8757],
        [-10.3431,  -1.9763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07541175186634064
RAW KL tensor(0.0401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 567: train/loss = 0.27291959524154663, train/raw-loss = 0.23384958505630493, train/logprobs = tensor([[ -3.2617, -11.4961],
        [ -4.8787,  -2.3364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0781400054693222
RAW KL tensor(0.1059, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 568: train/loss = 0.11175037175416946, train/raw-loss = 0.06052948161959648, train/logprobs = tensor([[ -4.2353, -13.9553],
        [-10.0978,  -2.0384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10244178026914597
RAW KL tensor(0.0806, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 569: train/loss = 0.0518215149641037, train/raw-loss = 0.0019120960496366024, train/logprobs = tensor([[ -3.7364, -10.7352],
        [-11.1972,  -3.2667]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09981884062290192
RAW KL tensor(0.1261, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 570: train/loss = 0.06825148314237595, train/raw-loss = 0.020942557603120804, train/logprobs = tensor([[-3.3828, -8.5054],
        [-9.8138, -1.7227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09461785107851028
RAW KL tensor(0.0983, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 571: train/loss = 0.05309874936938286, train/raw-loss = 0.007794479373842478, train/logprobs = tensor([[ -3.2032,  -9.3637],
        [-11.0417,  -2.1665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09060853719711304
RAW KL tensor(0.0792, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 572: train/loss = 0.04572903364896774, train/raw-loss = 0.0022120748180896044, train/logprobs = tensor([[ -2.9346, -12.5120],
        [-11.5788,  -2.6411]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08703391253948212
RAW KL tensor(0.1313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 573: train/loss = 1.017598032951355, train/raw-loss = 0.8995774984359741, train/logprobs = tensor([[ -6.1126, -12.4250],
        [ -5.8427,  -3.4237]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2360410839319229
RAW KL tensor(0.0732, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 574: train/loss = 0.1302650272846222, train/raw-loss = 0.09094966948032379, train/logprobs = tensor([[-2.6066, -8.5076],
        [-9.0767, -2.2713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.078630730509758
RAW KL tensor(0.0853, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 575: train/loss = 0.0650492012500763, train/raw-loss = 0.016472462564706802, train/logprobs = tensor([[-2.9831, -9.4413],
        [-8.5303, -2.0909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09715348482131958
RAW KL tensor(0.1126, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 576: train/loss = 0.06234682351350784, train/raw-loss = 0.021405814215540886, train/logprobs = tensor([[-1.9969, -6.4177],
        [-9.2220, -1.2696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08188201487064362
RAW KL tensor(0.0990, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 577: train/loss = 0.08576136827468872, train/raw-loss = 0.024193892255425453, train/logprobs = tensor([[ -4.6157, -13.3511],
        [-10.6096,  -3.1582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12313496321439743
RAW KL tensor(0.1277, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 578: train/loss = 0.4079631567001343, train/raw-loss = 0.3715100586414337, train/logprobs = tensor([[ -4.1401, -10.0176],
        [ -5.8307,  -3.2867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07290619611740112
RAW KL tensor(0.1138, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 579: train/loss = 0.05578979104757309, train/raw-loss = 0.011062707751989365, train/logprobs = tensor([[-2.0705, -7.1401],
        [-8.2318, -1.5017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08945416659116745
RAW KL tensor(0.0687, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 580: train/loss = 0.5305501222610474, train/raw-loss = 0.47563493251800537, train/logprobs = tensor([[-3.3951, -7.0561],
        [-9.6581, -4.3404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10983023047447205
RAW KL tensor(0.0832, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 581: train/loss = 0.0515960156917572, train/raw-loss = 0.004797416273504496, train/logprobs = tensor([[ -1.4987, -12.8313],
        [ -6.6947,  -3.2078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09359720349311829
RAW KL tensor(0.1225, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 582: train/loss = 0.05621249973773956, train/raw-loss = 0.00958303827792406, train/logprobs = tensor([[-3.0340, -7.3068],
        [-8.8437, -2.0384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09325893223285675
RAW KL tensor(0.0765, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 583: train/loss = 0.07908329367637634, train/raw-loss = 0.04900973662734032, train/logprobs = tensor([[ -2.4709, -11.1276],
        [ -5.4964,  -3.2900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06014711782336235
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 584: train/loss = 0.05154871195554733, train/raw-loss = 0.016393223777413368, train/logprobs = tensor([[-4.1169, -9.0414],
        [-8.8788, -1.8819]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07031097263097763
RAW KL tensor(0.0967, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 585: train/loss = 0.21842679381370544, train/raw-loss = 0.18345284461975098, train/logprobs = tensor([[-1.6071, -7.7242],
        [-5.5331, -2.1890]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06994790583848953
RAW KL tensor(0.0378, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 586: train/loss = 0.3230338990688324, train/raw-loss = 0.29116013646125793, train/logprobs = tensor([[-6.0902, -6.2486],
        [-6.7722, -2.5806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06374754756689072
RAW KL tensor(0.0749, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 587: train/loss = 0.3837481439113617, train/raw-loss = 0.34641823172569275, train/logprobs = tensor([[-1.4293, -5.1168],
        [-6.0019, -5.0132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07465985417366028
RAW KL tensor(0.1597, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 588: train/loss = 0.0754823163151741, train/raw-loss = 0.03412849083542824, train/logprobs = tensor([[-2.9162, -7.6134],
        [-8.5585, -1.5220]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08270764350891113
RAW KL tensor(0.0546, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 589: train/loss = 0.6480466723442078, train/raw-loss = 0.5818387269973755, train/logprobs = tensor([[-6.5090, -8.7268],
        [-4.6412, -5.1211]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13241596519947052
RAW KL tensor(0.0486, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 296])
Epoch 0, Step 590: train/loss = 0.15702149271965027, train/raw-loss = 0.11521288007497787, train/logprobs = tensor([[-3.5414, -9.0293],
        [-6.5915, -3.6176]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08361722528934479
RAW KL tensor(0.0915, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 591: train/loss = 0.09090591967105865, train/raw-loss = 0.049698419868946075, train/logprobs = tensor([[-2.7942, -7.3423],
        [-8.7356, -2.0894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08241501450538635
RAW KL tensor(0.0909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 592: train/loss = 0.24116961658000946, train/raw-loss = 0.18935313820838928, train/logprobs = tensor([[ -4.8476,  -8.5029],
        [-10.0033,  -2.8958]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10363295674324036
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 593: train/loss = 0.19002269208431244, train/raw-loss = 0.14439421892166138, train/logprobs = tensor([[ -3.7407, -12.0597],
        [ -7.4892,  -2.8136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09125695377588272
RAW KL tensor(0.1737, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 594: train/loss = 0.3853912949562073, train/raw-loss = 0.340221107006073, train/logprobs = tensor([[-2.5149, -5.8812],
        [-6.1355, -5.0872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09034039080142975
RAW KL tensor(0.0839, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 595: train/loss = 0.17434974014759064, train/raw-loss = 0.13002003729343414, train/logprobs = tensor([[-3.9144, -8.8331],
        [-9.4682, -2.0129]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08865943551063538
RAW KL tensor(0.0922, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 596: train/loss = 0.37797120213508606, train/raw-loss = 0.3392348289489746, train/logprobs = tensor([[-2.6094, -5.6922],
        [-8.6153, -1.6537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07747272402048111
RAW KL tensor(0.0564, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 597: train/loss = 0.08186881989240646, train/raw-loss = 0.04556223750114441, train/logprobs = tensor([[ -2.5009, -10.5090],
        [ -8.5238,  -2.1304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07261316478252411
RAW KL tensor(0.1143, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 598: train/loss = 0.15340465307235718, train/raw-loss = 0.10899113118648529, train/logprobs = tensor([[-2.0273, -7.6182],
        [-9.9258, -2.9994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08882703632116318
RAW KL tensor(0.0898, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 599: train/loss = 0.0532948262989521, train/raw-loss = 0.0034026356879621744, train/logprobs = tensor([[ -3.3428, -10.5934],
        [-10.8914,  -2.5266]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09978438913822174
RAW KL tensor(0.1043, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
