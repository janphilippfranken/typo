INFO 02-17 15:36:13 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 02-17 15:36:20 llm_engine.py:275] # GPU blocks: 27161, # CPU blocks: 2048
INFO 02-17 15:36:21 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 02-17 15:36:21 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 02-17 15:36:24 model_runner.py:547] Graph capturing finished in 3 secs.
0
1
2
Skipping example: 2
3
4
Skipping example: 4
5
6
7
8
9
10
11
12
13
14
15
Skipping example: 15
16
17
Skipping example: 17
18
19
20
21
Skipping example: 21
22
23
24
25
Skipping example: 25
26
27
28
29
30
Skipping example: 30
31
32
33
34
35
36
Skipping example: 36
37
38
39
40
41
42
Skipping example: 42
43
44
45
46
47
Skipping example: 47
48
Skipping example: 48
49
50
