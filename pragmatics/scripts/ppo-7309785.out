[2024-02-26 16:56:07,572][root][INFO] - beta: 0.5
[2024-02-26 16:56:07,573][root][INFO] - loss with_labels
[2024-02-26 16:56:07,573][root][INFO] - max_iter: 0
[2024-02-26 16:56:07,573][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels
clearing gpu cache for all ranks
Model with 214.180352M params prepared
n helpful: 5000
n harmless: 5000
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels after each epoch.
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-with-labels after each epoch.
> /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py(467)_run_batch()
-> if self.config.ppo.kl == 'average_kl':
> /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py(467)_run_batch()
-> if self.config.ppo.kl == 'average_kl':(Pdb) 

(Pdb) > /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py(467)_run_batch()
-> if self.config.ppo.kl == 'average_kl':> /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py(467)_run_batch()
-> if self.config.ppo.kl == 'average_kl':


(Pdb) (Pdb) 

