[2024-02-27 11:52:24,567][root][INFO] - beta: 0.3
[2024-02-27 11:52:24,567][root][INFO] - loss with_labels
[2024-02-27 11:52:24,568][root][INFO] - max_iter: 0
[2024-02-27 11:52:24,568][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-per-token-kl
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-per-token-kl after each epoch.
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-per-token-kl after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-per-token-kl after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-per-token-kl after each epoch.
RAW KL tensor(0., device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 0: train/loss = 0.698245108127594, train/raw-loss = 0.698245108127594, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
RAW KL tensor(0., device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 1: train/loss = 0.6927576065063477, train/raw-loss = 0.6927576065063477, train/logprobs = tensor([[-1.2483, -1.6199],
        [-1.2440, -1.6139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
RAW KL tensor(0.0003, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 2: train/loss = 0.6895034313201904, train/raw-loss = 0.6893494129180908, train/logprobs = tensor([[-1.0800, -2.0920],
        [-1.1098, -2.1063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005134812672622502
RAW KL tensor(0.0007, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 3: train/loss = 0.6810840964317322, train/raw-loss = 0.6807457208633423, train/logprobs = tensor([[-0.5910, -1.4466],
        [-0.5928, -1.3983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011277688900008798
RAW KL tensor(0.0030, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 4: train/loss = 0.670499324798584, train/raw-loss = 0.6692150235176086, train/logprobs = tensor([[-0.9606, -2.2400],
        [-0.9892, -2.1713]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0042811427265405655
RAW KL tensor(0.0096, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 5: train/loss = 0.6778687834739685, train/raw-loss = 0.6748994588851929, train/logprobs = tensor([[-1.2830, -1.8663],
        [-1.2478, -1.7572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00989769771695137
RAW KL tensor(0.0174, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 6: train/loss = 0.6804882884025574, train/raw-loss = 0.6747347116470337, train/logprobs = tensor([[-1.0910, -1.6203],
        [-1.0138, -1.4684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019178731366991997
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 7: train/loss = 0.6573760509490967, train/raw-loss = 0.6527828574180603, train/logprobs = tensor([[-1.0832, -1.7792],
        [-1.0685, -1.5954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015310576185584068
RAW KL tensor(0.0089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 8: train/loss = 0.6085008382797241, train/raw-loss = 0.6012153625488281, train/logprobs = tensor([[-0.8234, -2.1917],
        [-0.7461, -1.7143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024284953251481056
RAW KL tensor(0.0068, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 9: train/loss = 0.5851246118545532, train/raw-loss = 0.5790102481842041, train/logprobs = tensor([[-1.1536, -2.1138],
        [-1.0743, -1.5365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02038104459643364
RAW KL tensor(0.0422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 10: train/loss = 0.45895761251449585, train/raw-loss = 0.4460209310054779, train/logprobs = tensor([[-1.2625, -2.9734],
        [-1.1852, -1.6971]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04312213137745857
RAW KL tensor(0.2295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 11: train/loss = 0.5393190383911133, train/raw-loss = 0.5032137036323547, train/logprobs = tensor([[-0.8200, -2.0531],
        [-0.6513, -0.9086]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12035107612609863
RAW KL tensor(0.0371, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 12: train/loss = 0.5224741697311401, train/raw-loss = 0.49368220567703247, train/logprobs = tensor([[-0.9593, -2.7763],
        [-0.8952, -1.5867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09597315639257431
RAW KL tensor(0.0717, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 13: train/loss = 0.42289191484451294, train/raw-loss = 0.40341895818710327, train/logprobs = tensor([[-1.0496, -3.0520],
        [-0.7120, -1.0879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06490977108478546
RAW KL tensor(0.0769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 14: train/loss = 0.40643489360809326, train/raw-loss = 0.3810824453830719, train/logprobs = tensor([[-1.5930, -3.4416],
        [-1.0819, -1.1149]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08450821042060852
RAW KL tensor(0.1030, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 15: train/loss = 0.40864381194114685, train/raw-loss = 0.3790384829044342, train/logprobs = tensor([[-2.0488, -3.8247],
        [-1.1155, -0.8135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09868437051773071
RAW KL tensor(0.2062, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 16: train/loss = 0.1736040711402893, train/raw-loss = 0.12351588904857635, train/logprobs = tensor([[-2.1977, -6.7048],
        [-1.2586, -0.9756]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16696062684059143
RAW KL tensor(0.0507, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 17: train/loss = 0.39144963026046753, train/raw-loss = 0.36270976066589355, train/logprobs = tensor([[-1.8329, -4.0252],
        [-1.2836, -1.1411]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09579956531524658
RAW KL tensor(0.2540, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 18: train/loss = 0.2301366627216339, train/raw-loss = 0.17385399341583252, train/logprobs = tensor([[-3.0430, -6.2092],
        [-2.2356, -1.7669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18760892748832703
RAW KL tensor(0.1347, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 19: train/loss = 0.20534130930900574, train/raw-loss = 0.13908594846725464, train/logprobs = tensor([[-1.6654, -7.2951],
        [-0.7193, -1.1913]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22085118293762207
RAW KL tensor(0.2092, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 20: train/loss = 0.4304070472717285, train/raw-loss = 0.35735946893692017, train/logprobs = tensor([[-2.7426, -7.2151],
        [-1.7481, -3.1171]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24349188804626465
RAW KL tensor(0.1558, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 21: train/loss = 0.20600931346416473, train/raw-loss = 0.11018642783164978, train/logprobs = tensor([[-4.0703, -9.6963],
        [-2.0471, -2.2295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.31940957903862
RAW KL tensor(0.5260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 22: train/loss = 0.5066850185394287, train/raw-loss = 0.41597113013267517, train/logprobs = tensor([[-4.5942, -9.2011],
        [-1.9119, -1.5459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3023795485496521
RAW KL tensor(0.3368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 23: train/loss = 0.2684768736362457, train/raw-loss = 0.15025901794433594, train/logprobs = tensor([[ -8.1648, -14.7370],
        [ -2.4812,  -1.6069]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3940594792366028
RAW KL tensor(0.1340, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 24: train/loss = 0.3321099877357483, train/raw-loss = 0.17674683034420013, train/logprobs = tensor([[ -3.0083, -16.1472],
        [ -1.1646,  -1.3518]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5178772211074829
RAW KL tensor(0.5569, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 25: train/loss = 0.34628739953041077, train/raw-loss = 0.27229177951812744, train/logprobs = tensor([[-2.2332, -8.4364],
        [-0.8841, -1.9411]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2466520518064499
RAW KL tensor(0.2834, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 26: train/loss = 0.12350215762853622, train/raw-loss = 0.046091340482234955, train/logprobs = tensor([[ -1.4840, -12.9977],
        [ -0.5988,  -2.3255]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2580360472202301
RAW KL tensor(0.0689, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 27: train/loss = 0.3803277611732483, train/raw-loss = 0.2145417034626007, train/logprobs = tensor([[ -3.3219, -10.1782],
        [ -2.0141,  -1.7047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5526201725006104
RAW KL tensor(0.1125, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 28: train/loss = 0.25225552916526794, train/raw-loss = 0.18627679347991943, train/logprobs = tensor([[-2.2592, -6.8826],
        [-1.3504, -1.3103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21992912888526917
RAW KL tensor(0.0730, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 29: train/loss = 0.1645757406949997, train/raw-loss = 0.13055120408535004, train/logprobs = tensor([[-1.1491, -7.2832],
        [-0.6061, -1.4629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11341504752635956
RAW KL tensor(0.0648, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 219])
Epoch 0, Step 30: train/loss = 0.4685792326927185, train/raw-loss = 0.43187353014945984, train/logprobs = tensor([[-3.3733, -5.7222],
        [-0.9867, -1.3218]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12235233187675476
RAW KL tensor(0.2718, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 31: train/loss = 0.18555518984794617, train/raw-loss = 0.11409387737512589, train/logprobs = tensor([[ -4.1929, -15.8245],
        [ -2.3662,  -2.3499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23820438981056213
RAW KL tensor(0.3777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 32: train/loss = 0.14774943888187408, train/raw-loss = 0.005060409661382437, train/logprobs = tensor([[ -3.9568, -18.2663],
        [ -1.7309,  -1.8082]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4756300449371338
RAW KL tensor(0.1987, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 33: train/loss = 0.08718807250261307, train/raw-loss = 0.0027073638048022985, train/logprobs = tensor([[ -4.4229, -17.9980],
        [ -1.4938,  -2.0945]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2816023528575897
RAW KL tensor(0.1182, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 34: train/loss = 0.12014501541852951, train/raw-loss = 0.06378927826881409, train/logprobs = tensor([[ -5.1578, -11.5291],
        [ -1.6280,  -1.7860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1878524273633957
RAW KL tensor(0.3217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 35: train/loss = 0.12124012410640717, train/raw-loss = 0.019052037969231606, train/logprobs = tensor([[ -4.8685, -20.0973],
        [ -1.7180,  -2.6217]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.34062695503234863
RAW KL tensor(0.3277, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 36: train/loss = 0.3225838840007782, train/raw-loss = 0.22427017986774445, train/logprobs = tensor([[ -6.1521, -17.1164],
        [ -2.4632,  -2.1886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.32771235704421997
RAW KL tensor(0.1214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 235])
Epoch 0, Step 37: train/loss = 0.3189700245857239, train/raw-loss = 0.22841748595237732, train/logprobs = tensor([[ -3.5410, -16.7261],
        [ -0.6453,  -1.9931]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.30184176564216614
RAW KL tensor(0.1948, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 292])
Epoch 0, Step 38: train/loss = 0.23810507357120514, train/raw-loss = 0.16441448032855988, train/logprobs = tensor([[ -4.7049, -12.8879],
        [ -1.1007,  -1.8395]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24563534557819366
RAW KL tensor(0.2542, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 39: train/loss = 0.3383720815181732, train/raw-loss = 0.27014976739883423, train/logprobs = tensor([[ -4.0224, -11.7355],
        [ -0.7830,  -1.2593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2274075299501419
RAW KL tensor(0.2333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 40: train/loss = 0.3565601706504822, train/raw-loss = 0.28500664234161377, train/logprobs = tensor([[ -5.8940, -10.4726],
        [ -1.1198,  -1.2783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23851163685321808
RAW KL tensor(0.1735, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 329])
Epoch 0, Step 41: train/loss = 0.3125835061073303, train/raw-loss = 0.23688094317913055, train/logprobs = tensor([[ -5.3765, -11.8016],
        [ -1.6886,  -1.4496]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2523418664932251
RAW KL tensor(0.3376, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 42: train/loss = 0.23109027743339539, train/raw-loss = 0.17756828665733337, train/logprobs = tensor([[ -2.5295, -12.8040],
        [ -1.1852,  -1.5400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17840665578842163
RAW KL tensor(0.5156, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 43: train/loss = 0.8814719319343567, train/raw-loss = 0.788390040397644, train/logprobs = tensor([[ -8.3470, -13.7500],
        [ -2.0578,  -2.6106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3102729320526123
RAW KL tensor(0.1647, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 44: train/loss = 0.2260638177394867, train/raw-loss = 0.17328158020973206, train/logprobs = tensor([[-3.7267, -6.8611],
        [-1.9914, -1.1635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1759408563375473
RAW KL tensor(0.2839, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 45: train/loss = 0.4042934477329254, train/raw-loss = 0.3200814127922058, train/logprobs = tensor([[ -5.0908, -14.1457],
        [ -2.0276,  -1.9671]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.28070685267448425
RAW KL tensor(0.2148, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 46: train/loss = 0.1495007872581482, train/raw-loss = 0.024745145812630653, train/logprobs = tensor([[ -4.5053, -22.0024],
        [ -2.0350,  -2.9308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4158521294593811
RAW KL tensor(0.1421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 47: train/loss = 0.2500534653663635, train/raw-loss = 0.1916947364807129, train/logprobs = tensor([[ -2.5941, -11.3310],
        [ -0.9901,  -1.5117]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19452902674674988
RAW KL tensor(0.2393, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 48: train/loss = 0.778475821018219, train/raw-loss = 0.7285836935043335, train/logprobs = tensor([[-6.6985, -6.6283],
        [-2.1833, -1.6104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1663072407245636
RAW KL tensor(0.1804, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 49: train/loss = 0.10599176585674286, train/raw-loss = 0.0678931251168251, train/logprobs = tensor([[ -2.5838, -11.3211],
        [ -1.0962,  -1.5487]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12699544429779053
RAW KL tensor(0.0992, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 50: train/loss = 0.172824427485466, train/raw-loss = 0.13566632568836212, train/logprobs = tensor([[-3.6131, -9.4748],
        [-1.8478, -1.2981]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12386031448841095
RAW KL tensor(0.1112, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 51: train/loss = 0.11548642069101334, train/raw-loss = 0.07703366130590439, train/logprobs = tensor([[ -4.8594, -11.4270],
        [ -1.4041,  -1.6390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12817586958408356
RAW KL tensor(0.0745, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 52: train/loss = 0.6550147533416748, train/raw-loss = 0.6343710422515869, train/logprobs = tensor([[-2.7748, -2.8152],
        [-1.1931, -0.8551]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06881237030029297
RAW KL tensor(0.0316, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 53: train/loss = 0.5023140907287598, train/raw-loss = 0.4835028052330017, train/logprobs = tensor([[-2.1260, -5.8366],
        [-0.5189, -0.9007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06270428001880646
RAW KL tensor(0.2745, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 54: train/loss = 0.1301562339067459, train/raw-loss = 0.0783308893442154, train/logprobs = tensor([[ -3.5604, -10.5688],
        [ -1.7507,  -1.6408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17275111377239227
RAW KL tensor(0.1736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 55: train/loss = 0.2566906809806824, train/raw-loss = 0.2145366072654724, train/logprobs = tensor([[-4.6517, -8.3416],
        [-1.9390, -1.0674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14051350951194763
RAW KL tensor(0.1930, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 56: train/loss = 0.07720924913883209, train/raw-loss = 0.021888665854930878, train/logprobs = tensor([[ -4.1729, -12.8724],
        [ -1.5645,  -1.4785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18440192937850952
RAW KL tensor(0.2359, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 57: train/loss = 0.3295872211456299, train/raw-loss = 0.29210102558135986, train/logprobs = tensor([[ -2.8849, -10.4145],
        [ -1.2280,  -0.9934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12495411932468414
RAW KL tensor(0.0395, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 335])
Epoch 0, Step 58: train/loss = 0.23574557900428772, train/raw-loss = 0.19911471009254456, train/logprobs = tensor([[ -2.2657, -10.6368],
        [ -0.7894,  -1.3467]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12210287898778915
RAW KL tensor(0.0244, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 364])
Epoch 0, Step 59: train/loss = 0.2732115387916565, train/raw-loss = 0.24294373393058777, train/logprobs = tensor([[-3.4040, -7.5382],
        [-1.4090, -1.8306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10089268535375595
RAW KL tensor(0.0773, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 60: train/loss = 0.2241387963294983, train/raw-loss = 0.18085364997386932, train/logprobs = tensor([[ -5.3102, -16.1699],
        [ -1.6339,  -1.1474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14428381621837616
RAW KL tensor(0.1022, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 61: train/loss = 0.20730645954608917, train/raw-loss = 0.17683731019496918, train/logprobs = tensor([[ -1.4662, -13.6714],
        [ -0.5533,  -1.4087]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10156385600566864
RAW KL tensor(0.1636, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 62: train/loss = 0.15710866451263428, train/raw-loss = 0.11566714942455292, train/logprobs = tensor([[ -4.4393, -11.7618],
        [ -2.0736,  -2.1153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13813836872577667
RAW KL tensor(0.0074, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 63: train/loss = 0.2869170308113098, train/raw-loss = 0.250030517578125, train/logprobs = tensor([[-4.8780, -9.5547],
        [-2.1638, -1.7314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12295513600111008
RAW KL tensor(0.0593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 64: train/loss = 0.07281371206045151, train/raw-loss = 0.034832607954740524, train/logprobs = tensor([[ -3.5384, -12.9857],
        [ -1.8831,  -1.4628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12660369277000427
RAW KL tensor(0.1557, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 65: train/loss = 0.11890025436878204, train/raw-loss = 0.08085606247186661, train/logprobs = tensor([[ -5.0144, -16.7857],
        [ -1.5511,  -1.4627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12681394815444946
RAW KL tensor(0.0709, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 66: train/loss = 0.5609926581382751, train/raw-loss = 0.5181705355644226, train/logprobs = tensor([[ -4.8278, -20.0868],
        [ -1.6841,  -1.7495]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14274026453495026
RAW KL tensor(0.0852, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 67: train/loss = 0.03558213636279106, train/raw-loss = 0.007019133307039738, train/logprobs = tensor([[ -2.1434, -15.8463],
        [ -0.7350,  -1.5167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0952100083231926
RAW KL tensor(0.1179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 68: train/loss = 0.05668505281209946, train/raw-loss = 0.01447921060025692, train/logprobs = tensor([[ -3.5724, -22.1763],
        [ -1.3532,  -2.4619]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14068612456321716
RAW KL tensor(0.2176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 69: train/loss = 0.07661373913288116, train/raw-loss = 0.02493402548134327, train/logprobs = tensor([[ -6.3480, -15.1109],
        [ -2.8223,  -1.8973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17226570844650269
RAW KL tensor(0.1582, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 70: train/loss = 0.031410977244377136, train/raw-loss = 0.0016163811087608337, train/logprobs = tensor([[ -1.9517, -16.7313],
        [ -0.8025,  -1.4953]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09931531548500061
RAW KL tensor(0.1348, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 71: train/loss = 0.09222018718719482, train/raw-loss = 0.04677855223417282, train/logprobs = tensor([[ -5.2040, -18.3505],
        [ -1.8648,  -1.6851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15147210657596588
