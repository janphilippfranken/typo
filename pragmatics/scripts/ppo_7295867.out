[2024-02-18 23:07:54,036][root][INFO] - beta: 0.01
[2024-02-18 23:07:54,036][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 20000 training examples...
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01 after each epoch.
train dataset has 19000 examples.
eval dataset has 1000 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01 after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01 after each epoch.
Epoch 0, Step 15: loss/train = 0.04409227892756462, logprobs/train = tensor([[-0.5966, -0.8092],
        [-0.5724, -0.7484]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.055760450661182404, logprobs/train = tensor([[-0.7943, -1.4819],
        [-0.8081, -1.4112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.04394383355975151, logprobs/train = tensor([[-1.0227, -0.8616],
        [-1.0066, -0.8327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.66835068538785e-06
