[2024-02-10 20:47:38,153] torch.distributed.run: [WARNING] 
[2024-02-10 20:47:38,153] torch.distributed.run: [WARNING] *****************************************
[2024-02-10 20:47:38,153] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-10 20:47:38,153] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240210_204741-miutil7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft_hh_cdpo
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/miutil7t
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240210_204741-7czlo66g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft_hh_cdpo
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/7czlo66g
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240210_204741-j4gx4ln1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft_hh_cdpo
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/j4gx4ln1
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240210_204741-udp9z9aj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft_hh_cdpo
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/udp9z9aj
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240210_204741-64bt798p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft_hh_cdpo
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/64bt798p
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240210_204741-q1p6gl8g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sft_hh_cdpo
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai/runs/q1p6gl8g
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.34s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.42s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.45s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.49s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.49s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:04<00:04,  4.53s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  2.99s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.19s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.04s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.25s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.06s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.27s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.30s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.30s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.11s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.32s/it]
Error executing job with overrides: []
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 47, in main
    trainer = BasicTrainer(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 243, in __init__
    self.accelerator = Accelerator(gradient_accumulation_steps=config.training.gradient_accumulation_steps)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 380, in __init__
    self.state = AcceleratorState(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/state.py", line 761, in __init__
    PartialState(cpu, **kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/state.py", line 204, in __init__
    torch.cuda.set_device(self.device)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/cuda/__init__.py", line 404, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: []
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 47, in main
    trainer = BasicTrainer(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 243, in __init__
    self.accelerator = Accelerator(gradient_accumulation_steps=config.training.gradient_accumulation_steps)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 380, in __init__
    self.state = AcceleratorState(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/state.py", line 761, in __init__
    PartialState(cpu, **kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/state.py", line 204, in __init__
    torch.cuda.set_device(self.device)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/cuda/__init__.py", line 404, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: CUDA error: invalid device ordinal
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb: üöÄ View run sft_hh_cdpo at: https://wandb.ai/janphilipp-franken/scai/runs/miutil7t
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODg1NzMyOA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240210_204741-miutil7t/logs
wandb: üöÄ View run sft_hh_cdpo at: https://wandb.ai/janphilipp-franken/scai/runs/64bt798p
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODg1NzMyOA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240210_204741-64bt798p/logs
Using /sailhome/jphilipp/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/jphilipp/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Error executing job with overrides: []
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 47, in main
    trainer = BasicTrainer(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 246, in __init__
    self.model, self.optimizer, self.train_dataloader, self.eval_dataloader, self.scheduler = self.accelerator.prepare(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1219, in prepare
    result = self._prepare_deepspeed(*args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1604, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/__init__.py", line 157, in initialize
    config_class = DeepSpeedConfig(config, mpu)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 785, in __init__
    self._initialize_params(copy.copy(self._param_dict))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 805, in _initialize_params
    self.zero_config = get_zero_config(param_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/config.py", line 70, in get_zero_config
    return DeepSpeedZeroConfig(**zero_config_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config_utils.py", line 56, in __init__
    super().__init__(**data)
  File "pydantic/main.py", line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 2 validation errors for DeepSpeedZeroConfig
offload_param -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])
offload_optimizer -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using /sailhome/jphilipp/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/jphilipp/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Error executing job with overrides: []
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 47, in main
    trainer = BasicTrainer(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 246, in __init__
    self.model, self.optimizer, self.train_dataloader, self.eval_dataloader, self.scheduler = self.accelerator.prepare(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1219, in prepare
    result = self._prepare_deepspeed(*args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1604, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/__init__.py", line 157, in initialize
    config_class = DeepSpeedConfig(config, mpu)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 785, in __init__
    self._initialize_params(copy.copy(self._param_dict))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 805, in _initialize_params
    self.zero_config = get_zero_config(param_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/config.py", line 70, in get_zero_config
    return DeepSpeedZeroConfig(**zero_config_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config_utils.py", line 56, in __init__
    super().__init__(**data)
  File "pydantic/main.py", line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 2 validation errors for DeepSpeedZeroConfig
offload_param -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])
offload_optimizer -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Using /sailhome/jphilipp/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/jphilipp/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Using /sailhome/jphilipp/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /sailhome/jphilipp/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Error executing job with overrides: []
Error executing job with overrides: []
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 47, in main
    trainer = BasicTrainer(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 246, in __init__
    self.model, self.optimizer, self.train_dataloader, self.eval_dataloader, self.scheduler = self.accelerator.prepare(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1219, in prepare
    result = self._prepare_deepspeed(*args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1604, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/__init__.py", line 157, in initialize
    config_class = DeepSpeedConfig(config, mpu)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 785, in __init__
    self._initialize_params(copy.copy(self._param_dict))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 805, in _initialize_params
    self.zero_config = get_zero_config(param_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/config.py", line 70, in get_zero_config
    return DeepSpeedZeroConfig(**zero_config_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config_utils.py", line 56, in __init__
    super().__init__(**data)
  File "pydantic/main.py", line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 2 validation errors for DeepSpeedZeroConfig
offload_param -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])
offload_optimizer -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 47, in main
    trainer = BasicTrainer(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 246, in __init__
    self.model, self.optimizer, self.train_dataloader, self.eval_dataloader, self.scheduler = self.accelerator.prepare(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1219, in prepare
    result = self._prepare_deepspeed(*args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/accelerator.py", line 1604, in _prepare_deepspeed
    engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/__init__.py", line 157, in initialize
    config_class = DeepSpeedConfig(config, mpu)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 785, in __init__
    self._initialize_params(copy.copy(self._param_dict))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config.py", line 805, in _initialize_params
    self.zero_config = get_zero_config(param_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/zero/config.py", line 70, in get_zero_config
    return DeepSpeedZeroConfig(**zero_config_dict)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/deepspeed/runtime/config_utils.py", line 56, in __init__
    super().__init__(**data)
  File "pydantic/main.py", line 341, in pydantic.main.BaseModel.__init__
pydantic.error_wrappers.ValidationError: 2 validation errors for DeepSpeedZeroConfig
offload_param -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])
offload_optimizer -> device
  value is not a valid enumeration member; permitted: 'none', 'cpu', 'nvme' (type=type_error.enum; enum_values=[<OffloadDeviceEnum.none: 'none'>, <OffloadDeviceEnum.cpu: 'cpu'>, <OffloadDeviceEnum.nvme: 'nvme'>])

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: üöÄ View run sft_hh_cdpo at: https://wandb.ai/janphilipp-franken/scai/runs/j4gx4ln1
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODg1NzMyOA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240210_204741-j4gx4ln1/logs
[2024-02-10 20:50:43,337] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 545375 closing signal SIGTERM
[2024-02-10 20:50:43,337] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 545376 closing signal SIGTERM
[2024-02-10 20:50:43,338] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 545377 closing signal SIGTERM
[2024-02-10 20:50:43,339] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 545378 closing signal SIGTERM
wandb: üöÄ View run sft_hh_cdpo at: https://wandb.ai/janphilipp-franken/scai/runs/q1p6gl8g
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODg1NzMyOA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240210_204741-q1p6gl8g/logs
wandb: üöÄ View run sft_hh_cdpo at: https://wandb.ai/janphilipp-franken/scai/runs/udp9z9aj
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODg1NzMyOA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240210_204741-udp9z9aj/logs
wandb: üöÄ View run sft_hh_cdpo at: https://wandb.ai/janphilipp-franken/scai/runs/7czlo66g
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzODg1NzMyOA==/version_details/v1
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240210_204741-7czlo66g/logs
[2024-02-10 20:50:44,670] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 4 (pid: 545379) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 47, in main
    args.func(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1008, in launch_command
    deepspeed_launcher(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/accelerate/commands/launch.py", line 724, in deepspeed_launcher
    distrib_run.run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-10_20:50:43
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 545380)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-10_20:50:43
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 545379)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
