[2024-02-26 17:26:29,043][root][INFO] - beta: 0.01
[2024-02-26 17:26:29,043][root][INFO] - loss with_labels
[2024-02-26 17:26:29,044][root][INFO] - max_iter: 0
[2024-02-26 17:26:29,044][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-per-token-kl
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-per-token-kl after each epoch.
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-per-token-kl after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-per-token-kl after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-per-token-kl after each epoch.
RAW KL tensor(0.0989, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 0: train/loss = 0.7012884020805359, train/raw-loss = 0.700703501701355, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05849311128258705
RAW KL tensor(0.0533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 1: train/loss = 0.6951550245285034, train/raw-loss = 0.6946866512298584, train/logprobs = tensor([[-1.2483, -1.6199],
        [-1.2440, -1.6139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046839892864227295
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 2: train/loss = 0.6925154328346252, train/raw-loss = 0.6923112869262695, train/logprobs = tensor([[-1.0892, -2.0869],
        [-1.1206, -2.1103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02041739784181118
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 3: train/loss = 0.6865567564964294, train/raw-loss = 0.6862989068031311, train/logprobs = tensor([[-0.5922, -1.4420],
        [-0.5997, -1.4088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025789767503738403
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 4: train/loss = 0.6825052499771118, train/raw-loss = 0.6821818947792053, train/logprobs = tensor([[-0.9682, -2.2517],
        [-1.0031, -2.2372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032332804054021835
RAW KL tensor(0.2229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 5: train/loss = 0.6999635696411133, train/raw-loss = 0.6991856098175049, train/logprobs = tensor([[-1.2526, -1.8470],
        [-1.2661, -1.8698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07779429852962494
RAW KL tensor(0.0653, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 6: train/loss = 0.691232442855835, train/raw-loss = 0.6907837390899658, train/logprobs = tensor([[-0.9823, -1.5991],
        [-1.0011, -1.5974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04487462714314461
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 7: train/loss = 0.7026811838150024, train/raw-loss = 0.7020244598388672, train/logprobs = tensor([[-1.0630, -1.5507],
        [-1.1077, -1.6226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06567401438951492
RAW KL tensor(0.0341, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 8: train/loss = 0.6933778524398804, train/raw-loss = 0.6930682063102722, train/logprobs = tensor([[-0.8023, -1.7333],
        [-0.7871, -1.7088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030963294208049774
RAW KL tensor(0.0381, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 9: train/loss = 0.6967381834983826, train/raw-loss = 0.6962793469429016, train/logprobs = tensor([[-1.0835, -1.7181],
        [-1.0813, -1.6973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04588749259710312
RAW KL tensor(0.2401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 10: train/loss = 0.6974309682846069, train/raw-loss = 0.6965494155883789, train/logprobs = tensor([[-1.1971, -2.5060],
        [-1.2764, -2.5027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08815398067235947
RAW KL tensor(0.0386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 11: train/loss = 0.6866023540496826, train/raw-loss = 0.6862969398498535, train/logprobs = tensor([[-0.7466, -1.5640],
        [-0.7228, -1.5107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030534276738762856
RAW KL tensor(0.0910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 12: train/loss = 0.7153841257095337, train/raw-loss = 0.7148674726486206, train/logprobs = tensor([[-0.7433, -1.8789],
        [-0.7825, -1.8867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05166593939065933
RAW KL tensor(0.0755, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 13: train/loss = 0.6873892545700073, train/raw-loss = 0.6869643926620483, train/logprobs = tensor([[-0.7519, -1.6611],
        [-0.7745, -1.6382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04248277097940445
RAW KL tensor(0.0711, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 14: train/loss = 0.6854458451271057, train/raw-loss = 0.684917688369751, train/logprobs = tensor([[-1.1222, -1.7514],
        [-1.1230, -1.7108]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05281598120927811
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 15: train/loss = 0.7038630247116089, train/raw-loss = 0.7035808563232422, train/logprobs = tensor([[-1.0901, -1.5558],
        [-1.0901, -1.5823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02820965088903904
RAW KL tensor(0.2291, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 16: train/loss = 0.7067016363143921, train/raw-loss = 0.705668568611145, train/logprobs = tensor([[-0.9616, -2.0296],
        [-0.9940, -1.9801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10330930352210999
RAW KL tensor(0.0085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 17: train/loss = 0.6928829550743103, train/raw-loss = 0.6924867033958435, train/logprobs = tensor([[-1.0904, -1.5065],
        [-1.0878, -1.4924]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03962227329611778
RAW KL tensor(0.0882, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 18: train/loss = 0.6932921409606934, train/raw-loss = 0.6928467154502869, train/logprobs = tensor([[-1.1498, -1.6923],
        [-1.1412, -1.6685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04454222694039345
RAW KL tensor(0.0408, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 19: train/loss = 0.6978036165237427, train/raw-loss = 0.6973589658737183, train/logprobs = tensor([[-0.7575, -1.3558],
        [-0.7511, -1.3457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04445846751332283
RAW KL tensor(0.0451, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 20: train/loss = 0.752029538154602, train/raw-loss = 0.751427173614502, train/logprobs = tensor([[-0.6982, -1.6722],
        [-0.7384, -1.6710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0602305606007576
RAW KL tensor(0.0190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 21: train/loss = 0.6947212219238281, train/raw-loss = 0.6941919326782227, train/logprobs = tensor([[-0.9935, -1.2997],
        [-0.9919, -1.2949]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052927035838365555
RAW KL tensor(0.0285, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 22: train/loss = 0.6885553598403931, train/raw-loss = 0.6880555152893066, train/logprobs = tensor([[-1.1048, -1.2115],
        [-1.1166, -1.1903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049987539649009705
RAW KL tensor(0.1180, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 23: train/loss = 0.7257606983184814, train/raw-loss = 0.7249387502670288, train/logprobs = tensor([[-1.1101, -1.9440],
        [-1.1279, -2.0090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08219575881958008
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 24: train/loss = 0.690395176410675, train/raw-loss = 0.6900442838668823, train/logprobs = tensor([[-0.9510, -2.1814],
        [-0.9490, -2.1607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03509220853447914
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 25: train/loss = 0.6856286525726318, train/raw-loss = 0.6853428483009338, train/logprobs = tensor([[-0.8277, -2.2563],
        [-0.8247, -2.2167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028583889827132225
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 26: train/loss = 0.6984255909919739, train/raw-loss = 0.6981753706932068, train/logprobs = tensor([[-0.5759, -2.1066],
        [-0.6087, -2.1473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025020746514201164
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 27: train/loss = 0.6978297233581543, train/raw-loss = 0.6973809003829956, train/logprobs = tensor([[-1.0402, -2.5564],
        [-1.0804, -2.5702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04488641396164894
RAW KL tensor(0.0764, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 28: train/loss = 0.6985744833946228, train/raw-loss = 0.6975324153900146, train/logprobs = tensor([[-0.8392, -1.7205],
        [-0.8369, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1042046993970871
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 29: train/loss = 0.6936971545219421, train/raw-loss = 0.6934842467308044, train/logprobs = tensor([[-0.5879, -1.8914],
        [-0.5988, -1.9005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021290477365255356
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 219])
Epoch 0, Step 30: train/loss = 0.6921564340591431, train/raw-loss = 0.6918355822563171, train/logprobs = tensor([[-0.8561, -1.1920],
        [-0.8388, -1.1585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03208891302347183
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 31: train/loss = 0.6731812953948975, train/raw-loss = 0.672492265701294, train/logprobs = tensor([[-1.0550, -2.2670],
        [-1.0818, -2.1798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06890882551670074
RAW KL tensor(0.0422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 32: train/loss = 0.6891586184501648, train/raw-loss = 0.6886965036392212, train/logprobs = tensor([[-0.9240, -1.9170],
        [-0.9467, -1.8934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04621318727731705
RAW KL tensor(0.0468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 33: train/loss = 0.7014726996421814, train/raw-loss = 0.7009933590888977, train/logprobs = tensor([[-0.8779, -1.8828],
        [-0.8573, -1.8740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047930438071489334
RAW KL tensor(0.0315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 34: train/loss = 0.6963070034980774, train/raw-loss = 0.6958221793174744, train/logprobs = tensor([[-1.0511, -1.5128],
        [-1.0826, -1.5036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04848434031009674
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 35: train/loss = 0.7044469118118286, train/raw-loss = 0.7038596868515015, train/logprobs = tensor([[-0.9902, -1.8427],
        [-0.9916, -1.8163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05871553719043732
RAW KL tensor(0.0617, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 36: train/loss = 0.7001550197601318, train/raw-loss = 0.6996639966964722, train/logprobs = tensor([[-1.2729, -1.4568],
        [-1.2776, -1.4303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04909990355372429
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 235])
Epoch 0, Step 37: train/loss = 0.6886305809020996, train/raw-loss = 0.6881989240646362, train/logprobs = tensor([[-0.6132, -1.4763],
        [-0.6105, -1.4501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0431632362306118
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 292])
Epoch 0, Step 38: train/loss = 0.6920983791351318, train/raw-loss = 0.6917117834091187, train/logprobs = tensor([[-0.8017, -1.5622],
        [-0.8207, -1.5360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038653694093227386
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 39: train/loss = 0.6976761817932129, train/raw-loss = 0.6974338889122009, train/logprobs = tensor([[-0.6779, -1.3967],
        [-0.6694, -1.3926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02422407455742359
RAW KL tensor(0.0270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 40: train/loss = 0.6942631006240845, train/raw-loss = 0.6939688920974731, train/logprobs = tensor([[-0.7632, -1.1195],
        [-0.7654, -1.1192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029419410973787308
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 329])
Epoch 0, Step 41: train/loss = 0.6800301671028137, train/raw-loss = 0.6795097589492798, train/logprobs = tensor([[-0.8216, -1.7078],
        [-0.8400, -1.6511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05203207954764366
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 42: train/loss = 0.6949127912521362, train/raw-loss = 0.694367527961731, train/logprobs = tensor([[-0.6560, -1.7110],
        [-0.6654, -1.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05452563613653183
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 43: train/loss = 0.6871495246887207, train/raw-loss = 0.6866177916526794, train/logprobs = tensor([[-1.0934, -2.2801],
        [-1.0771, -2.2323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05317414551973343
RAW KL tensor(0.0493, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 44: train/loss = 0.690984845161438, train/raw-loss = 0.6903976798057556, train/logprobs = tensor([[-0.9415, -1.1740],
        [-0.9243, -1.1435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05871118605136871
RAW KL tensor(0.0539, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 45: train/loss = 0.7046841382980347, train/raw-loss = 0.7042211294174194, train/logprobs = tensor([[-0.8766, -1.8393],
        [-0.8959, -1.8327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046301472932100296
RAW KL tensor(0.1295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 46: train/loss = 0.7236000895500183, train/raw-loss = 0.7230007648468018, train/logprobs = tensor([[-1.1262, -2.6505],
        [-1.1680, -2.7227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059928908944129944
RAW KL tensor(0.0574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 47: train/loss = 0.6943569183349609, train/raw-loss = 0.6937203407287598, train/logprobs = tensor([[-0.7199, -1.4910],
        [-0.7111, -1.4710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06366247683763504
RAW KL tensor(0.1064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 48: train/loss = 0.7076858878135681, train/raw-loss = 0.7070211172103882, train/logprobs = tensor([[-1.3350, -1.4101],
        [-1.3527, -1.4624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0664793998003006
RAW KL tensor(0.0460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 49: train/loss = 0.6919316053390503, train/raw-loss = 0.6914311051368713, train/logprobs = tensor([[-0.8395, -1.8763],
        [-0.8370, -1.8425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05005505308508873
RAW KL tensor(0.0196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 50: train/loss = 0.6913439035415649, train/raw-loss = 0.6909155249595642, train/logprobs = tensor([[-1.0832, -1.3937],
        [-1.0782, -1.3770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042835790663957596
RAW KL tensor(0.0463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 51: train/loss = 0.6863089203834534, train/raw-loss = 0.6857802867889404, train/logprobs = tensor([[-0.9913, -1.7751],
        [-0.9906, -1.7283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052864715456962585
RAW KL tensor(0.0419, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 52: train/loss = 0.6877902150154114, train/raw-loss = 0.687508761882782, train/logprobs = tensor([[-0.7081, -0.7210],
        [-0.7282, -0.7153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028142552822828293
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 53: train/loss = 0.7090991139411926, train/raw-loss = 0.7088227868080139, train/logprobs = tensor([[-0.6088, -1.0795],
        [-0.5906, -1.0657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02763647772371769
RAW KL tensor(0.0467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 54: train/loss = 0.6830307245254517, train/raw-loss = 0.682507336139679, train/logprobs = tensor([[-1.2230, -1.6885],
        [-1.2433, -1.6452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052336983382701874
RAW KL tensor(0.0632, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 55: train/loss = 0.7040861248970032, train/raw-loss = 0.7028936743736267, train/logprobs = tensor([[-1.3000, -1.1052],
        [-1.3418, -1.1073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11924703419208527
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 56: train/loss = 0.6826920509338379, train/raw-loss = 0.6819606423377991, train/logprobs = tensor([[-0.8773, -1.4545],
        [-0.8995, -1.4236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07313912361860275
RAW KL tensor(0.0324, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 57: train/loss = 0.6855973601341248, train/raw-loss = 0.6851555109024048, train/logprobs = tensor([[-0.6412, -1.4265],
        [-0.6492, -1.3568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044186726212501526
RAW KL tensor(0.0098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 335])
Epoch 0, Step 58: train/loss = 0.673940122127533, train/raw-loss = 0.6735333204269409, train/logprobs = tensor([[-0.6794, -1.6542],
        [-0.6772, -1.5657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04068746790289879
RAW KL tensor(0.0064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 364])
Epoch 0, Step 59: train/loss = 0.6922350525856018, train/raw-loss = 0.6919022798538208, train/logprobs = tensor([[-0.8279, -1.2489],
        [-0.8414, -1.2448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03328267112374306
RAW KL tensor(0.0445, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 60: train/loss = 0.6918540596961975, train/raw-loss = 0.6914083957672119, train/logprobs = tensor([[-1.1025, -1.5208],
        [-1.0788, -1.4796]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04457211494445801
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 61: train/loss = 0.677606999874115, train/raw-loss = 0.6773156523704529, train/logprobs = tensor([[-0.5136, -1.4002],
        [-0.5257, -1.3338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029135417193174362
RAW KL tensor(0.0554, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 62: train/loss = 0.7097975611686707, train/raw-loss = 0.7092434763908386, train/logprobs = tensor([[-1.1946, -1.5974],
        [-1.1819, -1.5757]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055404916405677795
RAW KL tensor(0.0020, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 63: train/loss = 0.6967833042144775, train/raw-loss = 0.6964080333709717, train/logprobs = tensor([[-0.9767, -1.3460],
        [-0.9932, -1.3672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03752182796597481
RAW KL tensor(0.0663, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 64: train/loss = 0.6904047131538391, train/raw-loss = 0.6899641752243042, train/logprobs = tensor([[-0.9263, -1.6361],
        [-0.9205, -1.6097]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0440509058535099
RAW KL tensor(0.0481, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 65: train/loss = 0.6952937841415405, train/raw-loss = 0.6948447227478027, train/logprobs = tensor([[-0.8732, -1.5642],
        [-0.8636, -1.5526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044907957315444946
RAW KL tensor(0.0468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 66: train/loss = 0.7109975218772888, train/raw-loss = 0.710561215877533, train/logprobs = tensor([[-1.0520, -1.9491],
        [-1.0002, -1.9051]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04363113269209862
RAW KL tensor(0.0115, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 67: train/loss = 0.6939446926116943, train/raw-loss = 0.693669319152832, train/logprobs = tensor([[-0.6819, -1.8637],
        [-0.6696, -1.8171]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027540916576981544
RAW KL tensor(0.0060, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 68: train/loss = 0.7030650973320007, train/raw-loss = 0.7028182744979858, train/logprobs = tensor([[-0.8002, -2.7904],
        [-0.7831, -2.7557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024689074605703354
RAW KL tensor(0.0540, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 69: train/loss = 0.6880326271057129, train/raw-loss = 0.6873388290405273, train/logprobs = tensor([[-1.2738, -1.7102],
        [-1.2815, -1.6861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06938108801841736
RAW KL tensor(0.0204, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 70: train/loss = 0.6901374459266663, train/raw-loss = 0.6899594664573669, train/logprobs = tensor([[-0.5962, -1.9336],
        [-0.5912, -1.9044]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017797496169805527
RAW KL tensor(0.0531, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 71: train/loss = 0.6903543472290039, train/raw-loss = 0.6897768974304199, train/logprobs = tensor([[-1.0455, -1.9599],
        [-1.0358, -1.8803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0577419176697731
RAW KL tensor(0.0154, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 261])
Epoch 0, Step 72: train/loss = 0.6829736232757568, train/raw-loss = 0.6827189922332764, train/logprobs = tensor([[-0.4517, -1.9828],
        [-0.4544, -1.9352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025464776903390884
RAW KL tensor(0.0141, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 73: train/loss = 0.696906328201294, train/raw-loss = 0.6966177225112915, train/logprobs = tensor([[-0.7369, -1.1133],
        [-0.7708, -1.1433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028859399259090424
RAW KL tensor(0.0093, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 74: train/loss = 0.6966696381568909, train/raw-loss = 0.6963196992874146, train/logprobs = tensor([[-1.5981, -2.0020],
        [-1.5767, -1.9838]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03499604016542435
RAW KL tensor(0.0565, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 75: train/loss = 0.6920596957206726, train/raw-loss = 0.6916537284851074, train/logprobs = tensor([[-0.7796, -1.8943],
        [-0.7714, -1.8720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040598783642053604
RAW KL tensor(0.0284, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 76: train/loss = 0.6704718470573425, train/raw-loss = 0.6700690984725952, train/logprobs = tensor([[-0.9777, -1.5843],
        [-1.0509, -1.5459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040274009108543396
RAW KL tensor(0.0610, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 77: train/loss = 0.6928794980049133, train/raw-loss = 0.6922866106033325, train/logprobs = tensor([[-1.2750, -1.4930],
        [-1.2862, -1.4910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059286508709192276
RAW KL tensor(0.0265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 78: train/loss = 0.6849913597106934, train/raw-loss = 0.6845899224281311, train/logprobs = tensor([[-0.8869, -1.9019],
        [-0.8956, -1.8709]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040143851190805435
RAW KL tensor(0.0425, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 257])
Epoch 0, Step 79: train/loss = 0.6856880187988281, train/raw-loss = 0.6854345202445984, train/logprobs = tensor([[-0.5487, -1.2711],
        [-0.5516, -1.2343]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02534342184662819
RAW KL tensor(0.0280, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 80: train/loss = 0.6848790645599365, train/raw-loss = 0.6844770908355713, train/logprobs = tensor([[-1.0760, -2.1449],
        [-1.0760, -2.1038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04019489511847496
RAW KL tensor(0.0777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 81: train/loss = 0.703036904335022, train/raw-loss = 0.7025901675224304, train/logprobs = tensor([[-0.8242, -1.8966],
        [-0.8386, -1.9432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04467208310961723
RAW KL tensor(0.0503, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 82: train/loss = 0.6962107419967651, train/raw-loss = 0.6955835819244385, train/logprobs = tensor([[-1.2392, -1.0704],
        [-1.2422, -1.0612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06271198391914368
RAW KL tensor(0.0201, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 83: train/loss = 0.6917614936828613, train/raw-loss = 0.6914771199226379, train/logprobs = tensor([[-0.6380, -1.7151],
        [-0.6395, -1.6678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028434891253709793
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 344])
Epoch 0, Step 84: train/loss = 0.68636554479599, train/raw-loss = 0.6861332654953003, train/logprobs = tensor([[-0.5642, -2.0541],
        [-0.5590, -2.0173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023229533806443214
RAW KL tensor(0.0411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 85: train/loss = 0.7032700777053833, train/raw-loss = 0.7019620537757874, train/logprobs = tensor([[-0.9204, -2.1815],
        [-0.9300, -2.1860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13080327212810516
RAW KL tensor(0.0191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 86: train/loss = 0.6869778037071228, train/raw-loss = 0.6865781545639038, train/logprobs = tensor([[-0.6516, -1.8371],
        [-0.6546, -1.8059]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039963722229003906
RAW KL tensor(0.0317, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 87: train/loss = 0.7061035633087158, train/raw-loss = 0.7056586742401123, train/logprobs = tensor([[-1.3951, -1.5924],
        [-1.3877, -1.6200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044486239552497864
RAW KL tensor(0.0093, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 88: train/loss = 0.6997865438461304, train/raw-loss = 0.6992999911308289, train/logprobs = tensor([[-0.8133, -2.0623],
        [-0.8157, -2.0305]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048654213547706604
RAW KL tensor(0.0331, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 89: train/loss = 0.6974307894706726, train/raw-loss = 0.6969251036643982, train/logprobs = tensor([[-1.1086, -1.9215],
        [-1.0998, -1.8607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05057206749916077
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 90: train/loss = 0.6934877038002014, train/raw-loss = 0.6932564377784729, train/logprobs = tensor([[-0.8521, -1.8093],
        [-0.8935, -1.8295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023120339959859848
RAW KL tensor(0.1624, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 91: train/loss = 0.6877721548080444, train/raw-loss = 0.6871531009674072, train/logprobs = tensor([[-1.3090, -1.1750],
        [-1.3383, -1.1793]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061910148710012436
RAW KL tensor(0.0587, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 92: train/loss = 0.691642165184021, train/raw-loss = 0.6909563541412354, train/logprobs = tensor([[-0.9168, -1.7282],
        [-0.9825, -1.7274]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0685814917087555
RAW KL tensor(0.0380, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 93: train/loss = 0.6999514102935791, train/raw-loss = 0.6996270418167114, train/logprobs = tensor([[-1.0062, -1.4914],
        [-1.0173, -1.5012]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03243829309940338
RAW KL tensor(0.1462, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 94: train/loss = 0.7104413509368896, train/raw-loss = 0.7097605466842651, train/logprobs = tensor([[-1.2730, -2.7408],
        [-1.3163, -2.7096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0680842250585556
RAW KL tensor(0.0477, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 95: train/loss = 0.6919258236885071, train/raw-loss = 0.6912577748298645, train/logprobs = tensor([[-1.5593, -2.2820],
        [-1.5748, -2.2357]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06679873168468475
RAW KL tensor(0.0080, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 96: train/loss = 0.6916790008544922, train/raw-loss = 0.691291093826294, train/logprobs = tensor([[-0.8705, -1.7032],
        [-0.8668, -1.6821]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03879181295633316
RAW KL tensor(0.0187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 97: train/loss = 0.7016493678092957, train/raw-loss = 0.7012842297554016, train/logprobs = tensor([[-1.7931, -2.7687],
        [-1.7869, -2.7886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03651369735598564
RAW KL tensor(0.0318, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 98: train/loss = 0.6928791999816895, train/raw-loss = 0.6926079988479614, train/logprobs = tensor([[-1.0869, -1.6180],
        [-1.0983, -1.6032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027126047760248184
RAW KL tensor(0.0714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 99: train/loss = 0.6917019486427307, train/raw-loss = 0.6912419199943542, train/logprobs = tensor([[-0.8609, -1.5719],
        [-0.8570, -1.5489]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04599890112876892
RAW KL tensor(0.0720, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 100: train/loss = 0.6904800534248352, train/raw-loss = 0.6900198459625244, train/logprobs = tensor([[-1.0170, -1.3528],
        [-1.0141, -1.3128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046013474464416504
RAW KL tensor(0.0178, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 101: train/loss = 0.684394121170044, train/raw-loss = 0.6840503215789795, train/logprobs = tensor([[-1.1557, -2.3759],
        [-1.1512, -2.3111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034372828900814056
RAW KL tensor(0.0380, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 102: train/loss = 0.6711970567703247, train/raw-loss = 0.6707553863525391, train/logprobs = tensor([[-0.7618, -1.7901],
        [-0.7430, -1.6710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04416676610708237
RAW KL tensor(0.0567, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 103: train/loss = 0.6809406280517578, train/raw-loss = 0.6803330779075623, train/logprobs = tensor([[-0.8684, -1.7523],
        [-0.8952, -1.7089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060749586671590805
RAW KL tensor(0.0222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 104: train/loss = 0.687289297580719, train/raw-loss = 0.687110424041748, train/logprobs = tensor([[-0.8405, -1.5212],
        [-0.8403, -1.4759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017886387184262276
RAW KL tensor(0.0069, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 105: train/loss = 0.6900944709777832, train/raw-loss = 0.6897180676460266, train/logprobs = tensor([[-0.9835, -1.7935],
        [-0.9870, -1.7629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0376407764852047
RAW KL tensor(0.0158, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 106: train/loss = 0.6868875026702881, train/raw-loss = 0.6866775751113892, train/logprobs = tensor([[-0.7354, -1.7991],
        [-0.7069, -1.7304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020989390090107918
RAW KL tensor(0.0520, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 107: train/loss = 0.6951413154602051, train/raw-loss = 0.6945247650146484, train/logprobs = tensor([[-1.2862, -1.6873],
        [-1.3041, -1.6689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061650585383176804
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 108: train/loss = 0.6911971569061279, train/raw-loss = 0.6907690763473511, train/logprobs = tensor([[-0.7405, -0.8685],
        [-0.7378, -0.8381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042800478637218475
RAW KL tensor(0.0188, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 109: train/loss = 0.6971048712730408, train/raw-loss = 0.6968035101890564, train/logprobs = tensor([[-0.8073, -1.7850],
        [-0.8071, -1.7499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030136536806821823
RAW KL tensor(0.0565, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 110: train/loss = 0.6733492612838745, train/raw-loss = 0.6726171970367432, train/logprobs = tensor([[-0.9589, -2.1743],
        [-0.9288, -2.0481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07320556044578552
RAW KL tensor(0.0257, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 111: train/loss = 0.6991626024246216, train/raw-loss = 0.6989203095436096, train/logprobs = tensor([[-0.9152, -1.7955],
        [-0.9079, -1.8016]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024230992421507835
RAW KL tensor(0.0554, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 112: train/loss = 0.6968181133270264, train/raw-loss = 0.6962885856628418, train/logprobs = tensor([[-1.4829, -1.4339],
        [-1.5019, -1.4596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05295422673225403
RAW KL tensor(0.0289, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 113: train/loss = 0.6882978677749634, train/raw-loss = 0.6877992749214172, train/logprobs = tensor([[-1.0454, -1.4045],
        [-1.0305, -1.3610]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04984912648797035
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 353])
Epoch 0, Step 114: train/loss = 0.6877930760383606, train/raw-loss = 0.6875618696212769, train/logprobs = tensor([[-0.9348, -1.1853],
        [-0.9423, -1.1571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023117272183299065
RAW KL tensor(0.0507, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 115: train/loss = 0.6655760407447815, train/raw-loss = 0.6653377413749695, train/logprobs = tensor([[-0.7617, -1.7874],
        [-0.7380, -1.6418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023833630606532097
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 116: train/loss = 0.691371500492096, train/raw-loss = 0.6912192106246948, train/logprobs = tensor([[-0.6410, -1.4403],
        [-0.6480, -1.4385]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015233742073178291
RAW KL tensor(0.0155, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 117: train/loss = 0.6796227693557739, train/raw-loss = 0.6792714595794678, train/logprobs = tensor([[-0.9055, -1.7854],
        [-0.9148, -1.7152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0351368673145771
RAW KL tensor(0.0444, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 118: train/loss = 0.6856503486633301, train/raw-loss = 0.6854034662246704, train/logprobs = tensor([[-0.9820, -1.9321],
        [-1.0247, -1.9245]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024682853370904922
RAW KL tensor(0.1314, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 119: train/loss = 0.6977431774139404, train/raw-loss = 0.6971122026443481, train/logprobs = tensor([[-1.4230, -1.8218],
        [-1.4505, -1.8519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06309327483177185
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 120: train/loss = 0.7220796942710876, train/raw-loss = 0.7214913368225098, train/logprobs = tensor([[-1.1278, -2.1810],
        [-1.1117, -2.1933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05884099006652832
RAW KL tensor(0.0933, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 121: train/loss = 0.6904133558273315, train/raw-loss = 0.6900230050086975, train/logprobs = tensor([[-0.7475, -1.1891],
        [-0.7594, -1.1678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03903660178184509
RAW KL tensor(0.0168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 122: train/loss = 0.6903829574584961, train/raw-loss = 0.6900507211685181, train/logprobs = tensor([[-1.0430, -2.7964],
        [-1.0223, -2.7574]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03322792425751686
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 123: train/loss = 0.6898825168609619, train/raw-loss = 0.6895137429237366, train/logprobs = tensor([[-1.1873, -1.4198],
        [-1.1883, -1.3768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036879878491163254
RAW KL tensor(0.0204, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 124: train/loss = 0.6890449523925781, train/raw-loss = 0.6888111233711243, train/logprobs = tensor([[-0.6308, -1.4056],
        [-0.5832, -1.3212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02338295616209507
RAW KL tensor(0.0926, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 125: train/loss = 0.6801995038986206, train/raw-loss = 0.6795556545257568, train/logprobs = tensor([[-0.8304, -2.3628],
        [-0.8654, -2.3249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06438986957073212
RAW KL tensor(0.0551, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 126: train/loss = 0.695023775100708, train/raw-loss = 0.6947462558746338, train/logprobs = tensor([[-1.1804, -2.9301],
        [-1.2335, -2.9462]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027752745896577835
RAW KL tensor(0.0291, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 127: train/loss = 0.6860515475273132, train/raw-loss = 0.6856255531311035, train/logprobs = tensor([[-0.9778, -1.8415],
        [-0.9322, -1.7548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04260042682290077
RAW KL tensor(0.0430, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 128: train/loss = 0.7117312550544739, train/raw-loss = 0.7114008069038391, train/logprobs = tensor([[-1.0283, -1.5286],
        [-1.0046, -1.5581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033048346638679504
RAW KL tensor(0.0476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 129: train/loss = 0.7053354382514954, train/raw-loss = 0.7048170566558838, train/logprobs = tensor([[-1.1439, -1.9648],
        [-1.1360, -1.9843]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05183485150337219
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 430])
Epoch 0, Step 130: train/loss = 0.6976190805435181, train/raw-loss = 0.6972501277923584, train/logprobs = tensor([[-1.2154, -2.6517],
        [-1.1600, -2.5715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036900945007801056
RAW KL tensor(0.0149, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 131: train/loss = 0.6872280836105347, train/raw-loss = 0.6868311166763306, train/logprobs = tensor([[-1.5686, -1.4870],
        [-1.5987, -1.4761]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03969068452715874
RAW KL tensor(0.0124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 132: train/loss = 0.6902470588684082, train/raw-loss = 0.6895312666893005, train/logprobs = tensor([[-1.0207, -2.1859],
        [-1.1244, -2.2096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0715785026550293
RAW KL tensor(0.0567, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 133: train/loss = 0.691275417804718, train/raw-loss = 0.6908484697341919, train/logprobs = tensor([[-1.4804, -1.8524],
        [-1.4993, -1.8560]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042694006115198135
RAW KL tensor(0.0334, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 134: train/loss = 0.6850186586380005, train/raw-loss = 0.6847789287567139, train/logprobs = tensor([[-1.0396, -1.7180],
        [-1.0449, -1.6555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023975420743227005
RAW KL tensor(0.0214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 135: train/loss = 0.6861733198165894, train/raw-loss = 0.6858788132667542, train/logprobs = tensor([[-1.5312, -1.6234],
        [-1.5212, -1.5798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029448626562952995
RAW KL tensor(0.0285, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 136: train/loss = 0.6777311563491821, train/raw-loss = 0.677179753780365, train/logprobs = tensor([[-1.5444, -2.3548],
        [-1.5757, -2.2704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055143121629953384
RAW KL tensor(0.0590, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 137: train/loss = 0.6836367845535278, train/raw-loss = 0.6831316947937012, train/logprobs = tensor([[-1.6197, -1.5180],
        [-1.6430, -1.4967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05050276592373848
RAW KL tensor(0.0481, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 138: train/loss = 0.6910220980644226, train/raw-loss = 0.690771222114563, train/logprobs = tensor([[-1.1996, -2.0226],
        [-1.1529, -1.9489]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02508174255490303
RAW KL tensor(0.0137, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 139: train/loss = 0.7125755548477173, train/raw-loss = 0.7115408182144165, train/logprobs = tensor([[-1.1617, -2.4957],
        [-1.1860, -2.4824]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10347149521112442
RAW KL tensor(0.0189, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 140: train/loss = 0.6877224445343018, train/raw-loss = 0.6875033378601074, train/logprobs = tensor([[-1.0581, -2.2605],
        [-1.0730, -2.2353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02191597782075405
RAW KL tensor(0.0153, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 141: train/loss = 0.6817000508308411, train/raw-loss = 0.6814308762550354, train/logprobs = tensor([[-0.8710, -2.1167],
        [-0.8564, -2.0400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02691577561199665
RAW KL tensor(0.0353, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 142: train/loss = 0.6779554486274719, train/raw-loss = 0.6775135397911072, train/logprobs = tensor([[-1.3406, -1.9256],
        [-1.3522, -1.8566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044188328087329865
RAW KL tensor(0.0306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 143: train/loss = 0.6862354278564453, train/raw-loss = 0.6859237551689148, train/logprobs = tensor([[-1.5866, -1.7599],
        [-1.6126, -1.7199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031170452013611794
RAW KL tensor(0.0155, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 144: train/loss = 0.6896511316299438, train/raw-loss = 0.6894298791885376, train/logprobs = tensor([[-0.9471, -1.8416],
        [-0.9368, -1.7812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02212398685514927
RAW KL tensor(0.0515, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 145: train/loss = 0.6940630674362183, train/raw-loss = 0.693633496761322, train/logprobs = tensor([[-1.1675, -1.5838],
        [-1.1537, -1.5312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04295439273118973
RAW KL tensor(0.0133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 146: train/loss = 0.6848307847976685, train/raw-loss = 0.6844964623451233, train/logprobs = tensor([[-0.7833, -1.8363],
        [-0.7935, -1.7900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03343334421515465
RAW KL tensor(0.0287, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 147: train/loss = 0.6829537153244019, train/raw-loss = 0.6824808120727539, train/logprobs = tensor([[-1.3333, -1.8136],
        [-1.3186, -1.7475]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04728926718235016
RAW KL tensor(0.0301, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 148: train/loss = 0.6852102875709534, train/raw-loss = 0.6850422620773315, train/logprobs = tensor([[-1.0152, -1.4769],
        [-1.0180, -1.4421]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01679954305291176
RAW KL tensor(0.0091, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 149: train/loss = 0.6865414381027222, train/raw-loss = 0.6864089369773865, train/logprobs = tensor([[-1.0027, -1.7964],
        [-0.9976, -1.7640]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013252828270196915
RAW KL tensor(0.0766, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 150: train/loss = 0.6866767406463623, train/raw-loss = 0.6862710118293762, train/logprobs = tensor([[-1.1216, -1.8069],
        [-1.0824, -1.7329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04058033227920532
RAW KL tensor(0.0266, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 151: train/loss = 0.6998077034950256, train/raw-loss = 0.6996060609817505, train/logprobs = tensor([[-0.9393, -1.4932],
        [-0.9357, -1.4918]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020172908902168274
RAW KL tensor(0.0165, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 152: train/loss = 0.6846639513969421, train/raw-loss = 0.6844095587730408, train/logprobs = tensor([[-0.9576, -1.9739],
        [-0.9405, -1.9180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025444025173783302
RAW KL tensor(0.0265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 153: train/loss = 0.7002366781234741, train/raw-loss = 0.6998730897903442, train/logprobs = tensor([[-1.0865, -1.7831],
        [-1.0805, -1.7895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03636212646961212
RAW KL tensor(0.0263, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 154: train/loss = 0.6884204149246216, train/raw-loss = 0.6880753040313721, train/logprobs = tensor([[-1.3893, -2.1426],
        [-1.3933, -2.1056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03451327979564667
RAW KL tensor(0.0519, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 155: train/loss = 0.6904230117797852, train/raw-loss = 0.6899831295013428, train/logprobs = tensor([[-0.8926, -1.6926],
        [-0.8745, -1.6572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04399046301841736
RAW KL tensor(0.0539, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 156: train/loss = 0.6848867535591125, train/raw-loss = 0.6845393180847168, train/logprobs = tensor([[-1.0663, -2.3060],
        [-1.0463, -2.2446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03474752977490425
RAW KL tensor(0.0099, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 157: train/loss = 0.6928278207778931, train/raw-loss = 0.6926243305206299, train/logprobs = tensor([[-0.9887, -1.3393],
        [-0.9628, -1.2933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02034739963710308
RAW KL tensor(0.0054, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 158: train/loss = 0.6772266626358032, train/raw-loss = 0.6770004034042358, train/logprobs = tensor([[-0.8346, -0.8294],
        [-0.8448, -0.7620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02262626402080059
RAW KL tensor(0.0664, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 159: train/loss = 0.6861056089401245, train/raw-loss = 0.6857181191444397, train/logprobs = tensor([[-0.8596, -1.3322],
        [-0.8428, -1.2634]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03874686360359192
RAW KL tensor(0.0154, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 160: train/loss = 0.6864306926727295, train/raw-loss = 0.6861014366149902, train/logprobs = tensor([[-0.8506, -1.3718],
        [-0.8271, -1.3055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032920513302087784
RAW KL tensor(0.0186, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 161: train/loss = 0.6855535507202148, train/raw-loss = 0.6851769685745239, train/logprobs = tensor([[-0.9166, -1.9920],
        [-0.9491, -1.9848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03765752166509628
RAW KL tensor(0.0330, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 162: train/loss = 0.6653576493263245, train/raw-loss = 0.6650112271308899, train/logprobs = tensor([[-0.7115, -2.3488],
        [-0.6853, -2.1867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03464172035455704
RAW KL tensor(0.0668, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 163: train/loss = 0.6771993637084961, train/raw-loss = 0.6768215298652649, train/logprobs = tensor([[-1.2521, -1.8236],
        [-1.2337, -1.7141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037781570106744766
RAW KL tensor(0.0599, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 164: train/loss = 0.6808798909187317, train/raw-loss = 0.6805834174156189, train/logprobs = tensor([[-1.5453, -1.7987],
        [-1.5545, -1.7261]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029651515185832977
RAW KL tensor(0.0286, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 165: train/loss = 0.6871724724769592, train/raw-loss = 0.6868125796318054, train/logprobs = tensor([[-1.1228, -2.0374],
        [-1.1120, -1.9840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03599179536104202
RAW KL tensor(0.0633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 166: train/loss = 0.691642165184021, train/raw-loss = 0.6912738084793091, train/logprobs = tensor([[-0.9005, -2.1199],
        [-0.8957, -2.0928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03683626279234886
RAW KL tensor(0.0326, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 167: train/loss = 0.6778764724731445, train/raw-loss = 0.6775516271591187, train/logprobs = tensor([[-1.2627, -1.2678],
        [-1.3281, -1.2628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03249005600810051
RAW KL tensor(0.0122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 168: train/loss = 0.6872398853302002, train/raw-loss = 0.6869763135910034, train/logprobs = tensor([[-1.0139, -1.3471],
        [-1.0168, -1.3136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026357607915997505
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 169: train/loss = 0.6883068084716797, train/raw-loss = 0.6879808306694031, train/logprobs = tensor([[-0.8802, -1.5970],
        [-0.8981, -1.5872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032599858939647675
RAW KL tensor(0.0123, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 170: train/loss = 0.6805698275566101, train/raw-loss = 0.6803016662597656, train/logprobs = tensor([[-0.6267, -2.1017],
        [-0.6268, -2.0358]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026816774159669876
RAW KL tensor(0.0289, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 171: train/loss = 0.6941397786140442, train/raw-loss = 0.6937858462333679, train/logprobs = tensor([[-1.2680, -1.5454],
        [-1.2552, -1.4896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035398874431848526
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 172: train/loss = 0.6870602369308472, train/raw-loss = 0.6866616010665894, train/logprobs = tensor([[-1.0311, -1.8762],
        [-1.0209, -1.7828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03987067937850952
RAW KL tensor(0.0951, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 173: train/loss = 0.6900103092193604, train/raw-loss = 0.6896196603775024, train/logprobs = tensor([[-1.1169, -2.5126],
        [-1.1525, -2.4555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039067454636096954
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 174: train/loss = 0.6969164609909058, train/raw-loss = 0.6964269876480103, train/logprobs = tensor([[-1.2451, -2.7257],
        [-1.2128, -2.6214]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048950858414173126
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 175: train/loss = 0.6938157677650452, train/raw-loss = 0.6935153007507324, train/logprobs = tensor([[-1.6278, -1.7697],
        [-1.5477, -1.6891]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03005283698439598
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 176: train/loss = 0.6807318925857544, train/raw-loss = 0.6805022954940796, train/logprobs = tensor([[-0.8243, -2.0090],
        [-0.8053, -1.9090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022955764085054398
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 177: train/loss = 0.6751409769058228, train/raw-loss = 0.6749639511108398, train/logprobs = tensor([[-0.9328, -1.5906],
        [-0.9323, -1.5104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017703160643577576
RAW KL tensor(0.0132, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 259])
Epoch 0, Step 178: train/loss = 0.6856465935707092, train/raw-loss = 0.6854288578033447, train/logprobs = tensor([[-0.7699, -1.9147],
        [-0.7637, -1.8534]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021783702075481415
RAW KL tensor(0.0259, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 179: train/loss = 0.6849884390830994, train/raw-loss = 0.684802234172821, train/logprobs = tensor([[-1.3388, -2.0138],
        [-1.3266, -1.9565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01862679421901703
RAW KL tensor(0.0179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 180: train/loss = 0.6823368668556213, train/raw-loss = 0.6809632778167725, train/logprobs = tensor([[-1.1799, -1.5576],
        [-1.1879, -1.4869]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13735949993133545
RAW KL tensor(0.0050, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 326])
Epoch 0, Step 181: train/loss = 0.6980652213096619, train/raw-loss = 0.6976271867752075, train/logprobs = tensor([[-0.9661, -1.6468],
        [-0.9767, -1.6540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043809674680233
RAW KL tensor(0.0374, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 182: train/loss = 0.6817907691001892, train/raw-loss = 0.6814862489700317, train/logprobs = tensor([[-1.1115, -1.5201],
        [-1.1277, -1.4686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03044779971241951
RAW KL tensor(0.0109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 183: train/loss = 0.6723436713218689, train/raw-loss = 0.671960711479187, train/logprobs = tensor([[-0.8462, -1.8989],
        [-0.8614, -1.7846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038294240832328796
RAW KL tensor(0.0317, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 184: train/loss = 0.6736198663711548, train/raw-loss = 0.6733154058456421, train/logprobs = tensor([[-0.9882, -2.0546],
        [-1.0057, -1.9868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030453896149992943
RAW KL tensor(0.1023, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 185: train/loss = 0.703677773475647, train/raw-loss = 0.7032362222671509, train/logprobs = tensor([[-0.8782, -1.7688],
        [-0.8749, -1.6902]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044160593301057816
RAW KL tensor(0.0140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 186: train/loss = 0.6901801824569702, train/raw-loss = 0.68988037109375, train/logprobs = tensor([[-0.8313, -1.8831],
        [-0.8190, -1.8096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029985200613737106
RAW KL tensor(0.0701, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 187: train/loss = 0.6921191215515137, train/raw-loss = 0.6915900707244873, train/logprobs = tensor([[-0.9774, -1.6641],
        [-0.9713, -1.6317]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05291315168142319
RAW KL tensor(0.0179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 188: train/loss = 0.6839482188224792, train/raw-loss = 0.683596670627594, train/logprobs = tensor([[-1.2210, -1.9428],
        [-1.1949, -1.8737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03515768423676491
RAW KL tensor(0.0456, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 189: train/loss = 0.7006589770317078, train/raw-loss = 0.7003645896911621, train/logprobs = tensor([[-1.1151, -1.6393],
        [-1.1014, -1.6221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029429102316498756
RAW KL tensor(0.0656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 190: train/loss = 0.6798058748245239, train/raw-loss = 0.6794370412826538, train/logprobs = tensor([[-1.4924, -1.5867],
        [-1.5591, -1.5637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03688912093639374
RAW KL tensor(0.0421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 191: train/loss = 0.6870467662811279, train/raw-loss = 0.6867590546607971, train/logprobs = tensor([[-0.9934, -1.6190],
        [-0.9880, -1.5657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028761837631464005
RAW KL tensor(0.0202, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 192: train/loss = 0.6728105545043945, train/raw-loss = 0.6723777055740356, train/logprobs = tensor([[-1.1552, -1.3720],
        [-1.1978, -1.2782]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0432845875620842
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 193: train/loss = 0.7005759477615356, train/raw-loss = 0.7003645300865173, train/logprobs = tensor([[-0.6896, -1.2394],
        [-0.7292, -1.2943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021137503907084465
RAW KL tensor(0.0086, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 282])
Epoch 0, Step 194: train/loss = 0.6743929386138916, train/raw-loss = 0.6742165088653564, train/logprobs = tensor([[-1.0665, -1.1765],
        [-1.1180, -1.1436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017634527757763863
RAW KL tensor(0.0154, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 195: train/loss = 0.6743603944778442, train/raw-loss = 0.6741802096366882, train/logprobs = tensor([[-0.5920, -1.4435],
        [-0.5989, -1.3680]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01802208088338375
RAW KL tensor(0.0294, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 196: train/loss = 0.6858097314834595, train/raw-loss = 0.6855156421661377, train/logprobs = tensor([[-1.4560, -1.9879],
        [-1.3838, -1.8806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029411153867840767
RAW KL tensor(0.0539, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 197: train/loss = 0.6722983121871948, train/raw-loss = 0.6720248460769653, train/logprobs = tensor([[-0.9135, -1.3949],
        [-0.9033, -1.2717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027349593117833138
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 198: train/loss = 0.673062801361084, train/raw-loss = 0.6728498935699463, train/logprobs = tensor([[-0.8387, -0.9062],
        [-0.8647, -0.8401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021293746307492256
RAW KL tensor(0.0316, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 199: train/loss = 0.6726785898208618, train/raw-loss = 0.6723841428756714, train/logprobs = tensor([[-0.9079, -1.0627],
        [-0.9545, -1.0138]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029445728287100792
RAW KL tensor(0.0355, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 200: train/loss = 0.668398916721344, train/raw-loss = 0.6680384278297424, train/logprobs = tensor([[-1.1242, -2.2275],
        [-1.1485, -2.0725]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03605414554476738
RAW KL tensor(0.0305, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 201: train/loss = 0.6743366718292236, train/raw-loss = 0.6741489768028259, train/logprobs = tensor([[-0.6380, -1.7268],
        [-0.6555, -1.6391]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018771735951304436
RAW KL tensor(0.0516, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 202: train/loss = 0.6710448861122131, train/raw-loss = 0.6706466674804688, train/logprobs = tensor([[-0.6888, -1.9408],
        [-0.6988, -1.8420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03981946408748627
RAW KL tensor(0.0309, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 203: train/loss = 0.6294874548912048, train/raw-loss = 0.628741443157196, train/logprobs = tensor([[-1.0732, -1.2914],
        [-1.1844, -1.0939]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07460256665945053
RAW KL tensor(0.0350, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 204: train/loss = 0.6642218828201294, train/raw-loss = 0.6639553308486938, train/logprobs = tensor([[-1.3407, -1.3534],
        [-1.3566, -1.2443]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026648955419659615
RAW KL tensor(0.0078, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 328])
Epoch 0, Step 205: train/loss = 0.6672616600990295, train/raw-loss = 0.6670795679092407, train/logprobs = tensor([[-0.6503, -1.4074],
        [-0.6511, -1.2721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018213635310530663
RAW KL tensor(0.0675, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 206: train/loss = 0.6502999067306519, train/raw-loss = 0.6500043869018555, train/logprobs = tensor([[-0.8199, -1.3678],
        [-0.8312, -1.1800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029558049514889717
RAW KL tensor(0.0367, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 207: train/loss = 0.6603323221206665, train/raw-loss = 0.6600559949874878, train/logprobs = tensor([[-0.7470, -1.8716],
        [-0.7760, -1.7442]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027639005333185196
RAW KL tensor(0.0694, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 208: train/loss = 0.6652699708938599, train/raw-loss = 0.6649341583251953, train/logprobs = tensor([[-1.2925, -2.2335],
        [-1.3384, -2.1433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03358100354671478
RAW KL tensor(0.0444, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 209: train/loss = 0.687964141368866, train/raw-loss = 0.6875993609428406, train/logprobs = tensor([[-0.9180, -1.7703],
        [-0.9081, -1.7169]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036472778767347336
RAW KL tensor(0.0599, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 210: train/loss = 0.6574543118476868, train/raw-loss = 0.6571010947227478, train/logprobs = tensor([[-1.5178, -1.5243],
        [-1.5943, -1.4184]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035318538546562195
RAW KL tensor(0.0430, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 211: train/loss = 0.67666095495224, train/raw-loss = 0.6763549447059631, train/logprobs = tensor([[-1.1156, -2.0841],
        [-1.1220, -1.9856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030602265149354935
RAW KL tensor(0.0224, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 212: train/loss = 0.6781971454620361, train/raw-loss = 0.6779109835624695, train/logprobs = tensor([[-0.7336, -1.1573],
        [-0.7234, -1.0782]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028614221140742302
RAW KL tensor(0.0593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 213: train/loss = 0.6711593270301819, train/raw-loss = 0.670836329460144, train/logprobs = tensor([[-1.3600, -1.8991],
        [-1.3770, -1.7980]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03228956460952759
RAW KL tensor(0.0134, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 214: train/loss = 0.6926068067550659, train/raw-loss = 0.692229151725769, train/logprobs = tensor([[-0.8639, -1.1355],
        [-0.8666, -1.1088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037771277129650116
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 215: train/loss = 0.6553590893745422, train/raw-loss = 0.6550172567367554, train/logprobs = tensor([[-1.0781, -1.5270],
        [-1.1178, -1.4059]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034184399992227554
RAW KL tensor(0.0227, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 216: train/loss = 0.6812978982925415, train/raw-loss = 0.6810097694396973, train/logprobs = tensor([[-1.5688, -1.4735],
        [-1.5641, -1.4013]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028813418000936508
RAW KL tensor(0.0577, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 217: train/loss = 0.6770116090774536, train/raw-loss = 0.6767487525939941, train/logprobs = tensor([[-0.8536, -1.5868],
        [-0.8821, -1.5451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02628386579453945
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 218: train/loss = 0.6666603088378906, train/raw-loss = 0.6663217544555664, train/logprobs = tensor([[-0.8082, -2.0873],
        [-0.8260, -1.9514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033858802169561386
RAW KL tensor(0.0024, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 496])
Epoch 0, Step 219: train/loss = 0.6934965252876282, train/raw-loss = 0.6933385133743286, train/logprobs = tensor([[-0.9970, -0.9348],
        [-0.9662, -0.9028]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015800485387444496
RAW KL tensor(0.0530, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 220: train/loss = 0.660832941532135, train/raw-loss = 0.6604737639427185, train/logprobs = tensor([[-1.1815, -2.8774],
        [-1.2078, -2.7620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03591485321521759
RAW KL tensor(0.0110, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 221: train/loss = 0.6747838854789734, train/raw-loss = 0.6744478940963745, train/logprobs = tensor([[-1.4854, -0.8019],
        [-1.4872, -0.7219]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03359629586338997
RAW KL tensor(0.0452, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 222: train/loss = 0.6902825832366943, train/raw-loss = 0.6899899244308472, train/logprobs = tensor([[-0.9780, -1.2138],
        [-0.9715, -1.1740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029259908944368362
RAW KL tensor(0.0186, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 223: train/loss = 0.6823737025260925, train/raw-loss = 0.6819801330566406, train/logprobs = tensor([[-0.9211, -1.9502],
        [-0.9820, -1.8767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03936578333377838
RAW KL tensor(0.0426, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 224: train/loss = 0.6670966148376465, train/raw-loss = 0.6667160987854004, train/logprobs = tensor([[-1.3177, -2.7687],
        [-1.3082, -2.6209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038053564727306366
RAW KL tensor(0.0071, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 360])
Epoch 0, Step 225: train/loss = 0.6571094989776611, train/raw-loss = 0.6567973494529724, train/logprobs = tensor([[-0.8255, -2.0868],
        [-0.7860, -1.8848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031215334311127663
RAW KL tensor(0.0270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 226: train/loss = 0.662764847278595, train/raw-loss = 0.6624249219894409, train/logprobs = tensor([[-1.0314, -2.4538],
        [-1.0619, -2.3555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033992521464824677
RAW KL tensor(0.0769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 227: train/loss = 0.6769015192985535, train/raw-loss = 0.6766082048416138, train/logprobs = tensor([[-0.7544, -1.3950],
        [-0.7523, -1.3194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029339225962758064
RAW KL tensor(0.0595, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 228: train/loss = 0.6614108681678772, train/raw-loss = 0.6609882712364197, train/logprobs = tensor([[-1.1628, -2.5489],
        [-1.1558, -2.3900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04225644841790199
RAW KL tensor(0.0139, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 283])
Epoch 0, Step 229: train/loss = 0.6512657999992371, train/raw-loss = 0.6509864330291748, train/logprobs = tensor([[-0.9550, -1.7615],
        [-0.9838, -1.5886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027942664921283722
RAW KL tensor(0.0993, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 230: train/loss = 0.6680064797401428, train/raw-loss = 0.6674449443817139, train/logprobs = tensor([[-1.1771, -2.6223],
        [-1.1627, -2.4960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05615255981683731
RAW KL tensor(0.0274, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 231: train/loss = 0.6665428280830383, train/raw-loss = 0.6662261486053467, train/logprobs = tensor([[-1.2457, -1.4155],
        [-1.2547, -1.3060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0316682904958725
RAW KL tensor(0.0293, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 232: train/loss = 0.6618425846099854, train/raw-loss = 0.6615300178527832, train/logprobs = tensor([[-1.1902, -2.3165],
        [-1.1952, -2.1842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03125564008951187
RAW KL tensor(0.0178, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 233: train/loss = 0.6658501625061035, train/raw-loss = 0.6651980876922607, train/logprobs = tensor([[-1.6697, -1.9279],
        [-1.6288, -1.7517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06521286070346832
RAW KL tensor(0.0138, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 234: train/loss = 0.6285284161567688, train/raw-loss = 0.6282142400741577, train/logprobs = tensor([[-1.0611, -2.0305],
        [-1.0921, -1.7762]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031421951949596405
RAW KL tensor(0.1310, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 235: train/loss = 0.6862461566925049, train/raw-loss = 0.6848961114883423, train/logprobs = tensor([[-1.1254, -1.7235],
        [-1.0918, -1.6050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1350029557943344
RAW KL tensor(0.0124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 236: train/loss = 0.671522855758667, train/raw-loss = 0.6712213158607483, train/logprobs = tensor([[-0.6873, -1.4558],
        [-0.6788, -1.3528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030154980719089508
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 237: train/loss = 0.6723902225494385, train/raw-loss = 0.6722517609596252, train/logprobs = tensor([[-0.7624, -1.0509],
        [-0.7515, -0.9350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013847121968865395
RAW KL tensor(0.0219, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 238: train/loss = 0.6812746524810791, train/raw-loss = 0.681053876876831, train/logprobs = tensor([[-0.7323, -1.6465],
        [-0.7357, -1.5885]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02208019234240055
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 239: train/loss = 0.6933558583259583, train/raw-loss = 0.6925214529037476, train/logprobs = tensor([[-2.1056, -1.7768],
        [-2.0691, -1.6871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08343852311372757
RAW KL tensor(0.0172, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 240: train/loss = 0.6788799166679382, train/raw-loss = 0.6786974668502808, train/logprobs = tensor([[-1.0994, -1.4479],
        [-1.1154, -1.3586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01824449747800827
RAW KL tensor(0.0419, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 241: train/loss = 0.6642100214958191, train/raw-loss = 0.6639550924301147, train/logprobs = tensor([[-1.0309, -1.9692],
        [-1.0753, -1.8654]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02548878639936447
RAW KL tensor(0.0383, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 242: train/loss = 0.6597983837127686, train/raw-loss = 0.6594616770744324, train/logprobs = tensor([[-1.2699, -1.6574],
        [-1.2635, -1.4993]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033669620752334595
RAW KL tensor(0.0050, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 243: train/loss = 0.6673590540885925, train/raw-loss = 0.6671093702316284, train/logprobs = tensor([[-1.3659, -1.6971],
        [-1.3514, -1.5372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024970728904008865
RAW KL tensor(0.0785, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 244: train/loss = 0.6730902194976807, train/raw-loss = 0.6727695465087891, train/logprobs = tensor([[-1.0077, -1.7648],
        [-0.9738, -1.6441]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0320671945810318
RAW KL tensor(0.0603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 245: train/loss = 0.6774497032165527, train/raw-loss = 0.6771314740180969, train/logprobs = tensor([[-1.2329, -1.7463],
        [-1.2267, -1.6732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031821150332689285
RAW KL tensor(0.0458, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 246: train/loss = 0.6817956566810608, train/raw-loss = 0.6815875768661499, train/logprobs = tensor([[-1.0897, -1.5710],
        [-1.0834, -1.4909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020809222012758255
RAW KL tensor(0.0111, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 247: train/loss = 0.6611549854278564, train/raw-loss = 0.6609445810317993, train/logprobs = tensor([[-1.0039, -1.8843],
        [-0.9528, -1.6826]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021039923653006554
RAW KL tensor(0.0758, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 248: train/loss = 0.6565357446670532, train/raw-loss = 0.6561204791069031, train/logprobs = tensor([[-1.0205, -2.4789],
        [-0.9918, -2.2919]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04152649641036987
RAW KL tensor(0.0276, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 249: train/loss = 0.6672593951225281, train/raw-loss = 0.6669458150863647, train/logprobs = tensor([[-1.3766, -1.4679],
        [-1.4429, -1.4253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031355831772089005
RAW KL tensor(0.0134, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 250: train/loss = 0.6439475417137146, train/raw-loss = 0.6437617540359497, train/logprobs = tensor([[-0.5975, -2.2004],
        [-0.5659, -1.9566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018574362620711327
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 251: train/loss = 0.6886698603630066, train/raw-loss = 0.6882617473602295, train/logprobs = tensor([[-0.9716, -2.2569],
        [-1.0236, -2.2860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040809839963912964
RAW KL tensor(0.0315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 252: train/loss = 0.6975295543670654, train/raw-loss = 0.6970206499099731, train/logprobs = tensor([[-1.1801, -1.2186],
        [-1.1454, -1.1483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05089111626148224
RAW KL tensor(0.0201, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 253: train/loss = 0.6826631426811218, train/raw-loss = 0.6822282075881958, train/logprobs = tensor([[-1.2818, -1.5558],
        [-1.2867, -1.5135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043496135622262955
RAW KL tensor(0.0203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 254: train/loss = 0.6955173015594482, train/raw-loss = 0.6952747106552124, train/logprobs = tensor([[-0.9299, -1.1021],
        [-0.9524, -1.0921]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024260081350803375
RAW KL tensor(0.0450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 255: train/loss = 0.664046049118042, train/raw-loss = 0.6636612415313721, train/logprobs = tensor([[-0.9203, -2.2030],
        [-0.9470, -2.0696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038480330258607864
RAW KL tensor(0.0211, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 256: train/loss = 0.6534281969070435, train/raw-loss = 0.6532250642776489, train/logprobs = tensor([[-0.7902, -2.0016],
        [-0.7764, -1.8199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02031795307993889
RAW KL tensor(0.0508, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 257: train/loss = 0.6782640814781189, train/raw-loss = 0.6779988408088684, train/logprobs = tensor([[-1.4003, -2.5084],
        [-1.3663, -2.3731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026522181928157806
RAW KL tensor(0.1029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 116])
Epoch 0, Step 258: train/loss = 0.6689633131027222, train/raw-loss = 0.6684554815292358, train/logprobs = tensor([[-1.7066, -2.2016],
        [-1.7195, -2.0461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05078700929880142
RAW KL tensor(0.0315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 259: train/loss = 0.6828931570053101, train/raw-loss = 0.6826111674308777, train/logprobs = tensor([[-1.8183, -1.6979],
        [-1.7543, -1.5836]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028197571635246277
RAW KL tensor(0.0498, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 260: train/loss = 0.6493687629699707, train/raw-loss = 0.6487871408462524, train/logprobs = tensor([[-1.2212, -2.0664],
        [-1.1999, -1.8538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05815756693482399
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 261: train/loss = 0.6602187752723694, train/raw-loss = 0.6600278615951538, train/logprobs = tensor([[-0.9166, -2.3814],
        [-0.8214, -2.1195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019089099019765854
RAW KL tensor(0.0563, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 262: train/loss = 0.7030928134918213, train/raw-loss = 0.7026355862617493, train/logprobs = tensor([[-1.4584, -1.8832],
        [-1.4193, -1.8431]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04572422429919243
RAW KL tensor(0.0118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 263: train/loss = 0.6626734733581543, train/raw-loss = 0.6620473861694336, train/logprobs = tensor([[-1.8179, -2.5993],
        [-1.7784, -2.3880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06260243058204651
RAW KL tensor(0.0095, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 264: train/loss = 0.6656042337417603, train/raw-loss = 0.6654002666473389, train/logprobs = tensor([[-1.1597, -2.3298],
        [-1.0891, -2.1098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020392106845974922
RAW KL tensor(0.0114, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 265: train/loss = 0.6803317070007324, train/raw-loss = 0.6800230145454407, train/logprobs = tensor([[-1.3673, -1.9014],
        [-1.3594, -1.8299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03087308257818222
RAW KL tensor(0.0486, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 120])
Epoch 0, Step 266: train/loss = 0.682917058467865, train/raw-loss = 0.682590126991272, train/logprobs = tensor([[-1.2285, -1.1607],
        [-1.1893, -1.0607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03268411010503769
RAW KL tensor(0.0140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 267: train/loss = 0.6676796078681946, train/raw-loss = 0.667432427406311, train/logprobs = tensor([[-1.4056, -1.7245],
        [-1.4082, -1.5970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024721531197428703
RAW KL tensor(0.0148, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 268: train/loss = 0.654442548751831, train/raw-loss = 0.654253363609314, train/logprobs = tensor([[-1.1146, -1.8231],
        [-1.0865, -1.6221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018911708146333694
RAW KL tensor(0.0220, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 269: train/loss = 0.6393160820007324, train/raw-loss = 0.6380969882011414, train/logprobs = tensor([[-0.9840, -1.8184],
        [-1.0954, -1.6322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12190946191549301
RAW KL tensor(0.0179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 270: train/loss = 0.6814772486686707, train/raw-loss = 0.6813289523124695, train/logprobs = tensor([[-0.9108, -1.8225],
        [-0.8443, -1.7031]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014829897321760654
RAW KL tensor(0.0294, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 271: train/loss = 0.6406811475753784, train/raw-loss = 0.6404707431793213, train/logprobs = tensor([[-0.6785, -2.1124],
        [-0.6542, -1.8466]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021034326404333115
RAW KL tensor(0.0230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 272: train/loss = 0.6767736077308655, train/raw-loss = 0.6765486598014832, train/logprobs = tensor([[-0.9979, -1.6350],
        [-0.9663, -1.5161]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022497866302728653
RAW KL tensor(0.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 273: train/loss = 0.6562920212745667, train/raw-loss = 0.6561387777328491, train/logprobs = tensor([[-0.5478, -2.1321],
        [-0.5577, -1.9763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01532293576747179
RAW KL tensor(0.0860, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 274: train/loss = 0.6582396030426025, train/raw-loss = 0.6577540636062622, train/logprobs = tensor([[-1.3810, -2.0984],
        [-1.3905, -1.9253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04855361580848694
RAW KL tensor(0.0256, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 275: train/loss = 0.6540271639823914, train/raw-loss = 0.653828501701355, train/logprobs = tensor([[-1.0827, -2.1791],
        [-1.0150, -1.9089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019864670932292938
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 276: train/loss = 0.6512585282325745, train/raw-loss = 0.65096116065979, train/logprobs = tensor([[-0.6346, -2.5379],
        [-0.6235, -2.3403]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029740789905190468
RAW KL tensor(0.0398, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 277: train/loss = 0.6298982501029968, train/raw-loss = 0.62943434715271, train/logprobs = tensor([[-1.6332, -2.1563],
        [-1.6158, -1.8538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046393074095249176
RAW KL tensor(0.0193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 278: train/loss = 0.6556532979011536, train/raw-loss = 0.6552424430847168, train/logprobs = tensor([[-1.4296, -2.6039],
        [-1.4424, -2.3623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04107890650629997
RAW KL tensor(0.0162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 279: train/loss = 0.6643670797348022, train/raw-loss = 0.6640286445617676, train/logprobs = tensor([[-1.2574, -1.6899],
        [-1.2607, -1.5731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033840954303741455
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 280: train/loss = 0.652076005935669, train/raw-loss = 0.6518259644508362, train/logprobs = tensor([[-1.0086, -1.8484],
        [-0.9928, -1.6582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025006428360939026
RAW KL tensor(0.0237, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 281: train/loss = 0.6767783761024475, train/raw-loss = 0.6764052510261536, train/logprobs = tensor([[-1.5046, -1.8593],
        [-1.4574, -1.7256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0373145192861557
RAW KL tensor(0.0216, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 282: train/loss = 0.6493973135948181, train/raw-loss = 0.6491813659667969, train/logprobs = tensor([[-0.7142, -1.9414],
        [-0.6911, -1.7287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021593328565359116
RAW KL tensor(0.0163, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 283: train/loss = 0.6416164636611938, train/raw-loss = 0.6413974761962891, train/logprobs = tensor([[-0.7762, -2.3709],
        [-0.7858, -2.1352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021900922060012817
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 284: train/loss = 0.6801882982254028, train/raw-loss = 0.6799836158752441, train/logprobs = tensor([[-1.0236, -1.4490],
        [-1.0423, -1.3490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020468659698963165
RAW KL tensor(0.0383, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 285: train/loss = 0.672892689704895, train/raw-loss = 0.6726161241531372, train/logprobs = tensor([[-1.1112, -2.6323],
        [-1.0530, -2.4675]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027655960991978645
RAW KL tensor(0.0082, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 355])
Epoch 0, Step 286: train/loss = 0.6855525970458984, train/raw-loss = 0.6852743029594421, train/logprobs = tensor([[-0.9339, -1.5379],
        [-0.9086, -1.4360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027832908555865288
RAW KL tensor(0.0168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 287: train/loss = 0.6765896677970886, train/raw-loss = 0.6763216257095337, train/logprobs = tensor([[-1.1561, -1.3230],
        [-1.1680, -1.2327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026802685111761093
RAW KL tensor(0.2040, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 288: train/loss = 0.6515480875968933, train/raw-loss = 0.6476435661315918, train/logprobs = tensor([[-1.5465, -2.8533],
        [-1.5769, -2.3357]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.39045003056526184
RAW KL tensor(0.0238, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 289: train/loss = 0.6618117690086365, train/raw-loss = 0.6614372134208679, train/logprobs = tensor([[-1.1246, -1.5301],
        [-1.0606, -1.3297]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037456367164850235
RAW KL tensor(0.1598, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 290: train/loss = 0.6656323075294495, train/raw-loss = 0.6649730801582336, train/logprobs = tensor([[-1.1163, -1.8187],
        [-1.0552, -1.6065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06592024862766266
RAW KL tensor(0.0135, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 242])
Epoch 0, Step 291: train/loss = 0.6783419847488403, train/raw-loss = 0.6782066822052002, train/logprobs = tensor([[-0.7458, -1.0232],
        [-0.7166, -0.9166]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01353302039206028
RAW KL tensor(0.0256, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 292: train/loss = 0.6254958510398865, train/raw-loss = 0.625243067741394, train/logprobs = tensor([[-1.0534, -2.0084],
        [-0.9446, -1.5987]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025278614833950996
RAW KL tensor(0.0164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 294])
Epoch 0, Step 293: train/loss = 0.6095198392868042, train/raw-loss = 0.6092861294746399, train/logprobs = tensor([[-0.8950, -2.5251],
        [-0.9247, -2.1762]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023371443152427673
RAW KL tensor(0.0239, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 294: train/loss = 0.6504300832748413, train/raw-loss = 0.6501944065093994, train/logprobs = tensor([[-1.0490, -2.1437],
        [-1.0481, -1.9459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02357332408428192
RAW KL tensor(0.0713, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 295: train/loss = 0.6742626428604126, train/raw-loss = 0.6738072633743286, train/logprobs = tensor([[-1.7916, -1.9579],
        [-1.7372, -1.8080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04554036259651184
RAW KL tensor(0.0316, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 296: train/loss = 0.6135230660438538, train/raw-loss = 0.6132063865661621, train/logprobs = tensor([[-1.1325, -2.6664],
        [-1.1404, -2.3030]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03166846185922623
RAW KL tensor(0.0350, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 297: train/loss = 0.643896222114563, train/raw-loss = 0.6436375975608826, train/logprobs = tensor([[-1.2430, -2.3847],
        [-1.1873, -2.1008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02586475759744644
RAW KL tensor(0.0158, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 298: train/loss = 0.6284914612770081, train/raw-loss = 0.6282362937927246, train/logprobs = tensor([[-0.8226, -2.2196],
        [-0.8583, -1.9825]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025518255308270454
RAW KL tensor(0.0136, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 299: train/loss = 0.6603645086288452, train/raw-loss = 0.6600862145423889, train/logprobs = tensor([[-1.3387, -1.3099],
        [-1.3304, -1.1541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027823464944958687
RAW KL tensor(0.0676, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 300: train/loss = 0.6684805154800415, train/raw-loss = 0.6680868864059448, train/logprobs = tensor([[-1.4086, -1.5220],
        [-1.3225, -1.3238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03935820236802101
RAW KL tensor(0.0062, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 301: train/loss = 0.6612832546234131, train/raw-loss = 0.6610637903213501, train/logprobs = tensor([[-0.8639, -1.2977],
        [-0.8148, -1.1123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02194998972117901
RAW KL tensor(0.0061, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 302: train/loss = 0.6159414052963257, train/raw-loss = 0.6156483888626099, train/logprobs = tensor([[-1.0354, -2.2136],
        [-1.0443, -1.8694]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02930610626935959
RAW KL tensor(0.1223, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 303: train/loss = 0.6702449917793274, train/raw-loss = 0.6696918606758118, train/logprobs = tensor([[-0.8619, -1.7251],
        [-0.7750, -1.5272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05531615763902664
RAW KL tensor(0.0160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 304: train/loss = 0.6435114145278931, train/raw-loss = 0.6432921886444092, train/logprobs = tensor([[-1.2625, -2.1651],
        [-1.1986, -1.8910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021923232823610306
RAW KL tensor(0.1358, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 305: train/loss = 0.6645382642745972, train/raw-loss = 0.6640598773956299, train/logprobs = tensor([[-1.0314, -1.1213],
        [-0.9890, -0.9416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0478404238820076
RAW KL tensor(0.0442, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 306: train/loss = 0.6230490207672119, train/raw-loss = 0.6228094100952148, train/logprobs = tensor([[-0.9182, -2.0703],
        [-0.8986, -1.7495]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023960506543517113
RAW KL tensor(0.0528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 307: train/loss = 0.6283220648765564, train/raw-loss = 0.6279094219207764, train/logprobs = tensor([[-0.9018, -2.1453],
        [-0.8977, -1.8478]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04126477241516113
RAW KL tensor(0.0187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 308: train/loss = 0.6448565721511841, train/raw-loss = 0.6443784832954407, train/logprobs = tensor([[-1.2959, -2.0201],
        [-1.2890, -1.7566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04780657961964607
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 309: train/loss = 0.6470035910606384, train/raw-loss = 0.6468539237976074, train/logprobs = tensor([[-0.8482, -1.5270],
        [-0.8237, -1.3044]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014967601746320724
RAW KL tensor(0.0259, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 310: train/loss = 0.6304721236228943, train/raw-loss = 0.630196213722229, train/logprobs = tensor([[-1.1002, -2.6093],
        [-1.0536, -2.2840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027596279978752136
RAW KL tensor(0.0084, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 224])
Epoch 0, Step 311: train/loss = 0.6419666409492493, train/raw-loss = 0.641708493232727, train/logprobs = tensor([[-0.7419, -1.5955],
        [-0.7259, -1.3470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025812063366174698
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 312: train/loss = 0.6561349034309387, train/raw-loss = 0.6558624505996704, train/logprobs = tensor([[-1.3617, -1.9257],
        [-1.3718, -1.7732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027246937155723572
RAW KL tensor(0.0385, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 313: train/loss = 0.631609320640564, train/raw-loss = 0.6313614845275879, train/logprobs = tensor([[-0.9463, -2.8366],
        [-0.9198, -2.5126]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024785727262496948
RAW KL tensor(0.0423, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 447])
Epoch 0, Step 314: train/loss = 0.5884982347488403, train/raw-loss = 0.5882105827331543, train/logprobs = tensor([[-0.8670, -2.7803],
        [-0.9475, -2.3868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02877391315996647
RAW KL tensor(0.0453, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 315: train/loss = 0.651802122592926, train/raw-loss = 0.6514768600463867, train/logprobs = tensor([[-0.8116, -2.5734],
        [-0.7504, -2.3293]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03252489119768143
RAW KL tensor(0.0133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 239])
Epoch 0, Step 316: train/loss = 0.6254684925079346, train/raw-loss = 0.6251430511474609, train/logprobs = tensor([[-1.2544, -2.0630],
        [-1.2330, -1.7513]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032542839646339417
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 317: train/loss = 0.6488189101219177, train/raw-loss = 0.6485774517059326, train/logprobs = tensor([[-1.0850, -1.7688],
        [-1.0659, -1.5604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024142302572727203
RAW KL tensor(0.0256, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 318: train/loss = 0.6536558270454407, train/raw-loss = 0.6534504294395447, train/logprobs = tensor([[-1.1970, -2.1794],
        [-1.1702, -1.9619]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02053668722510338
RAW KL tensor(0.0226, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 319: train/loss = 0.6556152105331421, train/raw-loss = 0.655280351638794, train/logprobs = tensor([[-1.1869, -2.1212],
        [-1.1946, -1.9405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03348028287291527
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 320: train/loss = 0.6354736089706421, train/raw-loss = 0.6351804733276367, train/logprobs = tensor([[-0.7165, -1.6995],
        [-0.8106, -1.5384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029317788779735565
RAW KL tensor(0.0431, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 321: train/loss = 0.6173862218856812, train/raw-loss = 0.6170276999473572, train/logprobs = tensor([[-1.4099, -2.4756],
        [-1.5125, -2.2422]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035851482301950455
RAW KL tensor(0.0248, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 322: train/loss = 0.6543238162994385, train/raw-loss = 0.6537598371505737, train/logprobs = tensor([[-1.1349, -2.1545],
        [-1.2108, -1.9306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05640028789639473
RAW KL tensor(0.0292, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 323: train/loss = 0.6899887919425964, train/raw-loss = 0.6897499561309814, train/logprobs = tensor([[-1.0634, -0.8261],
        [-1.0924, -0.8339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023877110332250595
RAW KL tensor(0.0249, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 99])
Epoch 0, Step 324: train/loss = 0.6513242721557617, train/raw-loss = 0.6510487794876099, train/logprobs = tensor([[-1.5158, -2.2068],
        [-1.5918, -2.0950]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027547918260097504
RAW KL tensor(0.0199, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 325: train/loss = 0.6256471872329712, train/raw-loss = 0.6239991188049316, train/logprobs = tensor([[-2.1662, -1.9357],
        [-2.3367, -1.7101]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16480925679206848
RAW KL tensor(0.0611, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 326: train/loss = 0.6534917950630188, train/raw-loss = 0.652824342250824, train/logprobs = tensor([[-1.3788, -2.1277],
        [-1.5394, -2.1064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06674617528915405
RAW KL tensor(0.0379, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 327: train/loss = 0.6813068389892578, train/raw-loss = 0.680803656578064, train/logprobs = tensor([[-1.2860, -1.7476],
        [-1.3674, -1.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05031389743089676
RAW KL tensor(0.0577, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 328: train/loss = 0.6914783716201782, train/raw-loss = 0.6911443471908569, train/logprobs = tensor([[-1.1498, -1.4795],
        [-1.1118, -1.4100]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03340610861778259
RAW KL tensor(0.0314, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 329: train/loss = 0.633065938949585, train/raw-loss = 0.632736325263977, train/logprobs = tensor([[-1.4703, -2.0797],
        [-1.5934, -1.9170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03296428918838501
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 330: train/loss = 0.6671106219291687, train/raw-loss = 0.6667598485946655, train/logprobs = tensor([[-1.2280, -2.2773],
        [-1.3415, -2.2600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03508315235376358
RAW KL tensor(0.0780, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 331: train/loss = 0.6844703555107117, train/raw-loss = 0.6840054392814636, train/logprobs = tensor([[-1.4332, -2.0661],
        [-1.4221, -1.9943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04648773372173309
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 332: train/loss = 0.6577703952789307, train/raw-loss = 0.6574558019638062, train/logprobs = tensor([[-1.8747, -1.4717],
        [-1.9622, -1.3978]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031461432576179504
RAW KL tensor(0.0700, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 333: train/loss = 0.6143105626106262, train/raw-loss = 0.6136155128479004, train/logprobs = tensor([[-1.3732, -2.3945],
        [-1.4535, -2.0895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0695023238658905
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 334: train/loss = 0.6509326696395874, train/raw-loss = 0.6506134867668152, train/logprobs = tensor([[-1.5925, -2.2368],
        [-1.6134, -2.0533]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03191450983285904
RAW KL tensor(0.0201, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 335: train/loss = 0.6571880578994751, train/raw-loss = 0.6569641828536987, train/logprobs = tensor([[-0.8800, -1.5181],
        [-0.9482, -1.3957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022376997396349907
RAW KL tensor(0.0396, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 336: train/loss = 0.660058856010437, train/raw-loss = 0.6595791578292847, train/logprobs = tensor([[-1.0664, -1.6525],
        [-1.1379, -1.5684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047966763377189636
RAW KL tensor(0.0360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 337: train/loss = 0.6784958839416504, train/raw-loss = 0.6779618263244629, train/logprobs = tensor([[-1.2080, -2.5027],
        [-1.2922, -2.4613]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053406722843647
RAW KL tensor(0.0604, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 338: train/loss = 0.6843398809432983, train/raw-loss = 0.6839565634727478, train/logprobs = tensor([[-1.3473, -1.8638],
        [-1.3621, -1.7983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038332805037498474
RAW KL tensor(0.0375, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 339: train/loss = 0.6796801686286926, train/raw-loss = 0.6793466210365295, train/logprobs = tensor([[-1.5063, -1.5423],
        [-1.6019, -1.5695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03334999084472656
RAW KL tensor(0.0309, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 340: train/loss = 0.6233350038528442, train/raw-loss = 0.6229082345962524, train/logprobs = tensor([[-1.6301, -2.1767],
        [-1.7037, -1.9223]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04267328977584839
RAW KL tensor(0.0360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 341: train/loss = 0.6961367130279541, train/raw-loss = 0.6942449808120728, train/logprobs = tensor([[-1.6830, -2.0877],
        [-1.7624, -1.9755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18918119370937347
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 342: train/loss = 0.6343176364898682, train/raw-loss = 0.6338778734207153, train/logprobs = tensor([[-1.1722, -1.6701],
        [-1.2526, -1.4728]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04397121071815491
RAW KL tensor(0.0321, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 343: train/loss = 0.666111946105957, train/raw-loss = 0.6658854484558105, train/logprobs = tensor([[-0.8265, -1.9111],
        [-0.9104, -1.8523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02264666184782982
RAW KL tensor(0.0434, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 344: train/loss = 0.6776740550994873, train/raw-loss = 0.6773501634597778, train/logprobs = tensor([[-1.3669, -1.7452],
        [-1.3806, -1.6567]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03239138051867485
RAW KL tensor(0.0274, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 345: train/loss = 0.6663032174110413, train/raw-loss = 0.6658867597579956, train/logprobs = tensor([[-1.2236, -1.4196],
        [-1.2724, -1.3121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04164855182170868
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 346: train/loss = 0.6502432227134705, train/raw-loss = 0.6494594216346741, train/logprobs = tensor([[-1.1428, -2.3687],
        [-1.2289, -2.2481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07837984710931778
RAW KL tensor(0.0492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 347: train/loss = 0.6843318343162537, train/raw-loss = 0.683769941329956, train/logprobs = tensor([[-1.4633, -1.7464],
        [-1.4492, -1.6624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05619712173938751
RAW KL tensor(0.0549, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 348: train/loss = 0.635187029838562, train/raw-loss = 0.6345518827438354, train/logprobs = tensor([[-1.0299, -1.6069],
        [-1.2280, -1.5222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06351625174283981
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 349: train/loss = 0.6538112163543701, train/raw-loss = 0.6535488367080688, train/logprobs = tensor([[-1.0334, -1.7531],
        [-1.1367, -1.6594]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026234792545437813
RAW KL tensor(0.0520, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 350: train/loss = 0.6380360126495361, train/raw-loss = 0.6372785568237305, train/logprobs = tensor([[-0.8477, -3.2231],
        [-0.9332, -2.8933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07574594765901566
RAW KL tensor(0.0319, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 302])
Epoch 0, Step 351: train/loss = 0.6472684144973755, train/raw-loss = 0.6467974185943604, train/logprobs = tensor([[-1.0481, -2.5687],
        [-1.1667, -2.4571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04710392281413078
RAW KL tensor(0.0656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 352: train/loss = 0.5829463005065918, train/raw-loss = 0.5825238823890686, train/logprobs = tensor([[-1.2032, -2.6223],
        [-1.2458, -2.1703]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04224202781915665
RAW KL tensor(0.0418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 353: train/loss = 0.5697381496429443, train/raw-loss = 0.5693799257278442, train/logprobs = tensor([[-1.3954, -2.0546],
        [-1.4478, -1.5339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03582005202770233
RAW KL tensor(0.0323, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 354: train/loss = 0.5634018182754517, train/raw-loss = 0.5629662275314331, train/logprobs = tensor([[-0.7890, -3.4820],
        [-0.8368, -2.9350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043556373566389084
RAW KL tensor(0.0536, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 355: train/loss = 0.6579119563102722, train/raw-loss = 0.657558023929596, train/logprobs = tensor([[-0.8584, -1.7308],
        [-0.8598, -1.5592]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03539512678980827
RAW KL tensor(0.0162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 356: train/loss = 0.6501567363739014, train/raw-loss = 0.6499453783035278, train/logprobs = tensor([[-1.0984, -1.3722],
        [-1.0676, -1.1584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02113400772213936
RAW KL tensor(0.0148, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 345])
Epoch 0, Step 357: train/loss = 0.5594625473022461, train/raw-loss = 0.5590840578079224, train/logprobs = tensor([[-0.8459, -2.7588],
        [-0.8640, -2.1177]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03784741833806038
RAW KL tensor(0.0239, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 358: train/loss = 0.6117197275161743, train/raw-loss = 0.611377477645874, train/logprobs = tensor([[-1.2823, -2.2049],
        [-1.3467, -1.8928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034225139766931534
RAW KL tensor(0.0159, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 359: train/loss = 0.5841013789176941, train/raw-loss = 0.5838220119476318, train/logprobs = tensor([[-0.8662, -2.5798],
        [-0.9193, -2.1568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027943067252635956
RAW KL tensor(0.0267, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 249])
Epoch 0, Step 360: train/loss = 0.5862827897071838, train/raw-loss = 0.5859434604644775, train/logprobs = tensor([[-0.7743, -3.2433],
        [-0.8279, -2.7911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03392932564020157
RAW KL tensor(0.0479, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 361: train/loss = 0.5958986282348633, train/raw-loss = 0.5955854654312134, train/logprobs = tensor([[-0.7296, -2.6570],
        [-0.7516, -2.2319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031315796077251434
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 362: train/loss = 0.6130390167236328, train/raw-loss = 0.6125341653823853, train/logprobs = tensor([[-1.4959, -1.7209],
        [-1.6586, -1.5090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05048755556344986
RAW KL tensor(0.0435, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 363: train/loss = 0.6146930456161499, train/raw-loss = 0.6143117547035217, train/logprobs = tensor([[-1.3007, -2.1988],
        [-1.4210, -1.9740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03812533617019653
RAW KL tensor(0.0448, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 364: train/loss = 0.5964633822441101, train/raw-loss = 0.5960791110992432, train/logprobs = tensor([[-0.9487, -1.7987],
        [-1.0945, -1.5065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03842417150735855
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 365: train/loss = 0.5879091620445251, train/raw-loss = 0.5876474380493164, train/logprobs = tensor([[-1.3398, -1.6876],
        [-1.3814, -1.2491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0261717326939106
RAW KL tensor(0.0221, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 366: train/loss = 0.6147215366363525, train/raw-loss = 0.6143120527267456, train/logprobs = tensor([[-1.2988, -1.7860],
        [-1.3698, -1.4910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04094325006008148
RAW KL tensor(0.0223, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 367: train/loss = 0.6157714128494263, train/raw-loss = 0.6155920624732971, train/logprobs = tensor([[-1.1144, -1.7258],
        [-1.1469, -1.4296]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017934337258338928
RAW KL tensor(0.1092, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 368: train/loss = 0.6024234294891357, train/raw-loss = 0.601933479309082, train/logprobs = tensor([[-1.3436, -2.3887],
        [-1.4199, -2.0087]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04899977147579193
RAW KL tensor(0.0403, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 369: train/loss = 0.5746783018112183, train/raw-loss = 0.5742858648300171, train/logprobs = tensor([[-1.1565, -1.8487],
        [-1.2133, -1.3279]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03925219923257828
RAW KL tensor(0.0317, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 370: train/loss = 0.5783811807632446, train/raw-loss = 0.578048825263977, train/logprobs = tensor([[-1.0199, -2.3237],
        [-1.0367, -1.8144]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03323162719607353
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 461])
Epoch 0, Step 371: train/loss = 0.6524217128753662, train/raw-loss = 0.6521322131156921, train/logprobs = tensor([[-0.5713, -1.0712],
        [-0.6529, -0.9626]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028950121253728867
RAW KL tensor(0.0628, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 372: train/loss = 0.6378933191299438, train/raw-loss = 0.6374698877334595, train/logprobs = tensor([[-1.1262, -1.9767],
        [-1.1903, -1.7894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04234922677278519
RAW KL tensor(0.0323, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 373: train/loss = 0.5947790145874023, train/raw-loss = 0.5943780541419983, train/logprobs = tensor([[-1.1040, -2.7885],
        [-1.0894, -2.3094]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04009989649057388
RAW KL tensor(0.0235, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 374: train/loss = 0.6490118503570557, train/raw-loss = 0.6487270593643188, train/logprobs = tensor([[-0.7788, -1.1286],
        [-0.8515, -1.0014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028480730950832367
RAW KL tensor(0.0350, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 375: train/loss = 0.6326358914375305, train/raw-loss = 0.6322424411773682, train/logprobs = tensor([[-1.1067, -1.6775],
        [-1.2151, -1.5242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03934863209724426
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 376: train/loss = 0.6451700329780579, train/raw-loss = 0.6449384689331055, train/logprobs = tensor([[-1.4723, -1.8097],
        [-1.3861, -1.4829]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023153718560934067
RAW KL tensor(0.0081, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 377: train/loss = 0.6637998223304749, train/raw-loss = 0.6636595129966736, train/logprobs = tensor([[-0.6494, -0.8101],
        [-0.6965, -0.7355]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014033108949661255
RAW KL tensor(0.0445, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 378: train/loss = 0.6707699298858643, train/raw-loss = 0.6702992916107178, train/logprobs = tensor([[-1.9707, -1.8140],
        [-1.9587, -1.6385]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04705940932035446
RAW KL tensor(0.0593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 379: train/loss = 0.5819767713546753, train/raw-loss = 0.5815047025680542, train/logprobs = tensor([[-1.9462, -2.9067],
        [-1.8109, -2.1773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04720665514469147
RAW KL tensor(0.0260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 380: train/loss = 0.6235893964767456, train/raw-loss = 0.6233127117156982, train/logprobs = tensor([[-1.0139, -1.7551],
        [-1.0562, -1.4788]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027668233960866928
RAW KL tensor(0.0940, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 381: train/loss = 0.5705559253692627, train/raw-loss = 0.5701526999473572, train/logprobs = tensor([[-1.0134, -2.2624],
        [-1.0316, -1.6946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04031480848789215
RAW KL tensor(0.0362, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 382: train/loss = 0.639951765537262, train/raw-loss = 0.6396673321723938, train/logprobs = tensor([[-1.1452, -1.7744],
        [-1.0546, -1.4245]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02843894623219967
RAW KL tensor(0.0328, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 383: train/loss = 0.6401562690734863, train/raw-loss = 0.6399213671684265, train/logprobs = tensor([[-2.0782, -2.2906],
        [-1.8881, -1.8759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023492775857448578
RAW KL tensor(0.0372, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 384: train/loss = 0.5841132402420044, train/raw-loss = 0.5838146805763245, train/logprobs = tensor([[-0.9196, -2.0277],
        [-1.0140, -1.6414]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02985491417348385
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 385: train/loss = 0.6313733458518982, train/raw-loss = 0.631039559841156, train/logprobs = tensor([[-1.5697, -1.8756],
        [-1.5222, -1.5390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03337756171822548
RAW KL tensor(0.0855, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 386: train/loss = 0.5845937132835388, train/raw-loss = 0.5841244459152222, train/logprobs = tensor([[-1.4620, -2.3173],
        [-1.3761, -1.6985]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04692543298006058
RAW KL tensor(0.2840, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 387: train/loss = 0.5688787698745728, train/raw-loss = 0.5679885149002075, train/logprobs = tensor([[-1.3281, -2.1388],
        [-1.2523, -1.4874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08903183043003082
RAW KL tensor(0.0071, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 388: train/loss = 0.6345808506011963, train/raw-loss = 0.6343468427658081, train/logprobs = tensor([[-0.9520, -1.2748],
        [-0.8293, -0.8845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02340499684214592
RAW KL tensor(0.0469, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 389: train/loss = 0.6195606589317322, train/raw-loss = 0.6192407608032227, train/logprobs = tensor([[-0.8481, -1.7099],
        [-0.7901, -1.3292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0319882407784462
RAW KL tensor(0.0498, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 390: train/loss = 0.5286762714385986, train/raw-loss = 0.5283020734786987, train/logprobs = tensor([[-1.2742, -2.5177],
        [-1.1652, -1.6346]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03741585090756416
RAW KL tensor(0.0130, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 391: train/loss = 0.6069678068161011, train/raw-loss = 0.606376051902771, train/logprobs = tensor([[-1.6924, -2.2499],
        [-1.2762, -1.4417]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059176329523324966
RAW KL tensor(0.0395, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 392: train/loss = 0.534282922744751, train/raw-loss = 0.5337380170822144, train/logprobs = tensor([[-1.2285, -2.3863],
        [-1.2813, -1.6097]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05448906868696213
RAW KL tensor(0.1194, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 393: train/loss = 0.5756470561027527, train/raw-loss = 0.575053334236145, train/logprobs = tensor([[-1.5838, -2.4768],
        [-1.5369, -1.8989]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05937625840306282
RAW KL tensor(0.0932, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 394: train/loss = 0.5481588244438171, train/raw-loss = 0.5475736260414124, train/logprobs = tensor([[-1.3664, -2.6858],
        [-1.1793, -1.8253]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05851950868964195
RAW KL tensor(0.0141, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 395: train/loss = 0.6177405714988708, train/raw-loss = 0.6174787282943726, train/logprobs = tensor([[-1.1322, -1.9060],
        [-1.1017, -1.5440]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026189474388957024
RAW KL tensor(0.1010, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 396: train/loss = 0.49173200130462646, train/raw-loss = 0.49119701981544495, train/logprobs = tensor([[-0.9592, -2.7226],
        [-0.9651, -1.7061]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05349650979042053
RAW KL tensor(0.0346, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 397: train/loss = 0.6267895102500916, train/raw-loss = 0.6264644861221313, train/logprobs = tensor([[-1.7453, -1.6431],
        [-1.4403, -1.0360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03250189870595932
RAW KL tensor(0.0323, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 398: train/loss = 0.5388517379760742, train/raw-loss = 0.5384664535522461, train/logprobs = tensor([[-1.1093, -2.3683],
        [-1.1348, -1.6461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03852546215057373
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 399: train/loss = 0.5786188840866089, train/raw-loss = 0.578048586845398, train/logprobs = tensor([[-1.4101, -2.1587],
        [-1.3256, -1.5678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05702689290046692
RAW KL tensor(0.0367, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 400: train/loss = 0.6065120697021484, train/raw-loss = 0.6061104536056519, train/logprobs = tensor([[-1.7992, -1.9910],
        [-1.6893, -1.4588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040160171687603
RAW KL tensor(0.0844, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 401: train/loss = 0.480510950088501, train/raw-loss = 0.48009824752807617, train/logprobs = tensor([[-1.3357, -2.5639],
        [-1.2707, -1.4922]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04127337038516998
RAW KL tensor(0.0195, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 402: train/loss = 0.5883790850639343, train/raw-loss = 0.5880296230316162, train/logprobs = tensor([[-1.1664, -1.9591],
        [-1.0893, -1.4094]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034941837191581726
RAW KL tensor(0.0349, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 403: train/loss = 0.5234408378601074, train/raw-loss = 0.5230189561843872, train/logprobs = tensor([[-1.5382, -2.1747],
        [-1.6677, -1.5340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04219692200422287
RAW KL tensor(0.0383, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 404: train/loss = 0.5847234725952148, train/raw-loss = 0.5843881964683533, train/logprobs = tensor([[-1.3000, -2.3841],
        [-1.2983, -1.8980]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033520907163619995
RAW KL tensor(0.0191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 405: train/loss = 0.6341457962989807, train/raw-loss = 0.6339185237884521, train/logprobs = tensor([[-0.8183, -1.6849],
        [-0.8054, -1.4118]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02272317185997963
RAW KL tensor(0.0608, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 406: train/loss = 0.5421349406242371, train/raw-loss = 0.5416347980499268, train/logprobs = tensor([[-1.4304, -2.3638],
        [-1.4460, -1.6575]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05001822113990784
RAW KL tensor(0.0710, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 407: train/loss = 0.6056660413742065, train/raw-loss = 0.6051548719406128, train/logprobs = tensor([[-1.2791, -2.5682],
        [-1.2459, -2.1386]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0511159673333168
RAW KL tensor(0.0326, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 408: train/loss = 0.6061380505561829, train/raw-loss = 0.6056164503097534, train/logprobs = tensor([[-1.4499, -1.7453],
        [-1.3654, -1.2623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05216052383184433
RAW KL tensor(0.0438, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 409: train/loss = 0.5471659898757935, train/raw-loss = 0.546844482421875, train/logprobs = tensor([[-0.8519, -2.1394],
        [-0.8424, -1.4537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03214650973677635
RAW KL tensor(0.0321, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 410: train/loss = 0.5694355368614197, train/raw-loss = 0.5675350427627563, train/logprobs = tensor([[-1.1094, -1.9822],
        [-1.0098, -1.3287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19005051255226135
RAW KL tensor(0.0283, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 411: train/loss = 0.6863381862640381, train/raw-loss = 0.6860991716384888, train/logprobs = tensor([[-1.5169, -1.4449],
        [-1.1782, -1.0767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02390204556286335
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 412: train/loss = 0.5979264974594116, train/raw-loss = 0.5975496768951416, train/logprobs = tensor([[-1.0919, -1.8685],
        [-1.2734, -1.6205]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03767797350883484
RAW KL tensor(0.0107, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 413: train/loss = 0.659919023513794, train/raw-loss = 0.6595187187194824, train/logprobs = tensor([[-1.5279, -1.1595],
        [-1.4144, -0.8608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040034543722867966
RAW KL tensor(0.0402, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 414: train/loss = 0.6504645347595215, train/raw-loss = 0.6500707864761353, train/logprobs = tensor([[-1.1729, -1.4405],
        [-1.0176, -1.0393]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039372608065605164
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 263])
Epoch 0, Step 415: train/loss = 0.5340537428855896, train/raw-loss = 0.5336385369300842, train/logprobs = tensor([[-0.7228, -2.7161],
        [-0.6837, -1.9554]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041515469551086426
RAW KL tensor(0.1021, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 416: train/loss = 0.5127915740013123, train/raw-loss = 0.5120948553085327, train/logprobs = tensor([[-1.0206, -2.4969],
        [-0.9860, -1.6252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06966710835695267
RAW KL tensor(0.1068, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 417: train/loss = 0.45287024974823, train/raw-loss = 0.4520285129547119, train/logprobs = tensor([[-1.0879, -3.9478],
        [-1.1058, -2.8092]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08417482674121857
RAW KL tensor(0.1297, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 418: train/loss = 0.5992280840873718, train/raw-loss = 0.5984441637992859, train/logprobs = tensor([[-1.5857, -2.1086],
        [-1.3464, -1.4331]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07839280366897583
RAW KL tensor(0.0579, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 419: train/loss = 0.5840592384338379, train/raw-loss = 0.5833053588867188, train/logprobs = tensor([[-2.1957, -2.8153],
        [-1.7057, -1.7716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0753888487815857
RAW KL tensor(0.0888, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 420: train/loss = 0.4879137873649597, train/raw-loss = 0.48721063137054443, train/logprobs = tensor([[-1.2373, -2.7870],
        [-1.3067, -1.8748]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07031572610139847
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 421: train/loss = 0.5681583881378174, train/raw-loss = 0.56765216588974, train/logprobs = tensor([[-0.7415, -2.0562],
        [-0.6845, -1.4007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05062225088477135
RAW KL tensor(0.0386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 422: train/loss = 0.5311883091926575, train/raw-loss = 0.5305874347686768, train/logprobs = tensor([[-1.1326, -1.9389],
        [-1.1150, -1.1857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06008438020944595
RAW KL tensor(0.0878, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 423: train/loss = 0.5280170440673828, train/raw-loss = 0.5272485613822937, train/logprobs = tensor([[-1.5094, -1.9488],
        [-1.5229, -1.1973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07684843987226486
RAW KL tensor(0.1079, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 424: train/loss = 0.5012480020523071, train/raw-loss = 0.5006659030914307, train/logprobs = tensor([[-0.8560, -2.2314],
        [-0.8133, -1.2050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.058211326599121094
RAW KL tensor(0.0392, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 425: train/loss = 0.4435039460659027, train/raw-loss = 0.4430319666862488, train/logprobs = tensor([[-1.2144, -3.2213],
        [-1.1781, -1.9767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04719960689544678
RAW KL tensor(0.0666, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 426: train/loss = 0.539835512638092, train/raw-loss = 0.5392234325408936, train/logprobs = tensor([[-1.4847, -1.9969],
        [-1.3791, -1.1929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061201758682727814
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 427: train/loss = 0.6508871912956238, train/raw-loss = 0.6505258083343506, train/logprobs = tensor([[-0.8782, -1.3112],
        [-0.8984, -1.1393]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036144793033599854
RAW KL tensor(0.0319, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 428: train/loss = 0.5254422426223755, train/raw-loss = 0.5249460935592651, train/logprobs = tensor([[-1.5392, -2.0685],
        [-1.5286, -1.2598]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04961696267127991
RAW KL tensor(0.0165, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 285])
Epoch 0, Step 429: train/loss = 0.6460951566696167, train/raw-loss = 0.6457114219665527, train/logprobs = tensor([[-1.0591, -1.1429],
        [-0.9844, -0.8529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03836787864565849
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 430: train/loss = 0.5239231586456299, train/raw-loss = 0.5232596397399902, train/logprobs = tensor([[-1.2758, -3.0004],
        [-0.9654, -1.8610]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0663534477353096
RAW KL tensor(0.0414, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 431: train/loss = 0.5364328026771545, train/raw-loss = 0.5342957377433777, train/logprobs = tensor([[-1.7456, -2.8475],
        [-1.5261, -1.9026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21370816230773926
RAW KL tensor(0.0601, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 432: train/loss = 0.6289385557174683, train/raw-loss = 0.6285426616668701, train/logprobs = tensor([[-1.0913, -1.5801],
        [-1.1576, -1.3509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03959270566701889
RAW KL tensor(0.0859, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 433: train/loss = 0.4988671541213989, train/raw-loss = 0.49811774492263794, train/logprobs = tensor([[-1.5236, -2.9804],
        [-1.4892, -2.0174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07494378089904785
RAW KL tensor(0.1488, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 434: train/loss = 0.528165876865387, train/raw-loss = 0.5271781086921692, train/logprobs = tensor([[-1.2116, -2.6942],
        [-1.4114, -2.1430]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09877476096153259
RAW KL tensor(0.0702, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 435: train/loss = 0.6029799580574036, train/raw-loss = 0.6024746298789978, train/logprobs = tensor([[-1.5299, -1.8567],
        [-1.5527, -1.4637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05053439736366272
RAW KL tensor(0.0603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 436: train/loss = 0.5168845057487488, train/raw-loss = 0.5164586901664734, train/logprobs = tensor([[-1.2369, -1.8886],
        [-1.2465, -1.0307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0425792932510376
RAW KL tensor(0.0830, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 437: train/loss = 0.49161237478256226, train/raw-loss = 0.4910322427749634, train/logprobs = tensor([[-0.9544, -2.3166],
        [-1.0847, -1.5125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0580129511654377
RAW KL tensor(0.1261, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 438: train/loss = 0.6193154454231262, train/raw-loss = 0.6185881495475769, train/logprobs = tensor([[-1.5219, -2.2063],
        [-1.1971, -1.4969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07273081690073013
RAW KL tensor(0.0333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 439: train/loss = 0.5412372350692749, train/raw-loss = 0.5404366254806519, train/logprobs = tensor([[-1.1819, -2.0946],
        [-1.2494, -1.4721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08006344735622406
RAW KL tensor(0.0115, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 347])
Epoch 0, Step 440: train/loss = 0.5942742824554443, train/raw-loss = 0.593716561794281, train/logprobs = tensor([[-1.9225, -2.1155],
        [-1.6281, -1.3862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05577218160033226
RAW KL tensor(0.0561, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 441: train/loss = 0.4657098352909088, train/raw-loss = 0.4648358225822449, train/logprobs = tensor([[-1.1710, -3.1923],
        [-1.1895, -2.0926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08740099519491196
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 442: train/loss = 0.5877645611763, train/raw-loss = 0.5870190262794495, train/logprobs = tensor([[-1.8070, -1.8097],
        [-1.7455, -0.9551]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07455490529537201
RAW KL tensor(0.1006, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 443: train/loss = 0.5012028217315674, train/raw-loss = 0.500299334526062, train/logprobs = tensor([[-1.3820, -2.6667],
        [-1.4967, -1.8615]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09034660458564758
RAW KL tensor(0.0423, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 444: train/loss = 0.5810326337814331, train/raw-loss = 0.5806213617324829, train/logprobs = tensor([[-0.9408, -2.1356],
        [-0.9356, -1.6078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04112371429800987
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 445: train/loss = 0.5830041170120239, train/raw-loss = 0.5825780630111694, train/logprobs = tensor([[-1.7686, -1.8435],
        [-1.6855, -1.1877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04260648787021637
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 446: train/loss = 0.5339537858963013, train/raw-loss = 0.5330339670181274, train/logprobs = tensor([[-1.3108, -3.0240],
        [-1.1097, -2.0900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09198711067438126
RAW KL tensor(0.1640, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 447: train/loss = 0.4318172335624695, train/raw-loss = 0.4309545159339905, train/logprobs = tensor([[-1.1028, -2.7756],
        [-1.1596, -1.4586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08627377450466156
RAW KL tensor(0.0954, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 448: train/loss = 0.46953797340393066, train/raw-loss = 0.46850305795669556, train/logprobs = tensor([[-1.3203, -2.4069],
        [-1.1361, -1.0321]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.103491872549057
RAW KL tensor(0.0975, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 449: train/loss = 0.6127492189407349, train/raw-loss = 0.61199951171875, train/logprobs = tensor([[-1.5346, -1.5498],
        [-1.3464, -1.0041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07497061043977737
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 450: train/loss = 0.5120596885681152, train/raw-loss = 0.5115818977355957, train/logprobs = tensor([[-0.6381, -2.0712],
        [-0.6914, -1.2425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047782670706510544
RAW KL tensor(0.0633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 451: train/loss = 0.5530704855918884, train/raw-loss = 0.5523124933242798, train/logprobs = tensor([[-2.0407, -2.4668],
        [-1.8212, -1.5691]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0757974237203598
RAW KL tensor(0.0449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 452: train/loss = 0.4725821018218994, train/raw-loss = 0.4717642664909363, train/logprobs = tensor([[-1.3227, -2.1674],
        [-1.4430, -1.1714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08178040385246277
RAW KL tensor(0.0449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 453: train/loss = 0.5106590986251831, train/raw-loss = 0.5100277662277222, train/logprobs = tensor([[-0.8928, -2.0648],
        [-1.0697, -1.3423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0631357803940773
RAW KL tensor(0.0945, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 454: train/loss = 0.5516677498817444, train/raw-loss = 0.5509635210037231, train/logprobs = tensor([[-1.6170, -1.9789],
        [-1.5982, -1.3009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07042164355516434
RAW KL tensor(0.2009, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 374])
Epoch 0, Step 455: train/loss = 0.45759207010269165, train/raw-loss = 0.4565311074256897, train/logprobs = tensor([[-0.8272, -2.1109],
        [-0.8814, -0.9950]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1060982346534729
RAW KL tensor(0.0108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 423])
Epoch 0, Step 456: train/loss = 0.49240240454673767, train/raw-loss = 0.4917481541633606, train/logprobs = tensor([[-0.8641, -2.2030],
        [-0.8633, -1.0938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06542828679084778
RAW KL tensor(0.0803, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 457: train/loss = 0.5511065125465393, train/raw-loss = 0.5504400730133057, train/logprobs = tensor([[-0.7097, -1.6067],
        [-0.7810, -0.9198]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06664206087589264
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 118])
Epoch 0, Step 458: train/loss = 0.6305163502693176, train/raw-loss = 0.6297332048416138, train/logprobs = tensor([[-1.5239, -1.8911],
        [-1.2340, -1.3109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07831472158432007
RAW KL tensor(0.0251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 459: train/loss = 0.5756012201309204, train/raw-loss = 0.574876070022583, train/logprobs = tensor([[-0.8529, -1.8846],
        [-0.9923, -1.4714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07251593470573425
RAW KL tensor(0.0091, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 452])
Epoch 0, Step 460: train/loss = 0.5857300758361816, train/raw-loss = 0.5852468013763428, train/logprobs = tensor([[-0.8146, -1.4817],
        [-1.0342, -1.1271]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04832827299833298
RAW KL tensor(0.0777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 461: train/loss = 0.6417040228843689, train/raw-loss = 0.6406617760658264, train/logprobs = tensor([[-1.5833, -1.6561],
        [-1.2805, -1.0669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10422489792108536
RAW KL tensor(0.1592, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 361])
Epoch 0, Step 462: train/loss = 0.4735605716705322, train/raw-loss = 0.47270163893699646, train/logprobs = tensor([[-0.8609, -2.6256],
        [-0.9466, -1.6285]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08589619398117065
RAW KL tensor(0.0184, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 462])
Epoch 0, Step 463: train/loss = 0.5025408864021301, train/raw-loss = 0.501960813999176, train/logprobs = tensor([[-0.8575, -1.9742],
        [-0.9722, -1.1689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.058007389307022095
RAW KL tensor(0.2011, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 464: train/loss = 0.4757533669471741, train/raw-loss = 0.47435683012008667, train/logprobs = tensor([[-1.0352, -3.1744],
        [-1.0900, -2.0870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13965220749378204
RAW KL tensor(0.0477, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 465: train/loss = 0.6382331848144531, train/raw-loss = 0.6375550031661987, train/logprobs = tensor([[-0.9974, -1.2869],
        [-1.0794, -1.0792]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06781130284070969
RAW KL tensor(0.0848, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 466: train/loss = 0.48991766571998596, train/raw-loss = 0.48915228247642517, train/logprobs = tensor([[-1.5093, -2.4203],
        [-1.5663, -1.5014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07653972506523132
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 343])
Epoch 0, Step 467: train/loss = 0.5930225849151611, train/raw-loss = 0.5922159552574158, train/logprobs = tensor([[-1.5228, -1.8674],
        [-1.3261, -1.2095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08066242933273315
RAW KL tensor(0.0558, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 468: train/loss = 0.5907220244407654, train/raw-loss = 0.5899980068206787, train/logprobs = tensor([[-1.0936, -1.7354],
        [-1.1091, -1.2860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07239754498004913
RAW KL tensor(0.0758, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 469: train/loss = 0.5628982782363892, train/raw-loss = 0.5623608827590942, train/logprobs = tensor([[-0.9505, -1.5274],
        [-1.0227, -0.9890]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05374142900109291
RAW KL tensor(0.0959, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 470: train/loss = 0.4806365668773651, train/raw-loss = 0.47978582978248596, train/logprobs = tensor([[-0.9397, -2.7527],
        [-0.9062, -1.6970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08507578074932098
RAW KL tensor(0.1040, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 471: train/loss = 0.49477696418762207, train/raw-loss = 0.4935079514980316, train/logprobs = tensor([[-1.1033, -2.4671],
        [-1.1067, -1.4953]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12690085172653198
RAW KL tensor(0.1170, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 472: train/loss = 0.3132302761077881, train/raw-loss = 0.31231293082237244, train/logprobs = tensor([[-1.6095, -4.8552],
        [-1.6457, -1.9896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09173629432916641
RAW KL tensor(0.0729, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 473: train/loss = 0.5613301992416382, train/raw-loss = 0.5602405071258545, train/logprobs = tensor([[-1.6361, -2.0723],
        [-1.2494, -1.0319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10897082090377808
RAW KL tensor(0.0902, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 474: train/loss = 0.4547763764858246, train/raw-loss = 0.45406216382980347, train/logprobs = tensor([[-0.7718, -2.5040],
        [-0.8044, -1.3900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07142303884029388
RAW KL tensor(0.0453, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 475: train/loss = 0.4775453209877014, train/raw-loss = 0.4767686426639557, train/logprobs = tensor([[-0.9456, -2.2255],
        [-1.0487, -1.2597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07766848802566528
RAW KL tensor(0.0514, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 476: train/loss = 0.6420248746871948, train/raw-loss = 0.6414189338684082, train/logprobs = tensor([[-1.4414, -1.6246],
        [-1.1256, -1.0551]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060591306537389755
RAW KL tensor(0.1212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 477: train/loss = 0.5342676639556885, train/raw-loss = 0.5334175229072571, train/logprobs = tensor([[-1.6635, -1.9742],
        [-1.7396, -1.2173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08501485735177994
RAW KL tensor(0.0863, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 478: train/loss = 0.5876093506813049, train/raw-loss = 0.586648166179657, train/logprobs = tensor([[-1.7224, -1.9744],
        [-1.5813, -1.2710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09612161666154861
RAW KL tensor(0.0907, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 479: train/loss = 0.5224268436431885, train/raw-loss = 0.5218567848205566, train/logprobs = tensor([[-1.4859, -2.6915],
        [-1.2517, -1.6516]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05701186880469322
RAW KL tensor(0.1529, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 480: train/loss = 0.600616991519928, train/raw-loss = 0.5995150208473206, train/logprobs = tensor([[-1.0510, -1.6180],
        [-1.2257, -1.2640]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11019919812679291
RAW KL tensor(0.0976, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 481: train/loss = 0.585989236831665, train/raw-loss = 0.5848878622055054, train/logprobs = tensor([[-1.2793, -1.3358],
        [-1.5311, -1.0147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11013931781053543
RAW KL tensor(0.0723, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 278])
Epoch 0, Step 482: train/loss = 0.531690239906311, train/raw-loss = 0.5307289361953735, train/logprobs = tensor([[-1.0632, -2.5708],
        [-1.1503, -1.8160]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09612603485584259
RAW KL tensor(0.0955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 483: train/loss = 0.5037637948989868, train/raw-loss = 0.5029414296150208, train/logprobs = tensor([[-1.4314, -2.3109],
        [-1.5285, -1.4386]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08224109560251236
RAW KL tensor(0.1304, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 484: train/loss = 0.528831958770752, train/raw-loss = 0.527674674987793, train/logprobs = tensor([[-1.1115, -2.9290],
        [-1.3110, -2.2128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11572612822055817
RAW KL tensor(0.0512, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 485: train/loss = 0.5951201915740967, train/raw-loss = 0.5943694114685059, train/logprobs = tensor([[-1.3421, -1.6316],
        [-1.3764, -1.1608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07507196068763733
RAW KL tensor(0.0847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 486: train/loss = 0.6085173487663269, train/raw-loss = 0.6079196333885193, train/logprobs = tensor([[-0.6918, -1.2204],
        [-0.8947, -1.0420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059773534536361694
RAW KL tensor(0.0467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 487: train/loss = 0.4192051291465759, train/raw-loss = 0.4182473421096802, train/logprobs = tensor([[-1.1013, -3.8691],
        [-1.0768, -2.4272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09577612578868866
RAW KL tensor(0.1389, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 488: train/loss = 0.7120410799980164, train/raw-loss = 0.7106248140335083, train/logprobs = tensor([[-1.7176, -2.0836],
        [-1.5913, -1.8899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14162790775299072
RAW KL tensor(0.0434, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 489: train/loss = 0.5412843227386475, train/raw-loss = 0.5400136113166809, train/logprobs = tensor([[-1.3648, -2.0768],
        [-1.2976, -1.2437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.127075657248497
RAW KL tensor(0.0303, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 321])
Epoch 0, Step 490: train/loss = 0.6650117635726929, train/raw-loss = 0.6645752191543579, train/logprobs = tensor([[-0.6686, -1.5483],
        [-0.7762, -1.5343]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04365852475166321
RAW KL tensor(0.0793, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 491: train/loss = 0.5114570260047913, train/raw-loss = 0.5102574825286865, train/logprobs = tensor([[-1.1523, -2.1470],
        [-1.4238, -1.5145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11995314061641693
RAW KL tensor(0.2055, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 492: train/loss = 0.474009245634079, train/raw-loss = 0.4729212522506714, train/logprobs = tensor([[-0.6410, -2.3830],
        [-0.8302, -1.3747]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1088012307882309
RAW KL tensor(0.1104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 493: train/loss = 0.6096410155296326, train/raw-loss = 0.6085277795791626, train/logprobs = tensor([[-1.1618, -1.8199],
        [-1.1166, -1.2803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11132565885782242
RAW KL tensor(0.1078, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 494: train/loss = 0.6285305619239807, train/raw-loss = 0.6277099847793579, train/logprobs = tensor([[-1.4797, -1.4126],
        [-1.4697, -1.1068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08206257224082947
RAW KL tensor(0.0970, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 495: train/loss = 0.5828540325164795, train/raw-loss = 0.5821486711502075, train/logprobs = tensor([[-1.9536, -2.6446],
        [-1.8542, -1.9834]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07053213566541672
RAW KL tensor(0.0831, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 496: train/loss = 0.5556230545043945, train/raw-loss = 0.5546915531158447, train/logprobs = tensor([[-1.4445, -2.2931],
        [-1.6261, -1.5179]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09314900636672974
RAW KL tensor(0.1236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 497: train/loss = 0.448340505361557, train/raw-loss = 0.4470116198062897, train/logprobs = tensor([[-1.0734, -2.9814],
        [-1.3029, -1.8812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13289305567741394
RAW KL tensor(0.1354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 498: train/loss = 0.6921061277389526, train/raw-loss = 0.6910537481307983, train/logprobs = tensor([[-1.5641, -2.2990],
        [-1.5240, -1.7635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10523444414138794
RAW KL tensor(0.1105, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 499: train/loss = 0.6068640947341919, train/raw-loss = 0.6056279540061951, train/logprobs = tensor([[-0.8936, -2.2799],
        [-1.0648, -1.9655]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12361757457256317
RAW KL tensor(0.0908, device='cuda:0')
SHAPES: torch.Size([4, 122])
RAW KL tensor(0.0991, device='cuda:0')
SHAPES: torch.Size([4, 145])
RAW KL tensor(0.1621, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.2451, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.1330, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0692, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.1212, device='cuda:0')
SHAPES: torch.Size([4, 168])
RAW KL tensor(0.1148, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.1022, device='cuda:0')
SHAPES: torch.Size([4, 184])
RAW KL tensor(0.1151, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.1131, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.1231, device='cuda:0')
SHAPES: torch.Size([4, 189])
RAW KL tensor(0.1834, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0908, device='cuda:0')
SHAPES: torch.Size([4, 169])
RAW KL tensor(0.1555, device='cuda:0')
SHAPES: torch.Size([4, 239])
RAW KL tensor(0.0377, device='cuda:0')
SHAPES: torch.Size([4, 182])
RAW KL tensor(0.1396, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.1526, device='cuda:0')
SHAPES: torch.Size([4, 170])
RAW KL tensor(0.1929, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.1029, device='cuda:0')
SHAPES: torch.Size([4, 159])
RAW KL tensor(0.0733, device='cuda:0')
SHAPES: torch.Size([4, 158])
RAW KL tensor(0.0867, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.1600, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0602, device='cuda:0')
SHAPES: torch.Size([4, 201])
RAW KL tensor(0.0816, device='cuda:0')
SHAPES: torch.Size([4, 187])
RAW KL tensor(0.0861, device='cuda:0')
SHAPES: torch.Size([4, 317])
RAW KL tensor(0.0886, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0404, device='cuda:0')
SHAPES: torch.Size([4, 185])
RAW KL tensor(0.0553, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.1513, device='cuda:0')
SHAPES: torch.Size([4, 106])
RAW KL tensor(0.0651, device='cuda:0')
SHAPES: torch.Size([4, 173])
RAW KL tensor(0.1269, device='cuda:0')
SHAPES: torch.Size([4, 455])
RAW KL tensor(0.0436, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.0498, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.1976, device='cuda:0')
SHAPES: torch.Size([4, 263])
RAW KL tensor(0.0728, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.2001, device='cuda:0')
SHAPES: torch.Size([4, 155])
RAW KL tensor(0.0910, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0870, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0515, device='cuda:0')
SHAPES: torch.Size([4, 146])
RAW KL tensor(0.0494, device='cuda:0')
SHAPES: torch.Size([4, 226])
RAW KL tensor(0.0425, device='cuda:0')
SHAPES: torch.Size([4, 213])
RAW KL tensor(0.1362, device='cuda:0')
SHAPES: torch.Size([4, 204])
RAW KL tensor(0.0774, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.1001, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0663, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0552, device='cuda:0')
SHAPES: torch.Size([4, 130])
RAW KL tensor(0.1078, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.1336, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.0609, device='cuda:0')
SHAPES: torch.Size([4, 175])
RAW KL tensor(0.1022, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.1049, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0707, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.0487, device='cuda:0')
SHAPES: torch.Size([4, 208])
RAW KL tensor(0.1771, device='cuda:0')
SHAPES: torch.Size([4, 177])
RAW KL tensor(0.1106, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0534, device='cuda:0')
SHAPES: torch.Size([4, 164])
RAW KL tensor(0.0410, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.1518, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.0915, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0671, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.0905, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0896, device='cuda:0')
SHAPES: torch.Size([4, 194])
RAW KL tensor(0.0652, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.5407, device='cuda:0')
SHAPES: torch.Size([4, 125])
RAW KL tensor(0.0847, device='cuda:0')
SHAPES: torch.Size([4, 179])
RAW KL tensor(0.1880, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0597, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.2353, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.1547, device='cuda:0')
SHAPES: torch.Size([4, 229])
RAW KL tensor(0.0503, device='cuda:0')
SHAPES: torch.Size([4, 341])
RAW KL tensor(0.0842, device='cuda:0')
SHAPES: torch.Size([4, 237])
RAW KL tensor(0.1585, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.1138, device='cuda:0')
SHAPES: torch.Size([4, 118])
RAW KL tensor(0.1978, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0348, device='cuda:0')
SHAPES: torch.Size([4, 210])
RAW KL tensor(0.1739, device='cuda:0')
SHAPES: torch.Size([4, 216])
RAW KL tensor(0.0842, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.0777, device='cuda:0')
SHAPES: torch.Size([4, 191])
RAW KL tensor(0.1363, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.1063, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.1539, device='cuda:0')
SHAPES: torch.Size([4, 171])
RAW KL tensor(0.0599, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.0841, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.2545, device='cuda:0')
SHAPES: torch.Size([4, 195])
RAW KL tensor(0.0706, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0920, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.1021, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.0403, device='cuda:0')
SHAPES: torch.Size([4, 198])
RAW KL tensor(0.0166, device='cuda:0')
SHAPES: torch.Size([4, 450])
RAW KL tensor(0.0919, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0913, device='cuda:0')
SHAPES: torch.Size([4, 452])
RAW KL tensor(0.0794, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0878, device='cuda:0')
SHAPES: torch.Size([4, 188])
RAW KL tensor(0.1003, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0810, device='cuda:0')
SHAPES: torch.Size([4, 174])
RAW KL tensor(0.1049, device='cuda:0')
SHAPES: torch.Size([4, 137])
RAW KL tensor(0.0468, device='cuda:0')
SHAPES: torch.Size([4, 219])
RAW KL tensor(0.0991, device='cuda:0')
SHAPES: torch.Size([4, 140])
RAW KL tensor(0.1281, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0830, device='cuda:0')
SHAPES: torch.Size([4, 410])
RAW KL tensor(0.0407, device='cuda:0')
SHAPES: torch.Size([4, 214])
RAW KL tensor(0.0349, device='cuda:0')
SHAPES: torch.Size([4, 467])
RAW KL tensor(0.1653, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0669, device='cuda:0')
SHAPES: torch.Size([4, 205])
RAW KL tensor(0.1503, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0473, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.2120, device='cuda:0')
SHAPES: torch.Size([4, 206])
RAW KL tensor(0.0427, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0918, device='cuda:0')
SHAPES: torch.Size([4, 152])
RAW KL tensor(0.0464, device='cuda:0')
SHAPES: torch.Size([4, 227])
RAW KL tensor(0.1095, device='cuda:0')
SHAPES: torch.Size([4, 217])
RAW KL tensor(0.0580, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.0615, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0917, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.0601, device='cuda:0')
SHAPES: torch.Size([4, 183])
RAW KL tensor(0.1010, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.0906, device='cuda:0')
SHAPES: torch.Size([4, 126])
RAW KL tensor(0.1569, device='cuda:0')
SHAPES: torch.Size([4, 207])
RAW KL tensor(0.1708, device='cuda:0')
SHAPES: torch.Size([4, 121])
RAW KL tensor(0.0472, device='cuda:0')
SHAPES: torch.Size([4, 167])
RAW KL tensor(0.0684, device='cuda:0')
SHAPES: torch.Size([4, 236])
RAW KL tensor(0.1145, device='cuda:0')
SHAPES: torch.Size([4, 123])
RAW KL tensor(0.0374, device='cuda:0')
SHAPES: torch.Size([4, 128])
RAW KL tensor(0.0827, device='cuda:0')
SHAPES: torch.Size([4, 217])
eval/loss: 0.5449784994125366
RAW KL tensor(0.1838, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 500: train/loss = 0.6578167080879211, train/raw-loss = 0.6561973094940186, train/logprobs = tensor([[-1.1287, -2.0072],
        [-1.2875, -1.4667]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1619405448436737
RAW KL tensor(0.0508, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 501: train/loss = 0.5033324956893921, train/raw-loss = 0.5023216009140015, train/logprobs = tensor([[-1.1683, -2.2691],
        [-1.2960, -1.4695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10108532756567001
RAW KL tensor(0.0726, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 502: train/loss = 0.509459376335144, train/raw-loss = 0.5084220170974731, train/logprobs = tensor([[-0.7835, -2.4479],
        [-0.9857, -1.6902]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1037316620349884
RAW KL tensor(0.0880, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 503: train/loss = 0.5185763835906982, train/raw-loss = 0.5176068544387817, train/logprobs = tensor([[-1.4776, -2.7056],
        [-1.3935, -1.5185]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09694702178239822
RAW KL tensor(0.0750, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 504: train/loss = 0.6390653252601624, train/raw-loss = 0.6383876204490662, train/logprobs = tensor([[-1.2484, -1.5245],
        [-1.1369, -1.1752]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06777025759220123
RAW KL tensor(0.1995, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 505: train/loss = 0.509077250957489, train/raw-loss = 0.507914662361145, train/logprobs = tensor([[-1.2337, -2.0718],
        [-1.3438, -1.2004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11625418812036514
RAW KL tensor(0.1525, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 506: train/loss = 0.5397275686264038, train/raw-loss = 0.5387125015258789, train/logprobs = tensor([[-1.0676, -2.1953],
        [-1.2228, -1.4627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1015053540468216
RAW KL tensor(0.0622, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 507: train/loss = 0.44636934995651245, train/raw-loss = 0.445237398147583, train/logprobs = tensor([[-1.0579, -2.4377],
        [-1.2021, -1.3300]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11319561302661896
RAW KL tensor(0.1927, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 508: train/loss = 0.5013446807861328, train/raw-loss = 0.5003556609153748, train/logprobs = tensor([[-0.7867, -2.1163],
        [-1.1241, -1.4557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09890416264533997
RAW KL tensor(0.1031, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 509: train/loss = 0.558035671710968, train/raw-loss = 0.5567811131477356, train/logprobs = tensor([[-1.0501, -1.6174],
        [-1.2124, -1.0100]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12545549869537354
RAW KL tensor(0.1676, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 510: train/loss = 0.5295550227165222, train/raw-loss = 0.5286604166030884, train/logprobs = tensor([[-0.8327, -2.2832],
        [-1.0172, -1.6267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08946576714515686
RAW KL tensor(0.2145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 511: train/loss = 0.4145772457122803, train/raw-loss = 0.4132023751735687, train/logprobs = tensor([[-0.8945, -2.6207],
        [-0.9590, -1.1523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13748611509799957
RAW KL tensor(0.1220, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 217])
Epoch 0, Step 512: train/loss = 0.449865460395813, train/raw-loss = 0.44889459013938904, train/logprobs = tensor([[-0.6615, -2.3413],
        [-0.8439, -1.2428]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09708763659000397
RAW KL tensor(0.0601, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 513: train/loss = 0.4255251884460449, train/raw-loss = 0.42447924613952637, train/logprobs = tensor([[-1.4894, -2.3928],
        [-1.6550, -1.2520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10459598153829575
RAW KL tensor(0.1139, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 228])
Epoch 0, Step 514: train/loss = 0.660582423210144, train/raw-loss = 0.6587072610855103, train/logprobs = tensor([[-2.0189, -2.1372],
        [-1.9802, -1.5080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18751561641693115
RAW KL tensor(0.1642, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 515: train/loss = 0.5318425893783569, train/raw-loss = 0.5309592485427856, train/logprobs = tensor([[-0.8959, -2.6098],
        [-1.0694, -1.9329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0883314311504364
RAW KL tensor(0.3325, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 516: train/loss = 0.5898262858390808, train/raw-loss = 0.5874900817871094, train/logprobs = tensor([[-2.1398, -2.8893],
        [-1.6478, -1.7937]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23362049460411072
RAW KL tensor(0.0451, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 307])
Epoch 0, Step 517: train/loss = 0.5472103357315063, train/raw-loss = 0.5461865663528442, train/logprobs = tensor([[-0.7957, -2.0373],
        [-1.0018, -1.5013]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10237809270620346
RAW KL tensor(0.2647, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 518: train/loss = 0.5265489220619202, train/raw-loss = 0.5251319408416748, train/logprobs = tensor([[-1.2615, -2.0927],
        [-1.4921, -1.4559]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14170262217521667
RAW KL tensor(0.0340, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 519: train/loss = 0.4864254891872406, train/raw-loss = 0.48553234338760376, train/logprobs = tensor([[-0.8224, -2.3139],
        [-1.1078, -1.4748]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08931542932987213
RAW KL tensor(0.1084, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 520: train/loss = 0.5499132871627808, train/raw-loss = 0.5484181642532349, train/logprobs = tensor([[-1.2500, -2.5238],
        [-1.2413, -1.6760]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14950710535049438
RAW KL tensor(0.1138, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 521: train/loss = 0.49604272842407227, train/raw-loss = 0.49443671107292175, train/logprobs = tensor([[-1.3269, -2.6376],
        [-1.5992, -1.6908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16059806942939758
RAW KL tensor(0.2949, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 522: train/loss = 0.5007449984550476, train/raw-loss = 0.49898314476013184, train/logprobs = tensor([[-1.9083, -3.3943],
        [-1.8127, -2.2107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1761867105960846
RAW KL tensor(0.1017, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 253])
Epoch 0, Step 523: train/loss = 0.6521860957145691, train/raw-loss = 0.6511298418045044, train/logprobs = tensor([[-0.8191, -1.3785],
        [-1.0325, -1.1110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10562264919281006
RAW KL tensor(0.0672, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 524: train/loss = 0.54999840259552, train/raw-loss = 0.5488767027854919, train/logprobs = tensor([[-1.3183, -1.6076],
        [-1.4868, -1.0914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11217335611581802
RAW KL tensor(0.0974, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 238])
Epoch 0, Step 525: train/loss = 0.5069570541381836, train/raw-loss = 0.5059790015220642, train/logprobs = tensor([[-1.4483, -2.7398],
        [-1.5107, -1.8985]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09780201315879822
RAW KL tensor(0.0654, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 526: train/loss = 0.5042010545730591, train/raw-loss = 0.5033055543899536, train/logprobs = tensor([[-1.7939, -2.7382],
        [-1.7682, -1.7209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0895472839474678
RAW KL tensor(0.1393, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 527: train/loss = 0.4422977864742279, train/raw-loss = 0.4410208463668823, train/logprobs = tensor([[-1.0628, -3.1323],
        [-1.1868, -1.8171]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12769433856010437
RAW KL tensor(0.0264, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 528: train/loss = 0.6530065536499023, train/raw-loss = 0.6522530317306519, train/logprobs = tensor([[-0.9230, -1.1670],
        [-1.1994, -1.2384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07535069435834885
RAW KL tensor(0.1226, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 529: train/loss = 0.3006732761859894, train/raw-loss = 0.2991693913936615, train/logprobs = tensor([[-0.7790, -3.8712],
        [-1.1223, -2.0006]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1503884196281433
RAW KL tensor(0.2375, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 530: train/loss = 0.5869872570037842, train/raw-loss = 0.5855587720870972, train/logprobs = tensor([[-1.2511, -2.0576],
        [-1.1187, -1.2090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1428511142730713
RAW KL tensor(0.1217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 531: train/loss = 0.6414800882339478, train/raw-loss = 0.6404485106468201, train/logprobs = tensor([[-1.3429, -1.6815],
        [-1.3112, -1.3071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1031571701169014
RAW KL tensor(0.0708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 532: train/loss = 0.47672417759895325, train/raw-loss = 0.4757349193096161, train/logprobs = tensor([[-0.9701, -2.3474],
        [-1.1700, -1.4319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09892845153808594
RAW KL tensor(0.0301, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 419])
Epoch 0, Step 533: train/loss = 0.5272777080535889, train/raw-loss = 0.5265693664550781, train/logprobs = tensor([[-0.6598, -1.9924],
        [-0.8144, -1.2518]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07084161043167114
RAW KL tensor(0.0542, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 114])
Epoch 0, Step 534: train/loss = 0.6043140292167664, train/raw-loss = 0.6034364700317383, train/logprobs = tensor([[-1.5359, -2.0989],
        [-1.3428, -1.4454]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08775518834590912
RAW KL tensor(0.0629, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 535: train/loss = 0.5652056336402893, train/raw-loss = 0.5644319653511047, train/logprobs = tensor([[-1.0756, -1.5495],
        [-1.2207, -1.0846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07736723870038986
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 536: train/loss = 0.5581878423690796, train/raw-loss = 0.5572439432144165, train/logprobs = tensor([[-1.2135, -2.1330],
        [-1.4434, -1.5838]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09439705312252045
RAW KL tensor(0.1216, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 537: train/loss = 0.6017359495162964, train/raw-loss = 0.6002761721611023, train/logprobs = tensor([[-1.2982, -1.7092],
        [-1.2659, -1.1788]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14597897231578827
RAW KL tensor(0.1513, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 538: train/loss = 0.5441378951072693, train/raw-loss = 0.543215274810791, train/logprobs = tensor([[-1.4324, -2.3027],
        [-1.5450, -1.5650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09226163476705551
RAW KL tensor(0.2048, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 539: train/loss = 0.5162587761878967, train/raw-loss = 0.5144088864326477, train/logprobs = tensor([[-1.1008, -2.9325],
        [-1.4017, -2.0041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18498902022838593
RAW KL tensor(0.2306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 540: train/loss = 0.5280157327651978, train/raw-loss = 0.5269252061843872, train/logprobs = tensor([[-1.2809, -1.9438],
        [-1.1491, -0.9670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10905179381370544
RAW KL tensor(0.0836, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 541: train/loss = 0.4866471290588379, train/raw-loss = 0.48565366864204407, train/logprobs = tensor([[-1.6632, -2.6230],
        [-1.7938, -1.7249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09934673458337784
RAW KL tensor(0.0725, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 542: train/loss = 0.5204856395721436, train/raw-loss = 0.5197276473045349, train/logprobs = tensor([[-1.4886, -2.6837],
        [-1.7153, -1.9676]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07580223679542542
RAW KL tensor(0.1028, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 543: train/loss = 0.3914552927017212, train/raw-loss = 0.39056676626205444, train/logprobs = tensor([[-1.1690, -2.7586],
        [-1.2386, -1.2714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08885232359170914
RAW KL tensor(0.1349, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 544: train/loss = 0.49390408396720886, train/raw-loss = 0.49295997619628906, train/logprobs = tensor([[-0.9102, -1.9947],
        [-1.0896, -1.1338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09441417455673218
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 545: train/loss = 0.5558032989501953, train/raw-loss = 0.5550931692123413, train/logprobs = tensor([[-0.6595, -1.4101],
        [-0.8168, -0.9091]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07100754231214523
RAW KL tensor(0.1610, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 546: train/loss = 0.44241097569465637, train/raw-loss = 0.4409083127975464, train/logprobs = tensor([[-1.6298, -2.1245],
        [-1.8042, -1.0317]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15026558935642242
RAW KL tensor(0.0956, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 201])
Epoch 0, Step 547: train/loss = 0.40086132287979126, train/raw-loss = 0.39985090494155884, train/logprobs = tensor([[-0.6914, -2.4850],
        [-0.8941, -1.2411]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10103915631771088
RAW KL tensor(0.0526, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 548: train/loss = 0.5328408479690552, train/raw-loss = 0.5322396159172058, train/logprobs = tensor([[-1.2004, -2.1165],
        [-1.3032, -1.4371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06012900918722153
RAW KL tensor(0.0484, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 549: train/loss = 0.49177831411361694, train/raw-loss = 0.4905373752117157, train/logprobs = tensor([[-1.2381, -2.0668],
        [-1.4896, -1.3373]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.124094158411026
RAW KL tensor(0.1030, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 550: train/loss = 0.5145443677902222, train/raw-loss = 0.5136669874191284, train/logprobs = tensor([[-1.2878, -2.0077],
        [-1.2952, -1.0773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08773721754550934
RAW KL tensor(0.0427, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 551: train/loss = 0.45237642526626587, train/raw-loss = 0.4513465464115143, train/logprobs = tensor([[-0.8150, -2.2167],
        [-1.0107, -1.0923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1029873713850975
RAW KL tensor(0.1084, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 552: train/loss = 0.33461883664131165, train/raw-loss = 0.3334888517856598, train/logprobs = tensor([[-1.2199, -3.1291],
        [-1.2855, -1.2768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11299887299537659
RAW KL tensor(0.0159, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 553: train/loss = 0.45633625984191895, train/raw-loss = 0.45543938875198364, train/logprobs = tensor([[-1.4209, -2.7496],
        [-1.4296, -1.5196]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08968675881624222
RAW KL tensor(0.0424, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 554: train/loss = 0.503279447555542, train/raw-loss = 0.5019387006759644, train/logprobs = tensor([[-1.1756, -2.1733],
        [-1.3022, -1.2024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1340789496898651
RAW KL tensor(0.3039, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 555: train/loss = 0.3294077515602112, train/raw-loss = 0.3274938464164734, train/logprobs = tensor([[-1.6969, -3.7047],
        [-1.8463, -1.6148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19139225780963898
RAW KL tensor(0.1755, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 556: train/loss = 0.35686907172203064, train/raw-loss = 0.3551245331764221, train/logprobs = tensor([[-1.5392, -3.0998],
        [-1.7164, -1.4705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17445102334022522
RAW KL tensor(0.0835, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 557: train/loss = 0.4290981888771057, train/raw-loss = 0.42835116386413574, train/logprobs = tensor([[-0.7957, -2.4467],
        [-0.8858, -1.0958]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07469860464334488
RAW KL tensor(0.0678, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 558: train/loss = 0.5814488530158997, train/raw-loss = 0.5806264877319336, train/logprobs = tensor([[-1.7293, -1.9307],
        [-1.6400, -1.3053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08223170787096024
RAW KL tensor(0.1798, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 559: train/loss = 0.41224154829978943, train/raw-loss = 0.4109952449798584, train/logprobs = tensor([[-0.9817, -2.9866],
        [-1.2481, -1.7946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12463246285915375
RAW KL tensor(0.1315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 560: train/loss = 0.5589113235473633, train/raw-loss = 0.5580382347106934, train/logprobs = tensor([[-1.0931, -2.0014],
        [-1.1865, -1.4589]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08731038123369217
RAW KL tensor(0.1272, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 561: train/loss = 0.4351798892021179, train/raw-loss = 0.4339028596878052, train/logprobs = tensor([[-1.8468, -3.0701],
        [-1.5062, -1.4237]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12770575284957886
RAW KL tensor(0.0916, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 562: train/loss = 0.5827397108078003, train/raw-loss = 0.5821390748023987, train/logprobs = tensor([[-1.1993, -2.1581],
        [-1.3346, -1.7828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060067832469940186
RAW KL tensor(0.0973, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 563: train/loss = 0.441433846950531, train/raw-loss = 0.4397539496421814, train/logprobs = tensor([[-1.6329, -2.7014],
        [-1.4108, -1.2064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1679883897304535
RAW KL tensor(0.0968, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 564: train/loss = 0.6113982200622559, train/raw-loss = 0.6104977130889893, train/logprobs = tensor([[-1.4209, -1.5570],
        [-1.2522, -1.0301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09004758298397064
RAW KL tensor(0.1889, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 565: train/loss = 0.4546968936920166, train/raw-loss = 0.4533769488334656, train/logprobs = tensor([[-1.0788, -2.2668],
        [-1.1606, -1.0392]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1319916546344757
RAW KL tensor(0.0845, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 566: train/loss = 0.364044725894928, train/raw-loss = 0.36265242099761963, train/logprobs = tensor([[-1.1573, -3.0028],
        [-1.2826, -1.3314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1392287164926529
RAW KL tensor(0.0998, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 567: train/loss = 0.3593137264251709, train/raw-loss = 0.358038067817688, train/logprobs = tensor([[-1.0391, -3.5237],
        [-0.9380, -1.4284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1275649219751358
RAW KL tensor(0.1514, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 568: train/loss = 0.313944935798645, train/raw-loss = 0.312213659286499, train/logprobs = tensor([[-1.4121, -3.4011],
        [-1.5660, -1.5010]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17312589287757874
RAW KL tensor(0.1877, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 569: train/loss = 0.3549669682979584, train/raw-loss = 0.3532997965812683, train/logprobs = tensor([[-1.6995, -3.6536],
        [-1.7999, -1.8916]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16671431064605713
RAW KL tensor(0.2066, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 570: train/loss = 0.5227561593055725, train/raw-loss = 0.5214619040489197, train/logprobs = tensor([[-1.4425, -2.6699],
        [-1.3549, -1.6234]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1294291913509369
RAW KL tensor(0.0821, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 571: train/loss = 0.4567103385925293, train/raw-loss = 0.4557996988296509, train/logprobs = tensor([[-1.4025, -3.0292],
        [-1.4782, -1.8794]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09106491506099701
RAW KL tensor(0.0876, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 572: train/loss = 0.357156366109848, train/raw-loss = 0.3560095429420471, train/logprobs = tensor([[-1.0982, -3.8742],
        [-1.2149, -2.0350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11468261480331421
RAW KL tensor(0.0937, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 573: train/loss = 0.5311818718910217, train/raw-loss = 0.5299267768859863, train/logprobs = tensor([[-1.8547, -3.4626],
        [-1.3993, -1.9415]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1255141794681549
RAW KL tensor(0.0672, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 574: train/loss = 0.5671720504760742, train/raw-loss = 0.5659713745117188, train/logprobs = tensor([[-1.1574, -1.9458],
        [-1.1834, -1.1646]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12006871402263641
RAW KL tensor(0.0813, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 575: train/loss = 0.4095888137817383, train/raw-loss = 0.4080597758293152, train/logprobs = tensor([[-1.1175, -2.9451],
        [-1.2102, -1.4834]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1529063582420349
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 576: train/loss = 0.49574124813079834, train/raw-loss = 0.4951203465461731, train/logprobs = tensor([[-1.0123, -1.8620],
        [-1.1077, -0.9974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06208808347582817
RAW KL tensor(0.1003, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 577: train/loss = 0.3656863868236542, train/raw-loss = 0.3641543388366699, train/logprobs = tensor([[-1.9205, -3.9634],
        [-1.7646, -2.0101]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15320375561714172
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 578: train/loss = 0.4701661467552185, train/raw-loss = 0.4691842496395111, train/logprobs = tensor([[-1.5248, -2.6585],
        [-1.3478, -1.0215]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09818843752145767
RAW KL tensor(0.0528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 579: train/loss = 0.4514417350292206, train/raw-loss = 0.4506362974643707, train/logprobs = tensor([[-0.8916, -2.4019],
        [-0.8850, -1.0930]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08054588735103607
RAW KL tensor(0.1313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 580: train/loss = 0.5994131565093994, train/raw-loss = 0.5984481573104858, train/logprobs = tensor([[-1.4302, -2.4288],
        [-1.1691, -1.3758]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09648950397968292
RAW KL tensor(0.1122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 581: train/loss = 0.278688907623291, train/raw-loss = 0.27671146392822266, train/logprobs = tensor([[-0.6120, -4.3812],
        [-0.6537, -2.0789]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19774261116981506
RAW KL tensor(0.2633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 582: train/loss = 0.4376179575920105, train/raw-loss = 0.4362572133541107, train/logprobs = tensor([[-1.4189, -2.4995],
        [-1.4310, -1.2794]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1360759735107422
RAW KL tensor(0.0826, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 583: train/loss = 0.34972748160362244, train/raw-loss = 0.34833359718322754, train/logprobs = tensor([[-1.0536, -3.8203],
        [-1.0522, -2.0065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13938722014427185
RAW KL tensor(0.0808, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 584: train/loss = 0.44008785486221313, train/raw-loss = 0.438897967338562, train/logprobs = tensor([[-1.3988, -2.9785],
        [-1.4032, -1.5910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11899184435606003
RAW KL tensor(0.1338, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 585: train/loss = 0.46281349658966064, train/raw-loss = 0.4619048535823822, train/logprobs = tensor([[-0.7014, -2.9538],
        [-0.8302, -1.7399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09086190909147263
RAW KL tensor(0.1171, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 586: train/loss = 0.5988081693649292, train/raw-loss = 0.597885251045227, train/logprobs = tensor([[-2.4378, -1.8693],
        [-2.0541, -1.0578]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09229996800422668
RAW KL tensor(0.0549, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 587: train/loss = 0.47556740045547485, train/raw-loss = 0.47481948137283325, train/logprobs = tensor([[-0.6946, -2.1022],
        [-0.8261, -1.0018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07479080557823181
RAW KL tensor(0.0889, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 588: train/loss = 0.45221564173698425, train/raw-loss = 0.4514513611793518, train/logprobs = tensor([[-1.6793, -2.5613],
        [-1.5125, -1.2189]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07642543315887451
RAW KL tensor(0.1777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 589: train/loss = 0.5881659984588623, train/raw-loss = 0.5865381956100464, train/logprobs = tensor([[-2.0566, -2.9038],
        [-1.0878, -1.3244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16278281807899475
RAW KL tensor(0.1026, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 296])
Epoch 0, Step 590: train/loss = 0.4350094497203827, train/raw-loss = 0.4339861571788788, train/logprobs = tensor([[-1.5250, -3.0495],
        [-1.3311, -1.5649]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10233043134212494
RAW KL tensor(0.1014, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 591: train/loss = 0.4523152709007263, train/raw-loss = 0.45114320516586304, train/logprobs = tensor([[-1.2633, -2.3631],
        [-1.3508, -1.1566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1172088086605072
RAW KL tensor(0.1357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 592: train/loss = 0.5462687015533447, train/raw-loss = 0.5451049208641052, train/logprobs = tensor([[-2.1038, -2.1575],
        [-1.7358, -0.9897]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11637502908706665
RAW KL tensor(0.2261, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 593: train/loss = 0.33700668811798096, train/raw-loss = 0.33520573377609253, train/logprobs = tensor([[-1.4654, -3.5386],
        [-1.3779, -1.2601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1800972819328308
RAW KL tensor(0.1528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 594: train/loss = 0.5588890910148621, train/raw-loss = 0.5579646825790405, train/logprobs = tensor([[-1.0932, -2.0191],
        [-0.9775, -1.2096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09243719279766083
RAW KL tensor(0.0409, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 595: train/loss = 0.5165345668792725, train/raw-loss = 0.5150682926177979, train/logprobs = tensor([[-1.4863, -2.3734],
        [-1.2302, -1.2153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14662864804267883
RAW KL tensor(0.1327, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 596: train/loss = 0.49751168489456177, train/raw-loss = 0.49670928716659546, train/logprobs = tensor([[-1.0249, -2.1767],
        [-1.0045, -1.1336]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08024197071790695
RAW KL tensor(0.1467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 597: train/loss = 0.3101726770401001, train/raw-loss = 0.3089415431022644, train/logprobs = tensor([[-0.9441, -3.3981],
        [-0.9620, -1.1227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12311220914125443
RAW KL tensor(0.0203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 598: train/loss = 0.4070819020271301, train/raw-loss = 0.4060799181461334, train/logprobs = tensor([[-0.9657, -2.6841],
        [-0.9705, -1.0776]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10019773244857788
RAW KL tensor(0.1505, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 599: train/loss = 0.31470823287963867, train/raw-loss = 0.31310078501701355, train/logprobs = tensor([[-1.4110, -3.5566],
        [-1.2965, -1.3327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16074323654174805
RAW KL tensor(0.1368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 600: train/loss = 0.535511314868927, train/raw-loss = 0.5343796610832214, train/logprobs = tensor([[-1.3358, -1.8949],
        [-1.3736, -1.1879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11316417157649994
RAW KL tensor(0.0911, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 601: train/loss = 0.39310112595558167, train/raw-loss = 0.3918071687221527, train/logprobs = tensor([[-1.1886, -2.7851],
        [-1.1865, -1.2383]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12939420342445374
RAW KL tensor(0.3320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 602: train/loss = 0.45865917205810547, train/raw-loss = 0.4573594927787781, train/logprobs = tensor([[-1.4853, -2.3419],
        [-1.1829, -0.8914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12996666133403778
RAW KL tensor(0.0992, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 603: train/loss = 0.41660386323928833, train/raw-loss = 0.4152432382106781, train/logprobs = tensor([[-0.9102, -2.8379],
        [-1.0543, -1.3882]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13606607913970947
RAW KL tensor(0.1540, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 604: train/loss = 0.26994022727012634, train/raw-loss = 0.26845353841781616, train/logprobs = tensor([[-1.1708, -3.9230],
        [-1.2415, -1.3633]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1486707329750061
RAW KL tensor(0.0597, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 605: train/loss = 0.39275500178337097, train/raw-loss = 0.39144062995910645, train/logprobs = tensor([[-1.1807, -2.9091],
        [-1.2707, -1.2170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13143616914749146
RAW KL tensor(0.1262, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 606: train/loss = 0.4716634154319763, train/raw-loss = 0.47050756216049194, train/logprobs = tensor([[-1.4940, -2.5063],
        [-1.6680, -1.4424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11558393388986588
RAW KL tensor(0.1709, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 607: train/loss = 0.39104098081588745, train/raw-loss = 0.38847529888153076, train/logprobs = tensor([[-1.5734, -2.9445],
        [-1.5662, -1.2735]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2565646171569824
RAW KL tensor(0.1034, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 608: train/loss = 0.4531433582305908, train/raw-loss = 0.4522375762462616, train/logprobs = tensor([[-1.2328, -3.7995],
        [-1.1271, -2.0360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09058094769716263
RAW KL tensor(0.0838, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 609: train/loss = 0.376748651266098, train/raw-loss = 0.37595686316490173, train/logprobs = tensor([[-1.6337, -3.0624],
        [-1.5175, -1.2187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07917849719524384
RAW KL tensor(0.0885, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 251])
Epoch 0, Step 610: train/loss = 0.4734612703323364, train/raw-loss = 0.4727686941623688, train/logprobs = tensor([[-1.3257, -3.3627],
        [-0.7938, -1.4216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06925949454307556
RAW KL tensor(0.1423, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 611: train/loss = 0.5762873291969299, train/raw-loss = 0.5754019021987915, train/logprobs = tensor([[-1.4459, -1.9492],
        [-1.1927, -1.0564]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08854415267705917
RAW KL tensor(0.0664, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 612: train/loss = 0.46090641617774963, train/raw-loss = 0.4598037600517273, train/logprobs = tensor([[-1.7032, -3.0680],
        [-1.1958, -1.3911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11026500165462494
RAW KL tensor(0.1290, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 613: train/loss = 0.4875630736351013, train/raw-loss = 0.4864082932472229, train/logprobs = tensor([[-1.1653, -3.9682],
        [-0.7605, -2.2888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11547418683767319
RAW KL tensor(0.1212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 614: train/loss = 0.4366329312324524, train/raw-loss = 0.4354505240917206, train/logprobs = tensor([[-1.4176, -3.9081],
        [-0.9943, -2.1046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11823789775371552
RAW KL tensor(0.0307, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 615: train/loss = 0.3969886600971222, train/raw-loss = 0.39604005217552185, train/logprobs = tensor([[-0.7612, -2.9440],
        [-0.6416, -0.8203]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09486092627048492
RAW KL tensor(0.1205, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 616: train/loss = 0.3965722918510437, train/raw-loss = 0.3949936628341675, train/logprobs = tensor([[-2.3325, -4.6357],
        [-1.7120, -2.3849]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15786641836166382
RAW KL tensor(0.0701, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 617: train/loss = 0.4720551073551178, train/raw-loss = 0.4712963402271271, train/logprobs = tensor([[-1.1816, -2.4319],
        [-1.0278, -1.1806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07587762176990509
RAW KL tensor(0.1619, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 618: train/loss = 0.3830083906650543, train/raw-loss = 0.3819890320301056, train/logprobs = tensor([[-1.1871, -2.9032],
        [-0.9379, -1.0181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10193323343992233
RAW KL tensor(0.3450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 619: train/loss = 0.5739260911941528, train/raw-loss = 0.5719010829925537, train/logprobs = tensor([[-2.7434, -3.5821],
        [-1.2513, -1.0170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2025010734796524
RAW KL tensor(0.1177, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 620: train/loss = 0.46496766805648804, train/raw-loss = 0.46400800347328186, train/logprobs = tensor([[-1.1581, -2.7749],
        [-1.0143, -1.3478]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09596769511699677
RAW KL tensor(0.2064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 621: train/loss = 0.3771384656429291, train/raw-loss = 0.37601593136787415, train/logprobs = tensor([[-2.0525, -3.4507],
        [-1.6159, -1.2011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1122547835111618
RAW KL tensor(0.0787, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 622: train/loss = 0.5277537107467651, train/raw-loss = 0.5268712639808655, train/logprobs = tensor([[-1.1068, -3.4406],
        [-1.0070, -2.2555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08824743330478668
RAW KL tensor(0.1511, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 623: train/loss = 0.4048101007938385, train/raw-loss = 0.4037906527519226, train/logprobs = tensor([[-2.8560, -4.8579],
        [-2.4314, -2.3977]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10194563120603561
RAW KL tensor(0.1987, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 624: train/loss = 0.5209116339683533, train/raw-loss = 0.5195597410202026, train/logprobs = tensor([[-1.5645, -2.1543],
        [-1.3080, -0.9789]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13518431782722473
RAW KL tensor(0.0596, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 625: train/loss = 0.4823755621910095, train/raw-loss = 0.48034361004829407, train/logprobs = tensor([[-1.3706, -2.4428],
        [-1.5139, -1.2252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20319615304470062
RAW KL tensor(0.0902, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 626: train/loss = 0.45520174503326416, train/raw-loss = 0.45381850004196167, train/logprobs = tensor([[-1.6931, -3.2509],
        [-1.2882, -1.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13832338154315948
RAW KL tensor(0.1406, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 627: train/loss = 0.45013871788978577, train/raw-loss = 0.44887667894363403, train/logprobs = tensor([[-0.9444, -2.9230],
        [-0.8075, -1.3148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1262088268995285
RAW KL tensor(0.1234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 628: train/loss = 0.3500930666923523, train/raw-loss = 0.3480563163757324, train/logprobs = tensor([[-1.3456, -3.8669],
        [-0.9200, -1.4544]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20367538928985596
RAW KL tensor(0.1898, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 629: train/loss = 0.41352906823158264, train/raw-loss = 0.41230154037475586, train/logprobs = tensor([[-1.5303, -2.8765],
        [-1.2958, -1.0327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12275168299674988
RAW KL tensor(0.0508, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 630: train/loss = 0.2853531837463379, train/raw-loss = 0.2842538356781006, train/logprobs = tensor([[-0.8867, -3.9757],
        [-0.8395, -1.3220]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10993730276823044
RAW KL tensor(0.0755, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 631: train/loss = 0.6041335463523865, train/raw-loss = 0.6036057472229004, train/logprobs = tensor([[-1.0077, -1.4534],
        [-0.8836, -0.9186]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05277912691235542
RAW KL tensor(0.1066, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 632: train/loss = 0.43330517411231995, train/raw-loss = 0.43255579471588135, train/logprobs = tensor([[-1.1364, -2.8001],
        [-1.0814, -1.3314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07493755966424942
RAW KL tensor(0.2909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 633: train/loss = 0.39173659682273865, train/raw-loss = 0.3901146352291107, train/logprobs = tensor([[-0.9365, -3.7234],
        [-0.9742, -1.6609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16219647228717804
RAW KL tensor(0.1474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 634: train/loss = 0.38546818494796753, train/raw-loss = 0.3836587369441986, train/logprobs = tensor([[-1.6299, -3.5276],
        [-1.3733, -1.5137]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1809496283531189
RAW KL tensor(0.0344, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 635: train/loss = 0.46277421712875366, train/raw-loss = 0.4615170359611511, train/logprobs = tensor([[-1.4099, -2.4638],
        [-1.2651, -1.0015]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.125718891620636
RAW KL tensor(0.0931, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 636: train/loss = 0.4971250891685486, train/raw-loss = 0.49597930908203125, train/logprobs = tensor([[-2.0438, -2.8645],
        [-1.5253, -1.2535]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11457768827676773
RAW KL tensor(0.0263, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 637: train/loss = 0.5223242044448853, train/raw-loss = 0.5213133096694946, train/logprobs = tensor([[-1.4702, -2.3190],
        [-1.3977, -1.1740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10109078139066696
RAW KL tensor(0.0317, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 638: train/loss = 0.49333441257476807, train/raw-loss = 0.4926086962223053, train/logprobs = tensor([[-1.2055, -2.2358],
        [-1.1873, -1.0847]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0725691094994545
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 305])
Epoch 0, Step 639: train/loss = 0.5777922868728638, train/raw-loss = 0.5770010948181152, train/logprobs = tensor([[-1.6082, -2.1288],
        [-1.2485, -1.1668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07911834120750427
RAW KL tensor(0.0248, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 640: train/loss = 0.3509571850299835, train/raw-loss = 0.34997761249542236, train/logprobs = tensor([[-0.9296, -3.4143],
        [-0.7017, -0.9546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09795612096786499
RAW KL tensor(0.2133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 641: train/loss = 0.48535412549972534, train/raw-loss = 0.48434925079345703, train/logprobs = tensor([[-1.0610, -2.5083],
        [-1.2053, -1.2464]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10048974305391312
RAW KL tensor(0.0728, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 642: train/loss = 0.5998004674911499, train/raw-loss = 0.5992095470428467, train/logprobs = tensor([[-0.9409, -1.8029],
        [-0.7623, -1.1449]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0590871199965477
RAW KL tensor(0.1227, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 643: train/loss = 0.5403836965560913, train/raw-loss = 0.5391985774040222, train/logprobs = tensor([[-1.0014, -2.3267],
        [-0.9626, -1.4967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11851000785827637
RAW KL tensor(0.1306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 644: train/loss = 0.39805543422698975, train/raw-loss = 0.3964601159095764, train/logprobs = tensor([[-1.5557, -3.5892],
        [-1.6148, -1.8143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15953171253204346
RAW KL tensor(0.0205, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 645: train/loss = 0.5595728158950806, train/raw-loss = 0.5590667724609375, train/logprobs = tensor([[-1.2828, -2.0079],
        [-0.9039, -1.0106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.050595857203006744
RAW KL tensor(0.1322, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 646: train/loss = 0.38982653617858887, train/raw-loss = 0.38885658979415894, train/logprobs = tensor([[-1.3879, -3.5854],
        [-1.2229, -1.7661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09699629247188568
RAW KL tensor(0.1282, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 647: train/loss = 0.4660130441188812, train/raw-loss = 0.4647587835788727, train/logprobs = tensor([[-1.7169, -2.7435],
        [-1.5052, -1.2359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.125425785779953
RAW KL tensor(0.0963, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 648: train/loss = 0.4267057776451111, train/raw-loss = 0.42575138807296753, train/logprobs = tensor([[-1.1139, -3.9155],
        [-0.8190, -1.6533]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09543910622596741
RAW KL tensor(0.0330, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 649: train/loss = 0.5583509802818298, train/raw-loss = 0.5573834180831909, train/logprobs = tensor([[-0.9648, -2.1377],
        [-0.8422, -1.0661]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09675518423318863
RAW KL tensor(0.2679, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 650: train/loss = 0.5743502378463745, train/raw-loss = 0.5728791356086731, train/logprobs = tensor([[-1.8812, -2.8205],
        [-1.3267, -1.5815]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14710786938667297
RAW KL tensor(0.0704, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 651: train/loss = 0.4222303032875061, train/raw-loss = 0.4214105010032654, train/logprobs = tensor([[-1.8038, -2.4489],
        [-1.6365, -0.7831]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08198089897632599
RAW KL tensor(0.2001, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 652: train/loss = 0.35615110397338867, train/raw-loss = 0.354375422000885, train/logprobs = tensor([[-1.1291, -3.4240],
        [-0.8982, -0.9599]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1775682270526886
RAW KL tensor(0.2064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 653: train/loss = 0.49628567695617676, train/raw-loss = 0.49515870213508606, train/logprobs = tensor([[-0.9673, -2.0243],
        [-0.6889, -0.7371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1126965582370758
RAW KL tensor(0.0303, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 246])
Epoch 0, Step 654: train/loss = 0.46000421047210693, train/raw-loss = 0.45914143323898315, train/logprobs = tensor([[-1.2502, -2.1394],
        [-1.2347, -0.9767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08627597987651825
RAW KL tensor(0.2078, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 655: train/loss = 0.3885929584503174, train/raw-loss = 0.38715171813964844, train/logprobs = tensor([[-1.2551, -3.3404],
        [-1.2391, -1.7170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14412125945091248
RAW KL tensor(0.1051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 656: train/loss = 0.47214943170547485, train/raw-loss = 0.47134220600128174, train/logprobs = tensor([[-0.9040, -2.5670],
        [-0.7574, -1.2870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08072535693645477
RAW KL tensor(0.0291, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 657: train/loss = 0.5422651767730713, train/raw-loss = 0.5416346192359924, train/logprobs = tensor([[-1.1643, -1.9242],
        [-1.0478, -1.0015]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06305547058582306
RAW KL tensor(0.0356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 658: train/loss = 0.48853302001953125, train/raw-loss = 0.48760199546813965, train/logprobs = tensor([[-1.0941, -1.6914],
        [-1.2035, -0.7429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0931016355752945
RAW KL tensor(0.2548, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 659: train/loss = 0.7632899880409241, train/raw-loss = 0.7609823346138, train/logprobs = tensor([[-1.1612, -5.1518],
        [-1.0917, -3.3301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23077112436294556
RAW KL tensor(0.0988, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 660: train/loss = 0.604345440864563, train/raw-loss = 0.6030951738357544, train/logprobs = tensor([[-1.5026, -3.5381],
        [-0.8377, -1.8718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12502479553222656
RAW KL tensor(0.2316, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 661: train/loss = 0.38183748722076416, train/raw-loss = 0.380109965801239, train/logprobs = tensor([[-1.4848, -3.5165],
        [-1.5530, -1.7865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1727548986673355
RAW KL tensor(0.1151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 662: train/loss = 0.4054815173149109, train/raw-loss = 0.4044271409511566, train/logprobs = tensor([[-1.0635, -2.8686],
        [-1.0087, -1.2221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10543981939554214
RAW KL tensor(0.0889, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 663: train/loss = 0.3917611837387085, train/raw-loss = 0.39080825448036194, train/logprobs = tensor([[-1.4171, -2.8592],
        [-1.2169, -0.8818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09529362618923187
RAW KL tensor(0.1294, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 664: train/loss = 0.41240742802619934, train/raw-loss = 0.41114169359207153, train/logprobs = tensor([[-1.7608, -4.0923],
        [-1.6500, -2.3650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12657538056373596
RAW KL tensor(0.1711, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 665: train/loss = 0.3517918884754181, train/raw-loss = 0.3506309986114502, train/logprobs = tensor([[-1.3450, -3.6017],
        [-1.1569, -1.5846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11608871817588806
RAW KL tensor(0.0834, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 666: train/loss = 0.4333815574645996, train/raw-loss = 0.4322446584701538, train/logprobs = tensor([[-1.5342, -2.9220],
        [-1.4604, -1.5282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11369355022907257
RAW KL tensor(0.1135, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 667: train/loss = 0.534569263458252, train/raw-loss = 0.5336154103279114, train/logprobs = tensor([[-1.1814, -1.6734],
        [-1.1140, -0.8166]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0953826904296875
RAW KL tensor(0.0846, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 668: train/loss = 0.38249605894088745, train/raw-loss = 0.38167616724967957, train/logprobs = tensor([[-1.2845, -3.2833],
        [-1.1761, -1.3529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08198767900466919
RAW KL tensor(0.1982, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 669: train/loss = 0.3103099465370178, train/raw-loss = 0.30899789929389954, train/logprobs = tensor([[-1.1222, -4.3895],
        [-0.9837, -1.7961]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13120704889297485
RAW KL tensor(0.0247, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 670: train/loss = 0.6215735077857971, train/raw-loss = 0.6203435659408569, train/logprobs = tensor([[-1.8580, -2.0723],
        [-1.2226, -1.0874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1229916587471962
RAW KL tensor(0.0763, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 671: train/loss = 0.6418148279190063, train/raw-loss = 0.6412172913551331, train/logprobs = tensor([[-1.2326, -2.1225],
        [-0.7080, -1.1541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059749722480773926
RAW KL tensor(0.1446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 672: train/loss = 0.34255966544151306, train/raw-loss = 0.3409235179424286, train/logprobs = tensor([[-1.8369, -3.6250],
        [-1.3121, -1.1025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16361801326274872
RAW KL tensor(0.2040, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 117])
Epoch 0, Step 673: train/loss = 0.33400216698646545, train/raw-loss = 0.33240729570388794, train/logprobs = tensor([[-1.4977, -3.5700],
        [-1.1663, -1.2741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15948493778705597
RAW KL tensor(0.1578, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 674: train/loss = 0.26484930515289307, train/raw-loss = 0.26273906230926514, train/logprobs = tensor([[-1.4131, -4.2299],
        [-1.5073, -1.3945]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2110254168510437
RAW KL tensor(0.0609, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 675: train/loss = 0.37713149189949036, train/raw-loss = 0.37618735432624817, train/logprobs = tensor([[-1.0165, -3.1886],
        [-0.9928, -1.5334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09441535919904709
RAW KL tensor(0.1773, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 676: train/loss = 0.4133460521697998, train/raw-loss = 0.4118340015411377, train/logprobs = tensor([[-1.6060, -3.0208],
        [-1.3304, -1.2000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15120501816272736
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 677: train/loss = 0.3236162066459656, train/raw-loss = 0.32218995690345764, train/logprobs = tensor([[-1.3675, -3.6295],
        [-1.3756, -1.5245]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14262627065181732
RAW KL tensor(0.1517, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 678: train/loss = 0.28490304946899414, train/raw-loss = 0.283405601978302, train/logprobs = tensor([[-1.4723, -4.4850],
        [-1.4020, -1.7703]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14974462985992432
RAW KL tensor(0.0714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 679: train/loss = 0.5455052852630615, train/raw-loss = 0.5443963408470154, train/logprobs = tensor([[-1.4525, -1.8581],
        [-1.2769, -0.8692]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11089174449443817
RAW KL tensor(0.0960, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 680: train/loss = 0.43284446001052856, train/raw-loss = 0.4311605989933014, train/logprobs = tensor([[-2.0152, -3.2423],
        [-1.7777, -1.3558]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16838565468788147
RAW KL tensor(0.2287, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 681: train/loss = 0.4237499535083771, train/raw-loss = 0.42205947637557983, train/logprobs = tensor([[-1.0082, -3.0274],
        [-0.9156, -1.3296]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1690444052219391
RAW KL tensor(0.3559, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 682: train/loss = 0.2840155065059662, train/raw-loss = 0.28144699335098267, train/logprobs = tensor([[-1.4959, -4.2657],
        [-1.5854, -1.5887]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25685253739356995
RAW KL tensor(0.1357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 683: train/loss = 0.48132559657096863, train/raw-loss = 0.47909554839134216, train/logprobs = tensor([[-2.2330, -2.9920],
        [-1.6485, -1.1400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22300827503204346
RAW KL tensor(0.1125, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 684: train/loss = 0.4024839699268341, train/raw-loss = 0.40153229236602783, train/logprobs = tensor([[-1.0278, -2.5782],
        [-1.1727, -1.1169]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0951673686504364
RAW KL tensor(0.1210, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 685: train/loss = 0.32721632719039917, train/raw-loss = 0.3260076642036438, train/logprobs = tensor([[-1.3085, -3.6974],
        [-1.2865, -1.5258]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12086669355630875
RAW KL tensor(0.0463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 686: train/loss = 0.3656526505947113, train/raw-loss = 0.3644554018974304, train/logprobs = tensor([[-0.9169, -3.1939],
        [-0.8613, -0.9396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11972470581531525
RAW KL tensor(0.1140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 687: train/loss = 0.4029277265071869, train/raw-loss = 0.4015886187553406, train/logprobs = tensor([[-1.5625, -3.0051],
        [-1.4885, -1.2335]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13391223549842834
RAW KL tensor(0.1078, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 688: train/loss = 0.344940185546875, train/raw-loss = 0.34311699867248535, train/logprobs = tensor([[-1.5515, -4.3252],
        [-1.3402, -1.8419]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18231874704360962
RAW KL tensor(0.0698, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 689: train/loss = 0.37311768531799316, train/raw-loss = 0.3718336820602417, train/logprobs = tensor([[-0.8426, -3.0226],
        [-0.7711, -0.8615]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1284010112285614
RAW KL tensor(0.1442, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 690: train/loss = 0.3717871308326721, train/raw-loss = 0.37020695209503174, train/logprobs = tensor([[-1.5083, -3.4856],
        [-1.2125, -1.1895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1580209881067276
RAW KL tensor(0.2422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 691: train/loss = 0.27744483947753906, train/raw-loss = 0.2755289375782013, train/logprobs = tensor([[-0.9890, -3.9058],
        [-0.9484, -1.1136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19158929586410522
RAW KL tensor(0.1965, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 692: train/loss = 0.38802650570869446, train/raw-loss = 0.38655972480773926, train/logprobs = tensor([[-1.2250, -3.1089],
        [-1.0032, -1.2433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14667564630508423
RAW KL tensor(0.1126, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 693: train/loss = 0.4043552279472351, train/raw-loss = 0.403229683637619, train/logprobs = tensor([[-1.6268, -3.4822],
        [-1.5525, -1.3805]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11255644261837006
RAW KL tensor(0.1510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 694: train/loss = 0.34305334091186523, train/raw-loss = 0.3414013981819153, train/logprobs = tensor([[-1.5000, -3.7746],
        [-1.3006, -1.3170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16519266366958618
RAW KL tensor(0.2377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 695: train/loss = 0.32226356863975525, train/raw-loss = 0.32022789120674133, train/logprobs = tensor([[-1.2455, -3.4691],
        [-1.2073, -0.9973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2035687416791916
RAW KL tensor(0.1510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 696: train/loss = 0.2601901888847351, train/raw-loss = 0.2582453489303589, train/logprobs = tensor([[-1.1395, -4.0699],
        [-1.0432, -1.3691]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.194484680891037
RAW KL tensor(0.1387, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 697: train/loss = 0.34633320569992065, train/raw-loss = 0.3441427946090698, train/logprobs = tensor([[-1.2163, -3.3677],
        [-1.3628, -1.4538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2190406620502472
RAW KL tensor(0.1662, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 698: train/loss = 0.3220303952693939, train/raw-loss = 0.32031744718551636, train/logprobs = tensor([[-1.8079, -4.6144],
        [-1.1934, -1.3017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17129510641098022
RAW KL tensor(0.0449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 699: train/loss = 0.4265598654747009, train/raw-loss = 0.42534998059272766, train/logprobs = tensor([[-1.2915, -2.9992],
        [-1.1779, -1.1389]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12098877131938934
RAW KL tensor(0.0245, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 700: train/loss = 0.3822903633117676, train/raw-loss = 0.38100099563598633, train/logprobs = tensor([[-0.7330, -3.4595],
        [-0.6250, -1.2489]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1289362907409668
RAW KL tensor(0.1251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 701: train/loss = 0.40902164578437805, train/raw-loss = 0.4077390432357788, train/logprobs = tensor([[-1.1879, -2.8444],
        [-1.3645, -1.2505]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12826019525527954
RAW KL tensor(0.1934, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 289])
Epoch 0, Step 702: train/loss = 0.3339448869228363, train/raw-loss = 0.3323880136013031, train/logprobs = tensor([[-1.9756, -3.4288],
        [-1.9581, -1.2586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1556900292634964
RAW KL tensor(0.1464, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 703: train/loss = 0.39739036560058594, train/raw-loss = 0.39520493149757385, train/logprobs = tensor([[-1.8661, -4.5713],
        [-1.3499, -1.9790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21854332089424133
RAW KL tensor(0.0701, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 704: train/loss = 0.3148587942123413, train/raw-loss = 0.31371015310287476, train/logprobs = tensor([[-0.9958, -3.9830],
        [-1.1707, -2.0550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11486557871103287
RAW KL tensor(0.1503, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 705: train/loss = 0.4759056866168976, train/raw-loss = 0.4745537042617798, train/logprobs = tensor([[-1.5007, -2.9267],
        [-1.4583, -1.7448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13519622385501862
RAW KL tensor(0.2572, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 706: train/loss = 0.3245178163051605, train/raw-loss = 0.3227512836456299, train/logprobs = tensor([[-1.2558, -4.2347],
        [-0.8819, -1.3439]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17665386199951172
RAW KL tensor(0.2118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 707: train/loss = 0.49191099405288696, train/raw-loss = 0.4904139041900635, train/logprobs = tensor([[-1.5140, -2.5750],
        [-1.0946, -1.1163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1497119516134262
RAW KL tensor(0.2336, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 708: train/loss = 0.5279926061630249, train/raw-loss = 0.5267271995544434, train/logprobs = tensor([[-1.0939, -1.9257],
        [-1.3549, -1.3658]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1265319585800171
RAW KL tensor(0.1260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 709: train/loss = 0.45575395226478577, train/raw-loss = 0.4547235667705536, train/logprobs = tensor([[-0.8597, -2.2328],
        [-1.0193, -1.1781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10304156690835953
RAW KL tensor(0.0574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 710: train/loss = 0.3115408420562744, train/raw-loss = 0.3099491000175476, train/logprobs = tensor([[-0.7632, -3.6476],
        [-0.6814, -0.8724]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1591757982969284
RAW KL tensor(0.1816, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 711: train/loss = 0.3974483907222748, train/raw-loss = 0.39587655663490295, train/logprobs = tensor([[-1.8087, -3.1729],
        [-1.9557, -1.7000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1571846604347229
RAW KL tensor(0.0965, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 712: train/loss = 0.2816181778907776, train/raw-loss = 0.2799866497516632, train/logprobs = tensor([[-0.8582, -3.8182],
        [-0.8423, -1.2957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16315418481826782
RAW KL tensor(0.1102, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 713: train/loss = 0.2866513729095459, train/raw-loss = 0.28492748737335205, train/logprobs = tensor([[-1.0304, -3.5502],
        [-1.0318, -1.1624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17238788306713104
RAW KL tensor(0.1710, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 714: train/loss = 0.5152769684791565, train/raw-loss = 0.5141888856887817, train/logprobs = tensor([[-0.9784, -1.5544],
        [-1.2155, -0.8480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10880941897630692
RAW KL tensor(0.0903, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 715: train/loss = 0.5138130187988281, train/raw-loss = 0.5128070712089539, train/logprobs = tensor([[-1.1764, -1.6581],
        [-1.5336, -1.0648]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10059560835361481
RAW KL tensor(0.2553, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 716: train/loss = 0.3297315537929535, train/raw-loss = 0.32783186435699463, train/logprobs = tensor([[-1.5274, -3.6047],
        [-1.5183, -1.1029]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18997034430503845
RAW KL tensor(0.0980, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 717: train/loss = 0.2766796350479126, train/raw-loss = 0.27511411905288696, train/logprobs = tensor([[-1.4510, -3.5646],
        [-1.4978, -1.1083]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15655305981636047
RAW KL tensor(0.5255, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 108])
Epoch 0, Step 718: train/loss = 0.36268168687820435, train/raw-loss = 0.35889899730682373, train/logprobs = tensor([[-2.2212, -4.9145],
        [-1.2949, -1.4100]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.37827110290527344
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 719: train/loss = 0.5824085474014282, train/raw-loss = 0.581380307674408, train/logprobs = tensor([[-1.1422, -1.3878],
        [-1.2363, -0.9557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1028231605887413
RAW KL tensor(0.0446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 720: train/loss = 0.4462473690509796, train/raw-loss = 0.44519609212875366, train/logprobs = tensor([[-0.9557, -2.5781],
        [-1.0849, -1.1906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10512755811214447
RAW KL tensor(0.2455, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 721: train/loss = 0.43569672107696533, train/raw-loss = 0.4338447153568268, train/logprobs = tensor([[-1.4967, -2.9923],
        [-1.2518, -0.8873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18520605564117432
RAW KL tensor(0.0882, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 722: train/loss = 0.35199010372161865, train/raw-loss = 0.3508424162864685, train/logprobs = tensor([[-1.0217, -3.2831],
        [-1.1145, -1.1328]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11476873606443405
RAW KL tensor(0.2006, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 723: train/loss = 0.34535470604896545, train/raw-loss = 0.34331753849983215, train/logprobs = tensor([[-1.7328, -4.2157],
        [-1.0788, -1.5463]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20371562242507935
RAW KL tensor(0.0951, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 724: train/loss = 0.43709978461265564, train/raw-loss = 0.43596434593200684, train/logprobs = tensor([[-0.9217, -2.7354],
        [-1.1629, -1.3951]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11354480683803558
RAW KL tensor(0.1410, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 725: train/loss = 0.4255237579345703, train/raw-loss = 0.42374134063720703, train/logprobs = tensor([[-1.4253, -2.4576],
        [-1.5615, -1.1367]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1782475709915161
RAW KL tensor(0.0682, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 358])
Epoch 0, Step 726: train/loss = 0.4414011538028717, train/raw-loss = 0.4397279620170593, train/logprobs = tensor([[-1.1348, -2.2689],
        [-1.3972, -0.9504]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1673203706741333
RAW KL tensor(0.0416, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 727: train/loss = 0.46947726607322693, train/raw-loss = 0.46863383054733276, train/logprobs = tensor([[-0.8418, -2.2216],
        [-0.9803, -1.0950]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08434329181909561
RAW KL tensor(0.1954, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 728: train/loss = 0.43501952290534973, train/raw-loss = 0.433851420879364, train/logprobs = tensor([[-1.5573, -2.6937],
        [-1.7038, -1.4359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11681268364191055
RAW KL tensor(0.2562, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 323])
Epoch 0, Step 729: train/loss = 0.21431800723075867, train/raw-loss = 0.21229872107505798, train/logprobs = tensor([[-1.0063, -4.2950],
        [-1.0996, -1.3035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20193055272102356
RAW KL tensor(0.1191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 730: train/loss = 0.500890851020813, train/raw-loss = 0.4993390440940857, train/logprobs = tensor([[-1.7119, -2.2884],
        [-1.9389, -1.4207]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15517643094062805
RAW KL tensor(0.1728, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 731: train/loss = 0.49707460403442383, train/raw-loss = 0.4958093762397766, train/logprobs = tensor([[-2.3840, -2.4958],
        [-2.0052, -1.0620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12652702629566193
RAW KL tensor(0.1341, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 732: train/loss = 0.38928496837615967, train/raw-loss = 0.38738033175468445, train/logprobs = tensor([[-2.0033, -3.1479],
        [-2.1664, -1.4332]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19046594202518463
RAW KL tensor(0.1661, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 733: train/loss = 0.2955997586250305, train/raw-loss = 0.29427370429039, train/logprobs = tensor([[-1.0769, -3.5018],
        [-1.3588, -1.5064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.132605642080307
RAW KL tensor(0.0874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 734: train/loss = 0.5128881931304932, train/raw-loss = 0.5117359757423401, train/logprobs = tensor([[-1.6197, -2.1962],
        [-1.1471, -0.8161]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11521951854228973
RAW KL tensor(0.3360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 735: train/loss = 0.681249737739563, train/raw-loss = 0.6797804832458496, train/logprobs = tensor([[-2.1357, -1.8457],
        [-1.4397, -1.0145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14693090319633484
RAW KL tensor(0.1305, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 736: train/loss = 0.24164016544818878, train/raw-loss = 0.23954513669013977, train/logprobs = tensor([[-1.3639, -4.2264],
        [-1.7977, -1.6423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20950090885162354
RAW KL tensor(0.0545, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 737: train/loss = 0.5038630366325378, train/raw-loss = 0.5021263957023621, train/logprobs = tensor([[-1.7095, -2.4499],
        [-1.6653, -1.1311]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17366193234920502
RAW KL tensor(0.6259, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 738: train/loss = 0.16944143176078796, train/raw-loss = 0.16673129796981812, train/logprobs = tensor([[-1.2066, -4.7455],
        [-1.7041, -1.7319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2710118293762207
RAW KL tensor(0.0595, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 739: train/loss = 0.15883731842041016, train/raw-loss = 0.15683355927467346, train/logprobs = tensor([[-1.5650, -5.2136],
        [-1.4611, -0.7381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20037627220153809
RAW KL tensor(0.0697, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 740: train/loss = 0.38143253326416016, train/raw-loss = 0.3797992467880249, train/logprobs = tensor([[-0.7589, -3.4087],
        [-1.0710, -1.3542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16332441568374634
RAW KL tensor(0.2650, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 741: train/loss = 0.3051728904247284, train/raw-loss = 0.3032969832420349, train/logprobs = tensor([[-1.7224, -4.1418],
        [-1.4581, -1.1342]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18759119510650635
RAW KL tensor(0.1891, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 742: train/loss = 0.3829349875450134, train/raw-loss = 0.38153308629989624, train/logprobs = tensor([[-0.8173, -3.1104],
        [-0.9275, -0.8863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14019440114498138
RAW KL tensor(0.1946, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 743: train/loss = 0.4296363294124603, train/raw-loss = 0.4278556704521179, train/logprobs = tensor([[-1.2322, -3.1479],
        [-1.0407, -1.3578]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17806583642959595
RAW KL tensor(0.0371, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 744: train/loss = 0.3696942627429962, train/raw-loss = 0.3687010109424591, train/logprobs = tensor([[-1.2425, -3.4743],
        [-1.5112, -2.0158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09932328015565872
RAW KL tensor(0.2146, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 745: train/loss = 0.256202757358551, train/raw-loss = 0.2537442147731781, train/logprobs = tensor([[-1.0841, -4.1606],
        [-1.4625, -1.7099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24585400521755219
RAW KL tensor(0.0885, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 746: train/loss = 0.3865559995174408, train/raw-loss = 0.38535523414611816, train/logprobs = tensor([[-0.7371, -3.7336],
        [-0.9368, -1.7869]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12007806450128555
RAW KL tensor(0.1375, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 747: train/loss = 0.589740514755249, train/raw-loss = 0.5882436633110046, train/logprobs = tensor([[-1.3865, -1.7933],
        [-1.4949, -1.2854]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14968301355838776
RAW KL tensor(0.2046, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 748: train/loss = 0.428588330745697, train/raw-loss = 0.4269772171974182, train/logprobs = tensor([[-1.4345, -3.5482],
        [-1.6844, -2.2426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16111338138580322
RAW KL tensor(0.1260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 749: train/loss = 0.35886693000793457, train/raw-loss = 0.3574114441871643, train/logprobs = tensor([[-1.6827, -2.9055],
        [-1.6731, -1.1863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14555028080940247
RAW KL tensor(0.0874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 750: train/loss = 0.39022472500801086, train/raw-loss = 0.3887624740600586, train/logprobs = tensor([[-1.4016, -3.2258],
        [-1.1962, -1.3457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14622828364372253
RAW KL tensor(0.1245, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 751: train/loss = 0.37208887934684753, train/raw-loss = 0.370837539434433, train/logprobs = tensor([[-1.1272, -2.8708],
        [-1.5670, -1.3025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12513604760169983
RAW KL tensor(0.2452, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 752: train/loss = 0.4638446569442749, train/raw-loss = 0.4628644585609436, train/logprobs = tensor([[-0.7133, -2.1502],
        [-0.9781, -0.9539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09802067279815674
RAW KL tensor(0.2001, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 753: train/loss = 0.30397099256515503, train/raw-loss = 0.30271148681640625, train/logprobs = tensor([[-0.9396, -2.9091],
        [-1.4199, -1.2582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12595102190971375
RAW KL tensor(0.1640, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 754: train/loss = 0.30058661103248596, train/raw-loss = 0.29839929938316345, train/logprobs = tensor([[-1.0876, -3.7959],
        [-1.2125, -1.5973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21873079240322113
RAW KL tensor(0.2581, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 755: train/loss = 0.2512381970882416, train/raw-loss = 0.24828681349754333, train/logprobs = tensor([[-1.5152, -4.6427],
        [-1.9522, -1.6714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2951411306858063
RAW KL tensor(0.2807, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 756: train/loss = 0.2834230959415436, train/raw-loss = 0.28166836500167847, train/logprobs = tensor([[-1.2074, -3.7254],
        [-1.6168, -1.5163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1754743605852127
RAW KL tensor(0.1147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 757: train/loss = 0.2762491703033447, train/raw-loss = 0.2744777500629425, train/logprobs = tensor([[-1.4062, -4.5775],
        [-1.9458, -2.3610]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17714382708072662
RAW KL tensor(0.2379, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 758: train/loss = 0.37628084421157837, train/raw-loss = 0.3749698996543884, train/logprobs = tensor([[-1.2100, -2.7199],
        [-1.1456, -0.8629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.131093829870224
RAW KL tensor(0.1073, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 759: train/loss = 0.27165013551712036, train/raw-loss = 0.2691395580768585, train/logprobs = tensor([[-1.7725, -3.7075],
        [-2.0213, -1.0449]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25105923414230347
RAW KL tensor(0.2032, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 760: train/loss = 0.22569197416305542, train/raw-loss = 0.2230706363916397, train/logprobs = tensor([[-1.8892, -4.6455],
        [-1.9792, -1.6873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2621336281299591
RAW KL tensor(0.2164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 761: train/loss = 0.4206813871860504, train/raw-loss = 0.4192758798599243, train/logprobs = tensor([[-1.3462, -2.5376],
        [-1.3561, -0.9807]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1405525505542755
RAW KL tensor(0.1675, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 762: train/loss = 0.39818835258483887, train/raw-loss = 0.39699026942253113, train/logprobs = tensor([[-1.7450, -3.5900],
        [-1.5703, -1.5971]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11981000006198883
RAW KL tensor(0.1222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 763: train/loss = 0.5619769096374512, train/raw-loss = 0.5610033273696899, train/logprobs = tensor([[-1.0412, -1.4969],
        [-1.1822, -1.0290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0973530262708664
RAW KL tensor(0.3067, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 764: train/loss = 0.266970694065094, train/raw-loss = 0.2648318111896515, train/logprobs = tensor([[-1.3048, -3.3956],
        [-1.7417, -1.3334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.213888019323349
RAW KL tensor(0.2428, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 765: train/loss = 0.23090466856956482, train/raw-loss = 0.22818705439567566, train/logprobs = tensor([[-1.0696, -4.5801],
        [-1.5179, -1.1302]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.27176228165626526
RAW KL tensor(0.2220, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 766: train/loss = 0.45397359132766724, train/raw-loss = 0.45255422592163086, train/logprobs = tensor([[-1.2113, -3.0826],
        [-1.7221, -2.1442]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14193153381347656
RAW KL tensor(0.1603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 767: train/loss = 0.4847342073917389, train/raw-loss = 0.48277705907821655, train/logprobs = tensor([[-1.3407, -2.7865],
        [-1.5418, -1.8252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19571411609649658
RAW KL tensor(0.1741, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 768: train/loss = 0.3516731560230255, train/raw-loss = 0.34988081455230713, train/logprobs = tensor([[-1.1398, -2.4223],
        [-1.7229, -0.9999]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17923328280448914
RAW KL tensor(0.1334, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 769: train/loss = 0.4137876629829407, train/raw-loss = 0.41261026263237, train/logprobs = tensor([[-1.1772, -2.9343],
        [-1.3747, -1.5877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11774411052465439
RAW KL tensor(0.0441, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 770: train/loss = 0.5457672476768494, train/raw-loss = 0.5447870492935181, train/logprobs = tensor([[-0.8081, -1.2855],
        [-1.1551, -0.7875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09802039712667465
RAW KL tensor(0.1764, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 771: train/loss = 0.2531665861606598, train/raw-loss = 0.25119948387145996, train/logprobs = tensor([[-1.2596, -3.5894],
        [-1.7204, -1.1874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1967116892337799
RAW KL tensor(0.3029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 772: train/loss = 0.35831159353256226, train/raw-loss = 0.3561876118183136, train/logprobs = tensor([[-1.1901, -2.7397],
        [-1.6756, -1.0281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2123999297618866
RAW KL tensor(0.0785, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 773: train/loss = 0.5271760821342468, train/raw-loss = 0.5256389379501343, train/logprobs = tensor([[-1.7942, -2.2868],
        [-1.5435, -1.1099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15371793508529663
RAW KL tensor(0.1108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 774: train/loss = 0.3767242431640625, train/raw-loss = 0.3750344514846802, train/logprobs = tensor([[-1.4746, -3.1537],
        [-1.8469, -1.4935]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16897839307785034
RAW KL tensor(0.2442, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 775: train/loss = 0.3567522466182709, train/raw-loss = 0.3550163805484772, train/logprobs = tensor([[-1.1390, -3.2964],
        [-1.4945, -1.6974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1735852062702179
RAW KL tensor(0.1012, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 776: train/loss = 0.24333006143569946, train/raw-loss = 0.24077457189559937, train/logprobs = tensor([[-1.3592, -4.3262],
        [-1.6077, -0.9872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.255549818277359
RAW KL tensor(0.4538, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 777: train/loss = 0.3409656584262848, train/raw-loss = 0.33804619312286377, train/logprobs = tensor([[-1.3602, -3.7590],
        [-1.6746, -1.6223]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.29194775223731995
RAW KL tensor(0.1476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 778: train/loss = 0.3505898714065552, train/raw-loss = 0.34888148307800293, train/logprobs = tensor([[-1.2545, -2.7611],
        [-1.6209, -0.9934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17084017395973206
RAW KL tensor(0.1068, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 779: train/loss = 0.3089599907398224, train/raw-loss = 0.30679085850715637, train/logprobs = tensor([[-1.6595, -3.2439],
        [-1.8409, -1.0544]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21691448986530304
RAW KL tensor(0.1394, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 780: train/loss = 0.38320377469062805, train/raw-loss = 0.3822365999221802, train/logprobs = tensor([[-1.1787, -2.5649],
        [-1.3361, -1.1295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09671454131603241
RAW KL tensor(0.1680, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 781: train/loss = 0.4389762282371521, train/raw-loss = 0.4366038143634796, train/logprobs = tensor([[-1.7270, -3.5515],
        [-1.5698, -1.3600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23724554479122162
RAW KL tensor(0.3057, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 782: train/loss = 0.27155259251594543, train/raw-loss = 0.2695491313934326, train/logprobs = tensor([[-1.0108, -4.3690],
        [-1.2328, -1.2860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2003454566001892
RAW KL tensor(0.0957, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 234])
Epoch 0, Step 783: train/loss = 0.45476076006889343, train/raw-loss = 0.45351940393447876, train/logprobs = tensor([[-1.0213, -1.9041],
        [-1.2319, -0.9875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12413282692432404
RAW KL tensor(0.1361, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 784: train/loss = 0.22987060248851776, train/raw-loss = 0.22780102491378784, train/logprobs = tensor([[-1.0174, -3.9313],
        [-1.4561, -1.4483]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2069590985774994
RAW KL tensor(0.2464, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 785: train/loss = 0.20937705039978027, train/raw-loss = 0.20687203109264374, train/logprobs = tensor([[-1.2237, -4.0239],
        [-1.4774, -1.1434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25050151348114014
RAW KL tensor(0.3527, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 786: train/loss = 0.3164026737213135, train/raw-loss = 0.3134904205799103, train/logprobs = tensor([[-1.2615, -3.5571],
        [-1.8383, -1.8697]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.29122358560562134
RAW KL tensor(0.3448, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 255])
Epoch 0, Step 787: train/loss = 0.3429383933544159, train/raw-loss = 0.34095102548599243, train/logprobs = tensor([[-0.9153, -3.5941],
        [-1.3387, -1.7609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1987409144639969
RAW KL tensor(0.2290, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 788: train/loss = 0.3547220528125763, train/raw-loss = 0.3527647852897644, train/logprobs = tensor([[-0.7910, -3.3435],
        [-0.9854, -1.2926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1957286149263382
RAW KL tensor(0.1969, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 789: train/loss = 0.46842658519744873, train/raw-loss = 0.46651965379714966, train/logprobs = tensor([[-1.3533, -2.2355],
        [-1.7915, -1.4607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19069145619869232
RAW KL tensor(0.1330, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 250])
Epoch 0, Step 790: train/loss = 0.38689538836479187, train/raw-loss = 0.38536131381988525, train/logprobs = tensor([[-1.3620, -2.6814],
        [-1.3856, -1.0459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15340487658977509
RAW KL tensor(0.0827, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 280])
Epoch 0, Step 791: train/loss = 0.4636332392692566, train/raw-loss = 0.46251896023750305, train/logprobs = tensor([[-1.3175, -2.2260],
        [-1.4006, -1.0957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11142580211162567
RAW KL tensor(0.2168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 792: train/loss = 0.3167295455932617, train/raw-loss = 0.3147887885570526, train/logprobs = tensor([[-1.4320, -3.0192],
        [-1.9778, -1.4602]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19407688081264496
RAW KL tensor(0.3262, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 793: train/loss = 0.23873692750930786, train/raw-loss = 0.23673735558986664, train/logprobs = tensor([[-1.1302, -3.8876],
        [-1.6828, -1.6063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19995640218257904
RAW KL tensor(0.2392, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 794: train/loss = 0.35241520404815674, train/raw-loss = 0.35037022829055786, train/logprobs = tensor([[-1.1663, -3.4293],
        [-1.2843, -1.3793]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20449717342853546
RAW KL tensor(0.1748, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 795: train/loss = 0.44389307498931885, train/raw-loss = 0.4422786831855774, train/logprobs = tensor([[-1.1296, -2.6892],
        [-1.2673, -1.4308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1614414006471634
RAW KL tensor(0.0548, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 796: train/loss = 0.37787681818008423, train/raw-loss = 0.37641000747680664, train/logprobs = tensor([[-0.8863, -2.2007],
        [-1.3972, -0.8772]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14668363332748413
RAW KL tensor(0.0847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 797: train/loss = 0.42518386244773865, train/raw-loss = 0.42376530170440674, train/logprobs = tensor([[-1.4099, -2.5454],
        [-1.7647, -1.5644]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14185628294944763
RAW KL tensor(0.1032, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 798: train/loss = 0.4373502731323242, train/raw-loss = 0.43594661355018616, train/logprobs = tensor([[-1.2196, -2.1002],
        [-1.4310, -0.9137]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14036394655704498
RAW KL tensor(0.2524, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 799: train/loss = 0.3174968659877777, train/raw-loss = 0.31548914313316345, train/logprobs = tensor([[-1.0958, -3.7993],
        [-1.0903, -1.1668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20077285170555115
RAW KL tensor(0.0916, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 800: train/loss = 0.5229698419570923, train/raw-loss = 0.5214694142341614, train/logprobs = tensor([[-1.3763, -1.6929],
        [-1.7412, -1.0397]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15004143118858337
RAW KL tensor(0.2643, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 801: train/loss = 0.18073831498622894, train/raw-loss = 0.1787848025560379, train/logprobs = tensor([[-0.9761, -4.4411],
        [-1.3583, -1.4368]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19535279273986816
RAW KL tensor(0.2775, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 802: train/loss = 0.3623279929161072, train/raw-loss = 0.3589359521865845, train/logprobs = tensor([[-1.3023, -2.6095],
        [-2.2233, -1.6361]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.33920925855636597
RAW KL tensor(0.1601, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 803: train/loss = 0.5101470351219177, train/raw-loss = 0.5089738368988037, train/logprobs = tensor([[-0.7613, -1.8972],
        [-1.1119, -1.2985]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11732301861047745
RAW KL tensor(0.1426, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 804: train/loss = 0.34152281284332275, train/raw-loss = 0.3394659757614136, train/logprobs = tensor([[-1.0239, -3.4629],
        [-1.4515, -1.5589]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20568186044692993
RAW KL tensor(0.1488, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 805: train/loss = 0.35293078422546387, train/raw-loss = 0.3509275019168854, train/logprobs = tensor([[-1.7680, -3.6917],
        [-1.9811, -1.7487]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20032891631126404
RAW KL tensor(0.1444, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 806: train/loss = 0.40466830134391785, train/raw-loss = 0.40230250358581543, train/logprobs = tensor([[-1.3113, -3.2672],
        [-1.5211, -1.1660]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2365836799144745
RAW KL tensor(0.2748, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 807: train/loss = 0.304943323135376, train/raw-loss = 0.3023671805858612, train/logprobs = tensor([[-1.4161, -3.8753],
        [-1.8042, -1.1602]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25761210918426514
RAW KL tensor(0.1813, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 808: train/loss = 0.37079137563705444, train/raw-loss = 0.3687046766281128, train/logprobs = tensor([[-1.4567, -3.1387],
        [-1.8220, -1.6790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20867198705673218
RAW KL tensor(0.0959, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 809: train/loss = 0.3904745876789093, train/raw-loss = 0.38903433084487915, train/logprobs = tensor([[-0.9292, -3.2602],
        [-1.1332, -1.3701]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.144028902053833
RAW KL tensor(0.2598, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 810: train/loss = 0.2755892276763916, train/raw-loss = 0.2736920118331909, train/logprobs = tensor([[-0.8096, -3.7506],
        [-1.4423, -1.4880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18972289562225342
RAW KL tensor(0.2501, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 811: train/loss = 0.1663016676902771, train/raw-loss = 0.16375556588172913, train/logprobs = tensor([[-1.1556, -5.2054],
        [-1.6102, -1.8560]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.254611074924469
RAW KL tensor(0.1559, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 812: train/loss = 0.3650605082511902, train/raw-loss = 0.3617798388004303, train/logprobs = tensor([[-1.5784, -4.3391],
        [-2.0337, -2.6373]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3280661404132843
RAW KL tensor(0.2135, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 813: train/loss = 0.39729204773902893, train/raw-loss = 0.3948223292827606, train/logprobs = tensor([[-1.3862, -4.1582],
        [-1.4847, -1.9456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2469741255044937
RAW KL tensor(0.1491, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 814: train/loss = 0.3566710352897644, train/raw-loss = 0.3547940254211426, train/logprobs = tensor([[-1.7898, -3.5787],
        [-1.7458, -1.3269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18770067393779755
RAW KL tensor(0.1944, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 815: train/loss = 0.33612072467803955, train/raw-loss = 0.33400431275367737, train/logprobs = tensor([[-1.4468, -3.9282],
        [-1.4324, -1.6723]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21164296567440033
RAW KL tensor(0.3077, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 816: train/loss = 0.38874930143356323, train/raw-loss = 0.3869195878505707, train/logprobs = tensor([[-1.2176, -2.7894],
        [-1.4455, -1.2854]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18297117948532104
RAW KL tensor(0.1031, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 817: train/loss = 0.3023192882537842, train/raw-loss = 0.30051714181900024, train/logprobs = tensor([[-1.7708, -4.6006],
        [-2.0530, -0.8862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18021699786186218
RAW KL tensor(0.0628, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 818: train/loss = 0.38217461109161377, train/raw-loss = 0.3796757757663727, train/logprobs = tensor([[-0.9905, -2.4728],
        [-1.5267, -1.0605]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24988074600696564
RAW KL tensor(0.2502, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 819: train/loss = 0.3485497534275055, train/raw-loss = 0.34631258249282837, train/logprobs = tensor([[-1.8412, -3.6646],
        [-1.5945, -1.2615]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22371838986873627
RAW KL tensor(0.2563, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 820: train/loss = 0.4268253445625305, train/raw-loss = 0.4243852496147156, train/logprobs = tensor([[-1.0540, -2.7750],
        [-1.5555, -1.5507]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24400797486305237
RAW KL tensor(0.4381, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 821: train/loss = 0.2996445894241333, train/raw-loss = 0.29722869396209717, train/logprobs = tensor([[-1.4773, -3.7332],
        [-1.6286, -1.5903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24159091711044312
RAW KL tensor(0.0667, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 822: train/loss = 0.27786147594451904, train/raw-loss = 0.2758147120475769, train/logprobs = tensor([[-1.0377, -3.8786],
        [-1.4244, -1.0874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20467634499073029
RAW KL tensor(0.2217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 823: train/loss = 0.5192171335220337, train/raw-loss = 0.5175277590751648, train/logprobs = tensor([[-1.3437, -1.7917],
        [-1.9531, -1.4598]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16894179582595825
RAW KL tensor(0.1494, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 824: train/loss = 0.3436262905597687, train/raw-loss = 0.3419843018054962, train/logprobs = tensor([[-1.5596, -3.4261],
        [-1.4243, -0.7952]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1641940325498581
RAW KL tensor(0.2313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 825: train/loss = 0.45854073762893677, train/raw-loss = 0.4563259780406952, train/logprobs = tensor([[-1.5235, -2.2469],
        [-1.5421, -1.0581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22147618234157562
RAW KL tensor(0.2171, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 826: train/loss = 0.37798941135406494, train/raw-loss = 0.3762508034706116, train/logprobs = tensor([[-1.4799, -2.8547],
        [-1.6150, -1.1605]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1738617718219757
RAW KL tensor(0.2925, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 827: train/loss = 0.31461450457572937, train/raw-loss = 0.31054016947746277, train/logprobs = tensor([[-1.5981, -3.5206],
        [-1.8852, -1.2340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4074327349662781
RAW KL tensor(0.1918, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 828: train/loss = 0.28980112075805664, train/raw-loss = 0.28797268867492676, train/logprobs = tensor([[-1.1719, -3.3546],
        [-1.4181, -1.2700]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1828444004058838
RAW KL tensor(0.1369, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 829: train/loss = 0.5070809721946716, train/raw-loss = 0.5057768225669861, train/logprobs = tensor([[-1.2361, -2.0515],
        [-1.7762, -1.4371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13041828572750092
RAW KL tensor(0.1953, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 830: train/loss = 0.39522016048431396, train/raw-loss = 0.39361572265625, train/logprobs = tensor([[-0.9010, -2.4532],
        [-1.3673, -1.2066]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16044571995735168
RAW KL tensor(0.1187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 831: train/loss = 0.35641390085220337, train/raw-loss = 0.35511836409568787, train/logprobs = tensor([[-1.8115, -3.0825],
        [-1.7563, -1.1226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12955226004123688
RAW KL tensor(0.3722, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 832: train/loss = 0.3367459177970886, train/raw-loss = 0.3340193033218384, train/logprobs = tensor([[-2.0215, -4.0642],
        [-1.9758, -1.5419]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2726629972457886
RAW KL tensor(0.2207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 833: train/loss = 0.3332795202732086, train/raw-loss = 0.3318045735359192, train/logprobs = tensor([[-1.2750, -2.4862],
        [-2.0948, -1.3915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14749467372894287
RAW KL tensor(0.3018, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 834: train/loss = 0.3128758370876312, train/raw-loss = 0.31043511629104614, train/logprobs = tensor([[-0.7824, -3.2939],
        [-1.3373, -1.3960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24407726526260376
RAW KL tensor(0.1796, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 835: train/loss = 0.3602640926837921, train/raw-loss = 0.35850512981414795, train/logprobs = tensor([[-1.1861, -2.8588],
        [-1.5346, -1.2771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1758953034877777
RAW KL tensor(0.2472, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 836: train/loss = 0.5298027396202087, train/raw-loss = 0.527386486530304, train/logprobs = tensor([[-1.2742, -1.9619],
        [-1.8326, -1.5016]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24162554740905762
RAW KL tensor(0.1715, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 837: train/loss = 0.38241979479789734, train/raw-loss = 0.38054999709129333, train/logprobs = tensor([[-0.9024, -3.0150],
        [-1.3264, -1.1058]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18698157370090485
RAW KL tensor(0.2653, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 838: train/loss = 0.18003946542739868, train/raw-loss = 0.17797844111919403, train/logprobs = tensor([[-1.5830, -5.1797],
        [-1.8660, -1.8022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2061023712158203
RAW KL tensor(0.3093, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 839: train/loss = 0.5429764986038208, train/raw-loss = 0.540693998336792, train/logprobs = tensor([[-1.3673, -2.1425],
        [-2.4311, -2.1801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22825294733047485
RAW KL tensor(0.1659, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 840: train/loss = 0.31322407722473145, train/raw-loss = 0.31028950214385986, train/logprobs = tensor([[-1.6577, -4.9505],
        [-2.2777, -1.1840]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.29345884919166565
RAW KL tensor(0.1406, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 841: train/loss = 0.36115148663520813, train/raw-loss = 0.3588513731956482, train/logprobs = tensor([[-0.7651, -3.6371],
        [-1.2451, -1.7318]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23001177608966827
RAW KL tensor(0.2446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 842: train/loss = 0.2885647118091583, train/raw-loss = 0.2866835296154022, train/logprobs = tensor([[-1.4063, -3.5508],
        [-1.9511, -1.7192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18811842799186707
RAW KL tensor(0.3349, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 384])
Epoch 0, Step 843: train/loss = 0.22812733054161072, train/raw-loss = 0.22525468468666077, train/logprobs = tensor([[-0.6543, -4.4875],
        [-1.3182, -1.8059]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2872653603553772
RAW KL tensor(0.3701, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 844: train/loss = 0.237953782081604, train/raw-loss = 0.23568761348724365, train/logprobs = tensor([[-1.0317, -3.2675],
        [-1.7986, -1.3153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22661736607551575
RAW KL tensor(0.0808, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 845: train/loss = 0.4045259654521942, train/raw-loss = 0.4018711447715759, train/logprobs = tensor([[-1.1232, -2.5785],
        [-2.0631, -1.0696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2654823660850525
RAW KL tensor(0.2229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 846: train/loss = 0.5078309178352356, train/raw-loss = 0.5065548419952393, train/logprobs = tensor([[-1.4779, -2.4050],
        [-1.3899, -1.1174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12760820984840393
RAW KL tensor(0.7066, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 847: train/loss = 0.3334736227989197, train/raw-loss = 0.33018869161605835, train/logprobs = tensor([[-1.0369, -2.7120],
        [-1.8168, -1.1898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3284909427165985
RAW KL tensor(0.0923, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 848: train/loss = 0.3715478479862213, train/raw-loss = 0.3706306517124176, train/logprobs = tensor([[-1.1709, -2.6516],
        [-1.4908, -1.3114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09172293543815613
RAW KL tensor(0.1771, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 849: train/loss = 0.5559639930725098, train/raw-loss = 0.5546190142631531, train/logprobs = tensor([[-1.1794, -1.7749],
        [-1.4612, -1.3060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13449998199939728
RAW KL tensor(0.4187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 850: train/loss = 0.2927663326263428, train/raw-loss = 0.2892281413078308, train/logprobs = tensor([[-1.7088, -4.0623],
        [-2.1522, -1.3856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3538198173046112
RAW KL tensor(0.3390, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 851: train/loss = 0.5171011686325073, train/raw-loss = 0.5155139565467834, train/logprobs = tensor([[-1.1953, -2.1221],
        [-1.2593, -1.0024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15871667861938477
RAW KL tensor(0.2016, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 852: train/loss = 0.27319034934043884, train/raw-loss = 0.2698819935321808, train/logprobs = tensor([[-1.3677, -3.2980],
        [-2.4136, -1.0120]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3308330178260803
RAW KL tensor(0.0580, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 853: train/loss = 0.6067255139350891, train/raw-loss = 0.6049419641494751, train/logprobs = tensor([[-0.8429, -1.0124],
        [-1.3484, -1.0545]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17835277318954468
RAW KL tensor(0.1312, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 854: train/loss = 0.5455620288848877, train/raw-loss = 0.5427013635635376, train/logprobs = tensor([[-1.1093, -2.9313],
        [-1.3322, -1.9465]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.28606075048446655
RAW KL tensor(0.1548, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 855: train/loss = 0.47145140171051025, train/raw-loss = 0.4688313901424408, train/logprobs = tensor([[-1.4290, -2.3861],
        [-1.8848, -1.6166]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2620045244693756
RAW KL tensor(0.2657, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 856: train/loss = 0.34518861770629883, train/raw-loss = 0.343302458524704, train/logprobs = tensor([[-1.0628, -2.9501],
        [-1.6833, -1.5963]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18861445784568787
RAW KL tensor(0.2169, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 857: train/loss = 0.1756191998720169, train/raw-loss = 0.17353031039237976, train/logprobs = tensor([[-1.2944, -4.6686],
        [-1.8604, -1.5783]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20889036357402802
RAW KL tensor(0.1216, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 858: train/loss = 0.5486482381820679, train/raw-loss = 0.5456867218017578, train/logprobs = tensor([[-1.4387, -2.5930],
        [-1.6088, -1.4803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.29615768790245056
RAW KL tensor(0.1501, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 859: train/loss = 0.43032577633857727, train/raw-loss = 0.4282951354980469, train/logprobs = tensor([[-1.5740, -4.7797],
        [-1.4511, -2.3191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20306646823883057
RAW KL tensor(0.0375, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 860: train/loss = 0.49415162205696106, train/raw-loss = 0.49316221475601196, train/logprobs = tensor([[-0.8899, -1.3186],
        [-1.4492, -0.8780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0989389717578888
RAW KL tensor(0.1637, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 861: train/loss = 0.3151189386844635, train/raw-loss = 0.3128228485584259, train/logprobs = tensor([[-1.0989, -3.4754],
        [-1.3921, -1.1756]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2296079695224762
RAW KL tensor(0.1030, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 862: train/loss = 0.3946881592273712, train/raw-loss = 0.39352160692214966, train/logprobs = tensor([[-1.2633, -2.6460],
        [-1.6612, -1.3530]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11665277928113937
RAW KL tensor(0.1769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 863: train/loss = 0.43227294087409973, train/raw-loss = 0.42996150255203247, train/logprobs = tensor([[-1.2261, -3.0695],
        [-1.3357, -1.4856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23114289343357086
RAW KL tensor(0.2992, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 864: train/loss = 0.3768356442451477, train/raw-loss = 0.3747957646846771, train/logprobs = tensor([[-1.3469, -2.6330],
        [-1.9414, -1.3867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20398548245429993
RAW KL tensor(0.0979, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 865: train/loss = 0.3647037148475647, train/raw-loss = 0.3621390461921692, train/logprobs = tensor([[-1.1364, -3.8495],
        [-1.5172, -1.3689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2564676105976105
RAW KL tensor(0.1895, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 866: train/loss = 0.3162752687931061, train/raw-loss = 0.31427186727523804, train/logprobs = tensor([[-1.6591, -3.0340],
        [-2.2095, -1.5010]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20033931732177734
RAW KL tensor(0.1571, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 867: train/loss = 0.2426181435585022, train/raw-loss = 0.24087825417518616, train/logprobs = tensor([[-1.5020, -3.2684],
        [-1.8506, -0.8952]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1739864945411682
RAW KL tensor(0.0891, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 868: train/loss = 0.36098623275756836, train/raw-loss = 0.3588421046733856, train/logprobs = tensor([[-1.0972, -3.0885],
        [-1.5225, -1.1888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21440932154655457
RAW KL tensor(0.2689, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 869: train/loss = 0.14795686304569244, train/raw-loss = 0.14529262483119965, train/logprobs = tensor([[-1.0970, -5.1068],
        [-1.6773, -1.5831]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2664244771003723
RAW KL tensor(0.0837, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 437])
Epoch 0, Step 870: train/loss = 0.4227474629878998, train/raw-loss = 0.41966843605041504, train/logprobs = tensor([[-2.0072, -2.6505],
        [-2.0990, -0.9693]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.307905375957489
RAW KL tensor(0.1808, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 871: train/loss = 0.4398497939109802, train/raw-loss = 0.43828967213630676, train/logprobs = tensor([[-1.6329, -1.8380],
        [-2.0015, -0.9415]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15601110458374023
RAW KL tensor(0.2734, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 464])
Epoch 0, Step 872: train/loss = 0.3259020745754242, train/raw-loss = 0.32425186038017273, train/logprobs = tensor([[-0.7554, -4.3189],
        [-1.0143, -1.8712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16502578556537628
RAW KL tensor(0.3822, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 873: train/loss = 0.23070254921913147, train/raw-loss = 0.22818021476268768, train/logprobs = tensor([[-1.2159, -4.3189],
        [-1.8060, -1.9715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.252234548330307
RAW KL tensor(0.1869, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 874: train/loss = 0.48715272545814514, train/raw-loss = 0.4843938946723938, train/logprobs = tensor([[-1.0393, -2.0099],
        [-1.6206, -1.5230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.27588000893592834
RAW KL tensor(0.1495, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 875: train/loss = 0.3406845033168793, train/raw-loss = 0.33868443965911865, train/logprobs = tensor([[-1.2512, -2.7964],
        [-1.7709, -1.1597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2000083029270172
RAW KL tensor(0.3321, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 876: train/loss = 0.1584484875202179, train/raw-loss = 0.1561087667942047, train/logprobs = tensor([[-1.8892, -4.5077],
        [-2.2765, -1.0039]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23397113382816315
RAW KL tensor(0.1224, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 877: train/loss = 0.5371298789978027, train/raw-loss = 0.5355456471443176, train/logprobs = tensor([[-0.9248, -1.5800],
        [-1.2508, -1.1574]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1584237664937973
RAW KL tensor(0.2395, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 878: train/loss = 0.47327208518981934, train/raw-loss = 0.4714028239250183, train/logprobs = tensor([[-1.5662, -2.1721],
        [-1.9533, -1.2695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.186921626329422
RAW KL tensor(0.2082, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 253])
Epoch 0, Step 879: train/loss = 0.350863516330719, train/raw-loss = 0.3486323356628418, train/logprobs = tensor([[-0.7909, -2.5935],
        [-1.5968, -1.2577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22312133014202118
RAW KL tensor(0.2111, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 880: train/loss = 0.30598393082618713, train/raw-loss = 0.30414825677871704, train/logprobs = tensor([[-1.4255, -3.7566],
        [-1.9373, -2.0252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18356645107269287
RAW KL tensor(0.1297, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 881: train/loss = 0.3403235077857971, train/raw-loss = 0.3384012281894684, train/logprobs = tensor([[-0.8621, -3.6447],
        [-1.1213, -1.1040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1922283172607422
RAW KL tensor(0.0925, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 882: train/loss = 0.3009355962276459, train/raw-loss = 0.29765045642852783, train/logprobs = tensor([[-1.4740, -6.1420],
        [-1.9622, -1.5155]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3285117745399475
RAW KL tensor(0.1838, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 883: train/loss = 0.20287789404392242, train/raw-loss = 0.2008693814277649, train/logprobs = tensor([[-0.8213, -4.5389],
        [-1.5255, -1.7282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20085108280181885
RAW KL tensor(0.2147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 117])
Epoch 0, Step 884: train/loss = 0.47747811675071716, train/raw-loss = 0.47625425457954407, train/logprobs = tensor([[-1.0575, -2.2458],
        [-0.9927, -1.0802]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12238584458827972
RAW KL tensor(0.2646, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 885: train/loss = 0.29138630628585815, train/raw-loss = 0.288837730884552, train/logprobs = tensor([[-0.9337, -3.3889],
        [-1.5428, -1.1374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2548583149909973
RAW KL tensor(0.1970, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 886: train/loss = 0.3542318642139435, train/raw-loss = 0.35224005579948425, train/logprobs = tensor([[-1.2775, -3.2884],
        [-1.7559, -0.8113]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1991809904575348
RAW KL tensor(0.1471, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 887: train/loss = 0.4024219810962677, train/raw-loss = 0.3994513154029846, train/logprobs = tensor([[-2.4231, -3.9140],
        [-2.0851, -1.7060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2970666289329529
RAW KL tensor(0.1334, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 888: train/loss = 0.4877146780490875, train/raw-loss = 0.4864950478076935, train/logprobs = tensor([[-0.9376, -1.9057],
        [-1.3218, -1.2386]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12196342647075653
RAW KL tensor(0.2409, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 889: train/loss = 0.19642718136310577, train/raw-loss = 0.1937205195426941, train/logprobs = tensor([[-1.3838, -4.7682],
        [-2.0223, -1.6169]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.27066510915756226
RAW KL tensor(0.1536, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 890: train/loss = 0.30055826902389526, train/raw-loss = 0.2988581657409668, train/logprobs = tensor([[-1.8803, -4.0697],
        [-2.3458, -1.2786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17001113295555115
RAW KL tensor(0.1671, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 891: train/loss = 0.2370324432849884, train/raw-loss = 0.23518972098827362, train/logprobs = tensor([[-1.2984, -3.6989],
        [-1.5272, -1.0651]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18427297472953796
RAW KL tensor(0.2948, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 892: train/loss = 0.41364026069641113, train/raw-loss = 0.41149449348449707, train/logprobs = tensor([[-1.3903, -2.5148],
        [-1.7021, -0.9532]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21458014845848083
RAW KL tensor(0.1773, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 893: train/loss = 0.3718661069869995, train/raw-loss = 0.36933737993240356, train/logprobs = tensor([[-1.1062, -2.8036],
        [-1.9186, -1.4069]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2528715431690216
RAW KL tensor(0.1959, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 894: train/loss = 0.40132802724838257, train/raw-loss = 0.3993551433086395, train/logprobs = tensor([[-1.4092, -3.1654],
        [-1.4756, -1.4894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1972857266664505
RAW KL tensor(0.3339, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 895: train/loss = 0.30143487453460693, train/raw-loss = 0.29829680919647217, train/logprobs = tensor([[-1.2028, -3.0212],
        [-1.6496, -1.2209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3138056695461273
RAW KL tensor(0.3702, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 896: train/loss = 0.16223686933517456, train/raw-loss = 0.15899714827537537, train/logprobs = tensor([[-1.5146, -5.4339],
        [-2.2681, -1.8134]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.32397425174713135
RAW KL tensor(0.2847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 897: train/loss = 0.24251337349414825, train/raw-loss = 0.24045082926750183, train/logprobs = tensor([[-1.1509, -3.7238],
        [-1.8839, -1.0135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20625345408916473
RAW KL tensor(0.2421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 898: train/loss = 0.3394339084625244, train/raw-loss = 0.33654525876045227, train/logprobs = tensor([[-2.0854, -5.3850],
        [-1.4173, -1.3477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2888670265674591
RAW KL tensor(0.1810, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 899: train/loss = 0.28322672843933105, train/raw-loss = 0.28084152936935425, train/logprobs = tensor([[-1.5717, -4.9567],
        [-1.8911, -1.0873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23851674795150757
RAW KL tensor(0.2342, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 900: train/loss = 0.15182392299175262, train/raw-loss = 0.1498539000749588, train/logprobs = tensor([[-1.5340, -4.8031],
        [-2.4041, -1.1191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19700251519680023
RAW KL tensor(0.2585, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 901: train/loss = 0.1405085325241089, train/raw-loss = 0.13794738054275513, train/logprobs = tensor([[-1.7128, -4.4892],
        [-2.2380, -1.1047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25611722469329834
RAW KL tensor(0.3476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 902: train/loss = 0.32350754737854004, train/raw-loss = 0.3216642141342163, train/logprobs = tensor([[-0.8847, -3.2378],
        [-1.6461, -1.4554]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18433699011802673
RAW KL tensor(0.1241, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 903: train/loss = 0.3078896999359131, train/raw-loss = 0.30605486035346985, train/logprobs = tensor([[-1.1746, -3.9445],
        [-1.6839, -1.6873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1834857016801834
RAW KL tensor(0.1465, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 317])
Epoch 0, Step 904: train/loss = 0.36067599058151245, train/raw-loss = 0.35914960503578186, train/logprobs = tensor([[-1.0136, -2.3913],
        [-1.5248, -1.0194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1526382863521576
RAW KL tensor(0.0958, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 905: train/loss = 0.5352433919906616, train/raw-loss = 0.5337517261505127, train/logprobs = tensor([[-1.5151, -2.8158],
        [-1.3372, -1.3921]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14916536211967468
RAW KL tensor(0.3583, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 906: train/loss = 0.25008445978164673, train/raw-loss = 0.24734507501125336, train/logprobs = tensor([[-1.1311, -4.7596],
        [-1.6415, -1.7821]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.273939311504364
RAW KL tensor(0.2027, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 907: train/loss = 0.1991642713546753, train/raw-loss = 0.1969531774520874, train/logprobs = tensor([[-1.4269, -4.4621],
        [-1.8300, -1.5416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2211098074913025
RAW KL tensor(0.4433, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 908: train/loss = 0.35669174790382385, train/raw-loss = 0.35396480560302734, train/logprobs = tensor([[-1.8260, -2.8397],
        [-2.0828, -1.2827]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2726938724517822
RAW KL tensor(0.2536, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 909: train/loss = 0.37593990564346313, train/raw-loss = 0.37430477142333984, train/logprobs = tensor([[-0.9571, -2.3390],
        [-1.6220, -1.1671]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16351529955863953
RAW KL tensor(0.2498, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 910: train/loss = 0.3080461621284485, train/raw-loss = 0.30545079708099365, train/logprobs = tensor([[-1.3135, -3.1010],
        [-1.7436, -1.2614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2595336139202118
RAW KL tensor(0.2833, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 911: train/loss = 0.25299307703971863, train/raw-loss = 0.2499612420797348, train/logprobs = tensor([[-2.1049, -4.3106],
        [-2.3847, -1.7635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.30318498611450195
RAW KL tensor(0.1061, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 912: train/loss = 0.3937095105648041, train/raw-loss = 0.3918740153312683, train/logprobs = tensor([[-2.0029, -3.4041],
        [-1.6991, -0.9410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18354852497577667
RAW KL tensor(0.1782, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 913: train/loss = 0.3770662844181061, train/raw-loss = 0.3755365014076233, train/logprobs = tensor([[-1.0320, -2.8926],
        [-1.5804, -1.3778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15297716856002808
RAW KL tensor(0.3037, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 914: train/loss = 0.3594209551811218, train/raw-loss = 0.3574187755584717, train/logprobs = tensor([[-1.3180, -2.7253],
        [-1.9552, -1.4345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20021983981132507
RAW KL tensor(0.0955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 441])
Epoch 0, Step 915: train/loss = 0.49107515811920166, train/raw-loss = 0.48913419246673584, train/logprobs = tensor([[-0.9559, -1.7532],
        [-1.3397, -1.1104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19409693777561188
RAW KL tensor(0.2928, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 916: train/loss = 0.2443118542432785, train/raw-loss = 0.24085523188114166, train/logprobs = tensor([[-1.6797, -4.9490],
        [-2.4899, -1.6769]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3456639349460602
RAW KL tensor(0.4237, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 917: train/loss = 0.24484452605247498, train/raw-loss = 0.24252566695213318, train/logprobs = tensor([[-1.2914, -3.2088],
        [-1.8354, -1.0055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23188602924346924
RAW KL tensor(0.1073, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 255])
Epoch 0, Step 918: train/loss = 0.27948468923568726, train/raw-loss = 0.27588194608688354, train/logprobs = tensor([[-1.5728, -4.0867],
        [-2.0722, -1.5103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.36027365922927856
RAW KL tensor(0.1936, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 919: train/loss = 0.24977272748947144, train/raw-loss = 0.2472989857196808, train/logprobs = tensor([[-0.8618, -4.0098],
        [-1.8361, -1.8130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2473733276128769
RAW KL tensor(0.2849, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 920: train/loss = 0.27083924412727356, train/raw-loss = 0.2689851224422455, train/logprobs = tensor([[-1.7313, -6.0878],
        [-2.2800, -1.5790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18541470170021057
RAW KL tensor(0.1586, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 921: train/loss = 0.2748093605041504, train/raw-loss = 0.2723712921142578, train/logprobs = tensor([[-1.3614, -3.3654],
        [-2.0184, -1.1174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2438047230243683
RAW KL tensor(0.2303, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 922: train/loss = 0.4198448061943054, train/raw-loss = 0.4179152846336365, train/logprobs = tensor([[-1.2330, -2.5603],
        [-1.5667, -1.3931]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19295302033424377
RAW KL tensor(0.3648, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 923: train/loss = 0.28564080595970154, train/raw-loss = 0.2827923595905304, train/logprobs = tensor([[-2.0478, -4.3002],
        [-2.1469, -1.4526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2848445475101471
RAW KL tensor(0.1639, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 924: train/loss = 0.34945958852767944, train/raw-loss = 0.34783586859703064, train/logprobs = tensor([[-1.0764, -3.5132],
        [-1.5200, -1.7024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16237139701843262
RAW KL tensor(0.2594, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 925: train/loss = 0.5537266135215759, train/raw-loss = 0.5520397424697876, train/logprobs = tensor([[-1.0750, -1.8498],
        [-1.3722, -1.1969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16868412494659424
RAW KL tensor(0.3024, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 926: train/loss = 0.2767973244190216, train/raw-loss = 0.274819016456604, train/logprobs = tensor([[-1.3381, -4.9773],
        [-1.2677, -1.3861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19783136248588562
RAW KL tensor(0.2331, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 927: train/loss = 0.23075035214424133, train/raw-loss = 0.22836367785930634, train/logprobs = tensor([[-1.2904, -3.5697],
        [-2.1903, -1.2428]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2386663854122162
RAW KL tensor(0.2499, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 928: train/loss = 0.2955823838710785, train/raw-loss = 0.2934848368167877, train/logprobs = tensor([[-1.6071, -3.2848],
        [-2.4885, -1.5938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20975342392921448
RAW KL tensor(0.1930, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 387])
Epoch 0, Step 929: train/loss = 0.4710347354412079, train/raw-loss = 0.4696398377418518, train/logprobs = tensor([[-0.5717, -2.0736],
        [-1.2182, -1.4355]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13948673009872437
RAW KL tensor(0.2195, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 930: train/loss = 0.3743487596511841, train/raw-loss = 0.371975839138031, train/logprobs = tensor([[-1.2205, -3.4506],
        [-1.7604, -1.7137]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.237296000123024
RAW KL tensor(0.1122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 267])
Epoch 0, Step 931: train/loss = 0.39494943618774414, train/raw-loss = 0.39342719316482544, train/logprobs = tensor([[-0.6971, -2.8577],
        [-1.2471, -1.1360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15222394466400146
RAW KL tensor(0.1907, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 932: train/loss = 0.3120273947715759, train/raw-loss = 0.31014394760131836, train/logprobs = tensor([[-1.8803, -4.1248],
        [-2.3436, -2.0457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18834494054317474
RAW KL tensor(0.1392, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 933: train/loss = 0.5340111255645752, train/raw-loss = 0.5325844287872314, train/logprobs = tensor([[-1.2069, -2.3118],
        [-1.2276, -1.2776]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1426645666360855
RAW KL tensor(0.2304, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 934: train/loss = 0.15812097489833832, train/raw-loss = 0.15526847541332245, train/logprobs = tensor([[-1.3014, -5.4519],
        [-1.7291, -1.0984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2852494716644287
RAW KL tensor(0.4002, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 935: train/loss = 0.44306227564811707, train/raw-loss = 0.4399593472480774, train/logprobs = tensor([[-2.1813, -4.2014],
        [-1.5151, -1.1826]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3102913796901703
RAW KL tensor(0.1209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 936: train/loss = 0.4226832389831543, train/raw-loss = 0.42137742042541504, train/logprobs = tensor([[-0.8309, -1.8673],
        [-1.3205, -0.9215]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1305798888206482
RAW KL tensor(0.3547, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 937: train/loss = 0.38071268796920776, train/raw-loss = 0.3786312937736511, train/logprobs = tensor([[-1.5770, -3.4271],
        [-1.7478, -1.6390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20813970267772675
RAW KL tensor(0.3196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 938: train/loss = 0.3726009726524353, train/raw-loss = 0.3699426054954529, train/logprobs = tensor([[-2.3119, -2.7926],
        [-2.8867, -1.0687]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.26583895087242126
RAW KL tensor(0.1103, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 419])
Epoch 0, Step 939: train/loss = 0.6326848864555359, train/raw-loss = 0.6312038898468018, train/logprobs = tensor([[-0.9516, -1.1590],
        [-1.3993, -1.2855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1480989158153534
RAW KL tensor(0.1006, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 940: train/loss = 0.3475731611251831, train/raw-loss = 0.3453260362148285, train/logprobs = tensor([[-1.1599, -2.9811],
        [-1.9467, -1.4634]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2247101366519928
RAW KL tensor(0.1192, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 941: train/loss = 0.3651735186576843, train/raw-loss = 0.3626559376716614, train/logprobs = tensor([[-1.5070, -3.4854],
        [-1.9697, -1.9987]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2517596483230591
RAW KL tensor(0.1796, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 942: train/loss = 0.28999805450439453, train/raw-loss = 0.2885099947452545, train/logprobs = tensor([[-0.7812, -3.4706],
        [-1.0771, -0.7473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14880624413490295
RAW KL tensor(0.2633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 943: train/loss = 0.299769788980484, train/raw-loss = 0.29752904176712036, train/logprobs = tensor([[-1.3013, -3.3748],
        [-1.7593, -1.0515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22407624125480652
RAW KL tensor(0.1841, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 944: train/loss = 0.2763809263706207, train/raw-loss = 0.27365583181381226, train/logprobs = tensor([[-1.0712, -4.2266],
        [-1.6355, -1.2824]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2725107669830322
RAW KL tensor(0.3334, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 945: train/loss = 0.3595033884048462, train/raw-loss = 0.35692718625068665, train/logprobs = tensor([[-1.4958, -3.5016],
        [-2.0178, -1.8519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2576206922531128
RAW KL tensor(0.2113, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 946: train/loss = 0.35807570815086365, train/raw-loss = 0.35603636503219604, train/logprobs = tensor([[-0.8956, -2.9069],
        [-1.2556, -1.3131]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20393137633800507
RAW KL tensor(0.2995, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 947: train/loss = 0.20440955460071564, train/raw-loss = 0.20186568796634674, train/logprobs = tensor([[-1.3273, -4.2842],
        [-1.8853, -1.0072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25438863039016724
RAW KL tensor(0.3790, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 948: train/loss = 0.3674981892108917, train/raw-loss = 0.3654654920101166, train/logprobs = tensor([[-0.8640, -3.7849],
        [-1.5818, -1.6620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20326775312423706
RAW KL tensor(0.3651, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 949: train/loss = 0.23198948800563812, train/raw-loss = 0.22866490483283997, train/logprobs = tensor([[-2.1794, -5.2058],
        [-2.6373, -2.1034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3324572443962097
RAW KL tensor(0.1778, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 950: train/loss = 0.19679410755634308, train/raw-loss = 0.19443565607070923, train/logprobs = tensor([[-1.3266, -4.2902],
        [-1.7130, -0.9710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23584596812725067
RAW KL tensor(0.2191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 951: train/loss = 0.25489744544029236, train/raw-loss = 0.25306451320648193, train/logprobs = tensor([[-1.2703, -3.7076],
        [-1.7229, -1.4220]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1832921802997589
RAW KL tensor(0.5120, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 952: train/loss = 0.3192830979824066, train/raw-loss = 0.31629565358161926, train/logprobs = tensor([[-1.4293, -4.5671],
        [-1.9821, -2.1572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2987478971481323
RAW KL tensor(0.1960, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 953: train/loss = 0.5421905517578125, train/raw-loss = 0.5392768383026123, train/logprobs = tensor([[-1.6074, -2.4176],
        [-1.9101, -1.6449]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2913738191127777
RAW KL tensor(0.1333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 954: train/loss = 0.3622574806213379, train/raw-loss = 0.3607299327850342, train/logprobs = tensor([[-1.6940, -3.4946],
        [-2.1334, -1.9434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15275579690933228
RAW KL tensor(0.1326, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 494])
Epoch 0, Step 955: train/loss = 0.2038845419883728, train/raw-loss = 0.20152589678764343, train/logprobs = tensor([[-1.0028, -4.2365],
        [-1.4200, -1.0401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23586593568325043
RAW KL tensor(0.1887, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 956: train/loss = 0.22283026576042175, train/raw-loss = 0.22011399269104004, train/logprobs = tensor([[-1.4983, -5.7169],
        [-2.1138, -2.4984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2716270685195923
RAW KL tensor(0.1710, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 957: train/loss = 0.24583888053894043, train/raw-loss = 0.24247103929519653, train/logprobs = tensor([[-1.4462, -4.1804],
        [-1.6917, -0.9078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.33678433299064636
RAW KL tensor(0.1072, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 958: train/loss = 0.42430227994918823, train/raw-loss = 0.42258554697036743, train/logprobs = tensor([[-1.5295, -2.4243],
        [-1.6748, -1.1380]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17167231440544128
RAW KL tensor(0.1799, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 959: train/loss = 0.22150325775146484, train/raw-loss = 0.21953801810741425, train/logprobs = tensor([[-1.2551, -4.1322],
        [-1.5939, -0.7563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19652311503887177
RAW KL tensor(0.2658, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 455])
Epoch 0, Step 960: train/loss = 0.4438919723033905, train/raw-loss = 0.44200247526168823, train/logprobs = tensor([[-1.1688, -3.7553],
        [-0.9991, -1.4571]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18894590437412262
RAW KL tensor(0.1977, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 961: train/loss = 0.2745685875415802, train/raw-loss = 0.2711296081542969, train/logprobs = tensor([[-2.2430, -4.4911],
        [-1.9037, -1.0291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.34390074014663696
RAW KL tensor(0.3160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 962: train/loss = 0.19285480678081512, train/raw-loss = 0.19068150222301483, train/logprobs = tensor([[-1.5611, -3.9993],
        [-2.0002, -0.8923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2173311412334442
RAW KL tensor(0.4073, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 963: train/loss = 0.4778405427932739, train/raw-loss = 0.47535228729248047, train/logprobs = tensor([[-1.6323, -2.4617],
        [-2.1313, -1.6454]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24882398545742035
RAW KL tensor(0.3127, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 964: train/loss = 0.32352256774902344, train/raw-loss = 0.32120195031166077, train/logprobs = tensor([[-1.1809, -3.0159],
        [-1.9736, -1.4591]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23206079006195068
RAW KL tensor(0.4572, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
[2024-02-26 17:57:03,428][root][INFO] - beta: 0.1
[2024-02-26 17:57:03,428][root][INFO] - loss with_labels
[2024-02-26 17:57:03,428][root][INFO] - max_iter: 0
[2024-02-26 17:57:03,428][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-per-token-kl
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-per-token-kl after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-per-token-kl after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-per-token-kl after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-per-token-kl after each epoch.
RAW KL tensor(0.0989, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 0: train/loss = 0.7065528035163879, train/raw-loss = 0.700703501701355, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05849311128258705
RAW KL tensor(0.0533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 1: train/loss = 0.6993706226348877, train/raw-loss = 0.6946866512298584, train/logprobs = tensor([[-1.2483, -1.6199],
        [-1.2440, -1.6139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046839892864227295
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 2: train/loss = 0.6943529844284058, train/raw-loss = 0.6923112869262695, train/logprobs = tensor([[-1.0892, -2.0869],
        [-1.1206, -2.1103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02041739784181118
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 3: train/loss = 0.6888778805732727, train/raw-loss = 0.6862989068031311, train/logprobs = tensor([[-0.5922, -1.4420],
        [-0.5997, -1.4088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025789767503738403
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 4: train/loss = 0.6854151487350464, train/raw-loss = 0.6821818947792053, train/logprobs = tensor([[-0.9682, -2.2517],
        [-1.0031, -2.2372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032332804054021835
RAW KL tensor(0.2229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 5: train/loss = 0.7069650888442993, train/raw-loss = 0.6991856098175049, train/logprobs = tensor([[-1.2526, -1.8470],
        [-1.2661, -1.8698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07779429852962494
RAW KL tensor(0.0653, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 6: train/loss = 0.6952711939811707, train/raw-loss = 0.6907837390899658, train/logprobs = tensor([[-0.9823, -1.5991],
        [-1.0011, -1.5974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04487462714314461
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 7: train/loss = 0.7085918188095093, train/raw-loss = 0.7020244598388672, train/logprobs = tensor([[-1.0630, -1.5507],
        [-1.1077, -1.6226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06567401438951492
RAW KL tensor(0.0341, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 8: train/loss = 0.6961644887924194, train/raw-loss = 0.6930682063102722, train/logprobs = tensor([[-0.8023, -1.7333],
        [-0.7871, -1.7088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030963294208049774
RAW KL tensor(0.0381, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 9: train/loss = 0.7008680701255798, train/raw-loss = 0.6962793469429016, train/logprobs = tensor([[-1.0835, -1.7181],
        [-1.0813, -1.6973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04588749259710312
RAW KL tensor(0.2401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 10: train/loss = 0.7053648233413696, train/raw-loss = 0.6965494155883789, train/logprobs = tensor([[-1.1971, -2.5060],
        [-1.2764, -2.5027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08815398067235947
RAW KL tensor(0.0386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 11: train/loss = 0.6893503665924072, train/raw-loss = 0.6862969398498535, train/logprobs = tensor([[-0.7466, -1.5640],
        [-0.7228, -1.5107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030534276738762856
RAW KL tensor(0.0910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 12: train/loss = 0.720034122467041, train/raw-loss = 0.7148674726486206, train/logprobs = tensor([[-0.7433, -1.8789],
        [-0.7825, -1.8867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05166593939065933
RAW KL tensor(0.0755, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 13: train/loss = 0.6912126541137695, train/raw-loss = 0.6869643926620483, train/logprobs = tensor([[-0.7519, -1.6611],
        [-0.7745, -1.6382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04248277097940445
RAW KL tensor(0.0711, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 14: train/loss = 0.6901992559432983, train/raw-loss = 0.684917688369751, train/logprobs = tensor([[-1.1222, -1.7514],
        [-1.1230, -1.7108]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05281598120927811
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 15: train/loss = 0.7064018845558167, train/raw-loss = 0.7035808563232422, train/logprobs = tensor([[-1.0901, -1.5558],
        [-1.0901, -1.5823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02820965088903904
RAW KL tensor(0.2291, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 16: train/loss = 0.7159994840621948, train/raw-loss = 0.705668568611145, train/logprobs = tensor([[-0.9616, -2.0296],
        [-0.9940, -1.9801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10330930352210999
RAW KL tensor(0.0085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 17: train/loss = 0.6964489221572876, train/raw-loss = 0.6924867033958435, train/logprobs = tensor([[-1.0904, -1.5065],
        [-1.0878, -1.4924]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03962227329611778
RAW KL tensor(0.0882, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 18: train/loss = 0.6973009705543518, train/raw-loss = 0.6928467154502869, train/logprobs = tensor([[-1.1498, -1.6923],
        [-1.1412, -1.6685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04454222694039345
RAW KL tensor(0.0408, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 19: train/loss = 0.7018048763275146, train/raw-loss = 0.6973589658737183, train/logprobs = tensor([[-0.7575, -1.3558],
        [-0.7511, -1.3457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04445846751332283
RAW KL tensor(0.0451, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 20: train/loss = 0.7574503421783447, train/raw-loss = 0.751427173614502, train/logprobs = tensor([[-0.6982, -1.6722],
        [-0.7384, -1.6710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0602305606007576
RAW KL tensor(0.0190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 21: train/loss = 0.6994845867156982, train/raw-loss = 0.6941919326782227, train/logprobs = tensor([[-0.9935, -1.2997],
        [-0.9919, -1.2949]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052927035838365555
RAW KL tensor(0.0285, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 22: train/loss = 0.6930542588233948, train/raw-loss = 0.6880555152893066, train/logprobs = tensor([[-1.1048, -1.2115],
        [-1.1166, -1.1903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049987539649009705
RAW KL tensor(0.1180, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 23: train/loss = 0.7331583499908447, train/raw-loss = 0.7249387502670288, train/logprobs = tensor([[-1.1101, -1.9440],
        [-1.1279, -2.0090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08219575881958008
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 24: train/loss = 0.6935535073280334, train/raw-loss = 0.6900442838668823, train/logprobs = tensor([[-0.9510, -2.1814],
        [-0.9490, -2.1607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03509220853447914
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 25: train/loss = 0.6882011890411377, train/raw-loss = 0.6853428483009338, train/logprobs = tensor([[-0.8277, -2.2563],
        [-0.8247, -2.2167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028583889827132225
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 26: train/loss = 0.7006773948669434, train/raw-loss = 0.6981753706932068, train/logprobs = tensor([[-0.5759, -2.1066],
        [-0.6087, -2.1473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025020746514201164
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 27: train/loss = 0.7018694877624512, train/raw-loss = 0.6973809003829956, train/logprobs = tensor([[-1.0402, -2.5564],
        [-1.0804, -2.5702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04488641396164894
RAW KL tensor(0.0764, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 28: train/loss = 0.7079528570175171, train/raw-loss = 0.6975324153900146, train/logprobs = tensor([[-0.8392, -1.7205],
        [-0.8369, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1042046993970871
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 29: train/loss = 0.6956132650375366, train/raw-loss = 0.6934842467308044, train/logprobs = tensor([[-0.5879, -1.8914],
        [-0.5988, -1.9005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021290477365255356
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 219])
Epoch 0, Step 30: train/loss = 0.6950444579124451, train/raw-loss = 0.6918355822563171, train/logprobs = tensor([[-0.8561, -1.1920],
        [-0.8388, -1.1585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03208891302347183
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 31: train/loss = 0.6793830990791321, train/raw-loss = 0.672492265701294, train/logprobs = tensor([[-1.0550, -2.2670],
        [-1.0818, -2.1798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06890882551670074
RAW KL tensor(0.0422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 32: train/loss = 0.6933178305625916, train/raw-loss = 0.6886965036392212, train/logprobs = tensor([[-0.9240, -1.9170],
        [-0.9467, -1.8934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04621318727731705
RAW KL tensor(0.0468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 33: train/loss = 0.705786406993866, train/raw-loss = 0.7009933590888977, train/logprobs = tensor([[-0.8779, -1.8828],
        [-0.8573, -1.8740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047930438071489334
RAW KL tensor(0.0315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 34: train/loss = 0.7006706595420837, train/raw-loss = 0.6958221793174744, train/logprobs = tensor([[-1.0511, -1.5128],
        [-1.0826, -1.5036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04848434031009674
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 35: train/loss = 0.7097312211990356, train/raw-loss = 0.7038596868515015, train/logprobs = tensor([[-0.9902, -1.8427],
        [-0.9916, -1.8163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05871553719043732
RAW KL tensor(0.0617, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 36: train/loss = 0.7045739889144897, train/raw-loss = 0.6996639966964722, train/logprobs = tensor([[-1.2729, -1.4568],
        [-1.2776, -1.4303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04909990355372429
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 235])
Epoch 0, Step 37: train/loss = 0.6925152540206909, train/raw-loss = 0.6881989240646362, train/logprobs = tensor([[-0.6132, -1.4763],
        [-0.6105, -1.4501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0431632362306118
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 292])
Epoch 0, Step 38: train/loss = 0.6955771446228027, train/raw-loss = 0.6917117834091187, train/logprobs = tensor([[-0.8017, -1.5622],
        [-0.8207, -1.5360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038653694093227386
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 39: train/loss = 0.6998563408851624, train/raw-loss = 0.6974338889122009, train/logprobs = tensor([[-0.6779, -1.3967],
        [-0.6694, -1.3926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02422407455742359
RAW KL tensor(0.0270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 40: train/loss = 0.6969108581542969, train/raw-loss = 0.6939688920974731, train/logprobs = tensor([[-0.7632, -1.1195],
        [-0.7654, -1.1192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029419410973787308
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 329])
Epoch 0, Step 41: train/loss = 0.6847130060195923, train/raw-loss = 0.6795097589492798, train/logprobs = tensor([[-0.8216, -1.7078],
        [-0.8400, -1.6511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05203207954764366
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 42: train/loss = 0.6998201608657837, train/raw-loss = 0.694367527961731, train/logprobs = tensor([[-0.6560, -1.7110],
        [-0.6654, -1.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05452563613653183
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 43: train/loss = 0.6919351816177368, train/raw-loss = 0.6866177916526794, train/logprobs = tensor([[-1.0934, -2.2801],
        [-1.0771, -2.2323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05317414551973343
RAW KL tensor(0.0493, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 44: train/loss = 0.6962688565254211, train/raw-loss = 0.6903976798057556, train/logprobs = tensor([[-0.9415, -1.1740],
        [-0.9243, -1.1435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05871118605136871
RAW KL tensor(0.0539, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 45: train/loss = 0.7088512778282166, train/raw-loss = 0.7042211294174194, train/logprobs = tensor([[-0.8766, -1.8393],
        [-0.8959, -1.8327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046301472932100296
RAW KL tensor(0.1295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 46: train/loss = 0.7289936542510986, train/raw-loss = 0.7230007648468018, train/logprobs = tensor([[-1.1262, -2.6505],
        [-1.1680, -2.7227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059928908944129944
RAW KL tensor(0.0574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 47: train/loss = 0.7000865936279297, train/raw-loss = 0.6937203407287598, train/logprobs = tensor([[-0.7199, -1.4910],
        [-0.7111, -1.4710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06366247683763504
RAW KL tensor(0.1064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 48: train/loss = 0.7136690616607666, train/raw-loss = 0.7070211172103882, train/logprobs = tensor([[-1.3350, -1.4101],
        [-1.3527, -1.4624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0664793998003006
RAW KL tensor(0.0460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 49: train/loss = 0.6964365243911743, train/raw-loss = 0.6914311051368713, train/logprobs = tensor([[-0.8395, -1.8763],
        [-0.8370, -1.8425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05005505308508873
RAW KL tensor(0.0196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 50: train/loss = 0.6951990723609924, train/raw-loss = 0.6909155249595642, train/logprobs = tensor([[-1.0832, -1.3937],
        [-1.0782, -1.3770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042835790663957596
RAW KL tensor(0.0463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 51: train/loss = 0.6910667419433594, train/raw-loss = 0.6857802867889404, train/logprobs = tensor([[-0.9913, -1.7751],
        [-0.9906, -1.7283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052864715456962585
RAW KL tensor(0.0419, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 52: train/loss = 0.690322995185852, train/raw-loss = 0.687508761882782, train/logprobs = tensor([[-0.7081, -0.7210],
        [-0.7282, -0.7153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028142552822828293
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 53: train/loss = 0.7115864157676697, train/raw-loss = 0.7088227868080139, train/logprobs = tensor([[-0.6088, -1.0795],
        [-0.5906, -1.0657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02763647772371769
RAW KL tensor(0.0467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 54: train/loss = 0.6877409815788269, train/raw-loss = 0.682507336139679, train/logprobs = tensor([[-1.2230, -1.6885],
        [-1.2433, -1.6452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052336983382701874
RAW KL tensor(0.0632, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 55: train/loss = 0.7148183584213257, train/raw-loss = 0.7028936743736267, train/logprobs = tensor([[-1.3000, -1.1052],
        [-1.3418, -1.1073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11924703419208527
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 56: train/loss = 0.6892745494842529, train/raw-loss = 0.6819606423377991, train/logprobs = tensor([[-0.8773, -1.4545],
        [-0.8995, -1.4236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07313912361860275
RAW KL tensor(0.0324, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 57: train/loss = 0.6895741820335388, train/raw-loss = 0.6851555109024048, train/logprobs = tensor([[-0.6412, -1.4265],
        [-0.6492, -1.3568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044186726212501526
RAW KL tensor(0.0098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 335])
Epoch 0, Step 58: train/loss = 0.6776020526885986, train/raw-loss = 0.6735333204269409, train/logprobs = tensor([[-0.6794, -1.6542],
        [-0.6772, -1.5657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04068746790289879
RAW KL tensor(0.0064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 364])
Epoch 0, Step 59: train/loss = 0.6952304840087891, train/raw-loss = 0.6919022798538208, train/logprobs = tensor([[-0.8279, -1.2489],
        [-0.8414, -1.2448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03328267112374306
RAW KL tensor(0.0445, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 60: train/loss = 0.6958655118942261, train/raw-loss = 0.6914083957672119, train/logprobs = tensor([[-1.1025, -1.5208],
        [-1.0788, -1.4796]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04457211494445801
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 61: train/loss = 0.6802291870117188, train/raw-loss = 0.6773156523704529, train/logprobs = tensor([[-0.5136, -1.4002],
        [-0.5257, -1.3338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029135417193174362
RAW KL tensor(0.0554, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 62: train/loss = 0.7147839665412903, train/raw-loss = 0.7092434763908386, train/logprobs = tensor([[-1.1946, -1.5974],
        [-1.1819, -1.5757]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055404916405677795
RAW KL tensor(0.0020, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 63: train/loss = 0.7001602649688721, train/raw-loss = 0.6964080333709717, train/logprobs = tensor([[-0.9767, -1.3460],
        [-0.9932, -1.3672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03752182796597481
RAW KL tensor(0.0608, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 64: train/loss = 0.6937844753265381, train/raw-loss = 0.6896384954452515, train/logprobs = tensor([[-0.9369, -1.6423],
        [-0.9300, -1.6123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041460491716861725
RAW KL tensor(0.0504, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 65: train/loss = 0.7013130784034729, train/raw-loss = 0.6971468329429626, train/logprobs = tensor([[-0.8828, -1.5689],
        [-0.8697, -1.5631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0416625514626503
RAW KL tensor(0.0448, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 66: train/loss = 0.7131512761116028, train/raw-loss = 0.7089091539382935, train/logprobs = tensor([[-1.0587, -1.9493],
        [-1.0093, -1.9044]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04242096096277237
RAW KL tensor(0.0112, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 67: train/loss = 0.6950907111167908, train/raw-loss = 0.6923789381980896, train/logprobs = tensor([[-0.6842, -1.8718],
        [-0.6695, -1.8175]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027117934077978134
RAW KL tensor(0.0057, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 68: train/loss = 0.705316960811615, train/raw-loss = 0.7031043767929077, train/logprobs = tensor([[-0.8154, -2.8038],
        [-0.7968, -2.7721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02212587557733059
RAW KL tensor(0.0492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 69: train/loss = 0.6921756863594055, train/raw-loss = 0.6857651472091675, train/logprobs = tensor([[-1.2924, -1.7239],
        [-1.3023, -1.6967]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06410544365644455
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 70: train/loss = 0.6912198066711426, train/raw-loss = 0.6894952058792114, train/logprobs = tensor([[-0.5999, -1.9346],
        [-0.5944, -1.9033]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017246190458536148
RAW KL tensor(0.0526, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 71: train/loss = 0.6990053653717041, train/raw-loss = 0.6936639547348022, train/logprobs = tensor([[-1.0528, -1.9619],
        [-1.0462, -1.8995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05341416224837303
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 261])
Epoch 0, Step 72: train/loss = 0.6875698566436768, train/raw-loss = 0.6849978566169739, train/logprobs = tensor([[-0.4550, -1.9861],
        [-0.4576, -1.9467]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02572004869580269
RAW KL tensor(0.0126, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 73: train/loss = 0.6996365189552307, train/raw-loss = 0.6970072388648987, train/logprobs = tensor([[-0.7443, -1.1189],
        [-0.7801, -1.1528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02629288099706173
RAW KL tensor(0.0094, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 74: train/loss = 0.6986998319625854, train/raw-loss = 0.695360004901886, train/logprobs = tensor([[-1.6134, -2.0221],
        [-1.5921, -2.0000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033398378640413284
RAW KL tensor(0.0546, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 75: train/loss = 0.6939849853515625, train/raw-loss = 0.6900342106819153, train/logprobs = tensor([[-0.7829, -1.8985],
        [-0.7751, -1.8706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039508156478405
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 76: train/loss = 0.6721258759498596, train/raw-loss = 0.6684818267822266, train/logprobs = tensor([[-0.9952, -1.5911],
        [-1.0741, -1.5558]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0364399328827858
RAW KL tensor(0.0609, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 77: train/loss = 0.6983380913734436, train/raw-loss = 0.6926276683807373, train/logprobs = tensor([[-1.2840, -1.5010],
        [-1.2952, -1.5012]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05710339918732643
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 78: train/loss = 0.6892704963684082, train/raw-loss = 0.6856098175048828, train/logprobs = tensor([[-0.9006, -1.9094],
        [-0.9124, -1.8863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03660751134157181
RAW KL tensor(0.0442, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 257])
Epoch 0, Step 79: train/loss = 0.6886721253395081, train/raw-loss = 0.6861414909362793, train/logprobs = tensor([[-0.5546, -1.2737],
        [-0.5552, -1.2377]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025306761264801025
RAW KL tensor(0.0263, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 80: train/loss = 0.6888575553894043, train/raw-loss = 0.685078501701355, train/logprobs = tensor([[-1.0878, -2.1559],
        [-1.0843, -2.1148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03779090568423271
RAW KL tensor(0.0689, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 81: train/loss = 0.7073643207550049, train/raw-loss = 0.7034692764282227, train/logprobs = tensor([[-0.8418, -1.9089],
        [-0.8563, -1.9597]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038950636982917786
RAW KL tensor(0.0470, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 82: train/loss = 0.7028718590736389, train/raw-loss = 0.6968907713890076, train/logprobs = tensor([[-1.2558, -1.0788],
        [-1.2556, -1.0724]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05981137230992317
RAW KL tensor(0.0179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 83: train/loss = 0.6927980184555054, train/raw-loss = 0.690181314945221, train/logprobs = tensor([[-0.6417, -1.7307],
        [-0.6470, -1.6817]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026166807860136032
RAW KL tensor(0.0239, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 344])
Epoch 0, Step 84: train/loss = 0.6880495548248291, train/raw-loss = 0.6858158111572266, train/logprobs = tensor([[-0.5681, -2.0567],
        [-0.5617, -2.0173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022338278591632843
RAW KL tensor(0.0386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 85: train/loss = 0.7149969339370728, train/raw-loss = 0.7022321224212646, train/logprobs = tensor([[-0.9324, -2.1856],
        [-0.9444, -2.1941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12764844298362732
RAW KL tensor(0.0191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 86: train/loss = 0.6891407370567322, train/raw-loss = 0.6852616667747498, train/logprobs = tensor([[-0.6523, -1.8470],
        [-0.6577, -1.8129]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03879080340266228
RAW KL tensor(0.0300, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 87: train/loss = 0.7083456516265869, train/raw-loss = 0.7041488289833069, train/logprobs = tensor([[-1.4100, -1.5993],
        [-1.4035, -1.6218]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04196759685873985
RAW KL tensor(0.0089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 88: train/loss = 0.7008852958679199, train/raw-loss = 0.6963785886764526, train/logprobs = tensor([[-0.8211, -2.0780],
        [-0.8275, -2.0426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045066170394420624
RAW KL tensor(0.0323, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 89: train/loss = 0.6989282369613647, train/raw-loss = 0.6942613124847412, train/logprobs = tensor([[-1.1190, -1.9488],
        [-1.1092, -1.8823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04666915908455849
RAW KL tensor(0.0219, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 90: train/loss = 0.6966062784194946, train/raw-loss = 0.6943311095237732, train/logprobs = tensor([[-0.8650, -1.8132],
        [-0.9050, -1.8380]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02275163307785988
RAW KL tensor(0.1413, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 91: train/loss = 0.6935606002807617, train/raw-loss = 0.6880583763122559, train/logprobs = tensor([[-1.3337, -1.1920],
        [-1.3662, -1.2031]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05502128601074219
RAW KL tensor(0.0534, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 92: train/loss = 0.6971753835678101, train/raw-loss = 0.6906754970550537, train/logprobs = tensor([[-0.9328, -1.7460],
        [-0.9935, -1.7422]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06499921530485153
RAW KL tensor(0.0333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 93: train/loss = 0.6998075246810913, train/raw-loss = 0.6968961954116821, train/logprobs = tensor([[-1.0264, -1.5088],
        [-1.0402, -1.5136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02911289408802986
RAW KL tensor(0.1405, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 94: train/loss = 0.715236246585846, train/raw-loss = 0.7087640762329102, train/logprobs = tensor([[-1.2862, -2.7473],
        [-1.3263, -2.7098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06472156941890717
RAW KL tensor(0.0428, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 95: train/loss = 0.6964519619941711, train/raw-loss = 0.690026581287384, train/logprobs = tensor([[-1.5746, -2.2931],
        [-1.5958, -2.2494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06425369530916214
RAW KL tensor(0.0066, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 96: train/loss = 0.6958965063095093, train/raw-loss = 0.6924179792404175, train/logprobs = tensor([[-0.8870, -1.7322],
        [-0.8882, -1.7222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03478522226214409
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 97: train/loss = 0.7022027373313904, train/raw-loss = 0.6988433599472046, train/logprobs = tensor([[-1.8277, -2.7972],
        [-1.8241, -2.8091]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0335945300757885
RAW KL tensor(0.0265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 98: train/loss = 0.6917450428009033, train/raw-loss = 0.689576268196106, train/logprobs = tensor([[-1.1350, -1.6298],
        [-1.1503, -1.6167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0216875821352005
RAW KL tensor(0.0615, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 99: train/loss = 0.6938958168029785, train/raw-loss = 0.6898046731948853, train/logprobs = tensor([[-0.8776, -1.6102],
        [-0.8730, -1.5826]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04091169685125351
RAW KL tensor(0.0662, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 100: train/loss = 0.6935612559318542, train/raw-loss = 0.689713180065155, train/logprobs = tensor([[-1.0513, -1.3810],
        [-1.0426, -1.3350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03848063945770264
RAW KL tensor(0.0168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 101: train/loss = 0.6878745555877686, train/raw-loss = 0.6846531629562378, train/logprobs = tensor([[-1.1715, -2.3913],
        [-1.1641, -2.3275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032213471829891205
RAW KL tensor(0.0331, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 102: train/loss = 0.6712139844894409, train/raw-loss = 0.6675438284873962, train/logprobs = tensor([[-0.7742, -1.8215],
        [-0.7501, -1.6864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03670120984315872
RAW KL tensor(0.0494, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 103: train/loss = 0.6840092539787292, train/raw-loss = 0.67853182554245, train/logprobs = tensor([[-0.8826, -1.7661],
        [-0.9137, -1.7206]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05477435141801834
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 104: train/loss = 0.6893172264099121, train/raw-loss = 0.687708854675293, train/logprobs = tensor([[-0.8480, -1.5255],
        [-0.8520, -1.4878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016083242371678352
RAW KL tensor(0.0073, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 105: train/loss = 0.6926320791244507, train/raw-loss = 0.6892819404602051, train/logprobs = tensor([[-1.0135, -1.8293],
        [-1.0097, -1.7903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03350067511200905
RAW KL tensor(0.0132, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 106: train/loss = 0.6928831338882446, train/raw-loss = 0.6909016370773315, train/logprobs = tensor([[-0.7534, -1.8062],
        [-0.7267, -1.7562]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019815033301711082
RAW KL tensor(0.0500, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 107: train/loss = 0.69603431224823, train/raw-loss = 0.690582275390625, train/logprobs = tensor([[-1.3294, -1.7411],
        [-1.3461, -1.7140]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0545201301574707
RAW KL tensor(0.0111, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 108: train/loss = 0.6946934461593628, train/raw-loss = 0.6908484697341919, train/logprobs = tensor([[-0.7600, -0.8872],
        [-0.7508, -0.8499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03845009207725525
RAW KL tensor(0.0174, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 109: train/loss = 0.699942409992218, train/raw-loss = 0.6971114873886108, train/logprobs = tensor([[-0.8207, -1.8099],
        [-0.8174, -1.7737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0283089280128479
RAW KL tensor(0.0579, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 110: train/loss = 0.6845773458480835, train/raw-loss = 0.6775476932525635, train/logprobs = tensor([[-0.9764, -2.1783],
        [-0.9422, -2.0658]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07029619812965393
RAW KL tensor(0.0225, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 111: train/loss = 0.7006257176399231, train/raw-loss = 0.6985429525375366, train/logprobs = tensor([[-0.9467, -1.8142],
        [-0.9417, -1.8223]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020827796310186386
RAW KL tensor(0.0501, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 112: train/loss = 0.6989750266075134, train/raw-loss = 0.6948537230491638, train/logprobs = tensor([[-1.5262, -1.4787],
        [-1.5491, -1.5024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04121328145265579
RAW KL tensor(0.0307, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 113: train/loss = 0.6881667375564575, train/raw-loss = 0.6840395927429199, train/logprobs = tensor([[-1.0737, -1.4497],
        [-1.0573, -1.3913]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041270915418863297
RAW KL tensor(0.0163, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 353])
Epoch 0, Step 114: train/loss = 0.6881262063980103, train/raw-loss = 0.6861816048622131, train/logprobs = tensor([[-0.9559, -1.2121],
        [-0.9642, -1.1835]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019446933642029762
RAW KL tensor(0.0475, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 115: train/loss = 0.6658376455307007, train/raw-loss = 0.6635977625846863, train/logprobs = tensor([[-0.7741, -1.7998],
        [-0.7547, -1.6519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02239900641143322
RAW KL tensor(0.0164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 116: train/loss = 0.6926112174987793, train/raw-loss = 0.691116213798523, train/logprobs = tensor([[-0.6597, -1.4526],
        [-0.6669, -1.4499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014949945732951164
RAW KL tensor(0.0146, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 117: train/loss = 0.6857694983482361, train/raw-loss = 0.6826669573783875, train/logprobs = tensor([[-0.9184, -1.8083],
        [-0.9259, -1.7481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031025338917970657
RAW KL tensor(0.0322, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 118: train/loss = 0.6855416893959045, train/raw-loss = 0.6835558414459229, train/logprobs = tensor([[-1.0059, -1.9691],
        [-1.0510, -1.9583]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01985820382833481
RAW KL tensor(0.1039, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 119: train/loss = 0.6986637115478516, train/raw-loss = 0.6938228011131287, train/logprobs = tensor([[-1.4786, -1.8838],
        [-1.5053, -1.9008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04840892180800438
RAW KL tensor(0.0329, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 120: train/loss = 0.713090717792511, train/raw-loss = 0.7086261510848999, train/logprobs = tensor([[-1.1543, -2.1970],
        [-1.1418, -2.1865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04464574530720711
RAW KL tensor(0.0916, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 121: train/loss = 0.6930160522460938, train/raw-loss = 0.6893075108528137, train/logprobs = tensor([[-0.7602, -1.1867],
        [-0.7774, -1.1718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037085339426994324
RAW KL tensor(0.0142, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 122: train/loss = 0.6967655420303345, train/raw-loss = 0.6936624646186829, train/logprobs = tensor([[-1.0509, -2.8053],
        [-1.0315, -2.7822]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031031601130962372
RAW KL tensor(0.0209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 123: train/loss = 0.691683828830719, train/raw-loss = 0.6879751086235046, train/logprobs = tensor([[-1.2209, -1.4483],
        [-1.2185, -1.3974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037087902426719666
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 124: train/loss = 0.6927000880241394, train/raw-loss = 0.6905278563499451, train/logprobs = tensor([[-0.6435, -1.4232],
        [-0.5970, -1.3468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021722137928009033
RAW KL tensor(0.0859, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 125: train/loss = 0.6859005689620972, train/raw-loss = 0.6801546812057495, train/logprobs = tensor([[-0.8512, -2.3812],
        [-0.8899, -2.3531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.057458702474832535
RAW KL tensor(0.0476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 126: train/loss = 0.6931147575378418, train/raw-loss = 0.6905362606048584, train/logprobs = tensor([[-1.1916, -2.9370],
        [-1.2472, -2.9459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025785118341445923
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 127: train/loss = 0.6877326965332031, train/raw-loss = 0.684358537197113, train/logprobs = tensor([[-1.0056, -1.8691],
        [-0.9651, -1.7841]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03374194726347923
RAW KL tensor(0.0459, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 128: train/loss = 0.7174278497695923, train/raw-loss = 0.7146180868148804, train/logprobs = tensor([[-1.0695, -1.5697],
        [-1.0517, -1.6187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028097029775381088
RAW KL tensor(0.0390, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 129: train/loss = 0.7104012370109558, train/raw-loss = 0.705655574798584, train/logprobs = tensor([[-1.1867, -2.0080],
        [-1.1597, -2.0111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04745662212371826
RAW KL tensor(0.0203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 430])
Epoch 0, Step 130: train/loss = 0.6991050243377686, train/raw-loss = 0.6961541771888733, train/logprobs = tensor([[-1.2437, -2.6976],
        [-1.1811, -2.6111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02950863726437092
RAW KL tensor(0.0127, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 131: train/loss = 0.690805971622467, train/raw-loss = 0.6871145367622375, train/logprobs = tensor([[-1.6361, -1.5539],
        [-1.6508, -1.5286]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036913737654685974
RAW KL tensor(0.0108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 132: train/loss = 0.6961186528205872, train/raw-loss = 0.6893256306648254, train/logprobs = tensor([[-1.0800, -2.2600],
        [-1.1734, -2.2754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06793039292097092
RAW KL tensor(0.0391, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 133: train/loss = 0.6951125264167786, train/raw-loss = 0.6916468739509583, train/logprobs = tensor([[-1.5301, -1.9000],
        [-1.5455, -1.8956]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034656770527362823
RAW KL tensor(0.0253, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 134: train/loss = 0.6924207806587219, train/raw-loss = 0.690383791923523, train/logprobs = tensor([[-1.0645, -1.7266],
        [-1.0705, -1.6863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02036978304386139
RAW KL tensor(0.0196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 135: train/loss = 0.6886333227157593, train/raw-loss = 0.6864956021308899, train/logprobs = tensor([[-1.6395, -1.6799],
        [-1.6235, -1.6361]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02137717604637146
RAW KL tensor(0.0514, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 136: train/loss = 0.6839213371276855, train/raw-loss = 0.6789880990982056, train/logprobs = tensor([[-1.6365, -2.4455],
        [-1.6491, -2.3587]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049332428723573685
RAW KL tensor(0.0385, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 137: train/loss = 0.6864079236984253, train/raw-loss = 0.6821556091308594, train/logprobs = tensor([[-1.7083, -1.5726],
        [-1.7293, -1.5461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04252292215824127
RAW KL tensor(0.0379, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 138: train/loss = 0.6923335790634155, train/raw-loss = 0.6900333762168884, train/logprobs = tensor([[-1.2470, -2.0967],
        [-1.2051, -2.0246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02300206944346428
RAW KL tensor(0.0089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 139: train/loss = 0.7194544672966003, train/raw-loss = 0.7089399099349976, train/logprobs = tensor([[-1.2017, -2.5486],
        [-1.2199, -2.5240]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10514593869447708
RAW KL tensor(0.0159, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 140: train/loss = 0.6891559958457947, train/raw-loss = 0.6865456700325012, train/logprobs = tensor([[-1.1144, -2.3374],
        [-1.1089, -2.2944]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026103071868419647
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 141: train/loss = 0.688148021697998, train/raw-loss = 0.6856980323791504, train/logprobs = tensor([[-0.9057, -2.1479],
        [-0.8984, -2.0932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024500269442796707
RAW KL tensor(0.0296, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 142: train/loss = 0.6805614233016968, train/raw-loss = 0.6769504547119141, train/logprobs = tensor([[-1.3808, -1.9629],
        [-1.3918, -1.8926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036109521985054016
RAW KL tensor(0.0208, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 143: train/loss = 0.6886565685272217, train/raw-loss = 0.6856589913368225, train/logprobs = tensor([[-1.6765, -1.8239],
        [-1.7110, -1.7882]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029975995421409607
RAW KL tensor(0.0142, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 144: train/loss = 0.6939708590507507, train/raw-loss = 0.6921250820159912, train/logprobs = tensor([[-1.0009, -1.9211],
        [-0.9901, -1.8739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0184582881629467
RAW KL tensor(0.0543, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 145: train/loss = 0.7064334154129028, train/raw-loss = 0.7028396725654602, train/logprobs = tensor([[-1.2587, -1.6175],
        [-1.2224, -1.5491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035937268286943436
RAW KL tensor(0.0100, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 146: train/loss = 0.6877585649490356, train/raw-loss = 0.6847953796386719, train/logprobs = tensor([[-0.8154, -1.8713],
        [-0.8269, -1.8281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029632853344082832
RAW KL tensor(0.0391, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 147: train/loss = 0.689781904220581, train/raw-loss = 0.6852484941482544, train/logprobs = tensor([[-1.3982, -1.8450],
        [-1.3855, -1.7935]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04533389210700989
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 148: train/loss = 0.6840123534202576, train/raw-loss = 0.6821261644363403, train/logprobs = tensor([[-1.0746, -1.5016],
        [-1.0808, -1.4601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018862180411815643
RAW KL tensor(0.0088, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 149: train/loss = 0.6887621879577637, train/raw-loss = 0.6870791912078857, train/logprobs = tensor([[-1.0685, -1.8676],
        [-1.0511, -1.8252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016829300671815872
RAW KL tensor(0.0561, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 150: train/loss = 0.6919353604316711, train/raw-loss = 0.6883999109268188, train/logprobs = tensor([[-1.2003, -1.8578],
        [-1.1714, -1.7939]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03535430505871773
RAW KL tensor(0.0277, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 151: train/loss = 0.6978504657745361, train/raw-loss = 0.6956813931465149, train/logprobs = tensor([[-0.9831, -1.5461],
        [-0.9813, -1.5278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021690987050533295
RAW KL tensor(0.0138, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 152: train/loss = 0.6907371282577515, train/raw-loss = 0.6884952187538147, train/logprobs = tensor([[-0.9792, -1.9950],
        [-0.9632, -1.9570]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022419290617108345
RAW KL tensor(0.0288, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 153: train/loss = 0.7073425650596619, train/raw-loss = 0.7042675018310547, train/logprobs = tensor([[-1.1406, -1.8294],
        [-1.1330, -1.8530]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030750591307878494
RAW KL tensor(0.0333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 154: train/loss = 0.6912022829055786, train/raw-loss = 0.6880141496658325, train/logprobs = tensor([[-1.4783, -2.2440],
        [-1.4728, -2.2033]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0318809412419796
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 155: train/loss = 0.6952717900276184, train/raw-loss = 0.6910722851753235, train/logprobs = tensor([[-0.9250, -1.7360],
        [-0.9031, -1.7022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041994791477918625
RAW KL tensor(0.0482, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 156: train/loss = 0.6892296671867371, train/raw-loss = 0.6862154603004456, train/logprobs = tensor([[-1.0885, -2.3531],
        [-1.0737, -2.3059]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03014187328517437
RAW KL tensor(0.0119, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 157: train/loss = 0.6945911049842834, train/raw-loss = 0.6923613548278809, train/logprobs = tensor([[-1.0425, -1.3683],
        [-1.0177, -1.3160]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022297339513897896
RAW KL tensor(0.0051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 158: train/loss = 0.6815795302391052, train/raw-loss = 0.6799300909042358, train/logprobs = tensor([[-0.8698, -0.8618],
        [-0.8814, -0.8125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016494471579790115
RAW KL tensor(0.0441, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 159: train/loss = 0.6897410750389099, train/raw-loss = 0.6869632005691528, train/logprobs = tensor([[-0.9247, -1.3679],
        [-0.8952, -1.2929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027778802439570427
RAW KL tensor(0.0097, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 160: train/loss = 0.6888402104377747, train/raw-loss = 0.6859633326530457, train/logprobs = tensor([[-0.8907, -1.4214],
        [-0.8614, -1.3555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028768250718712807
RAW KL tensor(0.0139, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 161: train/loss = 0.6963254809379578, train/raw-loss = 0.6936750411987305, train/logprobs = tensor([[-1.0084, -2.0529],
        [-1.0269, -2.0604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026504147797822952
RAW KL tensor(0.0349, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 162: train/loss = 0.6766348481178284, train/raw-loss = 0.6738941669464111, train/logprobs = tensor([[-0.7447, -2.4279],
        [-0.7058, -2.2994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027406791225075722
RAW KL tensor(0.0534, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 163: train/loss = 0.6815382838249207, train/raw-loss = 0.6780105233192444, train/logprobs = tensor([[-1.3394, -1.9018],
        [-1.3110, -1.7778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035277340561151505
RAW KL tensor(0.0526, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 164: train/loss = 0.6893421411514282, train/raw-loss = 0.6865090131759644, train/logprobs = tensor([[-1.6428, -1.8474],
        [-1.6451, -1.7871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028331808745861053
RAW KL tensor(0.0170, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 165: train/loss = 0.6944103240966797, train/raw-loss = 0.6914033889770508, train/logprobs = tensor([[-1.2071, -2.1178],
        [-1.1814, -2.0710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03006945736706257
RAW KL tensor(0.0523, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 166: train/loss = 0.6901270747184753, train/raw-loss = 0.6870357990264893, train/logprobs = tensor([[-0.9450, -2.2206],
        [-0.9324, -2.1749]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030913371592760086
RAW KL tensor(0.0233, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 167: train/loss = 0.6846951246261597, train/raw-loss = 0.6820677518844604, train/logprobs = tensor([[-1.3511, -1.3402],
        [-1.4107, -1.3379]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026273541152477264
RAW KL tensor(0.0144, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 168: train/loss = 0.6986595392227173, train/raw-loss = 0.6956733465194702, train/logprobs = tensor([[-1.1155, -1.3898],
        [-1.1072, -1.3737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02986173704266548
RAW KL tensor(0.0187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 169: train/loss = 0.6925494074821472, train/raw-loss = 0.6892332434654236, train/logprobs = tensor([[-0.9380, -1.6485],
        [-0.9594, -1.6416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03316216915845871
RAW KL tensor(0.0112, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 170: train/loss = 0.6860479712486267, train/raw-loss = 0.6839193105697632, train/logprobs = tensor([[-0.6548, -2.1664],
        [-0.6496, -2.1172]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02128690853714943
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 171: train/loss = 0.6936511397361755, train/raw-loss = 0.6909216642379761, train/logprobs = tensor([[-1.3341, -1.6128],
        [-1.3067, -1.5504]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02729426696896553
RAW KL tensor(0.0176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 172: train/loss = 0.6944746971130371, train/raw-loss = 0.6905140280723572, train/logprobs = tensor([[-1.0665, -1.9246],
        [-1.0576, -1.8564]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03960694000124931
RAW KL tensor(0.0697, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 173: train/loss = 0.6953089833259583, train/raw-loss = 0.6917665004730225, train/logprobs = tensor([[-1.1995, -2.5688],
        [-1.2426, -2.5115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03542390465736389
RAW KL tensor(0.0429, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 174: train/loss = 0.6955791711807251, train/raw-loss = 0.6917356252670288, train/logprobs = tensor([[-1.3185, -2.7990],
        [-1.2978, -2.6826]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038436464965343475
RAW KL tensor(0.0163, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 175: train/loss = 0.700751543045044, train/raw-loss = 0.6980259418487549, train/logprobs = tensor([[-1.6762, -1.8052],
        [-1.6111, -1.7572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027255430817604065
RAW KL tensor(0.0230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 176: train/loss = 0.6799680590629578, train/raw-loss = 0.677987813949585, train/logprobs = tensor([[-0.9003, -2.1262],
        [-0.8578, -1.9954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019802941009402275
RAW KL tensor(0.0261, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 177: train/loss = 0.6852658987045288, train/raw-loss = 0.6832270622253418, train/logprobs = tensor([[-1.0152, -1.6666],
        [-1.0009, -1.6081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020388133823871613
RAW KL tensor(0.0140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 259])
Epoch 0, Step 178: train/loss = 0.6841439008712769, train/raw-loss = 0.6825711727142334, train/logprobs = tensor([[-0.8326, -1.9701],
        [-0.8218, -1.9018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01572669856250286
RAW KL tensor(0.0237, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 179: train/loss = 0.6942346096038818, train/raw-loss = 0.6922948360443115, train/logprobs = tensor([[-1.4340, -2.0740],
        [-1.4098, -2.0328]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019396811723709106
RAW KL tensor(0.0193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 180: train/loss = 0.6966692209243774, train/raw-loss = 0.6835402250289917, train/logprobs = tensor([[-1.2727, -1.6463],
        [-1.2820, -1.5905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.131290003657341
RAW KL tensor(0.0040, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 326])
Epoch 0, Step 181: train/loss = 0.7005564570426941, train/raw-loss = 0.6972655057907104, train/logprobs = tensor([[-1.0215, -1.6982],
        [-1.0276, -1.7034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032909002155065536
RAW KL tensor(0.0345, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 182: train/loss = 0.6881115436553955, train/raw-loss = 0.6854389309883118, train/logprobs = tensor([[-1.1845, -1.5819],
        [-1.1900, -1.5390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026726368814706802
RAW KL tensor(0.0080, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 183: train/loss = 0.6792240142822266, train/raw-loss = 0.675926685333252, train/logprobs = tensor([[-0.9236, -1.9744],
        [-0.9363, -1.8733]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03297343850135803
RAW KL tensor(0.0240, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 184: train/loss = 0.6801813244819641, train/raw-loss = 0.6774930953979492, train/logprobs = tensor([[-1.0698, -2.1210],
        [-1.0787, -2.0625]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026882590726017952
RAW KL tensor(0.0781, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 185: train/loss = 0.7144971489906311, train/raw-loss = 0.7110757231712341, train/logprobs = tensor([[-0.9524, -1.8295],
        [-0.9504, -1.7576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03421417251229286
RAW KL tensor(0.0092, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 186: train/loss = 0.6912297010421753, train/raw-loss = 0.688610851764679, train/logprobs = tensor([[-0.8713, -1.9647],
        [-0.8481, -1.8877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02618921548128128
RAW KL tensor(0.0504, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 187: train/loss = 0.6981684565544128, train/raw-loss = 0.6941688060760498, train/logprobs = tensor([[-1.0699, -1.7445],
        [-1.0677, -1.7222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039996784180402756
RAW KL tensor(0.0179, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 188: train/loss = 0.6880845427513123, train/raw-loss = 0.685585618019104, train/logprobs = tensor([[-1.3174, -2.0046],
        [-1.2943, -1.9463]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02498924359679222
RAW KL tensor(0.0680, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 189: train/loss = 0.7059684991836548, train/raw-loss = 0.7027037143707275, train/logprobs = tensor([[-1.1774, -1.6621],
        [-1.1764, -1.6447]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032647714018821716
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 190: train/loss = 0.6814852356910706, train/raw-loss = 0.6784719228744507, train/logprobs = tensor([[-1.5846, -1.6821],
        [-1.6503, -1.6501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030133076012134552
RAW KL tensor(0.0459, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 191: train/loss = 0.6963667273521423, train/raw-loss = 0.6936661005020142, train/logprobs = tensor([[-1.0901, -1.6911],
        [-1.0785, -1.6548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02700609713792801
RAW KL tensor(0.0102, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 192: train/loss = 0.6773144006729126, train/raw-loss = 0.6746066808700562, train/logprobs = tensor([[-1.2415, -1.4697],
        [-1.2640, -1.3738]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027076810598373413
RAW KL tensor(0.0133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 193: train/loss = 0.6957500576972961, train/raw-loss = 0.6943514347076416, train/logprobs = tensor([[-0.7544, -1.3651],
        [-0.7838, -1.3951]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013987001031637192
RAW KL tensor(0.0057, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 282])
Epoch 0, Step 194: train/loss = 0.6840775012969971, train/raw-loss = 0.6827582120895386, train/logprobs = tensor([[-1.1972, -1.2826],
        [-1.2181, -1.2485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013193231076002121
RAW KL tensor(0.0109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 195: train/loss = 0.688863217830658, train/raw-loss = 0.6870598793029785, train/logprobs = tensor([[-0.6259, -1.5361],
        [-0.6238, -1.4941]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01803359016776085
RAW KL tensor(0.0330, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 196: train/loss = 0.6892630457878113, train/raw-loss = 0.6863834857940674, train/logprobs = tensor([[-1.5556, -2.0999],
        [-1.4997, -2.0165]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028794897720217705
RAW KL tensor(0.0389, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 197: train/loss = 0.6822723150253296, train/raw-loss = 0.6805998682975769, train/logprobs = tensor([[-1.0177, -1.5407],
        [-0.9855, -1.4296]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01672397367656231
RAW KL tensor(0.0160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 198: train/loss = 0.675632894039154, train/raw-loss = 0.6741883158683777, train/logprobs = tensor([[-0.8898, -0.9988],
        [-0.9010, -0.9265]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014445400796830654
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 199: train/loss = 0.6781591773033142, train/raw-loss = 0.6761950850486755, train/logprobs = tensor([[-1.0164, -1.1945],
        [-1.0336, -1.1394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019641175866127014
RAW KL tensor(0.0133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 200: train/loss = 0.670512855052948, train/raw-loss = 0.668389618396759, train/logprobs = tensor([[-1.2281, -2.3833],
        [-1.2475, -2.2445]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02123224548995495
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 201: train/loss = 0.6761126518249512, train/raw-loss = 0.6748634576797485, train/logprobs = tensor([[-0.6920, -1.8774],
        [-0.6901, -1.7922]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012491848319768906
RAW KL tensor(0.0449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 202: train/loss = 0.6826133728027344, train/raw-loss = 0.6796776056289673, train/logprobs = tensor([[-0.7364, -2.0303],
        [-0.7309, -1.9543]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02935761772096157
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 203: train/loss = 0.6352097988128662, train/raw-loss = 0.628684401512146, train/logprobs = tensor([[-1.1922, -1.4156],
        [-1.2960, -1.2160]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06525376439094543
RAW KL tensor(0.0207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 204: train/loss = 0.6731675267219543, train/raw-loss = 0.6714348793029785, train/logprobs = tensor([[-1.3841, -1.4354],
        [-1.3874, -1.3491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017326513305306435
RAW KL tensor(0.0099, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 328])
Epoch 0, Step 205: train/loss = 0.6709619164466858, train/raw-loss = 0.6697061657905579, train/logprobs = tensor([[-0.6972, -1.5342],
        [-0.6912, -1.4216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012557296082377434
RAW KL tensor(0.0487, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 206: train/loss = 0.667446494102478, train/raw-loss = 0.6650622487068176, train/logprobs = tensor([[-0.8985, -1.4530],
        [-0.8750, -1.3096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023842712864279747
RAW KL tensor(0.0264, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 207: train/loss = 0.6597270965576172, train/raw-loss = 0.6573781967163086, train/logprobs = tensor([[-0.7935, -2.0703],
        [-0.8017, -1.9249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023488814011216164
RAW KL tensor(0.0874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 208: train/loss = 0.6787598133087158, train/raw-loss = 0.6748926639556885, train/logprobs = tensor([[-1.3886, -2.3494],
        [-1.4025, -2.2527]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03867097571492195
RAW KL tensor(0.0281, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 209: train/loss = 0.7001626491546631, train/raw-loss = 0.6974378228187561, train/logprobs = tensor([[-0.9875, -1.9051],
        [-0.9746, -1.8998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02724846825003624
RAW KL tensor(0.0180, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 210: train/loss = 0.6706265211105347, train/raw-loss = 0.6679524779319763, train/logprobs = tensor([[-1.6216, -1.6321],
        [-1.6818, -1.5414]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02674044854938984
RAW KL tensor(0.0370, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 211: train/loss = 0.6847296357154846, train/raw-loss = 0.6823368668556213, train/logprobs = tensor([[-1.2036, -2.1766],
        [-1.1747, -2.0745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023927856236696243
RAW KL tensor(0.0193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 212: train/loss = 0.6891139149665833, train/raw-loss = 0.6868906021118164, train/logprobs = tensor([[-0.8002, -1.2554],
        [-0.7720, -1.1968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022232677787542343
RAW KL tensor(0.0245, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 213: train/loss = 0.6719523668289185, train/raw-loss = 0.6701021790504456, train/logprobs = tensor([[-1.4693, -2.0268],
        [-1.4637, -1.8972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01850143074989319
RAW KL tensor(0.0110, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 214: train/loss = 0.6935881972312927, train/raw-loss = 0.6914643049240112, train/logprobs = tensor([[-0.9445, -1.2913],
        [-0.9369, -1.2643]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021238699555397034
RAW KL tensor(0.0183, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 215: train/loss = 0.6514025330543518, train/raw-loss = 0.6483634114265442, train/logprobs = tensor([[-1.1599, -1.6434],
        [-1.1722, -1.4652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030390897765755653
RAW KL tensor(0.0122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 216: train/loss = 0.6905733346939087, train/raw-loss = 0.6878324151039124, train/logprobs = tensor([[-1.7071, -1.5446],
        [-1.6857, -1.4656]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02740895375609398
RAW KL tensor(0.0578, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 217: train/loss = 0.6886853575706482, train/raw-loss = 0.6862953901290894, train/logprobs = tensor([[-0.9353, -1.6843],
        [-0.9551, -1.6741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02389911562204361
RAW KL tensor(0.0368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 218: train/loss = 0.67512047290802, train/raw-loss = 0.6711510419845581, train/logprobs = tensor([[-0.8737, -2.1768],
        [-0.8696, -2.0432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03969414904713631
RAW KL tensor(0.0022, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 496])
Epoch 0, Step 219: train/loss = 0.703741192817688, train/raw-loss = 0.7024784088134766, train/logprobs = tensor([[-1.0978, -1.0212],
        [-1.0341, -0.9894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012627901509404182
RAW KL tensor(0.0530, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 220: train/loss = 0.68890380859375, train/raw-loss = 0.6856772899627686, train/logprobs = tensor([[-1.3405, -2.9243],
        [-1.3245, -2.8588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03226609528064728
RAW KL tensor(0.0122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 221: train/loss = 0.6842080950737, train/raw-loss = 0.6812937259674072, train/logprobs = tensor([[-1.6557, -0.8894],
        [-1.6273, -0.8121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02914407104253769
RAW KL tensor(0.0312, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 222: train/loss = 0.683641791343689, train/raw-loss = 0.681803286075592, train/logprobs = tensor([[-1.0912, -1.3800],
        [-1.0563, -1.2850]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018384918570518494
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 223: train/loss = 0.6762127876281738, train/raw-loss = 0.6727120876312256, train/logprobs = tensor([[-1.0327, -2.1069],
        [-1.0602, -1.9771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03500701114535332
RAW KL tensor(0.0404, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 224: train/loss = 0.6694517135620117, train/raw-loss = 0.6665509939193726, train/logprobs = tensor([[-1.3949, -2.8561],
        [-1.3727, -2.6992]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02900771051645279
RAW KL tensor(0.0096, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 360])
Epoch 0, Step 225: train/loss = 0.660601019859314, train/raw-loss = 0.6577637195587158, train/logprobs = tensor([[-0.8603, -2.1777],
        [-0.8321, -1.9889]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0283722635358572
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 226: train/loss = 0.6704136729240417, train/raw-loss = 0.66777503490448, train/logprobs = tensor([[-1.0886, -2.5597],
        [-1.1112, -2.4714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026386234909296036
RAW KL tensor(0.0401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 227: train/loss = 0.6830570697784424, train/raw-loss = 0.68132483959198, train/logprobs = tensor([[-0.8017, -1.4718],
        [-0.7905, -1.4089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01732194609940052
RAW KL tensor(0.0290, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 228: train/loss = 0.6715689301490784, train/raw-loss = 0.6689693927764893, train/logprobs = tensor([[-1.2123, -2.6043],
        [-1.1977, -2.4769]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025996003299951553
RAW KL tensor(0.0125, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 283])
Epoch 0, Step 229: train/loss = 0.6528907418251038, train/raw-loss = 0.6511223912239075, train/logprobs = tensor([[-1.0428, -1.8292],
        [-1.0603, -1.6496]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0176834873855114
RAW KL tensor(0.0960, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 230: train/loss = 0.6679713129997253, train/raw-loss = 0.6625213623046875, train/logprobs = tensor([[-1.2317, -2.7192],
        [-1.2187, -2.5769]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05450006574392319
RAW KL tensor(0.0207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 231: train/loss = 0.6767153143882751, train/raw-loss = 0.6740293502807617, train/logprobs = tensor([[-1.3182, -1.4677],
        [-1.3106, -1.3760]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026859711855649948
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 232: train/loss = 0.6723071932792664, train/raw-loss = 0.6698737144470215, train/logprobs = tensor([[-1.2741, -2.3681],
        [-1.2544, -2.2419]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02433532476425171
RAW KL tensor(0.0119, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 233: train/loss = 0.6864317655563354, train/raw-loss = 0.6807667016983032, train/logprobs = tensor([[-1.7790, -1.9655],
        [-1.7260, -1.8273]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05665098503232002
RAW KL tensor(0.0105, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 234: train/loss = 0.629172146320343, train/raw-loss = 0.6262109875679016, train/logprobs = tensor([[-1.1584, -2.1321],
        [-1.1654, -1.8399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029612118378281593
RAW KL tensor(0.1120, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 235: train/loss = 0.7096126675605774, train/raw-loss = 0.6996294260025024, train/logprobs = tensor([[-1.2213, -1.7754],
        [-1.1774, -1.6790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09983226656913757
RAW KL tensor(0.0101, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 236: train/loss = 0.6785852313041687, train/raw-loss = 0.67646324634552, train/logprobs = tensor([[-0.7052, -1.4984],
        [-0.6994, -1.4228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021219251677393913
RAW KL tensor(0.0211, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 237: train/loss = 0.6785329580307007, train/raw-loss = 0.677264928817749, train/logprobs = tensor([[-0.8098, -1.0856],
        [-0.8009, -0.9896]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012679592706263065
RAW KL tensor(0.0143, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 238: train/loss = 0.683627188205719, train/raw-loss = 0.6820158362388611, train/logprobs = tensor([[-0.7630, -1.6903],
        [-0.7693, -1.6432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016113124787807465
RAW KL tensor(0.0224, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 239: train/loss = 0.6939976811408997, train/raw-loss = 0.6869949698448181, train/logprobs = tensor([[-2.1578, -1.8525],
        [-2.1096, -1.7450]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.070027194917202
RAW KL tensor(0.0169, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 240: train/loss = 0.6884467601776123, train/raw-loss = 0.6867370009422302, train/logprobs = tensor([[-1.1600, -1.4907],
        [-1.1696, -1.4063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01709703728556633
RAW KL tensor(0.0304, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 241: train/loss = 0.6795607805252075, train/raw-loss = 0.6776503324508667, train/logprobs = tensor([[-1.1016, -2.0738],
        [-1.1220, -1.9948]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01910416968166828
RAW KL tensor(0.0356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 242: train/loss = 0.6694574356079102, train/raw-loss = 0.6662051677703857, train/logprobs = tensor([[-1.3761, -1.7355],
        [-1.3725, -1.5992]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032522186636924744
RAW KL tensor(0.0035, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 243: train/loss = 0.6730701923370361, train/raw-loss = 0.6709548234939575, train/logprobs = tensor([[-1.4676, -1.7479],
        [-1.4424, -1.5751]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021153390407562256
RAW KL tensor(0.0614, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 244: train/loss = 0.6731318235397339, train/raw-loss = 0.6706530451774597, train/logprobs = tensor([[-1.0619, -1.8339],
        [-1.0232, -1.6997]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024786993861198425
RAW KL tensor(0.0499, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 245: train/loss = 0.6824319362640381, train/raw-loss = 0.6798006296157837, train/logprobs = tensor([[-1.2712, -1.7965],
        [-1.2565, -1.7240]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026314029470086098
RAW KL tensor(0.0427, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 246: train/loss = 0.6895358562469482, train/raw-loss = 0.6876420974731445, train/logprobs = tensor([[-1.1325, -1.6131],
        [-1.1167, -1.5472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01893746294081211
RAW KL tensor(0.0082, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 247: train/loss = 0.67816162109375, train/raw-loss = 0.6765786409378052, train/logprobs = tensor([[-1.0751, -1.9345],
        [-1.0094, -1.7781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015829769894480705
RAW KL tensor(0.0503, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 248: train/loss = 0.6766102313995361, train/raw-loss = 0.6732336282730103, train/logprobs = tensor([[-1.0976, -2.5141],
        [-1.0572, -2.3828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03376616910099983
RAW KL tensor(0.0339, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 249: train/loss = 0.6730343699455261, train/raw-loss = 0.6704244613647461, train/logprobs = tensor([[-1.4622, -1.5415],
        [-1.4759, -1.4588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026099303737282753
RAW KL tensor(0.0123, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 250: train/loss = 0.6492668390274048, train/raw-loss = 0.6474435329437256, train/logprobs = tensor([[-0.6256, -2.2678],
        [-0.5924, -2.0383]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018233109265565872
RAW KL tensor(0.0170, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 251: train/loss = 0.6925367116928101, train/raw-loss = 0.6893773078918457, train/logprobs = tensor([[-1.0306, -2.3216],
        [-1.0808, -2.3541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03159363195300102
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 252: train/loss = 0.698535144329071, train/raw-loss = 0.6945484280586243, train/logprobs = tensor([[-1.2472, -1.2720],
        [-1.2135, -1.1972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03986738249659538
RAW KL tensor(0.0168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 253: train/loss = 0.6855003833770752, train/raw-loss = 0.6813005208969116, train/logprobs = tensor([[-1.3297, -1.6202],
        [-1.3224, -1.5613]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0419979989528656
RAW KL tensor(0.0120, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 254: train/loss = 0.6925126314163208, train/raw-loss = 0.6909140348434448, train/logprobs = tensor([[-1.0081, -1.1516],
        [-1.0242, -1.1306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015986237674951553
RAW KL tensor(0.0516, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 255: train/loss = 0.6716610193252563, train/raw-loss = 0.6684135794639587, train/logprobs = tensor([[-0.9669, -2.2867],
        [-0.9761, -2.1696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03247435390949249
RAW KL tensor(0.0176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 256: train/loss = 0.6404085755348206, train/raw-loss = 0.6383479833602905, train/logprobs = tensor([[-0.8078, -1.9747],
        [-0.8373, -1.7721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020605433732271194
RAW KL tensor(0.0345, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 257: train/loss = 0.6741926670074463, train/raw-loss = 0.6716238260269165, train/logprobs = tensor([[-1.4458, -2.4989],
        [-1.4133, -2.3408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025688450783491135
RAW KL tensor(0.0842, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 116])
Epoch 0, Step 258: train/loss = 0.6753937005996704, train/raw-loss = 0.6708735227584839, train/logprobs = tensor([[-1.7778, -2.1775],
        [-1.7512, -1.9716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04520130902528763
RAW KL tensor(0.0308, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 259: train/loss = 0.6841952204704285, train/raw-loss = 0.6816810965538025, train/logprobs = tensor([[-1.8048, -1.6675],
        [-1.7073, -1.5095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025141190737485886
RAW KL tensor(0.0519, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 260: train/loss = 0.6575169563293457, train/raw-loss = 0.6519814133644104, train/logprobs = tensor([[-1.2323, -1.9638],
        [-1.1752, -1.7300]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055355172604322433
RAW KL tensor(0.0183, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 261: train/loss = 0.6483532786369324, train/raw-loss = 0.6466778516769409, train/logprobs = tensor([[-0.9111, -2.3368],
        [-0.8105, -2.0218]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01675444096326828
RAW KL tensor(0.0440, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 262: train/loss = 0.7075628042221069, train/raw-loss = 0.7031875848770142, train/logprobs = tensor([[-1.4866, -1.8688],
        [-1.4324, -1.8106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04375176131725311
RAW KL tensor(0.0104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 263: train/loss = 0.6774012446403503, train/raw-loss = 0.6702813506126404, train/logprobs = tensor([[-1.8590, -2.5640],
        [-1.7607, -2.2965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0711989626288414
RAW KL tensor(0.0150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 264: train/loss = 0.6636403799057007, train/raw-loss = 0.6617953777313232, train/logprobs = tensor([[-1.1601, -2.3110],
        [-1.1070, -2.0966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018449854105710983
RAW KL tensor(0.0133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 265: train/loss = 0.6778685450553894, train/raw-loss = 0.6753882765769958, train/logprobs = tensor([[-1.4170, -1.8870],
        [-1.4058, -1.7886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024802815169095993
RAW KL tensor(0.0492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 120])
Epoch 0, Step 266: train/loss = 0.6915073394775391, train/raw-loss = 0.6880733966827393, train/logprobs = tensor([[-1.2473, -1.1338],
        [-1.2289, -1.0701]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034339047968387604
RAW KL tensor(0.0141, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 267: train/loss = 0.662918210029602, train/raw-loss = 0.6603834629058838, train/logprobs = tensor([[-1.4301, -1.6799],
        [-1.4480, -1.5552]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025347614660859108
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 268: train/loss = 0.6406362652778625, train/raw-loss = 0.6386719346046448, train/logprobs = tensor([[-1.1548, -1.8111],
        [-1.1538, -1.5617]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01964341476559639
RAW KL tensor(0.0335, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 269: train/loss = 0.6344178915023804, train/raw-loss = 0.6229284405708313, train/logprobs = tensor([[-0.9846, -1.7394],
        [-1.1525, -1.5703]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11489436030387878
RAW KL tensor(0.0172, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 270: train/loss = 0.6770526766777039, train/raw-loss = 0.675632655620575, train/logprobs = tensor([[-0.9010, -1.8161],
        [-0.8569, -1.6948]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014200233854353428
RAW KL tensor(0.0298, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 271: train/loss = 0.628404974937439, train/raw-loss = 0.6264694929122925, train/logprobs = tensor([[-0.6926, -2.0577],
        [-0.6939, -1.7604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019354740157723427
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 272: train/loss = 0.6653079986572266, train/raw-loss = 0.6631951332092285, train/logprobs = tensor([[-1.0026, -1.5987],
        [-0.9640, -1.4238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021128661930561066
RAW KL tensor(0.0130, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 273: train/loss = 0.6561636328697205, train/raw-loss = 0.6548831462860107, train/logprobs = tensor([[-0.5675, -2.0858],
        [-0.5867, -1.9231]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012804398313164711
RAW KL tensor(0.0794, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 274: train/loss = 0.6529958248138428, train/raw-loss = 0.648080587387085, train/logprobs = tensor([[-1.3938, -2.0915],
        [-1.4333, -1.9185]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04915236681699753
RAW KL tensor(0.0188, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 275: train/loss = 0.6631654500961304, train/raw-loss = 0.6609727144241333, train/logprobs = tensor([[-1.0874, -2.1736],
        [-1.0629, -1.9806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021927986294031143
RAW KL tensor(0.0156, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 276: train/loss = 0.6198195815086365, train/raw-loss = 0.6174299716949463, train/logprobs = tensor([[-0.6635, -2.5068],
        [-0.6802, -2.1960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02389659360051155
RAW KL tensor(0.0381, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 277: train/loss = 0.632354736328125, train/raw-loss = 0.6281073093414307, train/logprobs = tensor([[-1.6887, -2.1438],
        [-1.6624, -1.7975]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04247432202100754
RAW KL tensor(0.0264, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 278: train/loss = 0.6597774028778076, train/raw-loss = 0.6558922529220581, train/logprobs = tensor([[-1.4664, -2.5809],
        [-1.4674, -2.3139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03885111212730408
RAW KL tensor(0.0116, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 279: train/loss = 0.6817009449005127, train/raw-loss = 0.6788684129714966, train/logprobs = tensor([[-1.2796, -1.6686],
        [-1.2879, -1.6158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028325146064162254
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 280: train/loss = 0.642350435256958, train/raw-loss = 0.6400535106658936, train/logprobs = tensor([[-1.0365, -1.8298],
        [-1.0626, -1.6279]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022969381883740425
RAW KL tensor(0.0210, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 281: train/loss = 0.692754864692688, train/raw-loss = 0.6894764304161072, train/logprobs = tensor([[-1.5412, -1.8187],
        [-1.4643, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03278442844748497
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 282: train/loss = 0.656640350818634, train/raw-loss = 0.6538788080215454, train/logprobs = tensor([[-0.7187, -1.9363],
        [-0.7577, -1.8062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02761545404791832
RAW KL tensor(0.0153, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 283: train/loss = 0.6444952487945557, train/raw-loss = 0.6421657800674438, train/logprobs = tensor([[-0.8026, -2.3625],
        [-0.8288, -2.1507]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023294556885957718
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 284: train/loss = 0.679700493812561, train/raw-loss = 0.677557647228241, train/logprobs = tensor([[-1.0584, -1.4523],
        [-1.1176, -1.3797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02142789401113987
RAW KL tensor(0.0320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 285: train/loss = 0.6648550629615784, train/raw-loss = 0.6621473431587219, train/logprobs = tensor([[-1.1576, -2.6411],
        [-1.1523, -2.4774]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027076715603470802
RAW KL tensor(0.0096, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 355])
Epoch 0, Step 286: train/loss = 0.6872498989105225, train/raw-loss = 0.6845218539237976, train/logprobs = tensor([[-0.9591, -1.5794],
        [-0.9458, -1.4617]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027280503883957863
RAW KL tensor(0.0290, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 287: train/loss = 0.6786764860153198, train/raw-loss = 0.676070511341095, train/logprobs = tensor([[-1.2111, -1.3326],
        [-1.2186, -1.2307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026059957221150398
RAW KL tensor(0.2258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 288: train/loss = 0.7505043745040894, train/raw-loss = 0.7406459450721741, train/logprobs = tensor([[-1.6077, -2.5450],
        [-1.7315, -2.3768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09858382493257523
RAW KL tensor(0.0347, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 289: train/loss = 0.6773137450218201, train/raw-loss = 0.6737132668495178, train/logprobs = tensor([[-1.1614, -1.4202],
        [-1.1691, -1.3344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036004289984703064
RAW KL tensor(0.2500, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 290: train/loss = 0.6789411902427673, train/raw-loss = 0.6692460179328918, train/logprobs = tensor([[-1.1194, -1.7055],
        [-1.1134, -1.5909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09695187211036682
RAW KL tensor(0.0216, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 242])
Epoch 0, Step 291: train/loss = 0.6840602159500122, train/raw-loss = 0.682081937789917, train/logprobs = tensor([[-0.7556, -0.9800],
        [-0.7896, -0.9557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01978326588869095
RAW KL tensor(0.0298, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 292: train/loss = 0.6802685260772705, train/raw-loss = 0.6768784523010254, train/logprobs = tensor([[-1.0622, -1.7310],
        [-1.0710, -1.6290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033900391310453415
RAW KL tensor(0.0259, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 294])
Epoch 0, Step 293: train/loss = 0.6926608681678772, train/raw-loss = 0.6893210411071777, train/logprobs = tensor([[-0.9847, -2.2571],
        [-1.0480, -2.2013]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03339778631925583
RAW KL tensor(0.0212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 294: train/loss = 0.6860976815223694, train/raw-loss = 0.6837130188941956, train/logprobs = tensor([[-1.1260, -2.0115],
        [-1.1648, -2.0047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023846108466386795
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 295: train/loss = 0.6682948470115662, train/raw-loss = 0.6644936203956604, train/logprobs = tensor([[-1.7937, -1.9252],
        [-1.7608, -1.7641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038011468946933746
RAW KL tensor(0.0591, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 296: train/loss = 0.6875815987586975, train/raw-loss = 0.6833306550979614, train/logprobs = tensor([[-1.2279, -2.3482],
        [-1.2566, -2.1586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04250919073820114
RAW KL tensor(0.0469, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 297: train/loss = 0.6812705993652344, train/raw-loss = 0.6777664422988892, train/logprobs = tensor([[-1.2544, -2.1825],
        [-1.3747, -2.2242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03504208102822304
RAW KL tensor(0.0227, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 298: train/loss = 0.666245698928833, train/raw-loss = 0.6620203256607056, train/logprobs = tensor([[-0.8870, -2.0289],
        [-0.9851, -1.9445]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04225414618849754
RAW KL tensor(0.0300, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 299: train/loss = 0.6827578544616699, train/raw-loss = 0.6782333254814148, train/logprobs = tensor([[-1.3986, -1.2523],
        [-1.5015, -1.2755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04524562880396843
RAW KL tensor(0.0364, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 300: train/loss = 0.6829503774642944, train/raw-loss = 0.6797487735748291, train/logprobs = tensor([[-1.3853, -1.4289],
        [-1.3863, -1.3652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03201526403427124
RAW KL tensor(0.0173, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 301: train/loss = 0.6876991987228394, train/raw-loss = 0.685108482837677, train/logprobs = tensor([[-0.8684, -1.1486],
        [-0.8957, -1.1264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025906894356012344
RAW KL tensor(0.0079, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 302: train/loss = 0.654803991317749, train/raw-loss = 0.650282621383667, train/logprobs = tensor([[-1.0638, -1.9956],
        [-1.1281, -1.8103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0452134869992733
RAW KL tensor(0.0477, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 303: train/loss = 0.6946264505386353, train/raw-loss = 0.6900973320007324, train/logprobs = tensor([[-0.8601, -1.6781],
        [-0.9212, -1.6962]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04529169946908951
RAW KL tensor(0.0349, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 304: train/loss = 0.6770755648612976, train/raw-loss = 0.6737582683563232, train/logprobs = tensor([[-1.2717, -1.8614],
        [-1.2966, -1.7551]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0331721305847168
RAW KL tensor(0.1385, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 305: train/loss = 0.7081069350242615, train/raw-loss = 0.702420711517334, train/logprobs = tensor([[-1.1379, -1.0232],
        [-1.1521, -1.0003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05686170607805252
RAW KL tensor(0.0445, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 306: train/loss = 0.6898422241210938, train/raw-loss = 0.6864341497421265, train/logprobs = tensor([[-0.9161, -1.7616],
        [-0.9831, -1.7476]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034080736339092255
RAW KL tensor(0.0690, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 307: train/loss = 0.6717652082443237, train/raw-loss = 0.667505145072937, train/logprobs = tensor([[-0.9235, -1.8766],
        [-1.0370, -1.8363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04260077327489853
RAW KL tensor(0.0132, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 308: train/loss = 0.6689499020576477, train/raw-loss = 0.6649242043495178, train/logprobs = tensor([[-1.3492, -1.8200],
        [-1.3818, -1.6780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0402572937309742
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 309: train/loss = 0.6778062582015991, train/raw-loss = 0.6749690771102905, train/logprobs = tensor([[-0.8956, -1.4522],
        [-0.9286, -1.3942]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028372040018439293
RAW KL tensor(0.0486, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 310: train/loss = 0.6581453084945679, train/raw-loss = 0.6540607213973999, train/logprobs = tensor([[-1.0859, -2.3718],
        [-1.1695, -2.2472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04084617272019386
RAW KL tensor(0.0137, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 224])
Epoch 0, Step 311: train/loss = 0.6823342442512512, train/raw-loss = 0.6786852478981018, train/logprobs = tensor([[-0.7828, -1.3643],
        [-0.8381, -1.3325]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03648924082517624
RAW KL tensor(0.0196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 312: train/loss = 0.6843699216842651, train/raw-loss = 0.6818283200263977, train/logprobs = tensor([[-1.4185, -1.7247],
        [-1.4475, -1.6928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025416461750864983
RAW KL tensor(0.0322, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 313: train/loss = 0.6623684763908386, train/raw-loss = 0.6590603590011597, train/logprobs = tensor([[-0.9515, -2.5654],
        [-0.9903, -2.4182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03308095410466194
RAW KL tensor(0.0883, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 447])
Epoch 0, Step 314: train/loss = 0.7230897545814514, train/raw-loss = 0.7170558571815491, train/logprobs = tensor([[-1.0137, -2.5058],
        [-1.0747, -2.5915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060339685529470444
RAW KL tensor(0.0394, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 315: train/loss = 0.66218501329422, train/raw-loss = 0.6591764092445374, train/logprobs = tensor([[-0.7872, -2.5147],
        [-0.8072, -2.3825]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030085600912570953
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 239])
Epoch 0, Step 316: train/loss = 0.68007892370224, train/raw-loss = 0.6763231158256531, train/logprobs = tensor([[-1.2624, -1.8426],
        [-1.2543, -1.7222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0375576876103878
RAW KL tensor(0.0286, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 317: train/loss = 0.6866117715835571, train/raw-loss = 0.682445764541626, train/logprobs = tensor([[-1.1601, -1.6204],
        [-1.2441, -1.6464]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04165896773338318
RAW KL tensor(0.0262, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 318: train/loss = 0.6831186413764954, train/raw-loss = 0.6805241107940674, train/logprobs = tensor([[-1.1921, -2.0407],
        [-1.2053, -1.9726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025945765897631645
RAW KL tensor(0.0305, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 319: train/loss = 0.6655161380767822, train/raw-loss = 0.6612484455108643, train/logprobs = tensor([[-1.3452, -1.9935],
        [-1.4016, -1.8995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042677149176597595
RAW KL tensor(0.0199, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 320: train/loss = 0.6297845840454102, train/raw-loss = 0.6273377537727356, train/logprobs = tensor([[-0.6985, -1.7834],
        [-0.7568, -1.5586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024468276649713516
RAW KL tensor(0.0351, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 321: train/loss = 0.6180605888366699, train/raw-loss = 0.6148995161056519, train/logprobs = tensor([[-1.4483, -2.5456],
        [-1.5425, -2.2954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03161069005727768
RAW KL tensor(0.0218, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 322: train/loss = 0.6438701152801514, train/raw-loss = 0.6388452053070068, train/logprobs = tensor([[-1.1992, -2.3066],
        [-1.2355, -1.9867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05024868994951248
RAW KL tensor(0.0215, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 323: train/loss = 0.6884259581565857, train/raw-loss = 0.6865330934524536, train/logprobs = tensor([[-1.1400, -0.9046],
        [-1.0925, -0.8227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018928347155451775
RAW KL tensor(0.0214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 99])
Epoch 0, Step 324: train/loss = 0.627680242061615, train/raw-loss = 0.6252080202102661, train/logprobs = tensor([[-1.5583, -2.3506],
        [-1.5956, -2.0972]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024722054600715637
RAW KL tensor(0.0178, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 325: train/loss = 0.6305540204048157, train/raw-loss = 0.6160190105438232, train/logprobs = tensor([[-2.2494, -2.0777],
        [-2.3158, -1.7148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1453503519296646
RAW KL tensor(0.0594, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 326: train/loss = 0.6486141681671143, train/raw-loss = 0.642711877822876, train/logprobs = tensor([[-1.4680, -2.2265],
        [-1.5387, -2.0742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059022799134254456
RAW KL tensor(0.0385, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 327: train/loss = 0.6581756472587585, train/raw-loss = 0.6539881229400635, train/logprobs = tensor([[-1.3560, -1.8808],
        [-1.3688, -1.6668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04187501221895218
RAW KL tensor(0.0462, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 328: train/loss = 0.6840751767158508, train/raw-loss = 0.6814348697662354, train/logprobs = tensor([[-1.2099, -1.5831],
        [-1.0914, -1.3873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026402536779642105
RAW KL tensor(0.0300, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 329: train/loss = 0.6180703639984131, train/raw-loss = 0.6153530478477478, train/logprobs = tensor([[-1.5023, -2.2305],
        [-1.5850, -1.9482]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02717302367091179
RAW KL tensor(0.0140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 330: train/loss = 0.6547713279724121, train/raw-loss = 0.6513538360595703, train/logprobs = tensor([[-1.2692, -2.3922],
        [-1.3434, -2.2715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03417450189590454
RAW KL tensor(0.0989, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 331: train/loss = 0.6832479238510132, train/raw-loss = 0.6784201264381409, train/logprobs = tensor([[-1.5298, -2.1805],
        [-1.4217, -1.9859]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04827804118394852
RAW KL tensor(0.0202, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 332: train/loss = 0.6603615283966064, train/raw-loss = 0.6574615240097046, train/logprobs = tensor([[-1.9173, -1.5221],
        [-1.9703, -1.4140]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029000628739595413
RAW KL tensor(0.0631, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 333: train/loss = 0.5863680839538574, train/raw-loss = 0.5802981853485107, train/logprobs = tensor([[-1.5059, -2.7375],
        [-1.4827, -2.2047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060698606073856354
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 334: train/loss = 0.6486048102378845, train/raw-loss = 0.6458001136779785, train/logprobs = tensor([[-1.5850, -2.2926],
        [-1.6018, -2.0923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02804736979305744
RAW KL tensor(0.0139, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 335: train/loss = 0.6363642811775208, train/raw-loss = 0.6346864104270935, train/logprobs = tensor([[-0.9130, -1.5981],
        [-0.9271, -1.3323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016778571531176567
RAW KL tensor(0.0319, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 336: train/loss = 0.6525734663009644, train/raw-loss = 0.6483240723609924, train/logprobs = tensor([[-1.1054, -1.7670],
        [-1.1275, -1.5866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04249422624707222
RAW KL tensor(0.0290, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 337: train/loss = 0.664151668548584, train/raw-loss = 0.6597309708595276, train/logprobs = tensor([[-1.2363, -2.5817],
        [-1.2858, -2.4411]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04420687258243561
RAW KL tensor(0.0473, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 338: train/loss = 0.6677943468093872, train/raw-loss = 0.6644994020462036, train/logprobs = tensor([[-1.4179, -2.0041],
        [-1.3507, -1.7781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032949723303318024
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 339: train/loss = 0.669536292552948, train/raw-loss = 0.666746199131012, train/logprobs = tensor([[-1.5521, -1.6344],
        [-1.6018, -1.5526]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02790018729865551
RAW KL tensor(0.0355, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 340: train/loss = 0.6128864884376526, train/raw-loss = 0.6089457273483276, train/logprobs = tensor([[-1.7186, -2.3307],
        [-1.7381, -1.9555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0394081175327301
RAW KL tensor(0.0336, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 341: train/loss = 0.6765952110290527, train/raw-loss = 0.6580077409744263, train/logprobs = tensor([[-1.7290, -2.2710],
        [-1.7610, -2.0180]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1858741194009781
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 342: train/loss = 0.6346017122268677, train/raw-loss = 0.6312289237976074, train/logprobs = tensor([[-1.2467, -1.7500],
        [-1.2730, -1.4845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033728498965501785
RAW KL tensor(0.0296, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 343: train/loss = 0.6577893495559692, train/raw-loss = 0.6559717059135437, train/logprobs = tensor([[-0.8435, -1.9929],
        [-0.8853, -1.8548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018176453188061714
RAW KL tensor(0.0497, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 344: train/loss = 0.676686704158783, train/raw-loss = 0.6730521321296692, train/logprobs = tensor([[-1.4488, -1.8441],
        [-1.3931, -1.6607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03634551167488098
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 345: train/loss = 0.6697266101837158, train/raw-loss = 0.6660023331642151, train/logprobs = tensor([[-1.3143, -1.5411],
        [-1.3005, -1.3627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037242889404296875
RAW KL tensor(0.0310, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 346: train/loss = 0.6237644553184509, train/raw-loss = 0.6166775226593018, train/logprobs = tensor([[-1.1630, -2.5095],
        [-1.2119, -2.2192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0708693414926529
RAW KL tensor(0.0383, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 347: train/loss = 0.6733477711677551, train/raw-loss = 0.6678504943847656, train/logprobs = tensor([[-1.5050, -1.8403],
        [-1.4534, -1.6603]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05497295409440994
RAW KL tensor(0.0342, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 348: train/loss = 0.6446896195411682, train/raw-loss = 0.6391568779945374, train/logprobs = tensor([[-1.0721, -1.6904],
        [-1.2016, -1.5657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05532752722501755
RAW KL tensor(0.0296, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 349: train/loss = 0.6499751210212708, train/raw-loss = 0.6475206017494202, train/logprobs = tensor([[-1.0635, -1.8137],
        [-1.1301, -1.6611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02454531006515026
RAW KL tensor(0.0473, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 350: train/loss = 0.6040917634963989, train/raw-loss = 0.5976977348327637, train/logprobs = tensor([[-0.8628, -3.4371],
        [-0.9114, -2.9741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06393953412771225
RAW KL tensor(0.0336, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 302])
Epoch 0, Step 351: train/loss = 0.637269914150238, train/raw-loss = 0.6332463026046753, train/logprobs = tensor([[-1.0897, -2.7056],
        [-1.1486, -2.4735]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040235795080661774
RAW KL tensor(0.0400, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 352: train/loss = 0.5761414766311646, train/raw-loss = 0.5726773738861084, train/logprobs = tensor([[-1.2101, -2.8525],
        [-1.1680, -2.2720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03464037925004959
RAW KL tensor(0.0370, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 353: train/loss = 0.622799813747406, train/raw-loss = 0.6196156144142151, train/logprobs = tensor([[-1.4212, -2.1125],
        [-1.3987, -1.7260]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031842101365327835
RAW KL tensor(0.0629, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 354: train/loss = 0.5348889231681824, train/raw-loss = 0.529205322265625, train/logprobs = tensor([[-0.8401, -3.7401],
        [-0.7877, -2.9245]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05683601647615433
RAW KL tensor(0.0535, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 355: train/loss = 0.6647537350654602, train/raw-loss = 0.6616977453231812, train/logprobs = tensor([[-0.9391, -1.8635],
        [-0.8068, -1.5946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030559677630662918
RAW KL tensor(0.0121, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 356: train/loss = 0.6487875580787659, train/raw-loss = 0.6466817855834961, train/logprobs = tensor([[-1.1360, -1.5108],
        [-1.0669, -1.2383]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021057898178696632
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 345])
Epoch 0, Step 357: train/loss = 0.5803734064102173, train/raw-loss = 0.5769314169883728, train/logprobs = tensor([[-0.8875, -2.9821],
        [-0.8138, -2.3544]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03442002832889557
RAW KL tensor(0.0160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 358: train/loss = 0.64110267162323, train/raw-loss = 0.6378448009490967, train/logprobs = tensor([[-1.4316, -2.3361],
        [-1.3503, -1.9743]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032579291611909866
RAW KL tensor(0.0248, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 359: train/loss = 0.5682722330093384, train/raw-loss = 0.5655831098556519, train/logprobs = tensor([[-0.9044, -2.7929],
        [-0.8878, -2.2118]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026891212910413742
RAW KL tensor(0.0388, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 249])
Epoch 0, Step 360: train/loss = 0.590822696685791, train/raw-loss = 0.5869436264038086, train/logprobs = tensor([[-0.8360, -3.5481],
        [-0.7499, -2.9882]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03879042714834213
RAW KL tensor(0.0301, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 361: train/loss = 0.6142193675041199, train/raw-loss = 0.6114853024482727, train/logprobs = tensor([[-0.7463, -2.8260],
        [-0.7109, -2.4355]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027340330183506012
RAW KL tensor(0.0357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 362: train/loss = 0.65336012840271, train/raw-loss = 0.6488548517227173, train/logprobs = tensor([[-1.5369, -1.8231],
        [-1.5262, -1.5606]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04505234956741333
RAW KL tensor(0.0276, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 363: train/loss = 0.6242164373397827, train/raw-loss = 0.6212897896766663, train/logprobs = tensor([[-1.3352, -2.3330],
        [-1.3338, -2.0219]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029265975579619408
RAW KL tensor(0.0376, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 364: train/loss = 0.6167646646499634, train/raw-loss = 0.6138392090797424, train/logprobs = tensor([[-0.9748, -1.9461],
        [-0.9871, -1.6165]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029254499822854996
RAW KL tensor(0.0102, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 365: train/loss = 0.6013528108596802, train/raw-loss = 0.5994495153427124, train/logprobs = tensor([[-1.3972, -1.9611],
        [-1.3589, -1.4930]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019033215939998627
RAW KL tensor(0.0192, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 366: train/loss = 0.6410943865776062, train/raw-loss = 0.6377979516983032, train/logprobs = tensor([[-1.3240, -1.9145],
        [-1.3248, -1.6485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03296434506773949
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 367: train/loss = 0.615095853805542, train/raw-loss = 0.613639771938324, train/logprobs = tensor([[-1.1203, -1.8585],
        [-1.0992, -1.4999]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014560754410922527
RAW KL tensor(0.0931, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 368: train/loss = 0.5815359354019165, train/raw-loss = 0.5775455236434937, train/logprobs = tensor([[-1.3688, -2.5823],
        [-1.4185, -2.0878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039903998374938965
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 369: train/loss = 0.6095905303955078, train/raw-loss = 0.6068111062049866, train/logprobs = tensor([[-1.1668, -2.0019],
        [-1.1800, -1.5756]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02779444120824337
RAW KL tensor(0.0228, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 370: train/loss = 0.5860074162483215, train/raw-loss = 0.5836147665977478, train/logprobs = tensor([[-1.0876, -2.5688],
        [-1.0419, -2.0001]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02392645552754402
RAW KL tensor(0.0134, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 461])
Epoch 0, Step 371: train/loss = 0.6352807283401489, train/raw-loss = 0.6332679986953735, train/logprobs = tensor([[-0.5976, -1.2114],
        [-0.5909, -0.9434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020127393305301666
RAW KL tensor(0.0622, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 372: train/loss = 0.6381831765174866, train/raw-loss = 0.6346346735954285, train/logprobs = tensor([[-1.1793, -2.2390],
        [-1.1636, -1.8980]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035484813153743744
RAW KL tensor(0.0295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 373: train/loss = 0.6155350208282471, train/raw-loss = 0.6118816137313843, train/logprobs = tensor([[-1.1410, -2.9258],
        [-1.0505, -2.4200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03653377294540405
RAW KL tensor(0.0203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 374: train/loss = 0.636915922164917, train/raw-loss = 0.6348673105239868, train/logprobs = tensor([[-0.7913, -1.2575],
        [-0.7787, -0.9905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02048652246594429
RAW KL tensor(0.0262, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 375: train/loss = 0.6306360960006714, train/raw-loss = 0.6269437670707703, train/logprobs = tensor([[-1.1521, -1.7991],
        [-1.1883, -1.5559]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036923568695783615
RAW KL tensor(0.0246, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 376: train/loss = 0.6426451206207275, train/raw-loss = 0.6404552459716797, train/logprobs = tensor([[-1.4885, -1.9078],
        [-1.4200, -1.5976]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02189793810248375
RAW KL tensor(0.0048, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 377: train/loss = 0.6752463579177856, train/raw-loss = 0.6741087436676025, train/logprobs = tensor([[-0.6432, -0.8927],
        [-0.6479, -0.8034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.011375369504094124
RAW KL tensor(0.0293, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 378: train/loss = 0.6758407950401306, train/raw-loss = 0.6720771193504333, train/logprobs = tensor([[-2.1637, -1.9509],
        [-1.9795, -1.6128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0376364029943943
RAW KL tensor(0.0463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 379: train/loss = 0.6015519499778748, train/raw-loss = 0.5970856547355652, train/logprobs = tensor([[-2.0696, -3.1306],
        [-1.8327, -2.4349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0446627214550972
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 380: train/loss = 0.6394870281219482, train/raw-loss = 0.6370419263839722, train/logprobs = tensor([[-1.0613, -1.9286],
        [-1.0089, -1.6186]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024450130760669708
RAW KL tensor(0.0679, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 381: train/loss = 0.5844546556472778, train/raw-loss = 0.5815623998641968, train/logprobs = tensor([[-1.0432, -2.4721],
        [-0.9628, -1.8932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028923260048031807
RAW KL tensor(0.0127, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 382: train/loss = 0.6411097645759583, train/raw-loss = 0.6388533115386963, train/logprobs = tensor([[-1.2606, -2.0082],
        [-1.0933, -1.5758]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022564124315977097
RAW KL tensor(0.0244, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 383: train/loss = 0.6657139658927917, train/raw-loss = 0.6629948616027832, train/logprobs = tensor([[-2.2162, -2.4809],
        [-1.9393, -2.0672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027192136272788048
RAW KL tensor(0.0442, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 384: train/loss = 0.5936429500579834, train/raw-loss = 0.5907880663871765, train/logprobs = tensor([[-0.9215, -2.0615],
        [-0.9569, -1.6491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02854887954890728
RAW KL tensor(0.0373, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 385: train/loss = 0.6383565664291382, train/raw-loss = 0.6352939605712891, train/logprobs = tensor([[-1.6558, -1.9417],
        [-1.5387, -1.5535]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030626507475972176
RAW KL tensor(0.0752, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 386: train/loss = 0.5992680788040161, train/raw-loss = 0.594399094581604, train/logprobs = tensor([[-1.5474, -2.4494],
        [-1.4109, -1.8200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04868993163108826
RAW KL tensor(0.2707, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 387: train/loss = 0.573567807674408, train/raw-loss = 0.5649862289428711, train/logprobs = tensor([[-1.3523, -2.2790],
        [-1.2529, -1.6009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08581608533859253
RAW KL tensor(0.0051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 388: train/loss = 0.6339609622955322, train/raw-loss = 0.6315861940383911, train/logprobs = tensor([[-1.0060, -1.3742],
        [-0.8832, -0.9663]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02374693937599659
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 389: train/loss = 0.6484513282775879, train/raw-loss = 0.6450086236000061, train/logprobs = tensor([[-0.8462, -1.7331],
        [-0.7863, -1.4481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03442712500691414
RAW KL tensor(0.0441, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 390: train/loss = 0.5394651293754578, train/raw-loss = 0.5359984040260315, train/logprobs = tensor([[-1.3429, -2.6301],
        [-1.2072, -1.7577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03466719761490822
RAW KL tensor(0.0123, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 391: train/loss = 0.6143880486488342, train/raw-loss = 0.6109576225280762, train/logprobs = tensor([[-1.7803, -2.3664],
        [-1.3732, -1.5858]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0343037024140358
RAW KL tensor(0.0317, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 392: train/loss = 0.535423994064331, train/raw-loss = 0.5322238206863403, train/logprobs = tensor([[-1.2648, -2.5241],
        [-1.2926, -1.6943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03200172632932663
RAW KL tensor(0.0777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 393: train/loss = 0.5880361199378967, train/raw-loss = 0.5830643773078918, train/logprobs = tensor([[-1.7237, -2.6116],
        [-1.5961, -1.9770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049717485904693604
RAW KL tensor(0.0669, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 394: train/loss = 0.5670035481452942, train/raw-loss = 0.5629843473434448, train/logprobs = tensor([[-1.4232, -2.7145],
        [-1.1865, -1.8719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040191881358623505
RAW KL tensor(0.0136, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 395: train/loss = 0.628738522529602, train/raw-loss = 0.6263882517814636, train/logprobs = tensor([[-1.1380, -1.9690],
        [-1.0784, -1.6063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023502416908740997
RAW KL tensor(0.0446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 396: train/loss = 0.49573230743408203, train/raw-loss = 0.49229106307029724, train/logprobs = tensor([[-0.9583, -2.9098],
        [-0.9301, -1.8698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03441225364804268
RAW KL tensor(0.0389, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 397: train/loss = 0.6451050043106079, train/raw-loss = 0.6423738598823547, train/logprobs = tensor([[-1.8131, -1.6736],
        [-1.4803, -1.0995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02731122449040413
RAW KL tensor(0.0367, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 398: train/loss = 0.5484455227851868, train/raw-loss = 0.5447998046875, train/logprobs = tensor([[-1.1969, -2.6514],
        [-1.1586, -1.8813]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03645702078938484
RAW KL tensor(0.0223, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 399: train/loss = 0.5892482995986938, train/raw-loss = 0.5835954546928406, train/logprobs = tensor([[-1.4370, -2.1856],
        [-1.2670, -1.5277]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05652840435504913
RAW KL tensor(0.0609, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 400: train/loss = 0.6143527030944824, train/raw-loss = 0.6104266047477722, train/logprobs = tensor([[-1.8509, -2.1037],
        [-1.7738, -1.6095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03926100954413414
RAW KL tensor(0.0732, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 401: train/loss = 0.5187715291976929, train/raw-loss = 0.5151833891868591, train/logprobs = tensor([[-1.3382, -2.6838],
        [-1.2710, -1.7710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03588113933801651
RAW KL tensor(0.0116, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 402: train/loss = 0.6033005714416504, train/raw-loss = 0.6000764966011047, train/logprobs = tensor([[-1.1731, -1.9792],
        [-1.1037, -1.4862]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032240934669971466
RAW KL tensor(0.0380, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 403: train/loss = 0.5609878301620483, train/raw-loss = 0.5576207637786865, train/logprobs = tensor([[-1.6028, -2.2104],
        [-1.6717, -1.6620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03367036208510399
RAW KL tensor(0.0317, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 404: train/loss = 0.5930156111717224, train/raw-loss = 0.5899841785430908, train/logprobs = tensor([[-1.3180, -2.4841],
        [-1.2804, -1.9705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030314244329929352
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 405: train/loss = 0.6281226873397827, train/raw-loss = 0.6262741684913635, train/logprobs = tensor([[-0.8423, -1.7094],
        [-0.8198, -1.3944]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018485454842448235
RAW KL tensor(0.0485, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 406: train/loss = 0.5451053380966187, train/raw-loss = 0.5407488942146301, train/logprobs = tensor([[-1.5537, -2.5155],
        [-1.4715, -1.6816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04356412589550018
RAW KL tensor(0.0574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 407: train/loss = 0.6177916526794434, train/raw-loss = 0.6137615442276001, train/logprobs = tensor([[-1.3731, -2.6849],
        [-1.2615, -2.1894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04030102491378784
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 408: train/loss = 0.5949118137359619, train/raw-loss = 0.5909146666526794, train/logprobs = tensor([[-1.4533, -1.7913],
        [-1.3824, -1.2562]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03997151553630829
RAW KL tensor(0.0246, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 409: train/loss = 0.5453708171844482, train/raw-loss = 0.5424936413764954, train/logprobs = tensor([[-0.8557, -2.2616],
        [-0.8313, -1.5227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028772227466106415
RAW KL tensor(0.0205, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 410: train/loss = 0.590802788734436, train/raw-loss = 0.5711179971694946, train/logprobs = tensor([[-1.1494, -2.0620],
        [-0.9899, -1.3579]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19684815406799316
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 411: train/loss = 0.6834707260131836, train/raw-loss = 0.6813175678253174, train/logprobs = tensor([[-1.5732, -1.5070],
        [-1.1752, -1.0601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021532513201236725
RAW KL tensor(0.0477, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 412: train/loss = 0.6049420833587646, train/raw-loss = 0.601577877998352, train/logprobs = tensor([[-1.1533, -1.9612],
        [-1.2795, -1.6653]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033642224967479706
RAW KL tensor(0.0093, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 413: train/loss = 0.6811361312866211, train/raw-loss = 0.677168607711792, train/logprobs = tensor([[-1.6530, -1.1802],
        [-1.4844, -0.8565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03967523202300072
RAW KL tensor(0.0313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 414: train/loss = 0.6463233232498169, train/raw-loss = 0.6424890160560608, train/logprobs = tensor([[-1.1711, -1.4621],
        [-1.0330, -1.0561]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03834296762943268
RAW KL tensor(0.0162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 263])
Epoch 0, Step 415: train/loss = 0.537157416343689, train/raw-loss = 0.5346031785011292, train/logprobs = tensor([[-0.7642, -2.8966],
        [-0.6645, -2.0843]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02554277330636978
RAW KL tensor(0.0809, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 416: train/loss = 0.5836426615715027, train/raw-loss = 0.5780239105224609, train/logprobs = tensor([[-1.0547, -2.2369],
        [-0.9778, -1.5969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.056187454611063004
RAW KL tensor(0.0909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 417: train/loss = 0.4984106123447418, train/raw-loss = 0.4907742738723755, train/logprobs = tensor([[-1.0437, -3.7743],
        [-1.0792, -2.8459]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07636343687772751
RAW KL tensor(0.0977, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 418: train/loss = 0.611674964427948, train/raw-loss = 0.6057665348052979, train/logprobs = tensor([[-1.5477, -2.0424],
        [-1.3340, -1.4175]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0590846873819828
RAW KL tensor(0.0520, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 419: train/loss = 0.6143537163734436, train/raw-loss = 0.6078709959983826, train/logprobs = tensor([[-2.3308, -2.7869],
        [-1.7945, -1.7827]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06482821702957153
RAW KL tensor(0.0821, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 420: train/loss = 0.5123354196548462, train/raw-loss = 0.5052037835121155, train/logprobs = tensor([[-1.2977, -2.7857],
        [-1.3288, -1.9322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07131662219762802
RAW KL tensor(0.0425, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 421: train/loss = 0.6292489171028137, train/raw-loss = 0.6245025992393494, train/logprobs = tensor([[-0.7286, -1.8284],
        [-0.6981, -1.4257]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0474630668759346
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 422: train/loss = 0.5496078729629517, train/raw-loss = 0.5441797971725464, train/logprobs = tensor([[-1.1527, -1.8845],
        [-1.1985, -1.2484]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05428053438663483
RAW KL tensor(0.0544, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 423: train/loss = 0.5754368305206299, train/raw-loss = 0.5700389742851257, train/logprobs = tensor([[-1.6385, -1.8880],
        [-1.6142, -1.2805]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053978465497493744
RAW KL tensor(0.1229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 424: train/loss = 0.5345446467399597, train/raw-loss = 0.5288490653038025, train/logprobs = tensor([[-0.8700, -2.0704],
        [-0.8388, -1.1637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05695576220750809
RAW KL tensor(0.0410, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 425: train/loss = 0.4628778100013733, train/raw-loss = 0.4582485556602478, train/logprobs = tensor([[-1.2127, -3.2408],
        [-1.2021, -2.1154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046292681246995926
RAW KL tensor(0.0801, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 426: train/loss = 0.554551899433136, train/raw-loss = 0.5486955642700195, train/logprobs = tensor([[-1.5351, -1.9252],
        [-1.4785, -1.2149]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05856351554393768
RAW KL tensor(0.0211, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 427: train/loss = 0.6636704206466675, train/raw-loss = 0.6603591442108154, train/logprobs = tensor([[-0.9120, -1.2960],
        [-0.9311, -1.1679]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033112965524196625
RAW KL tensor(0.0356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 428: train/loss = 0.5835285186767578, train/raw-loss = 0.5787211060523987, train/logprobs = tensor([[-1.5978, -1.8780],
        [-1.5842, -1.3055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0480743832886219
RAW KL tensor(0.0154, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 285])
Epoch 0, Step 429: train/loss = 0.665799617767334, train/raw-loss = 0.6627212762832642, train/logprobs = tensor([[-1.0731, -1.1305],
        [-0.9590, -0.8675]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030783001333475113
RAW KL tensor(0.0254, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 430: train/loss = 0.536028265953064, train/raw-loss = 0.5307247638702393, train/logprobs = tensor([[-1.1767, -2.7933],
        [-0.9665, -1.8118]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05303464084863663
RAW KL tensor(0.0517, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 431: train/loss = 0.5621097683906555, train/raw-loss = 0.5437222719192505, train/logprobs = tensor([[-1.7225, -2.7327],
        [-1.6631, -1.9943]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18387454748153687
RAW KL tensor(0.0521, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 432: train/loss = 0.6364318132400513, train/raw-loss = 0.6328331232070923, train/logprobs = tensor([[-1.1043, -1.5674],
        [-1.1740, -1.3552]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0359867699444294
RAW KL tensor(0.0603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 433: train/loss = 0.5257408022880554, train/raw-loss = 0.5198657512664795, train/logprobs = tensor([[-1.5052, -2.8384],
        [-1.4678, -1.9737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05875105410814285
RAW KL tensor(0.0857, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 434: train/loss = 0.5479650497436523, train/raw-loss = 0.5406195521354675, train/logprobs = tensor([[-1.2632, -2.6281],
        [-1.4275, -2.0835]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0734548568725586
RAW KL tensor(0.0692, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 435: train/loss = 0.6528013944625854, train/raw-loss = 0.6474505066871643, train/logprobs = tensor([[-1.5188, -1.7232],
        [-1.5440, -1.5029]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0535091795027256
RAW KL tensor(0.0449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 436: train/loss = 0.5640645027160645, train/raw-loss = 0.5601943731307983, train/logprobs = tensor([[-1.3107, -1.8329],
        [-1.3072, -1.1021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038701754063367844
RAW KL tensor(0.0643, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 437: train/loss = 0.5207995772361755, train/raw-loss = 0.5156890153884888, train/logprobs = tensor([[-1.0326, -2.3145],
        [-1.1036, -1.5663]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051105648279190063
RAW KL tensor(0.0980, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 438: train/loss = 0.6209290027618408, train/raw-loss = 0.6147714853286743, train/logprobs = tensor([[-1.5170, -2.2290],
        [-1.2384, -1.5558]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061574362218379974
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 439: train/loss = 0.5579515695571899, train/raw-loss = 0.5505034923553467, train/logprobs = tensor([[-1.1762, -2.0415],
        [-1.2764, -1.4996]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0744805708527565
RAW KL tensor(0.0107, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 347])
Epoch 0, Step 440: train/loss = 0.595528244972229, train/raw-loss = 0.5900070667266846, train/logprobs = tensor([[-1.8991, -2.0822],
        [-1.7048, -1.4338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05521174892783165
RAW KL tensor(0.0454, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 441: train/loss = 0.4806646406650543, train/raw-loss = 0.4727057218551636, train/logprobs = tensor([[-1.1654, -3.1874],
        [-1.2547, -2.1686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07958930730819702
RAW KL tensor(0.0455, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 442: train/loss = 0.6157129406929016, train/raw-loss = 0.6087087988853455, train/logprobs = tensor([[-1.8982, -1.7979],
        [-1.7856, -1.0112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07004203647375107
RAW KL tensor(0.1012, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 443: train/loss = 0.530070960521698, train/raw-loss = 0.5207728147506714, train/logprobs = tensor([[-1.4805, -2.6202],
        [-1.5158, -1.8021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09298165142536163
RAW KL tensor(0.0579, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 444: train/loss = 0.6080908179283142, train/raw-loss = 0.6040788888931274, train/logprobs = tensor([[-0.9431, -1.9883],
        [-0.9791, -1.6018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04011944681406021
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 445: train/loss = 0.584377646446228, train/raw-loss = 0.5801912546157837, train/logprobs = tensor([[-1.8278, -1.9264],
        [-1.7289, -1.1953]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041864365339279175
RAW KL tensor(0.0433, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 446: train/loss = 0.4942423105239868, train/raw-loss = 0.48566603660583496, train/logprobs = tensor([[-1.3595, -3.0778],
        [-1.2271, -1.9604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08576267957687378
RAW KL tensor(0.1362, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 447: train/loss = 0.43893206119537354, train/raw-loss = 0.4314514696598053, train/logprobs = tensor([[-1.1214, -2.8160],
        [-1.1929, -1.4999]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07480598241090775
RAW KL tensor(0.0685, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 448: train/loss = 0.486005574464798, train/raw-loss = 0.4763171672821045, train/logprobs = tensor([[-1.4523, -2.5400],
        [-1.1935, -1.1422]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09688428789377213
RAW KL tensor(0.0861, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 449: train/loss = 0.6196280717849731, train/raw-loss = 0.6131038665771484, train/logprobs = tensor([[-1.5999, -1.6196],
        [-1.3423, -1.0105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06524243950843811
RAW KL tensor(0.0328, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 450: train/loss = 0.5076627135276794, train/raw-loss = 0.5039239525794983, train/logprobs = tensor([[-0.6522, -2.2081],
        [-0.6321, -1.2928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03738760948181152
RAW KL tensor(0.0547, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 451: train/loss = 0.5578151345252991, train/raw-loss = 0.5516400337219238, train/logprobs = tensor([[-2.1019, -2.5231],
        [-1.8589, -1.5990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061750806868076324
RAW KL tensor(0.0366, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 452: train/loss = 0.48940229415893555, train/raw-loss = 0.48301708698272705, train/logprobs = tensor([[-1.3492, -2.1990],
        [-1.4136, -1.2281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06385207921266556
RAW KL tensor(0.0529, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 453: train/loss = 0.4995860457420349, train/raw-loss = 0.49345988035202026, train/logprobs = tensor([[-0.9997, -2.2971],
        [-1.0408, -1.3587]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06126116216182709
RAW KL tensor(0.0938, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 454: train/loss = 0.5453320145606995, train/raw-loss = 0.5385230183601379, train/logprobs = tensor([[-1.6587, -2.1305],
        [-1.5884, -1.3326]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06808998435735703
RAW KL tensor(0.2021, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 374])
Epoch 0, Step 455: train/loss = 0.47163042426109314, train/raw-loss = 0.4614708125591278, train/logprobs = tensor([[-0.8698, -2.1392],
        [-0.8749, -1.0104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10159599035978317
RAW KL tensor(0.0086, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 423])
Epoch 0, Step 456: train/loss = 0.5080856680870056, train/raw-loss = 0.5030929446220398, train/logprobs = tensor([[-0.8731, -2.3121],
        [-0.8050, -1.1171]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049926891922950745
RAW KL tensor(0.0698, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 457: train/loss = 0.5219132900238037, train/raw-loss = 0.5158784985542297, train/logprobs = tensor([[-0.7388, -1.8215],
        [-0.7508, -0.9899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06034756824374199
RAW KL tensor(0.0904, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 118])
Epoch 0, Step 458: train/loss = 0.6289373636245728, train/raw-loss = 0.6211060285568237, train/logprobs = tensor([[-1.6700, -2.0737],
        [-1.2102, -1.2705]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07831352949142456
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 459: train/loss = 0.5208491086959839, train/raw-loss = 0.5149059295654297, train/logprobs = tensor([[-0.8792, -2.2027],
        [-0.9421, -1.4403]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05943155661225319
RAW KL tensor(0.0076, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 452])
Epoch 0, Step 460: train/loss = 0.5811365842819214, train/raw-loss = 0.5774567723274231, train/logprobs = tensor([[-0.8790, -1.5727],
        [-1.0086, -1.0948]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03679835423827171
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 461: train/loss = 0.6723994016647339, train/raw-loss = 0.6647590398788452, train/logprobs = tensor([[-1.7668, -1.7833],
        [-1.3074, -1.0844]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07640357315540314
RAW KL tensor(0.1444, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 361])
Epoch 0, Step 462: train/loss = 0.4520781934261322, train/raw-loss = 0.44467347860336304, train/logprobs = tensor([[-0.8794, -2.8319],
        [-0.8991, -1.6501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07404723018407822
RAW KL tensor(0.0127, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 462])
Epoch 0, Step 463: train/loss = 0.49376919865608215, train/raw-loss = 0.48759186267852783, train/logprobs = tensor([[-0.9486, -2.2236],
        [-0.9821, -1.2431]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0617733970284462
RAW KL tensor(0.1967, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 464: train/loss = 0.4808940291404724, train/raw-loss = 0.46818047761917114, train/logprobs = tensor([[-1.0447, -3.2936],
        [-1.0134, -2.1032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12713554501533508
RAW KL tensor(0.0492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 465: train/loss = 0.6745469570159912, train/raw-loss = 0.6688682436943054, train/logprobs = tensor([[-1.0984, -1.3130],
        [-1.0350, -1.0539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05678652971982956
RAW KL tensor(0.0708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 466: train/loss = 0.503135085105896, train/raw-loss = 0.49701714515686035, train/logprobs = tensor([[-1.5430, -2.4706],
        [-1.5297, -1.5184]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06117973104119301
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 343])
Epoch 0, Step 467: train/loss = 0.6021833419799805, train/raw-loss = 0.5938930511474609, train/logprobs = tensor([[-1.6635, -1.9935],
        [-1.3374, -1.1986]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08290349692106247
RAW KL tensor(0.0470, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 468: train/loss = 0.6079955101013184, train/raw-loss = 0.6009117960929871, train/logprobs = tensor([[-1.3279, -1.9726],
        [-1.1184, -1.3474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07083755731582642
RAW KL tensor(0.0569, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 469: train/loss = 0.565258264541626, train/raw-loss = 0.5609061121940613, train/logprobs = tensor([[-0.9692, -1.5991],
        [-0.9874, -0.9965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04352099448442459
RAW KL tensor(0.0898, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 470: train/loss = 0.4759129285812378, train/raw-loss = 0.468474805355072, train/logprobs = tensor([[-0.9785, -2.8634],
        [-0.9029, -1.7128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07438122481107712
RAW KL tensor(0.1074, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 471: train/loss = 0.4986535906791687, train/raw-loss = 0.4866953194141388, train/logprobs = tensor([[-1.2159, -2.6800],
        [-1.1323, -1.6149]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11958255618810654
RAW KL tensor(0.0934, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 472: train/loss = 0.331251859664917, train/raw-loss = 0.32266658544540405, train/logprobs = tensor([[-1.8480, -5.1490],
        [-1.6835, -2.0596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0858527272939682
RAW KL tensor(0.0510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 473: train/loss = 0.5748839378356934, train/raw-loss = 0.5662685632705688, train/logprobs = tensor([[-1.7137, -2.1280],
        [-1.2730, -1.0681]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08615322411060333
RAW KL tensor(0.0784, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 474: train/loss = 0.4656217098236084, train/raw-loss = 0.45975014567375183, train/logprobs = tensor([[-0.7735, -2.5079],
        [-0.7556, -1.3848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.058716028928756714
RAW KL tensor(0.0385, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 475: train/loss = 0.5027884244918823, train/raw-loss = 0.4957817792892456, train/logprobs = tensor([[-0.9977, -2.2579],
        [-1.0363, -1.3434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07006669044494629
RAW KL tensor(0.0355, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 476: train/loss = 0.652786374092102, train/raw-loss = 0.6470657587051392, train/logprobs = tensor([[-1.4771, -1.6733],
        [-1.1060, -1.0581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05720580741763115
RAW KL tensor(0.1051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 477: train/loss = 0.5303811430931091, train/raw-loss = 0.522307813167572, train/logprobs = tensor([[-1.8426, -2.2005],
        [-1.7711, -1.2336]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08073386549949646
RAW KL tensor(0.0712, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 478: train/loss = 0.5896962285041809, train/raw-loss = 0.5810189247131348, train/logprobs = tensor([[-1.7640, -2.0615],
        [-1.5698, -1.2814]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08677342534065247
RAW KL tensor(0.0804, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 479: train/loss = 0.5425975322723389, train/raw-loss = 0.5374436974525452, train/logprobs = tensor([[-1.6640, -2.7617],
        [-1.2677, -1.6651]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05153856426477432
RAW KL tensor(0.1104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 480: train/loss = 0.5528765916824341, train/raw-loss = 0.5437722206115723, train/logprobs = tensor([[-1.1804, -2.0523],
        [-1.1735, -1.2867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09104353189468384
RAW KL tensor(0.0553, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 481: train/loss = 0.5194737315177917, train/raw-loss = 0.5122828483581543, train/logprobs = tensor([[-1.5427, -1.9367],
        [-1.5044, -1.0389]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07190900295972824
RAW KL tensor(0.0596, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 278])
Epoch 0, Step 482: train/loss = 0.48339879512786865, train/raw-loss = 0.4735615849494934, train/logprobs = tensor([[-1.1392, -2.9230],
        [-1.0859, -1.8055]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0983722135424614
RAW KL tensor(0.0624, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 483: train/loss = 0.4666605591773987, train/raw-loss = 0.45967280864715576, train/logprobs = tensor([[-1.4945, -2.6853],
        [-1.4427, -1.4898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06987747550010681
RAW KL tensor(0.1130, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 484: train/loss = 0.47215378284454346, train/raw-loss = 0.46138644218444824, train/logprobs = tensor([[-1.1673, -3.2859],
        [-1.2368, -2.2167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10767334699630737
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 485: train/loss = 0.5621103644371033, train/raw-loss = 0.5561507940292358, train/logprobs = tensor([[-1.4148, -1.8843],
        [-1.3312, -1.1345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05959521606564522
RAW KL tensor(0.0673, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 486: train/loss = 0.6117310523986816, train/raw-loss = 0.6072350740432739, train/logprobs = tensor([[-0.7422, -1.3395],
        [-0.7567, -0.9782]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04495999962091446
RAW KL tensor(0.0287, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 487: train/loss = 0.41311272978782654, train/raw-loss = 0.4065530300140381, train/logprobs = tensor([[-1.1381, -4.1749],
        [-0.9335, -2.5020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06559671461582184
RAW KL tensor(0.1067, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 488: train/loss = 0.6838073134422302, train/raw-loss = 0.6705315113067627, train/logprobs = tensor([[-2.0267, -2.5382],
        [-1.6080, -1.8878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13275770843029022
RAW KL tensor(0.0294, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 489: train/loss = 0.568394660949707, train/raw-loss = 0.5579251646995544, train/logprobs = tensor([[-1.5339, -2.1712],
        [-1.2713, -1.2319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10469498485326767
RAW KL tensor(0.0221, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 321])
Epoch 0, Step 490: train/loss = 0.6548905372619629, train/raw-loss = 0.6512539982795715, train/logprobs = tensor([[-0.6663, -1.5509],
        [-0.6902, -1.3930]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036364976316690445
RAW KL tensor(0.0695, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 491: train/loss = 0.5157409310340881, train/raw-loss = 0.506016731262207, train/logprobs = tensor([[-1.2505, -2.3957],
        [-1.2955, -1.5388]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0972423404455185
RAW KL tensor(0.1803, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 492: train/loss = 0.454426109790802, train/raw-loss = 0.4457249939441681, train/logprobs = tensor([[-0.6425, -2.6429],
        [-0.7270, -1.3564]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08701133728027344
RAW KL tensor(0.0852, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 493: train/loss = 0.456388920545578, train/raw-loss = 0.4454343616962433, train/logprobs = tensor([[-1.2000, -2.5642],
        [-1.0428, -1.2249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10954548418521881
RAW KL tensor(0.0952, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 494: train/loss = 0.6150723695755005, train/raw-loss = 0.6082395911216736, train/logprobs = tensor([[-1.6421, -1.5848],
        [-1.4626, -1.0235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0683276504278183
RAW KL tensor(0.0752, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 495: train/loss = 0.5897812843322754, train/raw-loss = 0.5837799906730652, train/logprobs = tensor([[-2.0959, -2.7700],
        [-1.8647, -1.9871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0600133091211319
RAW KL tensor(0.0714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 496: train/loss = 0.4319823086261749, train/raw-loss = 0.4226670563220978, train/logprobs = tensor([[-1.5336, -3.0874],
        [-1.5296, -1.4690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09315266460180283
RAW KL tensor(0.1107, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 497: train/loss = 0.4089925289154053, train/raw-loss = 0.39508187770843506, train/logprobs = tensor([[-1.1547, -3.3752],
        [-1.2049, -1.9238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13910669088363647
RAW KL tensor(0.1036, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 498: train/loss = 0.6485750079154968, train/raw-loss = 0.6391680836677551, train/logprobs = tensor([[-1.8183, -2.6689],
        [-1.5140, -1.7007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09406901895999908
RAW KL tensor(0.0910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 499: train/loss = 0.49379271268844604, train/raw-loss = 0.4825587570667267, train/logprobs = tensor([[-1.1103, -3.0121],
        [-1.0162, -1.9338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11233936995267868
RAW KL tensor(0.0720, device='cuda:0')
SHAPES: torch.Size([4, 122])
RAW KL tensor(0.0626, device='cuda:0')
SHAPES: torch.Size([4, 145])
RAW KL tensor(0.1309, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.1355, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.1224, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0894, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.0628, device='cuda:0')
SHAPES: torch.Size([4, 168])
RAW KL tensor(0.1216, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.1208, device='cuda:0')
SHAPES: torch.Size([4, 184])
RAW KL tensor(0.0880, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0827, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.1130, device='cuda:0')
SHAPES: torch.Size([4, 189])
RAW KL tensor(0.1310, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0674, device='cuda:0')
SHAPES: torch.Size([4, 169])
RAW KL tensor(0.1388, device='cuda:0')
SHAPES: torch.Size([4, 239])
RAW KL tensor(0.0261, device='cuda:0')
SHAPES: torch.Size([4, 182])
RAW KL tensor(0.1066, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.1052, device='cuda:0')
SHAPES: torch.Size([4, 170])
RAW KL tensor(0.1223, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0880, device='cuda:0')
SHAPES: torch.Size([4, 159])
RAW KL tensor(0.0519, device='cuda:0')
SHAPES: torch.Size([4, 158])
RAW KL tensor(0.0866, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.1522, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0468, device='cuda:0')
SHAPES: torch.Size([4, 201])
RAW KL tensor(0.0711, device='cuda:0')
SHAPES: torch.Size([4, 187])
RAW KL tensor(0.1037, device='cuda:0')
SHAPES: torch.Size([4, 317])
RAW KL tensor(0.0619, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0328, device='cuda:0')
SHAPES: torch.Size([4, 185])
RAW KL tensor(0.0461, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.1093, device='cuda:0')
SHAPES: torch.Size([4, 106])
RAW KL tensor(0.0438, device='cuda:0')
SHAPES: torch.Size([4, 173])
RAW KL tensor(0.1044, device='cuda:0')
SHAPES: torch.Size([4, 455])
RAW KL tensor(0.0554, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.0312, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.1644, device='cuda:0')
SHAPES: torch.Size([4, 263])
RAW KL tensor(0.0846, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.1254, device='cuda:0')
SHAPES: torch.Size([4, 155])
RAW KL tensor(0.0794, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.1109, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0229, device='cuda:0')
SHAPES: torch.Size([4, 146])
RAW KL tensor(0.0326, device='cuda:0')
SHAPES: torch.Size([4, 226])
RAW KL tensor(0.0239, device='cuda:0')
SHAPES: torch.Size([4, 213])
RAW KL tensor(0.1281, device='cuda:0')
SHAPES: torch.Size([4, 204])
RAW KL tensor(0.0586, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0870, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0462, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0437, device='cuda:0')
SHAPES: torch.Size([4, 130])
RAW KL tensor(0.0726, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0772, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.0586, device='cuda:0')
SHAPES: torch.Size([4, 175])
RAW KL tensor(0.0702, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0842, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0772, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.0375, device='cuda:0')
SHAPES: torch.Size([4, 208])
RAW KL tensor(0.1503, device='cuda:0')
SHAPES: torch.Size([4, 177])
RAW KL tensor(0.0984, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0420, device='cuda:0')
SHAPES: torch.Size([4, 164])
RAW KL tensor(0.0296, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.1320, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.0814, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0503, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.0669, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0688, device='cuda:0')
SHAPES: torch.Size([4, 194])
RAW KL tensor(0.0592, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.5812, device='cuda:0')
SHAPES: torch.Size([4, 125])
RAW KL tensor(0.0869, device='cuda:0')
SHAPES: torch.Size([4, 179])
RAW KL tensor(0.1603, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0396, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.1359, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.1244, device='cuda:0')
SHAPES: torch.Size([4, 229])
RAW KL tensor(0.0281, device='cuda:0')
SHAPES: torch.Size([4, 341])
RAW KL tensor(0.0678, device='cuda:0')
SHAPES: torch.Size([4, 237])
RAW KL tensor(0.1462, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.1030, device='cuda:0')
SHAPES: torch.Size([4, 118])
RAW KL tensor(0.1559, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0223, device='cuda:0')
SHAPES: torch.Size([4, 210])
RAW KL tensor(0.1590, device='cuda:0')
SHAPES: torch.Size([4, 216])
RAW KL tensor(0.0671, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.0621, device='cuda:0')
SHAPES: torch.Size([4, 191])
RAW KL tensor(0.1186, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.0858, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.1282, device='cuda:0')
SHAPES: torch.Size([4, 171])
RAW KL tensor(0.0485, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.0536, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.1674, device='cuda:0')
SHAPES: torch.Size([4, 195])
RAW KL tensor(0.0659, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0667, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.0655, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.0268, device='cuda:0')
SHAPES: torch.Size([4, 198])
RAW KL tensor(0.0114, device='cuda:0')
SHAPES: torch.Size([4, 450])
RAW KL tensor(0.0845, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0869, device='cuda:0')
SHAPES: torch.Size([4, 452])
RAW KL tensor(0.0613, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0657, device='cuda:0')
SHAPES: torch.Size([4, 188])
RAW KL tensor(0.0630, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0458, device='cuda:0')
SHAPES: torch.Size([4, 174])
RAW KL tensor(0.1386, device='cuda:0')
SHAPES: torch.Size([4, 137])
RAW KL tensor(0.0281, device='cuda:0')
SHAPES: torch.Size([4, 219])
RAW KL tensor(0.0841, device='cuda:0')
SHAPES: torch.Size([4, 140])
RAW KL tensor(0.1001, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0886, device='cuda:0')
SHAPES: torch.Size([4, 410])
RAW KL tensor(0.0248, device='cuda:0')
SHAPES: torch.Size([4, 214])
RAW KL tensor(0.0322, device='cuda:0')
SHAPES: torch.Size([4, 467])
RAW KL tensor(0.1280, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0656, device='cuda:0')
SHAPES: torch.Size([4, 205])
RAW KL tensor(0.1064, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0269, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.2137, device='cuda:0')
SHAPES: torch.Size([4, 206])
RAW KL tensor(0.0322, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.1535, device='cuda:0')
SHAPES: torch.Size([4, 152])
RAW KL tensor(0.0434, device='cuda:0')
SHAPES: torch.Size([4, 227])
RAW KL tensor(0.1236, device='cuda:0')
SHAPES: torch.Size([4, 217])
RAW KL tensor(0.0419, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.0491, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0574, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.0501, device='cuda:0')
SHAPES: torch.Size([4, 183])
RAW KL tensor(0.0773, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.1157, device='cuda:0')
SHAPES: torch.Size([4, 126])
RAW KL tensor(0.1213, device='cuda:0')
SHAPES: torch.Size([4, 207])
RAW KL tensor(0.1276, device='cuda:0')
SHAPES: torch.Size([4, 121])
RAW KL tensor(0.0399, device='cuda:0')
SHAPES: torch.Size([4, 167])
RAW KL tensor(0.0733, device='cuda:0')
SHAPES: torch.Size([4, 236])
RAW KL tensor(0.0864, device='cuda:0')
SHAPES: torch.Size([4, 123])
RAW KL tensor(0.0725, device='cuda:0')
SHAPES: torch.Size([4, 128])
RAW KL tensor(0.0601, device='cuda:0')
SHAPES: torch.Size([4, 217])
eval/loss: 0.5106493830680847
RAW KL tensor(0.0757, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 500: train/loss = 0.4168967008590698, train/raw-loss = 0.40742436051368713, train/logprobs = tensor([[-1.2203, -2.9880],
        [-1.2367, -1.4958]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09472353756427765
RAW KL tensor(0.0360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 501: train/loss = 0.503696620464325, train/raw-loss = 0.4940803050994873, train/logprobs = tensor([[-1.4053, -2.5891],
        [-1.2756, -1.4924]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09616312384605408
RAW KL tensor(0.0651, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 502: train/loss = 0.46469563245773315, train/raw-loss = 0.45658522844314575, train/logprobs = tensor([[-0.8866, -2.8598],
        [-0.9094, -1.6885]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0811043456196785
RAW KL tensor(0.0656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 503: train/loss = 0.43659284710884094, train/raw-loss = 0.42865628004074097, train/logprobs = tensor([[-1.5565, -3.1257],
        [-1.3073, -1.4720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0793658122420311
RAW KL tensor(0.0531, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 504: train/loss = 0.6402060389518738, train/raw-loss = 0.6350916624069214, train/logprobs = tensor([[-1.3093, -1.6333],
        [-1.0759, -1.1468]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05114363133907318
RAW KL tensor(0.2007, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 505: train/loss = 0.5065931081771851, train/raw-loss = 0.49623602628707886, train/logprobs = tensor([[-1.3044, -2.2549],
        [-1.2697, -1.2007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10357096791267395
RAW KL tensor(0.1167, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 506: train/loss = 0.4422185719013214, train/raw-loss = 0.432242751121521, train/logprobs = tensor([[-1.3528, -3.0925],
        [-1.1759, -1.5629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0997578427195549
RAW KL tensor(0.0521, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 507: train/loss = 0.4607534408569336, train/raw-loss = 0.45103856921195984, train/logprobs = tensor([[-1.1153, -2.5887],
        [-1.0848, -1.3630]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09714894741773605
RAW KL tensor(0.1400, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 508: train/loss = 0.47994786500930786, train/raw-loss = 0.4727642238140106, train/logprobs = tensor([[-0.8229, -2.4398],
        [-0.9710, -1.4147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07183640450239182
RAW KL tensor(0.0708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 509: train/loss = 0.5004395842552185, train/raw-loss = 0.4905930161476135, train/logprobs = tensor([[-1.5215, -2.3797],
        [-1.1661, -1.0129]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09846571087837219
RAW KL tensor(0.1479, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 510: train/loss = 0.48014384508132935, train/raw-loss = 0.47215723991394043, train/logprobs = tensor([[-0.8578, -2.6898],
        [-0.8648, -1.5994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07986598461866379
RAW KL tensor(0.2609, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 511: train/loss = 0.42621177434921265, train/raw-loss = 0.4135233163833618, train/logprobs = tensor([[-1.3042, -3.0980],
        [-0.8856, -1.1628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12688446044921875
RAW KL tensor(0.1125, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 217])
Epoch 0, Step 512: train/loss = 0.4077718257904053, train/raw-loss = 0.3965773284435272, train/logprobs = tensor([[-0.7079, -2.7893],
        [-0.7501, -1.2269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11194485425949097
RAW KL tensor(0.0512, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 513: train/loss = 0.4254590570926666, train/raw-loss = 0.41581279039382935, train/logprobs = tensor([[-1.5970, -2.5648],
        [-1.5812, -1.2035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09646271169185638
RAW KL tensor(0.0890, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 228])
Epoch 0, Step 514: train/loss = 0.6385001540184021, train/raw-loss = 0.6203098893165588, train/logprobs = tensor([[-2.1960, -2.4378],
        [-1.9698, -1.4887]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18190227448940277
RAW KL tensor(0.1281, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 515: train/loss = 0.48127368092536926, train/raw-loss = 0.4742199182510376, train/logprobs = tensor([[-0.9356, -2.9923],
        [-0.9758, -1.9181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07053786516189575
RAW KL tensor(0.2996, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 516: train/loss = 0.6520418524742126, train/raw-loss = 0.6345648765563965, train/logprobs = tensor([[-2.4552, -3.1049],
        [-1.6326, -1.8494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17476990818977356
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 307])
Epoch 0, Step 517: train/loss = 0.5723580121994019, train/raw-loss = 0.5641125440597534, train/logprobs = tensor([[-0.9123, -2.1392],
        [-0.9313, -1.4933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08245475590229034
RAW KL tensor(0.2822, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 518: train/loss = 0.46807947754859924, train/raw-loss = 0.4540836215019226, train/logprobs = tensor([[-1.3659, -2.6440],
        [-1.4632, -1.5841]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1399584263563156
RAW KL tensor(0.0256, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 519: train/loss = 0.4754429757595062, train/raw-loss = 0.46637922525405884, train/logprobs = tensor([[-0.8513, -2.5611],
        [-1.0074, -1.4901]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09063747525215149
RAW KL tensor(0.0756, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 520: train/loss = 0.4747002124786377, train/raw-loss = 0.46106430888175964, train/logprobs = tensor([[-1.3015, -3.2003],
        [-1.1469, -1.7167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1363590508699417
RAW KL tensor(0.0838, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 521: train/loss = 0.34945374727249146, train/raw-loss = 0.33652907609939575, train/logprobs = tensor([[-1.4496, -3.5860],
        [-1.5652, -1.7329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12924674153327942
RAW KL tensor(0.2560, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 522: train/loss = 0.4562734067440033, train/raw-loss = 0.4404999017715454, train/logprobs = tensor([[-2.0647, -3.9617],
        [-1.7649, -2.2147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15773501992225647
RAW KL tensor(0.0635, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 253])
Epoch 0, Step 523: train/loss = 0.5114715099334717, train/raw-loss = 0.503134548664093, train/logprobs = tensor([[-0.8455, -2.0189],
        [-0.9053, -1.0453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.083369679749012
RAW KL tensor(0.0424, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 524: train/loss = 0.5458883047103882, train/raw-loss = 0.5370990633964539, train/logprobs = tensor([[-1.3626, -1.7225],
        [-1.4223, -1.0549]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08789277821779251
RAW KL tensor(0.0848, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 238])
Epoch 0, Step 525: train/loss = 0.5133233666419983, train/raw-loss = 0.5052691698074341, train/logprobs = tensor([[-1.7875, -3.2032],
        [-1.4293, -1.9251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0805414542555809
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 526: train/loss = 0.43921083211898804, train/raw-loss = 0.4309907853603363, train/logprobs = tensor([[-1.8193, -3.3335],
        [-1.7544, -1.8096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08220069110393524
RAW KL tensor(0.1005, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 527: train/loss = 0.37067821621894836, train/raw-loss = 0.3597089648246765, train/logprobs = tensor([[-1.1446, -3.8124],
        [-1.1194, -1.8499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10969237983226776
RAW KL tensor(0.0214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 528: train/loss = 0.6645302176475525, train/raw-loss = 0.6586859226226807, train/logprobs = tensor([[-0.9195, -1.1641],
        [-1.1164, -1.1837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.058443207293748856
RAW KL tensor(0.0982, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 529: train/loss = 0.31864652037620544, train/raw-loss = 0.30595213174819946, train/logprobs = tensor([[-0.7897, -4.0541],
        [-0.9353, -2.0381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12694384157657623
RAW KL tensor(0.1203, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 530: train/loss = 0.45745110511779785, train/raw-loss = 0.44763216376304626, train/logprobs = tensor([[-1.3221, -2.7397],
        [-1.0804, -1.2601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09818971902132034
RAW KL tensor(0.0950, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 531: train/loss = 0.5946917533874512, train/raw-loss = 0.5867326259613037, train/logprobs = tensor([[-1.5153, -2.0845],
        [-1.2698, -1.2726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07959117740392685
RAW KL tensor(0.0783, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 532: train/loss = 0.44361162185668945, train/raw-loss = 0.43375474214553833, train/logprobs = tensor([[-1.1464, -2.8321],
        [-1.1196, -1.4766]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09856873005628586
RAW KL tensor(0.0285, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 419])
Epoch 0, Step 533: train/loss = 0.43705296516418457, train/raw-loss = 0.4310816824436188, train/logprobs = tensor([[-0.6725, -2.6298],
        [-0.7447, -1.3291]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059713106602430344
RAW KL tensor(0.0646, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 114])
Epoch 0, Step 534: train/loss = 0.5705578327178955, train/raw-loss = 0.5624914169311523, train/logprobs = tensor([[-1.5390, -2.3310],
        [-1.3057, -1.4740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0806637555360794
RAW KL tensor(0.0415, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 535: train/loss = 0.5394307971000671, train/raw-loss = 0.532528281211853, train/logprobs = tensor([[-1.0543, -1.7883],
        [-1.0926, -1.0480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06902491301298141
RAW KL tensor(0.0522, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 536: train/loss = 0.42750558257102966, train/raw-loss = 0.41943979263305664, train/logprobs = tensor([[-1.3762, -3.0566],
        [-1.3759, -1.6455]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08065767586231232
RAW KL tensor(0.1144, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 537: train/loss = 0.6038646101951599, train/raw-loss = 0.5911697745323181, train/logprobs = tensor([[-1.4932, -1.9614],
        [-1.2068, -1.1563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1269480586051941
RAW KL tensor(0.1355, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 538: train/loss = 0.5400846004486084, train/raw-loss = 0.5325636267662048, train/logprobs = tensor([[-1.4686, -2.4060],
        [-1.3764, -1.4821]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07520966231822968
RAW KL tensor(0.1817, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 539: train/loss = 0.3841806650161743, train/raw-loss = 0.371305912733078, train/logprobs = tensor([[-1.2234, -3.7316],
        [-1.3565, -2.1610]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12874753773212433
RAW KL tensor(0.2141, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 540: train/loss = 0.5270096063613892, train/raw-loss = 0.517007052898407, train/logprobs = tensor([[-1.3478, -2.1059],
        [-1.0930, -0.9864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10002532601356506
RAW KL tensor(0.0835, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 541: train/loss = 0.4927193820476532, train/raw-loss = 0.48292064666748047, train/logprobs = tensor([[-1.8632, -2.8579],
        [-1.7938, -1.7425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09798730164766312
RAW KL tensor(0.0745, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 542: train/loss = 0.48138365149497986, train/raw-loss = 0.4744933247566223, train/logprobs = tensor([[-1.6048, -3.0847],
        [-1.6431, -2.0010]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06890355050563812
RAW KL tensor(0.1150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 543: train/loss = 0.3983505964279175, train/raw-loss = 0.3899179995059967, train/logprobs = tensor([[-1.2595, -3.0168],
        [-1.1933, -1.4069]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08432590961456299
RAW KL tensor(0.1338, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 544: train/loss = 0.5024177432060242, train/raw-loss = 0.4934973418712616, train/logprobs = tensor([[-0.9599, -2.0311],
        [-1.0543, -1.0973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08920404314994812
RAW KL tensor(0.0192, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 545: train/loss = 0.5551835298538208, train/raw-loss = 0.5487017631530762, train/logprobs = tensor([[-0.7153, -1.5532],
        [-0.8169, -0.9572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06481794267892838
RAW KL tensor(0.1305, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 546: train/loss = 0.4725164473056793, train/raw-loss = 0.45947155356407166, train/logprobs = tensor([[-1.9347, -2.3602],
        [-1.8044, -1.0339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1304486244916916
RAW KL tensor(0.0930, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 201])
Epoch 0, Step 547: train/loss = 0.4091259241104126, train/raw-loss = 0.39948442578315735, train/logprobs = tensor([[-0.7199, -2.6071],
        [-0.8534, -1.2937]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09641508013010025
RAW KL tensor(0.0501, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 548: train/loss = 0.522323489189148, train/raw-loss = 0.5162308216094971, train/logprobs = tensor([[-1.2005, -2.2705],
        [-1.2491, -1.4446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06092705577611923
RAW KL tensor(0.0353, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 549: train/loss = 0.5073041319847107, train/raw-loss = 0.4955206513404846, train/logprobs = tensor([[-1.3789, -2.1579],
        [-1.4751, -1.3049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11783492565155029
RAW KL tensor(0.0723, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 550: train/loss = 0.5204625129699707, train/raw-loss = 0.5131102800369263, train/logprobs = tensor([[-1.3810, -2.0783],
        [-1.2674, -1.0578]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07352148741483688
RAW KL tensor(0.0392, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 551: train/loss = 0.46501436829566956, train/raw-loss = 0.45476028323173523, train/logprobs = tensor([[-0.8488, -2.2337],
        [-0.9848, -1.0797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10254091769456863
RAW KL tensor(0.0915, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 552: train/loss = 0.35461363196372986, train/raw-loss = 0.3456169366836548, train/logprobs = tensor([[-1.2761, -3.2049],
        [-1.2389, -1.3229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08996710926294327
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 553: train/loss = 0.4844849705696106, train/raw-loss = 0.47602754831314087, train/logprobs = tensor([[-1.6168, -2.8828],
        [-1.4359, -1.5808]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0845741331577301
RAW KL tensor(0.0391, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 554: train/loss = 0.5097966194152832, train/raw-loss = 0.49888527393341064, train/logprobs = tensor([[-1.2157, -2.2414],
        [-1.2524, -1.1979]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10911328345537186
RAW KL tensor(0.3016, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 555: train/loss = 0.37447965145111084, train/raw-loss = 0.35785090923309326, train/logprobs = tensor([[-1.9925, -3.9125],
        [-1.8528, -1.6870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16628730297088623
RAW KL tensor(0.1599, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 556: train/loss = 0.3672202229499817, train/raw-loss = 0.35165733098983765, train/logprobs = tensor([[-1.6769, -3.2737],
        [-1.7351, -1.4524]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15562887489795685
RAW KL tensor(0.0781, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 557: train/loss = 0.4378933906555176, train/raw-loss = 0.43030864000320435, train/logprobs = tensor([[-0.8393, -2.5172],
        [-0.8884, -1.1334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07584744691848755
RAW KL tensor(0.0701, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 558: train/loss = 0.5883039236068726, train/raw-loss = 0.5809580087661743, train/logprobs = tensor([[-1.9149, -2.1111],
        [-1.6486, -1.3153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07345911860466003
RAW KL tensor(0.1678, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 559: train/loss = 0.4347338080406189, train/raw-loss = 0.42430579662323, train/logprobs = tensor([[-1.0624, -3.0925],
        [-1.2163, -1.8950]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10428021103143692
RAW KL tensor(0.1163, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 560: train/loss = 0.5469998717308044, train/raw-loss = 0.53955078125, train/logprobs = tensor([[-1.1617, -2.1671],
        [-1.1646, -1.4489]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07449089735746384
RAW KL tensor(0.1378, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 561: train/loss = 0.471778929233551, train/raw-loss = 0.45868003368377686, train/logprobs = tensor([[-2.0406, -3.1789],
        [-1.5299, -1.5043]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13098891079425812
RAW KL tensor(0.0901, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 562: train/loss = 0.5909775495529175, train/raw-loss = 0.5852980613708496, train/logprobs = tensor([[-1.2330, -2.2394],
        [-1.3356, -1.8451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05679462477564812
RAW KL tensor(0.0856, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 563: train/loss = 0.45152348279953003, train/raw-loss = 0.439084529876709, train/logprobs = tensor([[-1.7860, -2.8686],
        [-1.4598, -1.2634]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12438952922821045
RAW KL tensor(0.0914, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 564: train/loss = 0.6224516034126282, train/raw-loss = 0.614508867263794, train/logprobs = tensor([[-1.5385, -1.6261],
        [-1.2503, -0.9860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07942716777324677
RAW KL tensor(0.1681, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 565: train/loss = 0.45583149790763855, train/raw-loss = 0.44440144300460815, train/logprobs = tensor([[-1.1289, -2.4053],
        [-1.1742, -1.1041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11430049687623978
RAW KL tensor(0.0792, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 566: train/loss = 0.3756050765514374, train/raw-loss = 0.3627224564552307, train/logprobs = tensor([[-1.2739, -3.1557],
        [-1.2889, -1.3982]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12882623076438904
RAW KL tensor(0.1107, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 567: train/loss = 0.3774050772190094, train/raw-loss = 0.3653249144554138, train/logprobs = tensor([[-1.0717, -3.5545],
        [-0.9292, -1.4947]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12080186605453491
RAW KL tensor(0.1141, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 568: train/loss = 0.3280211389064789, train/raw-loss = 0.31476324796676636, train/logprobs = tensor([[-1.5891, -3.5245],
        [-1.5509, -1.4706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1325788050889969
RAW KL tensor(0.1686, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 569: train/loss = 0.3759153187274933, train/raw-loss = 0.36144763231277466, train/logprobs = tensor([[-1.8509, -3.7736],
        [-1.8485, -1.9759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14467684924602509
RAW KL tensor(0.1857, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 570: train/loss = 0.5458647608757019, train/raw-loss = 0.534027099609375, train/logprobs = tensor([[-1.5439, -2.7091],
        [-1.3388, -1.6115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11837665736675262
RAW KL tensor(0.0796, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 571: train/loss = 0.46203088760375977, train/raw-loss = 0.45365703105926514, train/logprobs = tensor([[-1.4430, -3.0974],
        [-1.4537, -1.8690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08373841643333435
RAW KL tensor(0.0837, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 572: train/loss = 0.35604387521743774, train/raw-loss = 0.3451460003852844, train/logprobs = tensor([[-1.2022, -4.1025],
        [-1.1662, -1.9656]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10897865891456604
RAW KL tensor(0.1029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 573: train/loss = 0.5277833938598633, train/raw-loss = 0.5156755447387695, train/logprobs = tensor([[-1.9498, -3.6230],
        [-1.3914, -1.8945]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1210785061120987
RAW KL tensor(0.0677, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 574: train/loss = 0.5507923364639282, train/raw-loss = 0.5409892201423645, train/logprobs = tensor([[-1.2314, -2.0910],
        [-1.1616, -1.1037]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09803066402673721
RAW KL tensor(0.0993, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 575: train/loss = 0.4235980808734894, train/raw-loss = 0.4099065959453583, train/logprobs = tensor([[-1.1948, -2.9920],
        [-1.2130, -1.4868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13691480457782745
RAW KL tensor(0.0599, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 576: train/loss = 0.4633479416370392, train/raw-loss = 0.4572147727012634, train/logprobs = tensor([[-1.0084, -1.9577],
        [-1.2593, -1.0582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06133165955543518
RAW KL tensor(0.0632, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 577: train/loss = 0.3862968385219574, train/raw-loss = 0.3742790222167969, train/logprobs = tensor([[-2.1346, -4.0995],
        [-1.8049, -2.0322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12017833441495895
RAW KL tensor(0.0205, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 578: train/loss = 0.4583028554916382, train/raw-loss = 0.4484892785549164, train/logprobs = tensor([[-1.6460, -2.8667],
        [-1.4179, -1.0404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09813573956489563
RAW KL tensor(0.0489, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 579: train/loss = 0.44235455989837646, train/raw-loss = 0.434992253780365, train/logprobs = tensor([[-0.9027, -2.3923],
        [-0.9729, -1.0573]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07362291216850281
RAW KL tensor(0.1151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 580: train/loss = 0.6029731631278992, train/raw-loss = 0.5938568711280823, train/logprobs = tensor([[-1.5083, -2.4890],
        [-1.3206, -1.4508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09116324037313461
RAW KL tensor(0.1049, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 581: train/loss = 0.29790833592414856, train/raw-loss = 0.281011700630188, train/logprobs = tensor([[-0.6430, -4.4103],
        [-0.6836, -2.1520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16896626353263855
RAW KL tensor(0.2131, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 582: train/loss = 0.4302893280982971, train/raw-loss = 0.4181433320045471, train/logprobs = tensor([[-1.4990, -2.6497],
        [-1.4750, -1.2898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12146024405956268
RAW KL tensor(0.0804, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 583: train/loss = 0.3585380017757416, train/raw-loss = 0.3454661965370178, train/logprobs = tensor([[-1.1403, -3.9323],
        [-1.1691, -2.1078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13071812689304352
RAW KL tensor(0.1050, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 584: train/loss = 0.4332444965839386, train/raw-loss = 0.4200607240200043, train/logprobs = tensor([[-1.5033, -3.1610],
        [-1.6064, -1.6540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13183794915676117
RAW KL tensor(0.1286, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 585: train/loss = 0.4750188887119293, train/raw-loss = 0.4666944742202759, train/logprobs = tensor([[-0.7320, -2.9775],
        [-0.8740, -1.8207]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08324398100376129
RAW KL tensor(0.1320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 586: train/loss = 0.6175176501274109, train/raw-loss = 0.6085467338562012, train/logprobs = tensor([[-2.6353, -1.9683],
        [-2.1625, -1.1098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0897086039185524
RAW KL tensor(0.0598, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 587: train/loss = 0.4748809039592743, train/raw-loss = 0.46688058972358704, train/logprobs = tensor([[-0.7124, -2.0969],
        [-0.9107, -1.0246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08000302314758301
RAW KL tensor(0.0949, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 588: train/loss = 0.4455425441265106, train/raw-loss = 0.43863844871520996, train/logprobs = tensor([[-1.7383, -2.6526],
        [-1.5971, -1.2365]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06904098391532898
RAW KL tensor(0.1727, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 589: train/loss = 0.6006078124046326, train/raw-loss = 0.5873870849609375, train/logprobs = tensor([[-2.1999, -3.0221],
        [-1.1687, -1.3988]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13220727443695068
RAW KL tensor(0.0896, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 296])
Epoch 0, Step 590: train/loss = 0.43786555528640747, train/raw-loss = 0.42854923009872437, train/logprobs = tensor([[-1.6459, -3.1649],
        [-1.4168, -1.6234]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09316319972276688
RAW KL tensor(0.1011, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 591: train/loss = 0.44882988929748535, train/raw-loss = 0.43759870529174805, train/logprobs = tensor([[-1.3318, -2.4772],
        [-1.4231, -1.1994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11231156438589096
RAW KL tensor(0.1321, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 592: train/loss = 0.5627528429031372, train/raw-loss = 0.5527586936950684, train/logprobs = tensor([[-2.2820, -2.2668],
        [-1.8414, -1.0228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09994175285100937
RAW KL tensor(0.1544, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 593: train/loss = 0.3334203064441681, train/raw-loss = 0.31680864095687866, train/logprobs = tensor([[-1.5587, -3.6268],
        [-1.5603, -1.2576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16611674427986145
RAW KL tensor(0.1239, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 594: train/loss = 0.5651874542236328, train/raw-loss = 0.5563076138496399, train/logprobs = tensor([[-1.1712, -2.1307],
        [-1.1252, -1.4110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08879866451025009
RAW KL tensor(0.0491, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 595: train/loss = 0.5020738840103149, train/raw-loss = 0.48807671666145325, train/logprobs = tensor([[-1.5718, -2.5613],
        [-1.3397, -1.2685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13997149467468262
RAW KL tensor(0.1162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 596: train/loss = 0.47762492299079895, train/raw-loss = 0.4696006178855896, train/logprobs = tensor([[-1.0688, -2.3301],
        [-1.1060, -1.1451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08024328202009201
RAW KL tensor(0.1427, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 597: train/loss = 0.30886542797088623, train/raw-loss = 0.29600539803504944, train/logprobs = tensor([[-0.9934, -3.4125],
        [-1.0945, -1.1489]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12860003113746643
RAW KL tensor(0.0194, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 598: train/loss = 0.38470447063446045, train/raw-loss = 0.3727898597717285, train/logprobs = tensor([[-1.0158, -2.8916],
        [-1.1146, -1.1274]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11914585530757904
RAW KL tensor(0.1768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 599: train/loss = 0.30071383714675903, train/raw-loss = 0.28315138816833496, train/logprobs = tensor([[-1.5527, -3.7998],
        [-1.4785, -1.3617]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17562445998191833
RAW KL tensor(0.1234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 600: train/loss = 0.5503699779510498, train/raw-loss = 0.5395388603210449, train/logprobs = tensor([[-1.4682, -1.9997],
        [-1.4573, -1.2586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10831107199192047
RAW KL tensor(0.0846, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 601: train/loss = 0.4015217423439026, train/raw-loss = 0.38900676369667053, train/logprobs = tensor([[-1.2542, -2.9385],
        [-1.2749, -1.3163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12515003979206085
RAW KL tensor(0.2170, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 602: train/loss = 0.47667479515075684, train/raw-loss = 0.46661972999572754, train/logprobs = tensor([[-1.5837, -2.4068],
        [-1.2720, -1.0089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1005508154630661
RAW KL tensor(0.1071, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 603: train/loss = 0.42771977186203003, train/raw-loss = 0.4145708680152893, train/logprobs = tensor([[-0.9638, -2.9523],
        [-1.1229, -1.4907]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13148930668830872
RAW KL tensor(0.1126, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 604: train/loss = 0.2610347270965576, train/raw-loss = 0.24612337350845337, train/logprobs = tensor([[-1.2169, -4.2411],
        [-1.2758, -1.3960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1491134613752365
RAW KL tensor(0.0542, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 605: train/loss = 0.394048810005188, train/raw-loss = 0.38191407918930054, train/logprobs = tensor([[-1.2231, -2.9031],
        [-1.3226, -1.1846]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12134742736816406
RAW KL tensor(0.1377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 606: train/loss = 0.46197015047073364, train/raw-loss = 0.4507560133934021, train/logprobs = tensor([[-1.5771, -2.6999],
        [-1.7453, -1.4839]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11214113235473633
RAW KL tensor(0.1433, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 607: train/loss = 0.39759910106658936, train/raw-loss = 0.37662893533706665, train/logprobs = tensor([[-1.7193, -3.0895],
        [-1.6835, -1.3412]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2097017765045166
RAW KL tensor(0.1714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 608: train/loss = 0.3621644079685211, train/raw-loss = 0.34828582406044006, train/logprobs = tensor([[-1.2461, -3.9301],
        [-1.2960, -1.7259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13878554105758667
RAW KL tensor(0.0854, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 609: train/loss = 0.34387433528900146, train/raw-loss = 0.3344658613204956, train/logprobs = tensor([[-1.8077, -3.3794],
        [-1.7131, -1.2778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09408475458621979
RAW KL tensor(0.1015, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 251])
Epoch 0, Step 610: train/loss = 0.4587990939617157, train/raw-loss = 0.4503641128540039, train/logprobs = tensor([[-1.3414, -3.3979],
        [-1.1109, -1.5629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08434993773698807
RAW KL tensor(0.1117, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 611: train/loss = 0.5817078948020935, train/raw-loss = 0.5728523135185242, train/logprobs = tensor([[-1.5719, -2.0437],
        [-1.3617, -1.2033]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08855599164962769
RAW KL tensor(0.0919, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 612: train/loss = 0.44002440571784973, train/raw-loss = 0.4276612401008606, train/logprobs = tensor([[-1.7957, -3.2141],
        [-1.3419, -1.3786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12363169342279434
RAW KL tensor(0.1265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 613: train/loss = 0.36898529529571533, train/raw-loss = 0.3588537573814392, train/logprobs = tensor([[-1.2402, -4.1360],
        [-0.8887, -1.9665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10131533443927765
RAW KL tensor(0.0991, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 614: train/loss = 0.43012773990631104, train/raw-loss = 0.4176393449306488, train/logprobs = tensor([[-1.5254, -3.9921],
        [-1.2144, -2.1308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12488394230604172
RAW KL tensor(0.0453, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 615: train/loss = 0.382201611995697, train/raw-loss = 0.3699842095375061, train/logprobs = tensor([[-0.7824, -2.9404],
        [-0.8821, -0.9071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12217386066913605
RAW KL tensor(0.1568, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 616: train/loss = 0.4077577590942383, train/raw-loss = 0.3918190002441406, train/logprobs = tensor([[-2.5488, -4.9042],
        [-1.8386, -2.5097]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15938743948936462
RAW KL tensor(0.0856, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 617: train/loss = 0.4598384499549866, train/raw-loss = 0.4507482051849365, train/logprobs = tensor([[-1.2860, -2.5035],
        [-1.2373, -1.2339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09090214222669601
RAW KL tensor(0.1597, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 618: train/loss = 0.36387133598327637, train/raw-loss = 0.35065028071403503, train/logprobs = tensor([[-1.2266, -3.0890],
        [-1.1807, -1.1657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13221076130867004
RAW KL tensor(0.2854, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 619: train/loss = 0.5805367231369019, train/raw-loss = 0.5646035671234131, train/logprobs = tensor([[-2.9379, -3.8118],
        [-1.3491, -1.1051]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1593315303325653
RAW KL tensor(0.1418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 620: train/loss = 0.432207852602005, train/raw-loss = 0.42016321420669556, train/logprobs = tensor([[-1.2059, -2.7887],
        [-1.2955, -1.3075]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12044645845890045
RAW KL tensor(0.1932, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 621: train/loss = 0.3642968237400055, train/raw-loss = 0.35322844982147217, train/logprobs = tensor([[-2.1178, -3.6908],
        [-1.7058, -1.1897]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11068354547023773
RAW KL tensor(0.0667, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 622: train/loss = 0.40518879890441895, train/raw-loss = 0.39299285411834717, train/logprobs = tensor([[-1.1879, -3.4996],
        [-1.1260, -1.8105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12195947021245956
RAW KL tensor(0.1166, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 623: train/loss = 0.3608579635620117, train/raw-loss = 0.3497847616672516, train/logprobs = tensor([[-3.0348, -5.3124],
        [-2.2136, -1.9600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1107320636510849
RAW KL tensor(0.1837, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 624: train/loss = 0.5158730149269104, train/raw-loss = 0.5016200542449951, train/logprobs = tensor([[-1.7241, -2.2657],
        [-1.4981, -0.9944]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14252924919128418
RAW KL tensor(0.0544, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 625: train/loss = 0.4666338562965393, train/raw-loss = 0.4537920355796814, train/logprobs = tensor([[-1.5569, -2.6436],
        [-1.7114, -1.2759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12841778993606567
RAW KL tensor(0.0921, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 626: train/loss = 0.4277931749820709, train/raw-loss = 0.4131247401237488, train/logprobs = tensor([[-1.8069, -3.3917],
        [-1.4777, -1.6081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14668449759483337
RAW KL tensor(0.1510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 627: train/loss = 0.39218857884407043, train/raw-loss = 0.3797321021556854, train/logprobs = tensor([[-1.0241, -2.9827],
        [-1.0639, -1.0851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12456487119197845
RAW KL tensor(0.1356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 628: train/loss = 0.33022257685661316, train/raw-loss = 0.31158551573753357, train/logprobs = tensor([[-1.3696, -3.9424],
        [-1.1689, -1.4682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18637068569660187
RAW KL tensor(0.1808, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 629: train/loss = 0.41702550649642944, train/raw-loss = 0.40415525436401367, train/logprobs = tensor([[-1.5924, -3.0328],
        [-1.3677, -1.1053]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12870246171951294
RAW KL tensor(0.0800, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 630: train/loss = 0.22572964429855347, train/raw-loss = 0.21132972836494446, train/logprobs = tensor([[-0.9059, -4.1527],
        [-1.0840, -1.1439]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14399904012680054
RAW KL tensor(0.0886, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 631: train/loss = 0.5789228677749634, train/raw-loss = 0.5717611312866211, train/logprobs = tensor([[-1.0440, -1.4918],
        [-1.0277, -0.9357]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0716177299618721
RAW KL tensor(0.1249, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 632: train/loss = 0.3997163772583008, train/raw-loss = 0.38960859179496765, train/logprobs = tensor([[-1.1964, -2.8834],
        [-1.2974, -1.2608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10107774287462234
RAW KL tensor(0.2229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 633: train/loss = 0.2759072482585907, train/raw-loss = 0.26050758361816406, train/logprobs = tensor([[-0.9965, -3.7171],
        [-1.1823, -1.2723]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15399661660194397
RAW KL tensor(0.1577, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 634: train/loss = 0.3631381094455719, train/raw-loss = 0.34749969840049744, train/logprobs = tensor([[-1.7448, -3.5396],
        [-1.7006, -1.5082]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1563844382762909
RAW KL tensor(0.0452, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 635: train/loss = 0.4749923348426819, train/raw-loss = 0.46308550238609314, train/logprobs = tensor([[-1.5486, -2.5735],
        [-1.4618, -1.1176]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1190682128071785
RAW KL tensor(0.1561, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 636: train/loss = 0.4962298274040222, train/raw-loss = 0.48188239336013794, train/logprobs = tensor([[-2.1410, -2.9483],
        [-1.6832, -1.2407]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14347387850284576
RAW KL tensor(0.0708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 637: train/loss = 0.5034143328666687, train/raw-loss = 0.49249932169914246, train/logprobs = tensor([[-1.5615, -2.4155],
        [-1.6636, -1.2063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10915061086416245
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 638: train/loss = 0.47907358407974243, train/raw-loss = 0.47052258253097534, train/logprobs = tensor([[-1.2778, -2.3812],
        [-1.3267, -1.0960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08551017940044403
RAW KL tensor(0.0313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 305])
Epoch 0, Step 639: train/loss = 0.552357017993927, train/raw-loss = 0.5437083840370178, train/logprobs = tensor([[-1.7691, -2.2930],
        [-1.2649, -1.0302]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08648674935102463
RAW KL tensor(0.0600, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 640: train/loss = 0.32121938467025757, train/raw-loss = 0.30657920241355896, train/logprobs = tensor([[-0.9119, -3.5114],
        [-1.0477, -0.9602]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14640185236930847
RAW KL tensor(0.2327, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 641: train/loss = 0.4615921974182129, train/raw-loss = 0.45142149925231934, train/logprobs = tensor([[-1.1329, -2.7087],
        [-1.4373, -1.3070]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1017068475484848
RAW KL tensor(0.0874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 642: train/loss = 0.5693289041519165, train/raw-loss = 0.5624733567237854, train/logprobs = tensor([[-0.9827, -1.8135],
        [-1.0186, -1.1418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06855523586273193
RAW KL tensor(0.1169, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 643: train/loss = 0.5344784259796143, train/raw-loss = 0.5224786996841431, train/logprobs = tensor([[-1.0755, -2.4124],
        [-1.1879, -1.5909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11999741196632385
RAW KL tensor(0.1852, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 644: train/loss = 0.34410253167152405, train/raw-loss = 0.3284015655517578, train/logprobs = tensor([[-1.6297, -3.8891],
        [-1.8753, -1.8948]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1570097953081131
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 645: train/loss = 0.531101644039154, train/raw-loss = 0.5235093832015991, train/logprobs = tensor([[-1.3213, -2.0606],
        [-1.0444, -1.0003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0759221687912941
RAW KL tensor(0.1249, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 646: train/loss = 0.3410394787788391, train/raw-loss = 0.33025607466697693, train/logprobs = tensor([[-1.5613, -3.8034],
        [-1.5179, -1.7001]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10783404111862183
RAW KL tensor(0.1189, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 647: train/loss = 0.46453970670700073, train/raw-loss = 0.45297256112098694, train/logprobs = tensor([[-1.9477, -2.9543],
        [-1.7327, -1.3211]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1156717985868454
RAW KL tensor(0.1309, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 648: train/loss = 0.24639391899108887, train/raw-loss = 0.23220032453536987, train/logprobs = tensor([[-1.2024, -3.9807],
        [-1.0447, -0.9486]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14193600416183472
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 649: train/loss = 0.5391495823860168, train/raw-loss = 0.5293601751327515, train/logprobs = tensor([[-1.0106, -2.1494],
        [-1.1453, -1.1556]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09789373725652695
RAW KL tensor(0.2617, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 650: train/loss = 0.6075153350830078, train/raw-loss = 0.5931565165519714, train/logprobs = tensor([[-2.0213, -2.9574],
        [-1.5366, -1.8106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14358830451965332
RAW KL tensor(0.0602, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 651: train/loss = 0.4071529507637024, train/raw-loss = 0.3979775607585907, train/logprobs = tensor([[-1.8954, -2.5873],
        [-1.8779, -0.8910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09175397455692291
RAW KL tensor(0.1955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 652: train/loss = 0.35544219613075256, train/raw-loss = 0.33771663904190063, train/logprobs = tensor([[-1.2104, -3.4481],
        [-1.1491, -1.1255]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17725537717342377
RAW KL tensor(0.1334, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 653: train/loss = 0.4807914197444916, train/raw-loss = 0.47034263610839844, train/logprobs = tensor([[-1.0033, -2.0379],
        [-0.9656, -0.8600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10448792576789856
RAW KL tensor(0.0661, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 246])
Epoch 0, Step 654: train/loss = 0.45010778307914734, train/raw-loss = 0.4399626553058624, train/logprobs = tensor([[-1.4029, -2.2441],
        [-1.5771, -1.1344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1014510989189148
RAW KL tensor(0.1600, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 655: train/loss = 0.329454243183136, train/raw-loss = 0.31531018018722534, train/logprobs = tensor([[-1.4242, -3.5969],
        [-1.5412, -1.6744]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14144042134284973
RAW KL tensor(0.1582, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 656: train/loss = 0.4569394588470459, train/raw-loss = 0.44633254408836365, train/logprobs = tensor([[-0.9560, -2.6162],
        [-1.1358, -1.4979]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10606890171766281
RAW KL tensor(0.0636, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 657: train/loss = 0.4896341562271118, train/raw-loss = 0.47917482256889343, train/logprobs = tensor([[-1.2344, -2.1202],
        [-1.3542, -1.0685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10459350049495697
RAW KL tensor(0.0716, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 658: train/loss = 0.48288166522979736, train/raw-loss = 0.4742460250854492, train/logprobs = tensor([[-1.2061, -1.8943],
        [-1.4126, -0.9089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08635637164115906
RAW KL tensor(0.2202, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 659: train/loss = 0.27078336477279663, train/raw-loss = 0.24255657196044922, train/logprobs = tensor([[-1.2379, -5.5244],
        [-1.3386, -1.8641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.28226780891418457
RAW KL tensor(0.1366, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 660: train/loss = 0.4527783989906311, train/raw-loss = 0.43585801124572754, train/logprobs = tensor([[-1.6315, -3.5998],
        [-1.1198, -1.4161]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16920387744903564
RAW KL tensor(0.1275, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 661: train/loss = 0.3204687833786011, train/raw-loss = 0.3051837682723999, train/logprobs = tensor([[-1.5988, -3.7043],
        [-2.0141, -1.9000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15285024046897888
RAW KL tensor(0.1118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 662: train/loss = 0.36588799953460693, train/raw-loss = 0.3523739278316498, train/logprobs = tensor([[-1.1289, -3.1082],
        [-1.3650, -1.4024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13514050841331482
RAW KL tensor(0.1012, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 663: train/loss = 0.38476642966270447, train/raw-loss = 0.37217357754707336, train/logprobs = tensor([[-1.5632, -3.1388],
        [-1.3035, -0.8930]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12592846155166626
RAW KL tensor(0.1603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 664: train/loss = 0.4031798839569092, train/raw-loss = 0.3897276222705841, train/logprobs = tensor([[-1.8754, -4.2918],
        [-1.8497, -2.4960]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1345224827528
RAW KL tensor(0.1972, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 665: train/loss = 0.3164444863796234, train/raw-loss = 0.30490875244140625, train/logprobs = tensor([[-1.4865, -3.7028],
        [-1.4757, -1.5350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11535732448101044
RAW KL tensor(0.1376, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 666: train/loss = 0.33873891830444336, train/raw-loss = 0.3275313675403595, train/logprobs = tensor([[-1.6894, -3.1692],
        [-1.6568, -1.2078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11207549273967743
RAW KL tensor(0.1358, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 667: train/loss = 0.5156480073928833, train/raw-loss = 0.5022743940353394, train/logprobs = tensor([[-1.3009, -1.8258],
        [-1.4437, -0.9344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13373640179634094
RAW KL tensor(0.1083, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 668: train/loss = 0.3343907296657562, train/raw-loss = 0.3243529796600342, train/logprobs = tensor([[-1.4027, -3.3191],
        [-1.4676, -1.2436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10037754476070404
RAW KL tensor(0.1714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 669: train/loss = 0.27817070484161377, train/raw-loss = 0.2627764046192169, train/logprobs = tensor([[-1.2175, -4.4622],
        [-1.2562, -1.7111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15394312143325806
RAW KL tensor(0.0524, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 670: train/loss = 0.6033968925476074, train/raw-loss = 0.5879825353622437, train/logprobs = tensor([[-1.9379, -2.2179],
        [-1.4502, -1.2339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1541440486907959
RAW KL tensor(0.0696, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 671: train/loss = 0.5823869109153748, train/raw-loss = 0.5743125081062317, train/logprobs = tensor([[-1.3446, -2.1909],
        [-0.9865, -1.1032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0807444304227829
RAW KL tensor(0.1418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 672: train/loss = 0.34509560465812683, train/raw-loss = 0.32908421754837036, train/logprobs = tensor([[-1.9832, -3.8196],
        [-1.5364, -1.2304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16011400520801544
RAW KL tensor(0.2877, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 117])
Epoch 0, Step 673: train/loss = 0.3300185203552246, train/raw-loss = 0.3137868046760559, train/logprobs = tensor([[-1.6706, -3.8134],
        [-1.4265, -1.4349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16231700778007507
RAW KL tensor(0.1222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 674: train/loss = 0.26160019636154175, train/raw-loss = 0.24364422261714935, train/logprobs = tensor([[-1.5698, -4.4749],
        [-1.7246, -1.4512]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17955976724624634
RAW KL tensor(0.0945, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 675: train/loss = 0.3432508707046509, train/raw-loss = 0.33370959758758545, train/logprobs = tensor([[-1.0770, -3.2927],
        [-1.2752, -1.5833]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09541306644678116
RAW KL tensor(0.1642, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 676: train/loss = 0.40882620215415955, train/raw-loss = 0.39412665367126465, train/logprobs = tensor([[-1.6904, -3.1734],
        [-1.4001, -1.2425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14699535071849823
RAW KL tensor(0.0603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 677: train/loss = 0.3167823851108551, train/raw-loss = 0.3053607940673828, train/logprobs = tensor([[-1.5281, -3.9374],
        [-1.5759, -1.7131]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1142156645655632
RAW KL tensor(0.1898, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 678: train/loss = 0.2263757884502411, train/raw-loss = 0.20854562520980835, train/logprobs = tensor([[-1.5981, -4.7830],
        [-1.7454, -1.7181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1783013939857483
RAW KL tensor(0.0699, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 679: train/loss = 0.5294133424758911, train/raw-loss = 0.518554151058197, train/logprobs = tensor([[-1.6334, -2.0522],
        [-1.5003, -0.9266]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10859193652868271
RAW KL tensor(0.0956, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 680: train/loss = 0.44991976022720337, train/raw-loss = 0.4337344765663147, train/logprobs = tensor([[-2.1382, -3.4239],
        [-1.9467, -1.5851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16185282170772552
RAW KL tensor(0.1656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 681: train/loss = 0.4211941957473755, train/raw-loss = 0.40513119101524353, train/logprobs = tensor([[-1.1030, -3.2264],
        [-1.2236, -1.5544]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16063016653060913
RAW KL tensor(0.3051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 682: train/loss = 0.25246044993400574, train/raw-loss = 0.23007027804851532, train/logprobs = tensor([[-1.6381, -4.5440],
        [-1.8821, -1.5938]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22390148043632507
RAW KL tensor(0.1356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 683: train/loss = 0.486986368894577, train/raw-loss = 0.4626644253730774, train/logprobs = tensor([[-2.4886, -3.3524],
        [-1.7572, -1.2313]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24321947991847992
RAW KL tensor(0.0966, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 684: train/loss = 0.38550642132759094, train/raw-loss = 0.37520831823349, train/logprobs = tensor([[-1.1129, -2.6928],
        [-1.3366, -1.1771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10298097878694534
RAW KL tensor(0.1098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 685: train/loss = 0.3205633759498596, train/raw-loss = 0.30921250581741333, train/logprobs = tensor([[-1.4374, -3.7755],
        [-1.5564, -1.6667]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11350856721401215
RAW KL tensor(0.0783, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 686: train/loss = 0.3631584644317627, train/raw-loss = 0.35097450017929077, train/logprobs = tensor([[-0.9836, -3.2139],
        [-1.1442, -1.0962]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12183933705091476
RAW KL tensor(0.1113, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 687: train/loss = 0.3721878230571747, train/raw-loss = 0.3611201345920563, train/logprobs = tensor([[-1.7925, -3.3353],
        [-1.8570, -1.3787]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11067679524421692
RAW KL tensor(0.1132, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 688: train/loss = 0.3181453347206116, train/raw-loss = 0.2980693280696869, train/logprobs = tensor([[-1.7251, -4.4982],
        [-1.5826, -1.8022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2007601261138916
RAW KL tensor(0.0989, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 689: train/loss = 0.36713001132011414, train/raw-loss = 0.352995365858078, train/logprobs = tensor([[-0.8961, -3.0190],
        [-1.0659, -1.0229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14134646952152252
RAW KL tensor(0.1049, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 690: train/loss = 0.37669238448143005, train/raw-loss = 0.36274999380111694, train/logprobs = tensor([[-1.6170, -3.4486],
        [-1.4517, -1.3018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13942426443099976
RAW KL tensor(0.2106, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 691: train/loss = 0.2659987509250641, train/raw-loss = 0.24810633063316345, train/logprobs = tensor([[-1.0327, -3.9611],
        [-1.2325, -1.2405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17892421782016754
RAW KL tensor(0.1440, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 692: train/loss = 0.34748342633247375, train/raw-loss = 0.33368125557899475, train/logprobs = tensor([[-1.2906, -3.2102],
        [-1.4249, -1.3575]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13802166283130646
RAW KL tensor(0.1378, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 693: train/loss = 0.3813368082046509, train/raw-loss = 0.36457714438438416, train/logprobs = tensor([[-1.7055, -3.8429],
        [-1.8026, -1.3270]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1675964891910553
RAW KL tensor(0.0874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 694: train/loss = 0.29329538345336914, train/raw-loss = 0.27906614542007446, train/logprobs = tensor([[-1.5649, -3.9029],
        [-1.6115, -1.3232]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1422925591468811
RAW KL tensor(0.2808, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 695: train/loss = 0.3126380741596222, train/raw-loss = 0.2947414517402649, train/logprobs = tensor([[-1.3742, -3.5626],
        [-1.4693, -1.0114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1789664328098297
RAW KL tensor(0.1677, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 696: train/loss = 0.234117329120636, train/raw-loss = 0.21538186073303223, train/logprobs = tensor([[-1.2292, -4.2923],
        [-1.2944, -1.3786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1873546689748764
RAW KL tensor(0.1212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 697: train/loss = 0.35077041387557983, train/raw-loss = 0.3297690749168396, train/logprobs = tensor([[-1.3774, -3.6256],
        [-1.7368, -1.6899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21001318097114563
RAW KL tensor(0.1912, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 698: train/loss = 0.3197774887084961, train/raw-loss = 0.3024912476539612, train/logprobs = tensor([[-1.8770, -4.7085],
        [-1.4952, -1.4609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1728624850511551
RAW KL tensor(0.0603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 699: train/loss = 0.40726438164711, train/raw-loss = 0.3921024799346924, train/logprobs = tensor([[-1.4067, -3.1879],
        [-1.3976, -1.1635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1516190618276596
RAW KL tensor(0.0659, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 700: train/loss = 0.3187434673309326, train/raw-loss = 0.30638450384140015, train/logprobs = tensor([[-0.7723, -3.5308],
        [-0.9256, -1.1233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12358963489532471
RAW KL tensor(0.1540, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 701: train/loss = 0.38678938150405884, train/raw-loss = 0.3735700249671936, train/logprobs = tensor([[-1.2350, -2.9372],
        [-1.6983, -1.3992]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13219377398490906
RAW KL tensor(0.1589, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 289])
Epoch 0, Step 702: train/loss = 0.3298557698726654, train/raw-loss = 0.3172006607055664, train/logprobs = tensor([[-2.1843, -3.5603],
        [-2.2107, -1.3984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12655077874660492
RAW KL tensor(0.1325, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 703: train/loss = 0.27635273337364197, train/raw-loss = 0.25827139616012573, train/logprobs = tensor([[-1.9743, -4.7247],
        [-1.6655, -1.5485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1808132380247116
RAW KL tensor(0.0678, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 704: train/loss = 0.3070793151855469, train/raw-loss = 0.2969140112400055, train/logprobs = tensor([[-1.1424, -4.2838],
        [-1.3911, -2.2020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10165302455425262
RAW KL tensor(0.1348, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 705: train/loss = 0.4543556272983551, train/raw-loss = 0.4424619674682617, train/logprobs = tensor([[-1.6389, -3.3516],
        [-1.6757, -2.0547]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11893680691719055
RAW KL tensor(0.1910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 706: train/loss = 0.32229334115982056, train/raw-loss = 0.30632930994033813, train/logprobs = tensor([[-1.3978, -4.4629],
        [-1.0737, -1.5124]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15964031219482422
RAW KL tensor(0.1368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 707: train/loss = 0.5160121917724609, train/raw-loss = 0.5023389458656311, train/logprobs = tensor([[-1.7558, -2.7243],
        [-1.2984, -1.2683]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13673259317874908
RAW KL tensor(0.1460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 708: train/loss = 0.4997113049030304, train/raw-loss = 0.48760557174682617, train/logprobs = tensor([[-1.2310, -2.2481],
        [-1.5308, -1.4963]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1210571900010109
RAW KL tensor(0.1524, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 709: train/loss = 0.443294882774353, train/raw-loss = 0.43137413263320923, train/logprobs = tensor([[-0.9097, -2.4767],
        [-1.2843, -1.4702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11920776963233948
RAW KL tensor(0.0640, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 710: train/loss = 0.3233664035797119, train/raw-loss = 0.3064298629760742, train/logprobs = tensor([[-0.8249, -3.8300],
        [-0.7755, -1.0852]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16936522722244263
RAW KL tensor(0.1534, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 711: train/loss = 0.34777092933654785, train/raw-loss = 0.33399590849876404, train/logprobs = tensor([[-1.9406, -3.6042],
        [-2.1723, -1.8353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13774995505809784
RAW KL tensor(0.0795, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 712: train/loss = 0.27119624614715576, train/raw-loss = 0.25629091262817383, train/logprobs = tensor([[-0.9115, -4.0053],
        [-0.9970, -1.4103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14905329048633575
RAW KL tensor(0.1238, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 713: train/loss = 0.26614847779273987, train/raw-loss = 0.2510891556739807, train/logprobs = tensor([[-1.1351, -3.8879],
        [-1.2303, -1.2599]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15059350430965424
RAW KL tensor(0.1613, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 714: train/loss = 0.5101862549781799, train/raw-loss = 0.4986118674278259, train/logprobs = tensor([[-1.0606, -1.7689],
        [-1.3737, -0.9759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11574383080005646
RAW KL tensor(0.1341, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 715: train/loss = 0.48702922463417053, train/raw-loss = 0.475805401802063, train/logprobs = tensor([[-1.3544, -1.9132],
        [-1.8224, -1.1879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11223801970481873
RAW KL tensor(0.2510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 716: train/loss = 0.3045101463794708, train/raw-loss = 0.2853930592536926, train/logprobs = tensor([[-1.6577, -4.2067],
        [-1.8456, -1.2283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19117048382759094
RAW KL tensor(0.0905, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 717: train/loss = 0.24727775156497955, train/raw-loss = 0.2318459004163742, train/logprobs = tensor([[-1.6352, -4.0677],
        [-1.7787, -1.2279]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15431852638721466
RAW KL tensor(0.4109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 108])
Epoch 0, Step 718: train/loss = 0.37624669075012207, train/raw-loss = 0.3457193970680237, train/logprobs = tensor([[-2.4154, -5.6750],
        [-1.3422, -1.4153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3052726686000824
RAW KL tensor(0.0284, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 719: train/loss = 0.6002085208892822, train/raw-loss = 0.5902246832847595, train/logprobs = tensor([[-1.2666, -1.4437],
        [-1.4250, -1.1211]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09983911365270615
RAW KL tensor(0.0614, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 720: train/loss = 0.45033904910087585, train/raw-loss = 0.43966323137283325, train/logprobs = tensor([[-1.0328, -2.7093],
        [-1.3274, -1.3467]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10675843060016632
RAW KL tensor(0.2002, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 721: train/loss = 0.4548288583755493, train/raw-loss = 0.4394645392894745, train/logprobs = tensor([[-1.6410, -3.0975],
        [-1.4604, -1.1051]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15364320576190948
RAW KL tensor(0.0937, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 722: train/loss = 0.3412078022956848, train/raw-loss = 0.32920390367507935, train/logprobs = tensor([[-1.1511, -3.6619],
        [-1.3394, -1.3343]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12003938853740692
RAW KL tensor(0.1862, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 723: train/loss = 0.388374924659729, train/raw-loss = 0.37054163217544556, train/logprobs = tensor([[-2.0790, -4.3252],
        [-1.3080, -1.8259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17833296954631805
RAW KL tensor(0.1019, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 724: train/loss = 0.432449072599411, train/raw-loss = 0.42231354117393494, train/logprobs = tensor([[-1.0247, -2.9162],
        [-1.3055, -1.4872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10135523229837418
RAW KL tensor(0.1273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 725: train/loss = 0.4309137463569641, train/raw-loss = 0.41476812958717346, train/logprobs = tensor([[-1.6463, -2.6812],
        [-1.7766, -1.2807]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16145606338977814
RAW KL tensor(0.0743, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 358])
Epoch 0, Step 726: train/loss = 0.43314605951309204, train/raw-loss = 0.419036328792572, train/logprobs = tensor([[-1.3456, -2.5921],
        [-1.6119, -0.9727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14109735190868378
RAW KL tensor(0.0456, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 727: train/loss = 0.4606624245643616, train/raw-loss = 0.45159655809402466, train/logprobs = tensor([[-0.8866, -2.3583],
        [-1.1792, -1.2441]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09065863490104675
RAW KL tensor(0.1916, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 728: train/loss = 0.4110473394393921, train/raw-loss = 0.39761820435523987, train/logprobs = tensor([[-1.6979, -3.0294],
        [-1.9324, -1.5317]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13429154455661774
RAW KL tensor(0.1590, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 323])
Epoch 0, Step 729: train/loss = 0.20959575474262238, train/raw-loss = 0.19106680154800415, train/logprobs = tensor([[-1.0563, -4.5334],
        [-1.3961, -1.4653]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18528945744037628
RAW KL tensor(0.1096, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 730: train/loss = 0.49577242136001587, train/raw-loss = 0.4807598888874054, train/logprobs = tensor([[-1.8437, -2.5738],
        [-2.2176, -1.6295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15012522041797638
RAW KL tensor(0.1085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 731: train/loss = 0.47510549426078796, train/raw-loss = 0.46380993723869324, train/logprobs = tensor([[-2.6213, -2.7922],
        [-2.2589, -1.2021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11295577883720398
RAW KL tensor(0.1250, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 732: train/loss = 0.36674177646636963, train/raw-loss = 0.35108572244644165, train/logprobs = tensor([[-2.1232, -3.3669],
        [-2.4034, -1.4824]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1565602719783783
RAW KL tensor(0.1855, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 733: train/loss = 0.27989351749420166, train/raw-loss = 0.2663668096065521, train/logprobs = tensor([[-1.2005, -3.7277],
        [-1.5819, -1.6422]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13526707887649536
RAW KL tensor(0.0810, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 734: train/loss = 0.4841674864292145, train/raw-loss = 0.4719921052455902, train/logprobs = tensor([[-1.7439, -2.4119],
        [-1.3682, -0.8856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12175405025482178
RAW KL tensor(0.2437, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 735: train/loss = 0.6878942251205444, train/raw-loss = 0.6748461723327637, train/logprobs = tensor([[-2.2998, -1.9799],
        [-1.5950, -1.0931]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1304808408021927
RAW KL tensor(0.1324, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 736: train/loss = 0.23940855264663696, train/raw-loss = 0.21826228499412537, train/logprobs = tensor([[-1.5435, -4.8274],
        [-2.1471, -1.8646]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21146267652511597
RAW KL tensor(0.0740, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 737: train/loss = 0.5119226574897766, train/raw-loss = 0.49637967348098755, train/logprobs = tensor([[-1.9837, -2.8068],
        [-1.8949, -1.2163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1554296314716339
RAW KL tensor(0.3118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 738: train/loss = 0.17318040132522583, train/raw-loss = 0.15138943493366241, train/logprobs = tensor([[-1.3395, -5.3503],
        [-1.9311, -2.0228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21790975332260132
RAW KL tensor(0.0879, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 739: train/loss = 0.18003056943416595, train/raw-loss = 0.15840117633342743, train/logprobs = tensor([[-1.6585, -6.1523],
        [-1.7616, -1.3438]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21629376709461212
RAW KL tensor(0.0871, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 740: train/loss = 0.35158663988113403, train/raw-loss = 0.33731308579444885, train/logprobs = tensor([[-0.8422, -3.5862],
        [-1.3538, -1.4494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14273546636104584
RAW KL tensor(0.1615, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 741: train/loss = 0.34282469749450684, train/raw-loss = 0.3261106312274933, train/logprobs = tensor([[-1.9835, -4.3052],
        [-1.7700, -1.4756]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1671404391527176
RAW KL tensor(0.1502, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 742: train/loss = 0.40381962060928345, train/raw-loss = 0.3897213935852051, train/logprobs = tensor([[-0.9365, -3.3268],
        [-1.1470, -1.2042]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14098256826400757
RAW KL tensor(0.1948, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 743: train/loss = 0.4412676692008972, train/raw-loss = 0.42660778760910034, train/logprobs = tensor([[-1.3143, -3.3826],
        [-1.3991, -1.7990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14659839868545532
RAW KL tensor(0.0416, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 744: train/loss = 0.3650705814361572, train/raw-loss = 0.35425281524658203, train/logprobs = tensor([[-1.3589, -3.7485],
        [-1.7847, -2.2847]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10817781090736389
RAW KL tensor(0.2043, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 745: train/loss = 0.23895683884620667, train/raw-loss = 0.21755799651145935, train/logprobs = tensor([[-1.2077, -4.5519],
        [-1.6809, -1.8166]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21398837864398956
RAW KL tensor(0.1512, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 746: train/loss = 0.41925305128097534, train/raw-loss = 0.4045078158378601, train/logprobs = tensor([[-0.8304, -3.9605],
        [-1.0736, -2.3485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14745235443115234
RAW KL tensor(0.1178, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 747: train/loss = 0.6212356090545654, train/raw-loss = 0.6070027351379395, train/logprobs = tensor([[-1.6934, -2.0326],
        [-1.7508, -1.5005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14232823252677917
RAW KL tensor(0.2285, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 748: train/loss = 0.4250326156616211, train/raw-loss = 0.4077373147010803, train/logprobs = tensor([[-1.6234, -3.8326],
        [-1.9221, -2.3720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17295333743095398
RAW KL tensor(0.1295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 749: train/loss = 0.3473912477493286, train/raw-loss = 0.3335934281349182, train/logprobs = tensor([[-2.0588, -3.3235],
        [-2.0924, -1.4757]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.137978196144104
RAW KL tensor(0.0657, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 750: train/loss = 0.41347813606262207, train/raw-loss = 0.39934855699539185, train/logprobs = tensor([[-1.6106, -3.5942],
        [-1.4045, -1.7302]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14129579067230225
RAW KL tensor(0.1276, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 751: train/loss = 0.35635441541671753, train/raw-loss = 0.34321558475494385, train/logprobs = tensor([[-1.2839, -3.2947],
        [-1.8020, -1.5031]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1313886195421219
RAW KL tensor(0.2978, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 752: train/loss = 0.4464656114578247, train/raw-loss = 0.433988094329834, train/logprobs = tensor([[-0.8311, -2.3619],
        [-1.2319, -1.1542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12477532029151917
RAW KL tensor(0.2051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 753: train/loss = 0.30757009983062744, train/raw-loss = 0.29461124539375305, train/logprobs = tensor([[-1.0269, -3.1618],
        [-1.6158, -1.5244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12958833575248718
RAW KL tensor(0.2320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 754: train/loss = 0.34860798716545105, train/raw-loss = 0.32822513580322266, train/logprobs = tensor([[-1.2191, -4.2009],
        [-1.4480, -2.1732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.203828826546669
RAW KL tensor(0.2476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 755: train/loss = 0.24128291010856628, train/raw-loss = 0.21608829498291016, train/logprobs = tensor([[-1.7286, -5.2796],
        [-2.2332, -1.7821]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2519460916519165
RAW KL tensor(0.3279, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 756: train/loss = 0.2676825523376465, train/raw-loss = 0.247676283121109, train/logprobs = tensor([[-1.3959, -4.2113],
        [-2.0363, -1.8132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20006251335144043
RAW KL tensor(0.1184, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 757: train/loss = 0.26732248067855835, train/raw-loss = 0.24826104938983917, train/logprobs = tensor([[-1.5536, -4.8681],
        [-2.1648, -2.5263]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1906142234802246
RAW KL tensor(0.2408, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 758: train/loss = 0.384240984916687, train/raw-loss = 0.370585560798645, train/logprobs = tensor([[-1.4074, -2.9974],
        [-1.2725, -1.0375]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1365543007850647
RAW KL tensor(0.1026, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 759: train/loss = 0.2536812722682953, train/raw-loss = 0.2336980700492859, train/logprobs = tensor([[-1.8923, -4.1090],
        [-2.3176, -1.3158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1998319774866104
RAW KL tensor(0.1725, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 760: train/loss = 0.24016346037387848, train/raw-loss = 0.2195524275302887, train/logprobs = tensor([[-2.1743, -5.0644],
        [-2.2510, -1.8816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2061104029417038
RAW KL tensor(0.2199, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 761: train/loss = 0.42596980929374695, train/raw-loss = 0.41110002994537354, train/logprobs = tensor([[-1.6579, -2.9610],
        [-1.5640, -1.1049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14869776368141174
RAW KL tensor(0.1683, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 762: train/loss = 0.40633201599121094, train/raw-loss = 0.3924812078475952, train/logprobs = tensor([[-1.9387, -3.9704],
        [-1.7917, -1.8037]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13850800693035126
RAW KL tensor(0.1109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 763: train/loss = 0.537711501121521, train/raw-loss = 0.5273908972740173, train/logprobs = tensor([[-1.2418, -1.7984],
        [-1.3635, -1.1175]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10320581495761871
RAW KL tensor(0.2027, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 764: train/loss = 0.24660910665988922, train/raw-loss = 0.22974073886871338, train/logprobs = tensor([[-1.4944, -3.8768],
        [-2.0263, -1.5003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16868366301059723
RAW KL tensor(0.2104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 765: train/loss = 0.2686328887939453, train/raw-loss = 0.24307358264923096, train/logprobs = tensor([[-1.2249, -4.9318],
        [-1.7801, -1.7650]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25559312105178833
RAW KL tensor(0.2017, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 766: train/loss = 0.4583030939102173, train/raw-loss = 0.44406789541244507, train/logprobs = tensor([[-1.3985, -3.4446],
        [-1.9885, -2.4858]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14235198497772217
RAW KL tensor(0.1629, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 767: train/loss = 0.5227621793746948, train/raw-loss = 0.5077484846115112, train/logprobs = tensor([[-1.5158, -3.0393],
        [-1.7326, -2.2111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15013660490512848
RAW KL tensor(0.1332, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 768: train/loss = 0.3552696704864502, train/raw-loss = 0.338294118642807, train/logprobs = tensor([[-1.3105, -2.7642],
        [-1.9905, -1.3091]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16975589096546173
RAW KL tensor(0.1706, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 769: train/loss = 0.4034661054611206, train/raw-loss = 0.39033105969429016, train/logprobs = tensor([[-1.2807, -3.1628],
        [-1.6520, -1.7962]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1313508152961731
RAW KL tensor(0.0585, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 770: train/loss = 0.47080063819885254, train/raw-loss = 0.457894504070282, train/logprobs = tensor([[-0.9927, -1.8803],
        [-1.4041, -0.9235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12906141579151154
RAW KL tensor(0.1428, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 771: train/loss = 0.2512664496898651, train/raw-loss = 0.2331829071044922, train/logprobs = tensor([[-1.4909, -4.0037],
        [-1.9665, -1.3423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1808355450630188
RAW KL tensor(0.3069, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 772: train/loss = 0.3213363289833069, train/raw-loss = 0.29843977093696594, train/logprobs = tensor([[-1.6046, -3.4382],
        [-1.8606, -1.0214]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22896558046340942
RAW KL tensor(0.0985, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 773: train/loss = 0.5386357307434082, train/raw-loss = 0.5221558809280396, train/logprobs = tensor([[-2.1888, -2.7066],
        [-1.6943, -1.2902]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16479875147342682
RAW KL tensor(0.1239, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 774: train/loss = 0.3262143135070801, train/raw-loss = 0.3108985126018524, train/logprobs = tensor([[-1.7446, -3.9313],
        [-2.1029, -1.6670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15315788984298706
RAW KL tensor(0.2312, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 775: train/loss = 0.3095296025276184, train/raw-loss = 0.29229292273521423, train/logprobs = tensor([[-1.3357, -4.0128],
        [-1.6871, -1.9492]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1723666489124298
RAW KL tensor(0.0906, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 776: train/loss = 0.2787230610847473, train/raw-loss = 0.2558385133743286, train/logprobs = tensor([[-1.5203, -4.7916],
        [-1.8923, -1.6001]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22884546220302582
RAW KL tensor(0.4209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 777: train/loss = 0.3398659825325012, train/raw-loss = 0.3158576488494873, train/logprobs = tensor([[-1.5646, -4.1821],
        [-1.9941, -1.8517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24008360505104065
RAW KL tensor(0.1910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 778: train/loss = 0.33437150716781616, train/raw-loss = 0.3164673149585724, train/logprobs = tensor([[-1.4382, -3.2593],
        [-1.8521, -1.1899]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1790420413017273
RAW KL tensor(0.1175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 779: train/loss = 0.2879945635795593, train/raw-loss = 0.2684118449687958, train/logprobs = tensor([[-2.0095, -4.0403],
        [-2.1012, -1.2062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1958271861076355
RAW KL tensor(0.1768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 780: train/loss = 0.3587135970592499, train/raw-loss = 0.347034752368927, train/logprobs = tensor([[-1.3593, -2.9560],
        [-1.4822, -1.1424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.116788350045681
RAW KL tensor(0.1409, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 781: train/loss = 0.42473143339157104, train/raw-loss = 0.40633171796798706, train/logprobs = tensor([[-2.0851, -4.0505],
        [-1.7548, -1.5557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18399736285209656
RAW KL tensor(0.2638, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 782: train/loss = 0.2806307077407837, train/raw-loss = 0.2633788585662842, train/logprobs = tensor([[-1.2061, -4.9766],
        [-1.4352, -1.7344]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1725182831287384
RAW KL tensor(0.0929, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 234])
Epoch 0, Step 783: train/loss = 0.4069520831108093, train/raw-loss = 0.3945022225379944, train/logprobs = tensor([[-1.2049, -2.3119],
        [-1.4346, -1.0440]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1244983896613121
RAW KL tensor(0.1223, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 784: train/loss = 0.24543136358261108, train/raw-loss = 0.2239404320716858, train/logprobs = tensor([[-1.1902, -4.2233],
        [-1.7377, -1.8015]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21490925550460815
RAW KL tensor(0.1886, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 785: train/loss = 0.1809215396642685, train/raw-loss = 0.15621958673000336, train/logprobs = tensor([[-1.4506, -5.1910],
        [-1.7284, -1.5670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24701957404613495
RAW KL tensor(0.1935, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 786: train/loss = 0.26854193210601807, train/raw-loss = 0.2435547560453415, train/logprobs = tensor([[-1.4341, -4.4744],
        [-2.0935, -2.0737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2498718649148941
RAW KL tensor(0.2682, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 255])
Epoch 0, Step 787: train/loss = 0.32550132274627686, train/raw-loss = 0.30729246139526367, train/logprobs = tensor([[-1.0633, -3.9955],
        [-1.4909, -1.8064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1820886731147766
RAW KL tensor(0.2024, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 788: train/loss = 0.3609580099582672, train/raw-loss = 0.3433879017829895, train/logprobs = tensor([[-0.8777, -3.6875],
        [-1.1472, -1.6007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17570124566555023
RAW KL tensor(0.1998, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 789: train/loss = 0.43567904829978943, train/raw-loss = 0.41990530490875244, train/logprobs = tensor([[-1.5292, -2.6284],
        [-1.9548, -1.5302]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15773767232894897
RAW KL tensor(0.0783, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 250])
Epoch 0, Step 790: train/loss = 0.3694079518318176, train/raw-loss = 0.3557211756706238, train/logprobs = tensor([[-1.6216, -3.2028],
        [-1.5142, -1.1883]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.136867493391037
RAW KL tensor(0.0827, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 280])
Epoch 0, Step 791: train/loss = 0.4074059724807739, train/raw-loss = 0.3964283764362335, train/logprobs = tensor([[-1.5550, -2.8400],
        [-1.6259, -1.1765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1097760796546936
RAW KL tensor(0.1021, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 792: train/loss = 0.2810317575931549, train/raw-loss = 0.26742395758628845, train/logprobs = tensor([[-1.6937, -3.6121],
        [-2.2031, -1.6023]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13607800006866455
RAW KL tensor(0.2379, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 793: train/loss = 0.23773466050624847, train/raw-loss = 0.21925491094589233, train/logprobs = tensor([[-1.3925, -4.5379],
        [-1.9259, -1.9540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18479734659194946
RAW KL tensor(0.2214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 794: train/loss = 0.34546756744384766, train/raw-loss = 0.33064064383506775, train/logprobs = tensor([[-1.3503, -3.9667],
        [-1.4794, -1.7309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14826950430870056
RAW KL tensor(0.1897, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 795: train/loss = 0.43693679571151733, train/raw-loss = 0.4223858714103699, train/logprobs = tensor([[-1.2904, -3.0802],
        [-1.4672, -1.6971]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.145509272813797
RAW KL tensor(0.0637, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 796: train/loss = 0.34569570422172546, train/raw-loss = 0.33128252625465393, train/logprobs = tensor([[-1.0723, -2.9266],
        [-1.6581, -1.0848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1441315859556198
RAW KL tensor(0.0835, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 797: train/loss = 0.439807265996933, train/raw-loss = 0.4266650378704071, train/logprobs = tensor([[-1.6141, -2.8606],
        [-1.8484, -1.7786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13142237067222595
RAW KL tensor(0.1321, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 798: train/loss = 0.4116307199001312, train/raw-loss = 0.3959519863128662, train/logprobs = tensor([[-1.3955, -2.4382],
        [-1.6846, -1.0923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15678741037845612
RAW KL tensor(0.2119, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 799: train/loss = 0.3246544599533081, train/raw-loss = 0.303955614566803, train/logprobs = tensor([[-1.2130, -4.0679],
        [-1.3371, -1.3863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20698890089988708
RAW KL tensor(0.0982, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 800: train/loss = 0.5249643325805664, train/raw-loss = 0.5101984143257141, train/logprobs = tensor([[-1.5448, -1.9196],
        [-1.8801, -1.1637]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14765915274620056
RAW KL tensor(0.2161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 801: train/loss = 0.1800970435142517, train/raw-loss = 0.16262684762477875, train/logprobs = tensor([[-1.1400, -4.9062],
        [-1.5667, -1.6362]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17470191419124603
RAW KL tensor(0.2657, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 802: train/loss = 0.2792949378490448, train/raw-loss = 0.2576711177825928, train/logprobs = tensor([[-1.7529, -4.2841],
        [-2.3854, -1.8951]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21623842418193817
RAW KL tensor(0.1413, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 803: train/loss = 0.4928175210952759, train/raw-loss = 0.4813883900642395, train/logprobs = tensor([[-0.8859, -2.2570],
        [-1.2818, -1.5349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11429145932197571
RAW KL tensor(0.1802, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 804: train/loss = 0.29880231618881226, train/raw-loss = 0.2800731658935547, train/logprobs = tensor([[-1.1715, -4.1612],
        [-1.7053, -1.6021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18729159235954285
RAW KL tensor(0.1266, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 805: train/loss = 0.3406304121017456, train/raw-loss = 0.3241347074508667, train/logprobs = tensor([[-2.0751, -4.4785],
        [-2.2026, -1.9189]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1649571806192398
RAW KL tensor(0.1029, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 806: train/loss = 0.45697420835494995, train/raw-loss = 0.4385179579257965, train/logprobs = tensor([[-1.6650, -3.9584],
        [-1.6990, -1.4420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18456247448921204
RAW KL tensor(0.1357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 807: train/loss = 0.2782692313194275, train/raw-loss = 0.256378173828125, train/logprobs = tensor([[-1.7361, -4.5797],
        [-2.0423, -1.2651]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21891066431999207
RAW KL tensor(0.1150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 808: train/loss = 0.3694853186607361, train/raw-loss = 0.35079073905944824, train/logprobs = tensor([[-1.7858, -3.6702],
        [-2.0720, -1.8950]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18694543838500977
RAW KL tensor(0.1151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 809: train/loss = 0.36777061223983765, train/raw-loss = 0.35363274812698364, train/logprobs = tensor([[-1.2039, -3.8986],
        [-1.2004, -1.4801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1413787603378296
RAW KL tensor(0.2022, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 810: train/loss = 0.23722273111343384, train/raw-loss = 0.2205968052148819, train/logprobs = tensor([[-0.9517, -4.3956],
        [-1.6519, -1.6165]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1662590652704239
RAW KL tensor(0.2130, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 811: train/loss = 0.14978013932704926, train/raw-loss = 0.12803703546524048, train/logprobs = tensor([[-1.3439, -5.8920],
        [-1.8431, -1.9746]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2174309641122818
RAW KL tensor(0.2006, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 812: train/loss = 0.36149948835372925, train/raw-loss = 0.3348972201347351, train/logprobs = tensor([[-1.9786, -5.0521],
        [-2.1949, -2.8426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.26602277159690857
RAW KL tensor(0.1730, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 813: train/loss = 0.43911468982696533, train/raw-loss = 0.41861796379089355, train/logprobs = tensor([[-1.7779, -4.7356],
        [-1.6846, -2.2906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20496734976768494
RAW KL tensor(0.1081, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 814: train/loss = 0.33558550477027893, train/raw-loss = 0.3204033374786377, train/logprobs = tensor([[-2.0913, -4.2081],
        [-1.8652, -1.4556]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.151821568608284
RAW KL tensor(0.1339, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 815: train/loss = 0.34038129448890686, train/raw-loss = 0.3225821852684021, train/logprobs = tensor([[-1.7174, -4.5053],
        [-1.6050, -1.9644]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17799115180969238
RAW KL tensor(0.2407, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 816: train/loss = 0.3709051012992859, train/raw-loss = 0.3557496666908264, train/logprobs = tensor([[-1.5076, -3.2678],
        [-1.6512, -1.4100]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15155431628227234
RAW KL tensor(0.1060, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 817: train/loss = 0.3044053316116333, train/raw-loss = 0.28626662492752075, train/logprobs = tensor([[-2.0628, -6.0819],
        [-2.2214, -1.0685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18138670921325684
RAW KL tensor(0.0884, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 818: train/loss = 0.30983006954193115, train/raw-loss = 0.29261165857315063, train/logprobs = tensor([[-1.2124, -3.9604],
        [-1.7117, -1.2876]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1721840798854828
RAW KL tensor(0.1762, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 819: train/loss = 0.3565845489501953, train/raw-loss = 0.33528122305870056, train/logprobs = tensor([[-2.2553, -4.5159],
        [-1.7210, -1.4855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21303343772888184
RAW KL tensor(0.1528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 820: train/loss = 0.25611361861228943, train/raw-loss = 0.2355056256055832, train/logprobs = tensor([[-1.3055, -4.4880],
        [-1.7806, -1.7154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20607990026474
RAW KL tensor(0.3813, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 821: train/loss = 0.2752983868122101, train/raw-loss = 0.25467148423194885, train/logprobs = tensor([[-1.8215, -4.4585],
        [-1.8001, -1.7150]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2062690556049347
RAW KL tensor(0.0983, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 822: train/loss = 0.24427849054336548, train/raw-loss = 0.22250762581825256, train/logprobs = tensor([[-1.1772, -4.9537],
        [-1.7147, -1.2355]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21770861744880676
RAW KL tensor(0.2112, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 823: train/loss = 0.5078486204147339, train/raw-loss = 0.49251487851142883, train/logprobs = tensor([[-1.5334, -2.1539],
        [-2.0834, -1.5864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15333737432956696
RAW KL tensor(0.1654, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 824: train/loss = 0.3414757549762726, train/raw-loss = 0.3263773024082184, train/logprobs = tensor([[-1.8456, -4.0282],
        [-1.5101, -0.9226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15098431706428528
RAW KL tensor(0.1085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 825: train/loss = 0.4592377543449402, train/raw-loss = 0.44204720854759216, train/logprobs = tensor([[-1.7750, -2.6091],
        [-1.6163, -1.1252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17190542817115784
RAW KL tensor(0.2303, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 826: train/loss = 0.3018704652786255, train/raw-loss = 0.2867327034473419, train/logprobs = tensor([[-1.8875, -3.7942],
        [-1.7463, -1.2968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15137755870819092
RAW KL tensor(0.2974, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 827: train/loss = 0.2954618036746979, train/raw-loss = 0.2619442939758301, train/logprobs = tensor([[-2.0652, -4.7407],
        [-2.0328, -1.4480]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.33517515659332275
RAW KL tensor(0.2005, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 828: train/loss = 0.2642698287963867, train/raw-loss = 0.24857383966445923, train/logprobs = tensor([[-1.4046, -3.9515],
        [-1.5550, -1.3244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1569599062204361
RAW KL tensor(0.1621, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 829: train/loss = 0.4538307785987854, train/raw-loss = 0.44138452410697937, train/logprobs = tensor([[-1.5999, -2.7271],
        [-1.9842, -1.6864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12446267902851105
RAW KL tensor(0.1801, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 830: train/loss = 0.34416401386260986, train/raw-loss = 0.3318624496459961, train/logprobs = tensor([[-1.1591, -3.6193],
        [-1.5266, -1.3854]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1230158731341362
RAW KL tensor(0.1311, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 831: train/loss = 0.3379743695259094, train/raw-loss = 0.32435131072998047, train/logprobs = tensor([[-2.1095, -3.6791],
        [-1.8674, -1.2153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13623061776161194
RAW KL tensor(0.3546, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 832: train/loss = 0.34469160437583923, train/raw-loss = 0.31565189361572266, train/logprobs = tensor([[-2.8330, -5.1268],
        [-2.0862, -1.6229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.29039710760116577
RAW KL tensor(0.2411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 833: train/loss = 0.2945998013019562, train/raw-loss = 0.2786675691604614, train/logprobs = tensor([[-1.6785, -3.1781],
        [-2.3104, -1.4873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15932244062423706
RAW KL tensor(0.2650, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 834: train/loss = 0.2675554156303406, train/raw-loss = 0.24579107761383057, train/logprobs = tensor([[-1.0921, -4.6227],
        [-1.4862, -1.3739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2176433503627777
RAW KL tensor(0.1371, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 835: train/loss = 0.31104665994644165, train/raw-loss = 0.2949437201023102, train/logprobs = tensor([[-1.4286, -3.6988],
        [-1.6170, -1.3303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16102921962738037
RAW KL tensor(0.1085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 836: train/loss = 0.37588417530059814, train/raw-loss = 0.3620939552783966, train/logprobs = tensor([[-1.7229, -3.2683],
        [-2.0334, -1.6503]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1379024237394333
RAW KL tensor(0.1953, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 837: train/loss = 0.3247068226337433, train/raw-loss = 0.3034423887729645, train/logprobs = tensor([[-1.2049, -3.9553],
        [-1.4874, -1.2646]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21264436841011047
RAW KL tensor(0.2045, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 838: train/loss = 0.17058247327804565, train/raw-loss = 0.14847302436828613, train/logprobs = tensor([[-2.0195, -6.4278],
        [-1.9667, -1.8477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2210945338010788
RAW KL tensor(0.1411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 839: train/loss = 0.6390228867530823, train/raw-loss = 0.6218358874320984, train/logprobs = tensor([[-2.0695, -2.6536],
        [-2.5120, -2.3502]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1718699336051941
RAW KL tensor(0.1437, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 840: train/loss = 0.313339501619339, train/raw-loss = 0.28940439224243164, train/logprobs = tensor([[-2.0989, -6.3194],
        [-2.4242, -1.3915]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2393510937690735
RAW KL tensor(0.2202, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 841: train/loss = 0.20799259841442108, train/raw-loss = 0.18831434845924377, train/logprobs = tensor([[-1.0029, -5.0905],
        [-1.4353, -1.8273]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19678257405757904
RAW KL tensor(0.1906, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 842: train/loss = 0.28168314695358276, train/raw-loss = 0.26339030265808105, train/logprobs = tensor([[-1.8007, -4.3372],
        [-2.0519, -1.8215]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18292826414108276
RAW KL tensor(0.2049, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 384])
Epoch 0, Step 843: train/loss = 0.18719275295734406, train/raw-loss = 0.16795489192008972, train/logprobs = tensor([[-0.8001, -5.3422],
        [-1.4700, -1.8913]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19237872958183289
RAW KL tensor(0.3028, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 844: train/loss = 0.19139760732650757, train/raw-loss = 0.17083901166915894, train/logprobs = tensor([[-1.5330, -4.4365],
        [-1.9331, -1.3800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2055860161781311
RAW KL tensor(0.0894, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 845: train/loss = 0.4350288510322571, train/raw-loss = 0.4152299761772156, train/logprobs = tensor([[-1.5992, -3.2433],
        [-2.3411, -1.2647]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1979885995388031
RAW KL tensor(0.1788, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 846: train/loss = 0.5092285871505737, train/raw-loss = 0.49581944942474365, train/logprobs = tensor([[-1.7617, -2.9624],
        [-1.4211, -1.1200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13409146666526794
RAW KL tensor(0.2560, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 847: train/loss = 0.20183773338794708, train/raw-loss = 0.17974743247032166, train/logprobs = tensor([[-1.4362, -4.4154],
        [-2.0051, -1.3020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2209029495716095
RAW KL tensor(0.1563, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 848: train/loss = 0.3219625949859619, train/raw-loss = 0.31147947907447815, train/logprobs = tensor([[-1.5995, -3.6155],
        [-1.6186, -1.3309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10483140498399734
RAW KL tensor(0.1430, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 849: train/loss = 0.5586669445037842, train/raw-loss = 0.5434812307357788, train/logprobs = tensor([[-1.6956, -2.5637],
        [-1.5773, -1.3432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1518571376800537
RAW KL tensor(0.2472, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 850: train/loss = 0.3088657259941101, train/raw-loss = 0.2861766517162323, train/logprobs = tensor([[-1.9169, -4.5725],
        [-2.2831, -1.5785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22689056396484375
RAW KL tensor(0.3055, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 851: train/loss = 0.511709988117218, train/raw-loss = 0.4975723326206207, train/logprobs = tensor([[-1.5035, -2.5926],
        [-1.3414, -1.0874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14137662947177887
RAW KL tensor(0.2035, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 852: train/loss = 0.25359678268432617, train/raw-loss = 0.2204507291316986, train/logprobs = tensor([[-1.6225, -4.0818],
        [-2.5857, -1.1124]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3314601182937622
RAW KL tensor(0.0492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 853: train/loss = 0.5387206077575684, train/raw-loss = 0.5242699384689331, train/logprobs = tensor([[-1.3195, -1.9452],
        [-1.4417, -1.1372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14450596272945404
RAW KL tensor(0.1554, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 854: train/loss = 0.4030364453792572, train/raw-loss = 0.3838050663471222, train/logprobs = tensor([[-1.4422, -4.1442],
        [-1.4711, -2.0632]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19231398403644562
RAW KL tensor(0.1663, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 855: train/loss = 0.39795035123825073, train/raw-loss = 0.3776545822620392, train/logprobs = tensor([[-1.9172, -3.3771],
        [-2.0907, -1.7868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20295770466327667
RAW KL tensor(0.2650, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 856: train/loss = 0.2994394600391388, train/raw-loss = 0.2807246446609497, train/logprobs = tensor([[-1.4354, -3.8937],
        [-1.8320, -1.6725]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18714827299118042
RAW KL tensor(0.1765, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 857: train/loss = 0.17020657658576965, train/raw-loss = 0.1545332819223404, train/logprobs = tensor([[-1.5640, -5.4064],
        [-2.0128, -1.7538]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15673300623893738
RAW KL tensor(0.1078, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 858: train/loss = 0.47256457805633545, train/raw-loss = 0.44398635625839233, train/logprobs = tensor([[-2.0758, -3.9969],
        [-1.7359, -1.5525]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2857820987701416
RAW KL tensor(0.2369, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 859: train/loss = 0.4315447509288788, train/raw-loss = 0.4100528359413147, train/logprobs = tensor([[-1.9463, -5.7081],
        [-1.5697, -2.3758]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21491925418376923
RAW KL tensor(0.0351, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 860: train/loss = 0.4893609285354614, train/raw-loss = 0.4809325635433197, train/logprobs = tensor([[-1.0402, -1.5862],
        [-1.5292, -0.9577]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08428400754928589
RAW KL tensor(0.2168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 861: train/loss = 0.29355674982070923, train/raw-loss = 0.2719134986400604, train/logprobs = tensor([[-1.2651, -4.3058],
        [-1.5919, -1.2753]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2164323925971985
RAW KL tensor(0.1299, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 862: train/loss = 0.38967299461364746, train/raw-loss = 0.37767887115478516, train/logprobs = tensor([[-1.5214, -3.1545],
        [-1.8718, -1.5210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11994102597236633
RAW KL tensor(0.1209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 863: train/loss = 0.28078266978263855, train/raw-loss = 0.2631857991218567, train/logprobs = tensor([[-1.6846, -4.8122],
        [-1.5107, -1.6259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17596888542175293
RAW KL tensor(0.3262, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 864: train/loss = 0.3625192642211914, train/raw-loss = 0.34132155776023865, train/logprobs = tensor([[-1.7480, -3.2344],
        [-2.0587, -1.4540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21197724342346191
RAW KL tensor(0.1143, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 865: train/loss = 0.35666903853416443, train/raw-loss = 0.33614271879196167, train/logprobs = tensor([[-1.3221, -4.5198],
        [-1.6717, -1.4849]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2052634060382843
RAW KL tensor(0.1895, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 866: train/loss = 0.297344833612442, train/raw-loss = 0.27807313203811646, train/logprobs = tensor([[-1.9665, -3.6880],
        [-2.3597, -1.6541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19271712005138397
RAW KL tensor(0.2085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 867: train/loss = 0.21973265707492828, train/raw-loss = 0.2011060267686844, train/logprobs = tensor([[-2.0689, -4.5385],
        [-1.9760, -0.9360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18626654148101807
RAW KL tensor(0.1022, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 868: train/loss = 0.3726879954338074, train/raw-loss = 0.34983235597610474, train/logprobs = tensor([[-1.3979, -3.8229],
        [-1.6861, -1.4033]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2285565882921219
RAW KL tensor(0.2307, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 869: train/loss = 0.14915591478347778, train/raw-loss = 0.12421423196792603, train/logprobs = tensor([[-1.4201, -6.0331],
        [-1.8294, -1.6428]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24941681325435638
RAW KL tensor(0.0972, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 437])
Epoch 0, Step 870: train/loss = 0.4205760359764099, train/raw-loss = 0.39223843812942505, train/logprobs = tensor([[-2.4594, -3.5846],
        [-2.2974, -1.0404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.28337612748146057
RAW KL tensor(0.1162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 871: train/loss = 0.46900081634521484, train/raw-loss = 0.45508214831352234, train/logprobs = tensor([[-2.1097, -2.2512],
        [-2.0985, -1.0804]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1391867697238922
RAW KL tensor(0.1862, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 464])
Epoch 0, Step 872: train/loss = 0.309791624546051, train/raw-loss = 0.295672208070755, train/logprobs = tensor([[-0.9178, -4.8496],
        [-1.1432, -1.8906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14119404554367065
RAW KL tensor(0.1706, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 873: train/loss = 0.20806503295898438, train/raw-loss = 0.1861306130886078, train/logprobs = tensor([[-1.6403, -5.1511],
        [-1.9465, -2.0387]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21934396028518677
RAW KL tensor(0.1475, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 874: train/loss = 0.4477010667324066, train/raw-loss = 0.42540857195854187, train/logprobs = tensor([[-1.3406, -2.6305],
        [-1.7585, -1.6022]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22292494773864746
RAW KL tensor(0.1159, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 875: train/loss = 0.3327098488807678, train/raw-loss = 0.3157574236392975, train/logprobs = tensor([[-1.5155, -3.4050],
        [-1.8697, -1.2818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16952407360076904
RAW KL tensor(0.2647, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 876: train/loss = 0.12739300727844238, train/raw-loss = 0.10620559751987457, train/logprobs = tensor([[-2.1740, -5.7984],
        [-2.3868, -1.1381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21187396347522736
RAW KL tensor(0.1459, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 877: train/loss = 0.4919828176498413, train/raw-loss = 0.47469204664230347, train/logprobs = tensor([[-1.1592, -2.0611],
        [-1.4816, -1.2786]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17290757596492767
RAW KL tensor(0.2573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 878: train/loss = 0.4828675389289856, train/raw-loss = 0.4650574326515198, train/logprobs = tensor([[-1.9305, -2.7352],
        [-2.0817, -1.4970]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1781013011932373
RAW KL tensor(0.1516, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 253])
Epoch 0, Step 879: train/loss = 0.30833199620246887, train/raw-loss = 0.291080117225647, train/logprobs = tensor([[-0.9033, -3.1502],
        [-1.8491, -1.4349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17251893877983093
RAW KL tensor(0.1358, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 880: train/loss = 0.3107879161834717, train/raw-loss = 0.29831820726394653, train/logprobs = tensor([[-1.7854, -4.3411],
        [-1.9422, -2.0025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12469740211963654
RAW KL tensor(0.1162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 881: train/loss = 0.31481945514678955, train/raw-loss = 0.3005185127258301, train/logprobs = tensor([[-1.0166, -4.2023],
        [-1.3048, -1.0904]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14300945401191711
RAW KL tensor(0.0852, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 882: train/loss = 0.30749017000198364, train/raw-loss = 0.2760428190231323, train/logprobs = tensor([[-1.9289, -7.5444],
        [-2.0966, -1.6462]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.31447356939315796
RAW KL tensor(0.2176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 883: train/loss = 0.2208474576473236, train/raw-loss = 0.20069162547588348, train/logprobs = tensor([[-1.0097, -5.4523],
        [-1.6005, -1.9760]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20155836641788483
RAW KL tensor(0.2012, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 117])
Epoch 0, Step 884: train/loss = 0.4243316054344177, train/raw-loss = 0.4124225378036499, train/logprobs = tensor([[-1.2783, -2.8533],
        [-1.0805, -1.1313]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11909037828445435
RAW KL tensor(0.2214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 885: train/loss = 0.29552894830703735, train/raw-loss = 0.2785961925983429, train/logprobs = tensor([[-1.1943, -3.9818],
        [-1.7095, -1.2875]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16932758688926697
RAW KL tensor(0.1624, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 886: train/loss = 0.3573225736618042, train/raw-loss = 0.33361390233039856, train/logprobs = tensor([[-1.5696, -4.3779],
        [-1.8927, -0.9576]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2370869517326355
RAW KL tensor(0.0949, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 887: train/loss = 0.4238726496696472, train/raw-loss = 0.4008752703666687, train/logprobs = tensor([[-2.6869, -4.4706],
        [-2.2460, -1.8505]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22997383773326874
RAW KL tensor(0.1441, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 888: train/loss = 0.46878910064697266, train/raw-loss = 0.45515257120132446, train/logprobs = tensor([[-1.1057, -2.3021],
        [-1.5056, -1.3982]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13636526465415955
RAW KL tensor(0.1358, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 889: train/loss = 0.20398461818695068, train/raw-loss = 0.17855292558670044, train/logprobs = tensor([[-1.6662, -5.3780],
        [-2.1340, -1.6598]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25431689620018005
RAW KL tensor(0.1801, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 890: train/loss = 0.2720797061920166, train/raw-loss = 0.2547544836997986, train/logprobs = tensor([[-2.2791, -5.3806],
        [-2.4001, -1.3036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17325210571289062
RAW KL tensor(0.2257, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 891: train/loss = 0.21011710166931152, train/raw-loss = 0.18673039972782135, train/logprobs = tensor([[-1.5869, -4.5043],
        [-1.6636, -1.0515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23386724293231964
RAW KL tensor(0.2173, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 892: train/loss = 0.4085844159126282, train/raw-loss = 0.3877309560775757, train/logprobs = tensor([[-1.9231, -3.3348],
        [-1.8048, -1.0607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20853450894355774
RAW KL tensor(0.1286, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 893: train/loss = 0.3872653841972351, train/raw-loss = 0.3700200915336609, train/logprobs = tensor([[-1.4076, -3.2478],
        [-2.1316, -1.4581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17245244979858398
RAW KL tensor(0.1644, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 894: train/loss = 0.38415858149528503, train/raw-loss = 0.36274564266204834, train/logprobs = tensor([[-1.8317, -3.9577],
        [-1.5985, -1.6561]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21412917971611023
RAW KL tensor(0.1357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 895: train/loss = 0.2953763008117676, train/raw-loss = 0.2666996121406555, train/logprobs = tensor([[-1.6006, -4.1556],
        [-1.7934, -1.2050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.28676676750183105
RAW KL tensor(0.2512, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 896: train/loss = 0.14716194570064545, train/raw-loss = 0.12196943908929825, train/logprobs = tensor([[-1.7811, -6.3533],
        [-2.4568, -1.9451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.25192514061927795
RAW KL tensor(0.2941, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 897: train/loss = 0.2257486879825592, train/raw-loss = 0.2051076740026474, train/logprobs = tensor([[-1.4598, -4.8606],
        [-2.0174, -1.1211]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20641033351421356
RAW KL tensor(0.2124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 898: train/loss = 0.3700537383556366, train/raw-loss = 0.3496733009815216, train/logprobs = tensor([[-2.6087, -6.2720],
        [-1.4662, -1.3973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2038043886423111
RAW KL tensor(0.1635, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 899: train/loss = 0.2969433665275574, train/raw-loss = 0.27596133947372437, train/logprobs = tensor([[-1.8410, -5.8481],
        [-1.9399, -1.1396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20982030034065247
RAW KL tensor(0.2234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 900: train/loss = 0.13387030363082886, train/raw-loss = 0.11529678851366043, train/logprobs = tensor([[-1.7882, -6.0771],
        [-2.6077, -1.1726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18573527038097382
RAW KL tensor(0.2404, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 901: train/loss = 0.14065402746200562, train/raw-loss = 0.11809183657169342, train/logprobs = tensor([[-2.1737, -5.7555],
        [-2.2722, -1.2114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22562193870544434
RAW KL tensor(0.3424, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 902: train/loss = 0.3023778796195984, train/raw-loss = 0.28307288885116577, train/logprobs = tensor([[-1.0744, -3.9268],
        [-1.6860, -1.4474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19304966926574707
RAW KL tensor(0.1479, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 903: train/loss = 0.30010107159614563, train/raw-loss = 0.2799307107925415, train/logprobs = tensor([[-1.4671, -4.6507],
        [-1.8490, -1.7672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20170357823371887
RAW KL tensor(0.1573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 317])
Epoch 0, Step 904: train/loss = 0.33714184165000916, train/raw-loss = 0.3212738037109375, train/logprobs = tensor([[-1.2548, -2.9879],
        [-1.6266, -1.0630]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15868030488491058
RAW KL tensor(0.1021, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 905: train/loss = 0.48460665345191956, train/raw-loss = 0.4669071137905121, train/logprobs = tensor([[-1.8289, -3.6893],
        [-1.5080, -1.4511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17699532210826874
RAW KL tensor(0.2474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 906: train/loss = 0.24528300762176514, train/raw-loss = 0.22231578826904297, train/logprobs = tensor([[-1.5677, -5.8881],
        [-1.8184, -1.7763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22967228293418884
RAW KL tensor(0.1919, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 907: train/loss = 0.16462208330631256, train/raw-loss = 0.1453457921743393, train/logprobs = tensor([[-1.7265, -5.2102],
        [-2.0489, -1.5174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19276303052902222
RAW KL tensor(0.2477, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 908: train/loss = 0.35671761631965637, train/raw-loss = 0.33679625391960144, train/logprobs = tensor([[-2.2498, -3.5508],
        [-2.2140, -1.4616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19921374320983887
RAW KL tensor(0.1736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 909: train/loss = 0.2999933660030365, train/raw-loss = 0.28350919485092163, train/logprobs = tensor([[-1.1235, -2.9341],
        [-1.8557, -1.1890]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16484174132347107
RAW KL tensor(0.1417, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 910: train/loss = 0.2834544777870178, train/raw-loss = 0.26497548818588257, train/logprobs = tensor([[-1.6751, -4.1188],
        [-1.8817, -1.4243]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18478989601135254
RAW KL tensor(0.1790, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 911: train/loss = 0.25950661301612854, train/raw-loss = 0.23772047460079193, train/logprobs = tensor([[-2.6882, -5.4131],
        [-2.5118, -1.8965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21786139905452728
RAW KL tensor(0.1069, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 912: train/loss = 0.41663676500320435, train/raw-loss = 0.39598745107650757, train/logprobs = tensor([[-2.5169, -4.1407],
        [-1.7507, -0.9207]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2064928263425827
RAW KL tensor(0.2116, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 913: train/loss = 0.362121045589447, train/raw-loss = 0.34473395347595215, train/logprobs = tensor([[-1.3331, -3.7435],
        [-1.7189, -1.4519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1738709956407547
RAW KL tensor(0.2119, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 914: train/loss = 0.3370289206504822, train/raw-loss = 0.3211151361465454, train/logprobs = tensor([[-1.4795, -3.2574],
        [-2.0684, -1.5855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15913815796375275
RAW KL tensor(0.0926, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 441])
Epoch 0, Step 915: train/loss = 0.4724113345146179, train/raw-loss = 0.4548671245574951, train/logprobs = tensor([[-1.2727, -2.3469],
        [-1.3992, -1.1694]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17544223368167877
RAW KL tensor(0.2998, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 916: train/loss = 0.25654107332229614, train/raw-loss = 0.22161361575126648, train/logprobs = tensor([[-2.2215, -6.0557],
        [-2.6173, -1.7229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.34927457571029663
RAW KL tensor(0.3272, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 917: train/loss = 0.19835400581359863, train/raw-loss = 0.1802842766046524, train/logprobs = tensor([[-1.5694, -4.0558],
        [-2.0227, -1.0135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18069744110107422
RAW KL tensor(0.0955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 255])
Epoch 0, Step 918: train/loss = 0.2840096354484558, train/raw-loss = 0.26192155480384827, train/logprobs = tensor([[-1.7811, -4.7311],
        [-2.2372, -1.4775]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2208806872367859
RAW KL tensor(0.2265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 919: train/loss = 0.23387180268764496, train/raw-loss = 0.21506540477275848, train/logprobs = tensor([[-1.0310, -4.7764],
        [-1.9467, -1.8689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18806418776512146
RAW KL tensor(0.1930, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 920: train/loss = 0.2481936812400818, train/raw-loss = 0.23382560908794403, train/logprobs = tensor([[-1.9943, -7.7745],
        [-2.4177, -1.6465]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14368094503879547
RAW KL tensor(0.1824, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 921: train/loss = 0.2612721920013428, train/raw-loss = 0.2395295798778534, train/logprobs = tensor([[-1.5705, -3.9667],
        [-2.2371, -1.2272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21742625534534454
RAW KL tensor(0.1810, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 922: train/loss = 0.3776143193244934, train/raw-loss = 0.35830846428871155, train/logprobs = tensor([[-1.6618, -3.4243],
        [-1.7098, -1.4640]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19305838644504547
RAW KL tensor(0.2637, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 923: train/loss = 0.2972604036331177, train/raw-loss = 0.27886074781417847, train/logprobs = tensor([[-2.6146, -5.1210],
        [-2.2696, -1.4640]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1839970052242279
RAW KL tensor(0.1569, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 924: train/loss = 0.32961422204971313, train/raw-loss = 0.3140970468521118, train/logprobs = tensor([[-1.3526, -4.1572],
        [-1.7549, -1.7340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15517204999923706
RAW KL tensor(0.2326, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 925: train/loss = 0.5578473806381226, train/raw-loss = 0.5404775142669678, train/logprobs = tensor([[-1.2677, -2.2179],
        [-1.6343, -1.4209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1736988127231598
RAW KL tensor(0.3147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 926: train/loss = 0.2626112997531891, train/raw-loss = 0.24344787001609802, train/logprobs = tensor([[-1.6475, -5.8860],
        [-1.4346, -1.4471]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1916341781616211
RAW KL tensor(0.2226, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 927: train/loss = 0.22988800704479218, train/raw-loss = 0.20723530650138855, train/logprobs = tensor([[-1.5330, -4.0492],
        [-2.3956, -1.2225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22652706503868103
RAW KL tensor(0.2259, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 928: train/loss = 0.2961006760597229, train/raw-loss = 0.27746909856796265, train/logprobs = tensor([[-1.9373, -4.2373],
        [-2.6266, -1.7446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18631568551063538
RAW KL tensor(0.1499, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 387])
Epoch 0, Step 929: train/loss = 0.44163778424263, train/raw-loss = 0.42884907126426697, train/logprobs = tensor([[-0.7005, -2.6055],
        [-1.3333, -1.4814]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12788726389408112
RAW KL tensor(0.2866, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 930: train/loss = 0.3454357087612152, train/raw-loss = 0.3237203359603882, train/logprobs = tensor([[-1.5717, -4.2117],
        [-1.8812, -1.7457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21715393662452698
RAW KL tensor(0.1132, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 267])
Epoch 0, Step 931: train/loss = 0.39249667525291443, train/raw-loss = 0.37447816133499146, train/logprobs = tensor([[-0.9265, -3.9591],
        [-1.4023, -1.2410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1801849901676178
RAW KL tensor(0.1971, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 932: train/loss = 0.2940882742404938, train/raw-loss = 0.27320754528045654, train/logprobs = tensor([[-2.2606, -5.0808],
        [-2.3966, -2.0322]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20880736410617828
RAW KL tensor(0.1235, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 933: train/loss = 0.5250729918479919, train/raw-loss = 0.5108802318572998, train/logprobs = tensor([[-1.5891, -2.8896],
        [-1.4017, -1.3712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14192822575569153
RAW KL tensor(0.1871, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 934: train/loss = 0.1507735699415207, train/raw-loss = 0.12728360295295715, train/logprobs = tensor([[-1.7309, -7.1023],
        [-1.9029, -1.1340]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2348996102809906
RAW KL tensor(0.1951, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 935: train/loss = 0.4293345808982849, train/raw-loss = 0.40443724393844604, train/logprobs = tensor([[-2.6157, -5.3121],
        [-1.6111, -1.2628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2489730566740036
RAW KL tensor(0.1422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 936: train/loss = 0.3631299138069153, train/raw-loss = 0.3496894836425781, train/logprobs = tensor([[-1.0554, -2.4585],
        [-1.5831, -1.0037]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13440433144569397
RAW KL tensor(0.2190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 937: train/loss = 0.38561105728149414, train/raw-loss = 0.36809495091438293, train/logprobs = tensor([[-2.2138, -4.2655],
        [-1.8372, -1.7765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17516154050827026
RAW KL tensor(0.2595, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 938: train/loss = 0.4610033631324768, train/raw-loss = 0.4386101961135864, train/logprobs = tensor([[-2.9293, -3.5810],
        [-2.9146, -1.1735]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2239314764738083
RAW KL tensor(0.1085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 419])
Epoch 0, Step 939: train/loss = 0.638911247253418, train/raw-loss = 0.6240327954292297, train/logprobs = tensor([[-1.1803, -1.3899],
        [-1.5895, -1.4493]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.148784339427948
RAW KL tensor(0.1418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 940: train/loss = 0.31369635462760925, train/raw-loss = 0.2914598286151886, train/logprobs = tensor([[-1.4564, -3.8677],
        [-2.1445, -1.6652]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22236549854278564
RAW KL tensor(0.1771, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 941: train/loss = 0.336883008480072, train/raw-loss = 0.31423404812812805, train/logprobs = tensor([[-1.9557, -4.2914],
        [-2.1797, -2.0641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22648964822292328
RAW KL tensor(0.1862, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 942: train/loss = 0.27353978157043457, train/raw-loss = 0.2565159797668457, train/logprobs = tensor([[-1.0454, -4.4207],
        [-1.1860, -0.7231]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.17023812234401703
RAW KL tensor(0.2328, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 943: train/loss = 0.28970760107040405, train/raw-loss = 0.2681610882282257, train/logprobs = tensor([[-1.7425, -4.3719],
        [-1.8979, -1.1700]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21546481549739838
RAW KL tensor(0.1768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 944: train/loss = 0.27118635177612305, train/raw-loss = 0.2472802996635437, train/logprobs = tensor([[-1.4162, -5.2197],
        [-1.8067, -1.3777]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23906061053276062
RAW KL tensor(0.3584, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 945: train/loss = 0.3056645691394806, train/raw-loss = 0.2831936180591583, train/logprobs = tensor([[-1.7174, -4.5466],
        [-2.1210, -1.8330]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22470957040786743
RAW KL tensor(0.1984, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 946: train/loss = 0.279310405254364, train/raw-loss = 0.25756147503852844, train/logprobs = tensor([[-1.1653, -3.9417],
        [-1.5105, -1.3555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21748939156532288
RAW KL tensor(0.2414, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 947: train/loss = 0.184082493185997, train/raw-loss = 0.16165991127490997, train/logprobs = tensor([[-1.6372, -5.2735],
        [-2.0308, -1.0776]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22422581911087036
RAW KL tensor(0.1714, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 948: train/loss = 0.34564903378486633, train/raw-loss = 0.3301808834075928, train/logprobs = tensor([[-1.0400, -4.5091],
        [-1.8168, -1.7328]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15468141436576843
RAW KL tensor(0.2424, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 949: train/loss = 0.22835727035999298, train/raw-loss = 0.2011033147573471, train/logprobs = tensor([[-2.7803, -6.2957],
        [-2.8382, -2.1183]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.27253979444503784
RAW KL tensor(0.1269, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 950: train/loss = 0.16718733310699463, train/raw-loss = 0.14686360955238342, train/logprobs = tensor([[-1.5378, -5.5613],
        [-1.7880, -0.8370]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20323732495307922
RAW KL tensor(0.2774, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 951: train/loss = 0.21057483553886414, train/raw-loss = 0.18799154460430145, train/logprobs = tensor([[-1.6265, -4.7539],
        [-1.8915, -1.3975]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2258329689502716
RAW KL tensor(0.5088, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 952: train/loss = 0.35662421584129333, train/raw-loss = 0.3276066482067108, train/logprobs = tensor([[-1.8488, -5.6411],
        [-2.1206, -2.2546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2901756763458252
RAW KL tensor(0.1572, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 953: train/loss = 0.5556027293205261, train/raw-loss = 0.5264533758163452, train/logprobs = tensor([[-2.1587, -3.0124],
        [-2.0253, -1.6691]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.29149365425109863
RAW KL tensor(0.1333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 954: train/loss = 0.32969123125076294, train/raw-loss = 0.31653499603271484, train/logprobs = tensor([[-1.9932, -4.0617],
        [-2.2517, -1.8857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1315622329711914
RAW KL tensor(0.1246, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 494])
Epoch 0, Step 955: train/loss = 0.18467079102993011, train/raw-loss = 0.16222485899925232, train/logprobs = tensor([[-1.2242, -5.4030],
        [-1.5507, -1.1198]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22445937991142273
RAW KL tensor(0.2199, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 956: train/loss = 0.19330057501792908, train/raw-loss = 0.16544823348522186, train/logprobs = tensor([[-1.8458, -7.0736],
        [-2.2485, -2.4861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2785235047340393
RAW KL tensor(0.1967, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 957: train/loss = 0.24770376086235046, train/raw-loss = 0.22058486938476562, train/logprobs = tensor([[-1.9669, -5.4439],
        [-1.8028, -1.0338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2711888551712036
RAW KL tensor(0.1186, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 958: train/loss = 0.3864097595214844, train/raw-loss = 0.3701880872249603, train/logprobs = tensor([[-1.9517, -3.4009],
        [-1.7732, -1.1580]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16221672296524048
RAW KL tensor(0.1941, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 959: train/loss = 0.1959039866924286, train/raw-loss = 0.17385181784629822, train/logprobs = tensor([[-1.7710, -5.4867],
        [-1.7694, -0.7616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22052165865898132
RAW KL tensor(0.1909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 455])
Epoch 0, Step 960: train/loss = 0.4666471779346466, train/raw-loss = 0.4476650059223175, train/logprobs = tensor([[-1.5363, -4.8435],
        [-0.9722, -1.5126]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1898217499256134
RAW KL tensor(0.2218, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 961: train/loss = 0.26419588923454285, train/raw-loss = 0.23605738580226898, train/logprobs = tensor([[-3.4029, -6.5508],
        [-2.0065, -1.0759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2813848853111267
RAW KL tensor(0.5734, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 962: train/loss = 0.17724435031414032, train/raw-loss = 0.14542128145694733, train/logprobs = tensor([[-2.0952, -6.0731],
        [-2.1323, -0.9143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.31823059916496277
RAW KL tensor(0.2345, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 963: train/loss = 0.46153637766838074, train/raw-loss = 0.4395384192466736, train/logprobs = tensor([[-2.2013, -3.5273],
        [-2.2634, -1.7214]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21997961401939392
RAW KL tensor(0.2872, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 964: train/loss = 0.2978549301624298, train/raw-loss = 0.27455994486808777, train/logprobs = tensor([[-1.6462, -4.3755],
        [-2.0982, -1.6397]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.23294979333877563
RAW KL tensor(0.3418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 965: train/loss = 0.10506126284599304, train/raw-loss = 0.07250726222991943, train/logprobs = tensor([[-1.9635, -7.9732],
        [-2.3481, -1.4278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.32554003596305847
RAW KL tensor(0.1652, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 966: train/loss = 0.37506920099258423, train/raw-loss = 0.3560594320297241, train/logprobs = tensor([[-2.2330, -3.9504],
        [-2.4434, -1.8691]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19009768962860107
RAW KL tensor(0.2372, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 967: train/loss = 0.23130539059638977, train/raw-loss = 0.2105274349451065, train/logprobs = tensor([[-1.4859, -4.8824],
        [-1.5569, -1.1717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.20777952671051025
RAW KL tensor(0.2144, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 968: train/loss = 0.4790576696395874, train/raw-loss = 0.45803195238113403, train/logprobs = tensor([[-2.3756, -3.3072],
        [-2.4169, -1.6427]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.21025723218917847
RAW KL tensor(0.0933, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 246])
Epoch 0, Step 969: train/loss = 0.39774438738822937, train/raw-loss = 0.3786405920982361, train/logprobs = tensor([[-1.2408, -3.6627],
        [-1.6206, -1.4517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19103801250457764
RAW KL tensor(0.2181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 970: train/loss = 0.08880443125963211, train/raw-loss = 0.0634833350777626, train/logprobs = tensor([[-1.5426, -7.3753],
        [-1.9713, -1.0307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2532109320163727
RAW KL tensor(0.1413, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 971: train/loss = 0.2741568386554718, train/raw-loss = 0.2518136203289032, train/logprobs = tensor([[-1.3799, -4.6571],
        [-1.8234, -1.0402]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.22343218326568604
RAW KL tensor(0.1632, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 262])
Epoch 0, Step 972: train/loss = 0.22286462783813477, train/raw-loss = 0.19221964478492737, train/logprobs = tensor([[-2.5058, -6.3064],
        [-2.3516, -1.3477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3064497709274292
RAW KL tensor(0.2250, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 973: train/loss = 0.25867652893066406, train/raw-loss = 0.2407347857952118, train/logprobs = tensor([[-1.8031, -4.9920],
        [-1.8997, -1.2125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1794174313545227
RAW KL tensor(0.2217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
[2024-02-26 18:28:17,322][root][INFO] - beta: 0.5
[2024-02-26 18:28:17,322][root][INFO] - loss with_labels
[2024-02-26 18:28:17,322][root][INFO] - max_iter: 0
[2024-02-26 18:28:17,322][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
RAW KL tensor(0.0989, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 0: train/loss = 0.7299500703811646, train/raw-loss = 0.700703501701355, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05849311128258705
RAW KL tensor(0.0533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 1: train/loss = 0.7100189328193665, train/raw-loss = 0.6903213262557983, train/logprobs = tensor([[-1.2331, -1.8702],
        [-1.2290, -1.8384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039395272731781006
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 2: train/loss = 0.7352927923202515, train/raw-loss = 0.6985180377960205, train/logprobs = tensor([[-1.1089, -1.9215],
        [-1.0918, -1.9190]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07354964315891266
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 3: train/loss = 0.6991938352584839, train/raw-loss = 0.6862989068031311, train/logprobs = tensor([[-0.5922, -1.4420],
        [-0.5997, -1.4088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025789767503738403
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 4: train/loss = 0.7582293152809143, train/raw-loss = 0.7163535952568054, train/logprobs = tensor([[-0.9912, -2.3954],
        [-0.9972, -2.4071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08375133574008942
RAW KL tensor(0.2229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 5: train/loss = 0.7380828261375427, train/raw-loss = 0.6991856098175049, train/logprobs = tensor([[-1.2526, -1.8470],
        [-1.2661, -1.8698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07779429852962494
RAW KL tensor(0.0653, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 6: train/loss = 0.71322101354599, train/raw-loss = 0.6907837390899658, train/logprobs = tensor([[-0.9823, -1.5991],
        [-1.0011, -1.5974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04487462714314461
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 7: train/loss = 0.7328072786331177, train/raw-loss = 0.6982183456420898, train/logprobs = tensor([[-1.1515, -1.6502],
        [-1.1941, -1.7062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06917789578437805
RAW KL tensor(0.0341, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 8: train/loss = 0.716881275177002, train/raw-loss = 0.69573575258255, train/logprobs = tensor([[-0.8533, -2.0334],
        [-0.8402, -2.0024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042290978133678436
RAW KL tensor(0.0381, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 9: train/loss = 0.7058921456336975, train/raw-loss = 0.6875514984130859, train/logprobs = tensor([[-0.6628, -1.6327],
        [-0.6530, -1.5866]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036681272089481354
RAW KL tensor(0.2401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 10: train/loss = 0.7420194149017334, train/raw-loss = 0.6977429389953613, train/logprobs = tensor([[-1.1884, -2.1423],
        [-1.2507, -2.1329]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08855302631855011
RAW KL tensor(0.0386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 11: train/loss = 0.7059262990951538, train/raw-loss = 0.6897475719451904, train/logprobs = tensor([[-0.5671, -1.4607],
        [-0.5399, -1.4123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0323573499917984
RAW KL tensor(0.0910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 12: train/loss = 0.7407004833221436, train/raw-loss = 0.7148674726486206, train/logprobs = tensor([[-0.7433, -1.8789],
        [-0.7825, -1.8867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05166593939065933
RAW KL tensor(0.0755, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 13: train/loss = 0.7082057595252991, train/raw-loss = 0.6869643926620483, train/logprobs = tensor([[-0.7519, -1.6611],
        [-0.7745, -1.6382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04248277097940445
RAW KL tensor(0.0711, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 14: train/loss = 0.7113256454467773, train/raw-loss = 0.684917688369751, train/logprobs = tensor([[-1.1222, -1.7514],
        [-1.1230, -1.7108]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05281598120927811
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 15: train/loss = 0.7176856994628906, train/raw-loss = 0.7035808563232422, train/logprobs = tensor([[-1.0901, -1.5558],
        [-1.0901, -1.5823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02820965088903904
RAW KL tensor(0.2291, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 16: train/loss = 0.7545833587646484, train/raw-loss = 0.7150740623474121, train/logprobs = tensor([[-0.9096, -1.7540],
        [-0.8979, -1.7117]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07901863753795624
RAW KL tensor(0.0085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 17: train/loss = 0.7108995318412781, train/raw-loss = 0.6927703022956848, train/logprobs = tensor([[-1.0581, -1.0386],
        [-1.0542, -1.0246]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03625836595892906
RAW KL tensor(0.0882, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 18: train/loss = 0.7229589819908142, train/raw-loss = 0.6956425309181213, train/logprobs = tensor([[-1.2519, -1.9719],
        [-1.2495, -1.9626]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05463284254074097
RAW KL tensor(0.0408, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 19: train/loss = 0.7163711786270142, train/raw-loss = 0.6946158409118652, train/logprobs = tensor([[-0.7459, -1.6709],
        [-0.7437, -1.6565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04351072385907173
RAW KL tensor(0.0451, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 20: train/loss = 0.7815424799919128, train/raw-loss = 0.751427173614502, train/logprobs = tensor([[-0.6982, -1.6722],
        [-0.7384, -1.6710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0602305606007576
RAW KL tensor(0.0190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 21: train/loss = 0.7129022479057312, train/raw-loss = 0.690139651298523, train/logprobs = tensor([[-0.8415, -1.0015],
        [-0.8475, -0.9916]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04552532359957695
RAW KL tensor(0.0285, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 22: train/loss = 0.7130492925643921, train/raw-loss = 0.6880555152893066, train/logprobs = tensor([[-1.1048, -1.2115],
        [-1.1166, -1.1903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049987539649009705
RAW KL tensor(0.1180, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 23: train/loss = 0.7660366296768188, train/raw-loss = 0.7249387502670288, train/logprobs = tensor([[-1.1101, -1.9440],
        [-1.1279, -2.0090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08219575881958008
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 24: train/loss = 0.7075904011726379, train/raw-loss = 0.6900442838668823, train/logprobs = tensor([[-0.9510, -2.1814],
        [-0.9490, -2.1607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03509220853447914
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 25: train/loss = 0.7088181972503662, train/raw-loss = 0.6901594996452332, train/logprobs = tensor([[-1.1993, -2.4077],
        [-1.2073, -2.3693]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03731732815504074
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 26: train/loss = 0.7106857299804688, train/raw-loss = 0.6981753706932068, train/logprobs = tensor([[-0.5759, -2.1066],
        [-0.6087, -2.1473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025020746514201164
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 27: train/loss = 0.7198241353034973, train/raw-loss = 0.6973809003829956, train/logprobs = tensor([[-1.0402, -2.5564],
        [-1.0804, -2.5702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04488641396164894
RAW KL tensor(0.0764, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 28: train/loss = 0.7584951519966125, train/raw-loss = 0.6961343288421631, train/logprobs = tensor([[-0.8319, -1.5135],
        [-0.8387, -1.4780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12472169101238251
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 29: train/loss = 0.7041295170783997, train/raw-loss = 0.6934842467308044, train/logprobs = tensor([[-0.5879, -1.8914],
        [-0.5988, -1.9005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021290477365255356
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 219])
Epoch 0, Step 30: train/loss = 0.7078800201416016, train/raw-loss = 0.6918355822563171, train/logprobs = tensor([[-0.8561, -1.1920],
        [-0.8388, -1.1585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03208891302347183
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 31: train/loss = 0.7169345617294312, train/raw-loss = 0.6783269047737122, train/logprobs = tensor([[-1.1475, -2.6249],
        [-1.1880, -2.5630]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07721538096666336
RAW KL tensor(0.0422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 32: train/loss = 0.7316939830780029, train/raw-loss = 0.6982897520065308, train/logprobs = tensor([[-0.8837, -2.1160],
        [-0.8833, -2.1158]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06680852174758911
RAW KL tensor(0.0468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 33: train/loss = 0.7193527221679688, train/raw-loss = 0.6995583772659302, train/logprobs = tensor([[-0.7489, -1.6609],
        [-0.7399, -1.6582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039588652551174164
RAW KL tensor(0.0315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 34: train/loss = 0.7200644016265869, train/raw-loss = 0.6958221793174744, train/logprobs = tensor([[-1.0511, -1.5128],
        [-1.0826, -1.5036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04848434031009674
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 35: train/loss = 0.7332174777984619, train/raw-loss = 0.7038596868515015, train/logprobs = tensor([[-0.9902, -1.8427],
        [-0.9916, -1.8163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05871553719043732
RAW KL tensor(0.0617, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 36: train/loss = 0.7242140173912048, train/raw-loss = 0.6996639966964722, train/logprobs = tensor([[-1.2729, -1.4568],
        [-1.2776, -1.4303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04909990355372429
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 235])
Epoch 0, Step 37: train/loss = 0.7097805738449097, train/raw-loss = 0.6881989240646362, train/logprobs = tensor([[-0.6132, -1.4763],
        [-0.6105, -1.4501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0431632362306118
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 292])
Epoch 0, Step 38: train/loss = 0.750110924243927, train/raw-loss = 0.7293005585670471, train/logprobs = tensor([[-0.7407, -2.9592],
        [-0.7440, -2.9488]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041620612144470215
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 39: train/loss = 0.7095459699630737, train/raw-loss = 0.6974338889122009, train/logprobs = tensor([[-0.6779, -1.3967],
        [-0.6694, -1.3926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02422407455742359
RAW KL tensor(0.0270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 40: train/loss = 0.7086786031723022, train/raw-loss = 0.6939688920974731, train/logprobs = tensor([[-0.7632, -1.1195],
        [-0.7654, -1.1192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029419410973787308
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 329])
Epoch 0, Step 41: train/loss = 0.705525815486908, train/raw-loss = 0.6795097589492798, train/logprobs = tensor([[-0.8216, -1.7078],
        [-0.8400, -1.6511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05203207954764366
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 42: train/loss = 0.7216303944587708, train/raw-loss = 0.694367527961731, train/logprobs = tensor([[-0.6560, -1.7110],
        [-0.6654, -1.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05452563613653183
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 43: train/loss = 0.712816596031189, train/raw-loss = 0.6800717115402222, train/logprobs = tensor([[-1.2107, -2.4576],
        [-1.1998, -2.3878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0654897466301918
RAW KL tensor(0.0493, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 44: train/loss = 0.7197532653808594, train/raw-loss = 0.6903976798057556, train/logprobs = tensor([[-0.9415, -1.1740],
        [-0.9243, -1.1435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05871118605136871
RAW KL tensor(0.0539, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 45: train/loss = 0.7071778774261475, train/raw-loss = 0.6871699094772339, train/logprobs = tensor([[-0.9022, -1.2574],
        [-0.9285, -1.2529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040015809237957
RAW KL tensor(0.1295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 46: train/loss = 0.7529652118682861, train/raw-loss = 0.7230007648468018, train/logprobs = tensor([[-1.1262, -2.6505],
        [-1.1680, -2.7227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059928908944129944
RAW KL tensor(0.0574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 47: train/loss = 0.7255516052246094, train/raw-loss = 0.6937203407287598, train/logprobs = tensor([[-0.7199, -1.4910],
        [-0.7111, -1.4710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06366247683763504
RAW KL tensor(0.1064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 48: train/loss = 0.7402607798576355, train/raw-loss = 0.7070211172103882, train/logprobs = tensor([[-1.3350, -1.4101],
        [-1.3527, -1.4624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0664793998003006
RAW KL tensor(0.0460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 49: train/loss = 0.7099758386611938, train/raw-loss = 0.6940168738365173, train/logprobs = tensor([[-0.7639, -1.6572],
        [-0.7800, -1.6528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03191794455051422
RAW KL tensor(0.0196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 50: train/loss = 0.7106039524078369, train/raw-loss = 0.6908168792724609, train/logprobs = tensor([[-1.0774, -1.4999],
        [-1.0844, -1.4799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039574120193719864
RAW KL tensor(0.0463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 51: train/loss = 0.7122126817703247, train/raw-loss = 0.6857802867889404, train/logprobs = tensor([[-0.9913, -1.7751],
        [-0.9906, -1.7283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052864715456962585
RAW KL tensor(0.0419, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 52: train/loss = 0.707892119884491, train/raw-loss = 0.6902652978897095, train/logprobs = tensor([[-0.7323, -0.8830],
        [-0.7461, -0.8815]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03525370731949806
RAW KL tensor(0.0152, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 53: train/loss = 0.7226409912109375, train/raw-loss = 0.7088227868080139, train/logprobs = tensor([[-0.6088, -1.0795],
        [-0.5906, -1.0657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02763647772371769
RAW KL tensor(0.0467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 54: train/loss = 0.7086758613586426, train/raw-loss = 0.682507336139679, train/logprobs = tensor([[-1.2230, -1.6885],
        [-1.2433, -1.6452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052336983382701874
RAW KL tensor(0.0632, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 55: train/loss = 0.7479618191719055, train/raw-loss = 0.6945031881332397, train/logprobs = tensor([[-1.3827, -1.1114],
        [-1.4643, -1.1231]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10691732168197632
RAW KL tensor(0.0480, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 56: train/loss = 0.7185302376747131, train/raw-loss = 0.6819606423377991, train/logprobs = tensor([[-0.8773, -1.4545],
        [-0.8995, -1.4236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07313912361860275
RAW KL tensor(0.0324, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 57: train/loss = 0.7032923698425293, train/raw-loss = 0.6829138398170471, train/logprobs = tensor([[-0.6305, -1.2882],
        [-0.6485, -1.2477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04075711965560913
RAW KL tensor(0.0098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 335])
Epoch 0, Step 58: train/loss = 0.6949813365936279, train/raw-loss = 0.6728382110595703, train/logprobs = tensor([[-0.7372, -1.5976],
        [-0.7321, -1.5018]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04428628832101822
RAW KL tensor(0.0064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 364])
Epoch 0, Step 59: train/loss = 0.7085435390472412, train/raw-loss = 0.6919022798538208, train/logprobs = tensor([[-0.8279, -1.2489],
        [-0.8414, -1.2448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03328267112374306
RAW KL tensor(0.0445, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 60: train/loss = 0.7748109102249146, train/raw-loss = 0.7513378858566284, train/logprobs = tensor([[-1.3190, -2.5738],
        [-1.3075, -2.4514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0469459593296051
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 61: train/loss = 0.6918833255767822, train/raw-loss = 0.6773156523704529, train/logprobs = tensor([[-0.5136, -1.4002],
        [-0.5257, -1.3338]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029135417193174362
RAW KL tensor(0.0554, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 62: train/loss = 0.7447111010551453, train/raw-loss = 0.706255316734314, train/logprobs = tensor([[-1.0352, -1.7081],
        [-1.0603, -1.7088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07691150903701782
RAW KL tensor(0.0020, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 63: train/loss = 0.7157446146011353, train/raw-loss = 0.6971982717514038, train/logprobs = tensor([[-1.0380, -1.0872],
        [-1.0625, -1.0946]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03709257021546364
RAW KL tensor(0.0488, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 64: train/loss = 0.7119298577308655, train/raw-loss = 0.6937227249145508, train/logprobs = tensor([[-0.9694, -1.6467],
        [-0.9606, -1.6261]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03641428053379059
RAW KL tensor(0.0518, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 65: train/loss = 0.716440737247467, train/raw-loss = 0.6990442872047424, train/logprobs = tensor([[-0.9072, -1.6026],
        [-0.8992, -1.6116]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03479292243719101
RAW KL tensor(0.0406, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 66: train/loss = 0.7251634001731873, train/raw-loss = 0.706115186214447, train/logprobs = tensor([[-1.0822, -1.9498],
        [-1.0354, -1.9026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03809636831283569
RAW KL tensor(0.0108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 67: train/loss = 0.7057663202285767, train/raw-loss = 0.6920357346534729, train/logprobs = tensor([[-0.6880, -1.8941],
        [-0.6729, -1.8431]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027461066842079163
RAW KL tensor(0.0065, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 68: train/loss = 0.7096672654151917, train/raw-loss = 0.6930681467056274, train/logprobs = tensor([[-0.7963, -2.3885],
        [-0.7873, -2.3517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03319829702377319
RAW KL tensor(0.0370, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 69: train/loss = 0.714778482913971, train/raw-loss = 0.6943738460540771, train/logprobs = tensor([[-1.4423, -2.0175],
        [-1.4666, -2.0352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04080934077501297
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 70: train/loss = 0.6982614398002625, train/raw-loss = 0.6898855566978455, train/logprobs = tensor([[-0.6137, -1.9486],
        [-0.6021, -1.9107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016751732677221298
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 71: train/loss = 0.7089052796363831, train/raw-loss = 0.6931037902832031, train/logprobs = tensor([[-1.0149, -2.1742],
        [-1.0361, -2.1528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03160301223397255
RAW KL tensor(0.0166, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 261])
Epoch 0, Step 72: train/loss = 0.7029573917388916, train/raw-loss = 0.690091073513031, train/logprobs = tensor([[-0.4665, -2.0000],
        [-0.4656, -1.9752]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025732751935720444
RAW KL tensor(0.0089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 73: train/loss = 0.7069769501686096, train/raw-loss = 0.6965222358703613, train/logprobs = tensor([[-0.7101, -1.3950],
        [-0.7367, -1.4213]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020909346640110016
RAW KL tensor(0.0087, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 74: train/loss = 0.7182298302650452, train/raw-loss = 0.6980204582214355, train/logprobs = tensor([[-1.2824, -2.1764],
        [-1.2673, -2.1690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04041866213083267
RAW KL tensor(0.0466, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 75: train/loss = 0.7079690098762512, train/raw-loss = 0.6908951997756958, train/logprobs = tensor([[-0.7980, -1.9168],
        [-0.7874, -1.8923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03414764255285263
RAW KL tensor(0.0162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 76: train/loss = 0.6878376007080078, train/raw-loss = 0.6733859181404114, train/logprobs = tensor([[-1.0412, -1.6139],
        [-1.1209, -1.6046]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028903312981128693
RAW KL tensor(0.0584, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 77: train/loss = 0.71220463514328, train/raw-loss = 0.6876100301742554, train/logprobs = tensor([[-1.3128, -1.5354],
        [-1.3300, -1.5231]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049189116805791855
RAW KL tensor(0.0241, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 78: train/loss = 0.6962173581123352, train/raw-loss = 0.6855587363243103, train/logprobs = tensor([[-0.8462, -1.5666],
        [-0.8602, -1.5467]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021317265927791595
RAW KL tensor(0.0460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 257])
Epoch 0, Step 79: train/loss = 0.6976849436759949, train/raw-loss = 0.6853398680686951, train/logprobs = tensor([[-0.5680, -1.2860],
        [-0.5699, -1.2494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02469008043408394
RAW KL tensor(0.0255, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 80: train/loss = 0.7028125524520874, train/raw-loss = 0.6852773427963257, train/logprobs = tensor([[-1.1219, -2.1798],
        [-1.1145, -2.1374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03507041931152344
RAW KL tensor(0.0503, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 81: train/loss = 0.6972559094429016, train/raw-loss = 0.6849780678749084, train/logprobs = tensor([[-0.8313, -1.3386],
        [-0.8502, -1.3213]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024555670097470284
RAW KL tensor(0.0384, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 82: train/loss = 0.7202479839324951, train/raw-loss = 0.6939911842346191, train/logprobs = tensor([[-1.2867, -1.0969],
        [-1.3016, -1.0969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052513591945171356
RAW KL tensor(0.0143, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 83: train/loss = 0.6968536376953125, train/raw-loss = 0.685767412185669, train/logprobs = tensor([[-0.7176, -1.7595],
        [-0.7361, -1.7170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022172417491674423
RAW KL tensor(0.0225, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 344])
Epoch 0, Step 84: train/loss = 0.6976449489593506, train/raw-loss = 0.6874499320983887, train/logprobs = tensor([[-0.5756, -2.0562],
        [-0.5672, -2.0208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020390111953020096
RAW KL tensor(0.0332, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 85: train/loss = 0.758599042892456, train/raw-loss = 0.699424684047699, train/logprobs = tensor([[-0.9642, -2.2147],
        [-0.9722, -2.2102]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11834868788719177
RAW KL tensor(0.0196, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 86: train/loss = 0.7120608687400818, train/raw-loss = 0.689421534538269, train/logprobs = tensor([[-0.8139, -1.5832],
        [-0.8190, -1.5528]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04527844116091728
RAW KL tensor(0.0279, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 87: train/loss = 0.7148034572601318, train/raw-loss = 0.7017902135848999, train/logprobs = tensor([[-1.3109, -1.3642],
        [-1.3165, -1.3995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02602650597691536
RAW KL tensor(0.0085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 88: train/loss = 0.7109968662261963, train/raw-loss = 0.6940239667892456, train/logprobs = tensor([[-0.9623, -2.0813],
        [-0.9884, -2.0707]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0339457206428051
RAW KL tensor(0.0312, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 89: train/loss = 0.7004250288009644, train/raw-loss = 0.6850441694259644, train/logprobs = tensor([[-0.9335, -2.1124],
        [-0.9202, -2.0174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03076186217367649
RAW KL tensor(0.0184, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 90: train/loss = 0.7166203856468201, train/raw-loss = 0.6958543658256531, train/logprobs = tensor([[-1.0539, -1.9238],
        [-1.0636, -1.9272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041531991213560104
RAW KL tensor(0.0893, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 91: train/loss = 0.7080191373825073, train/raw-loss = 0.688800573348999, train/logprobs = tensor([[-1.4111, -1.2494],
        [-1.4379, -1.2572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03843720629811287
RAW KL tensor(0.0419, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 92: train/loss = 0.7014308571815491, train/raw-loss = 0.686682939529419, train/logprobs = tensor([[-0.6416, -1.4972],
        [-0.6458, -1.4623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02949589118361473
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 93: train/loss = 0.7085914611816406, train/raw-loss = 0.6968578696250916, train/logprobs = tensor([[-1.0787, -1.5234],
        [-1.1025, -1.5463]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023467138409614563
RAW KL tensor(0.1218, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 94: train/loss = 0.7249549031257629, train/raw-loss = 0.7003031373023987, train/logprobs = tensor([[-1.3155, -2.5529],
        [-1.3570, -2.5157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049303553998470306
RAW KL tensor(0.0354, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 95: train/loss = 0.7107865810394287, train/raw-loss = 0.6850242018699646, train/logprobs = tensor([[-1.2961, -1.4673],
        [-1.3161, -1.4457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0515248104929924
RAW KL tensor(0.0060, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 96: train/loss = 0.7106050252914429, train/raw-loss = 0.695344865322113, train/logprobs = tensor([[-0.9245, -1.7533],
        [-0.9382, -1.7649]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030520368367433548
RAW KL tensor(0.0218, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 97: train/loss = 0.7174248099327087, train/raw-loss = 0.7039389610290527, train/logprobs = tensor([[-1.8181, -2.5350],
        [-1.8197, -2.5517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026971738785505295
RAW KL tensor(0.0256, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 98: train/loss = 0.7054195404052734, train/raw-loss = 0.6952242851257324, train/logprobs = tensor([[-1.0228, -1.7968],
        [-1.0197, -1.7979]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02039041742682457
RAW KL tensor(0.0490, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 99: train/loss = 0.7068939208984375, train/raw-loss = 0.6901707649230957, train/logprobs = tensor([[-0.8924, -1.6524],
        [-0.8912, -1.6341]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03344622254371643
RAW KL tensor(0.0523, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 100: train/loss = 0.703693687915802, train/raw-loss = 0.691409707069397, train/logprobs = tensor([[-1.2448, -1.5464],
        [-1.2689, -1.5470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024567943066358566
RAW KL tensor(0.0125, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 101: train/loss = 0.6883255243301392, train/raw-loss = 0.6776803731918335, train/logprobs = tensor([[-0.8501, -2.5901],
        [-0.8561, -2.5056]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02129034511744976
RAW KL tensor(0.0292, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 102: train/loss = 0.6848958730697632, train/raw-loss = 0.6717261075973511, train/logprobs = tensor([[-0.8063, -1.8428],
        [-0.7830, -1.7260]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02633959800004959
RAW KL tensor(0.0318, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 103: train/loss = 0.7050070762634277, train/raw-loss = 0.6845003962516785, train/logprobs = tensor([[-0.9249, -1.7995],
        [-0.9629, -1.7860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041013386100530624
RAW KL tensor(0.0176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 104: train/loss = 0.7000168561935425, train/raw-loss = 0.686448335647583, train/logprobs = tensor([[-0.9894, -1.6280],
        [-0.9915, -1.5986]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02713710069656372
RAW KL tensor(0.0113, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 105: train/loss = 0.7048569917678833, train/raw-loss = 0.6919866800308228, train/logprobs = tensor([[-1.0719, -1.8831],
        [-1.0630, -1.8552]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025740651413798332
RAW KL tensor(0.0061, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 106: train/loss = 0.7042118310928345, train/raw-loss = 0.694330096244812, train/logprobs = tensor([[-0.8034, -1.8042],
        [-0.7780, -1.7674]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01976354420185089
RAW KL tensor(0.0402, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 107: train/loss = 0.6991930603981018, train/raw-loss = 0.6841168403625488, train/logprobs = tensor([[-1.4184, -1.7119],
        [-1.4591, -1.7035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030152469873428345
RAW KL tensor(0.0086, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 108: train/loss = 0.7038593888282776, train/raw-loss = 0.6872736215591431, train/logprobs = tensor([[-0.7579, -1.0063],
        [-0.7617, -0.9612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03317158669233322
RAW KL tensor(0.0150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 109: train/loss = 0.7210023403167725, train/raw-loss = 0.7084422707557678, train/logprobs = tensor([[-0.8583, -1.8383],
        [-0.8443, -1.8279]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025120234116911888
RAW KL tensor(0.0727, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 110: train/loss = 0.7207187414169312, train/raw-loss = 0.6880940198898315, train/logprobs = tensor([[-1.0081, -2.1577],
        [-0.9765, -2.0670]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06524962931871414
RAW KL tensor(0.0301, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 111: train/loss = 0.7055748701095581, train/raw-loss = 0.6953016519546509, train/logprobs = tensor([[-0.9880, -1.8282],
        [-0.9983, -1.8427]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020546341314911842
RAW KL tensor(0.0443, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 112: train/loss = 0.7008904814720154, train/raw-loss = 0.6897714138031006, train/logprobs = tensor([[-1.2869, -1.2704],
        [-1.3171, -1.2845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022238140925765038
RAW KL tensor(0.0450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 113: train/loss = 0.6990262269973755, train/raw-loss = 0.6820945143699646, train/logprobs = tensor([[-1.1262, -1.5676],
        [-1.1210, -1.5141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03386348858475685
RAW KL tensor(0.0131, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 353])
Epoch 0, Step 114: train/loss = 0.7015026807785034, train/raw-loss = 0.693418562412262, train/logprobs = tensor([[-0.8718, -1.3507],
        [-0.9116, -1.3812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016168324276804924
RAW KL tensor(0.0368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 115: train/loss = 0.6789600849151611, train/raw-loss = 0.6690905094146729, train/logprobs = tensor([[-0.7615, -1.9724],
        [-0.7532, -1.8601]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01973918452858925
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 116: train/loss = 0.7024818062782288, train/raw-loss = 0.6930261850357056, train/logprobs = tensor([[-0.6991, -1.4594],
        [-0.7048, -1.4580]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018911141902208328
RAW KL tensor(0.0153, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 117: train/loss = 0.7038689851760864, train/raw-loss = 0.6904243230819702, train/logprobs = tensor([[-0.9575, -1.8104],
        [-0.9654, -1.7870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02688932605087757
RAW KL tensor(0.0172, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 118: train/loss = 0.6895304918289185, train/raw-loss = 0.6799542307853699, train/logprobs = tensor([[-1.2819, -2.1730],
        [-1.3307, -2.1620]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01915241777896881
RAW KL tensor(0.0684, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 119: train/loss = 0.7088384032249451, train/raw-loss = 0.691184937953949, train/logprobs = tensor([[-1.5834, -1.9229],
        [-1.6212, -1.9492]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035307057201862335
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 120: train/loss = 0.7061224579811096, train/raw-loss = 0.6845849752426147, train/logprobs = tensor([[-1.5273, -1.8484],
        [-1.5438, -1.8276]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04307498037815094
RAW KL tensor(0.0755, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 121: train/loss = 0.7077171206474304, train/raw-loss = 0.6879890561103821, train/logprobs = tensor([[-0.7294, -1.6863],
        [-0.7494, -1.6741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039456117898225784
RAW KL tensor(0.0150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 122: train/loss = 0.6996378898620605, train/raw-loss = 0.6882066130638123, train/logprobs = tensor([[-0.9941, -2.4593],
        [-0.9837, -2.4221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02286258339881897
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 123: train/loss = 0.7052692174911499, train/raw-loss = 0.6834416389465332, train/logprobs = tensor([[-1.2761, -1.4969],
        [-1.2765, -1.4358]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04365513473749161
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 124: train/loss = 0.7048159837722778, train/raw-loss = 0.694568932056427, train/logprobs = tensor([[-0.6810, -1.4520],
        [-0.6359, -1.3942]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02049407921731472
RAW KL tensor(0.0841, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 125: train/loss = 0.7118360996246338, train/raw-loss = 0.6925027370452881, train/logprobs = tensor([[-0.8447, -1.8333],
        [-0.8843, -1.8583]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038666732609272
RAW KL tensor(0.0294, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 126: train/loss = 0.6962330341339111, train/raw-loss = 0.6850422024726868, train/logprobs = tensor([[-1.2271, -2.9036],
        [-1.2716, -2.9058]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022381598129868507
RAW KL tensor(0.0204, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 127: train/loss = 0.6979882121086121, train/raw-loss = 0.6880282163619995, train/logprobs = tensor([[-1.0885, -1.9380],
        [-1.0613, -1.8801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019920023158192635
RAW KL tensor(0.0530, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 128: train/loss = 0.7335354685783386, train/raw-loss = 0.7210192084312439, train/logprobs = tensor([[-0.9630, -1.1688],
        [-0.9518, -1.2458]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025032563135027885
RAW KL tensor(0.0168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 129: train/loss = 0.7196987867355347, train/raw-loss = 0.704765796661377, train/logprobs = tensor([[-1.3395, -2.1519],
        [-1.3065, -2.1364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02986595407128334
RAW KL tensor(0.0279, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 430])
Epoch 0, Step 130: train/loss = 0.7096840143203735, train/raw-loss = 0.6972298622131348, train/logprobs = tensor([[-1.2843, -2.6698],
        [-1.2508, -2.6272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024908294901251793
RAW KL tensor(0.0166, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 131: train/loss = 0.7090722322463989, train/raw-loss = 0.6940383315086365, train/logprobs = tensor([[-1.4104, -1.7085],
        [-1.4183, -1.6541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030067842453718185
RAW KL tensor(0.0174, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 132: train/loss = 0.7223173975944519, train/raw-loss = 0.6771868467330933, train/logprobs = tensor([[-1.4018, -2.7169],
        [-1.4920, -2.6773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09026102721691132
RAW KL tensor(0.0327, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 133: train/loss = 0.7120839357376099, train/raw-loss = 0.6958444118499756, train/logprobs = tensor([[-1.6665, -1.8986],
        [-1.6928, -1.8877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0324789322912693
RAW KL tensor(0.0148, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 134: train/loss = 0.7115715146064758, train/raw-loss = 0.7000542879104614, train/logprobs = tensor([[-1.1532, -1.7267],
        [-1.1572, -1.7145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02303445339202881
RAW KL tensor(0.0246, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 135: train/loss = 0.7012361884117126, train/raw-loss = 0.6888039112091064, train/logprobs = tensor([[-1.6281, -1.6342],
        [-1.6151, -1.5868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024864504113793373
RAW KL tensor(0.0920, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 136: train/loss = 0.7125301361083984, train/raw-loss = 0.6831098198890686, train/logprobs = tensor([[-1.7648, -2.4774],
        [-1.7880, -2.4309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05884061008691788
RAW KL tensor(0.0189, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 137: train/loss = 0.7039985060691833, train/raw-loss = 0.6826474666595459, train/logprobs = tensor([[-1.4918, -1.5929],
        [-1.5170, -1.5669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04270203039050102
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 138: train/loss = 0.7048524618148804, train/raw-loss = 0.6944208741188049, train/logprobs = tensor([[-1.2076, -2.2388],
        [-1.1870, -2.2154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020863201469182968
RAW KL tensor(0.0176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 139: train/loss = 0.7891750335693359, train/raw-loss = 0.7195417284965515, train/logprobs = tensor([[-1.2641, -2.6650],
        [-1.2895, -2.6447]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13926658034324646
RAW KL tensor(0.0187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 140: train/loss = 0.7198755741119385, train/raw-loss = 0.7009170651435852, train/logprobs = tensor([[-1.4690, -2.6567],
        [-1.4907, -2.6864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0379171259701252
RAW KL tensor(0.0307, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 141: train/loss = 0.7126791477203369, train/raw-loss = 0.6928249597549438, train/logprobs = tensor([[-1.0874, -2.2802],
        [-1.0621, -2.2352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03970848768949509
RAW KL tensor(0.0307, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 142: train/loss = 0.6973212957382202, train/raw-loss = 0.6841621398925781, train/logprobs = tensor([[-1.4079, -1.9197],
        [-1.4020, -1.8649]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026318229734897614
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 143: train/loss = 0.7127310037612915, train/raw-loss = 0.6854807138442993, train/logprobs = tensor([[-1.8526, -2.4490],
        [-1.8894, -2.4147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.054500628262758255
RAW KL tensor(0.0233, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 144: train/loss = 0.7055981159210205, train/raw-loss = 0.69087153673172, train/logprobs = tensor([[-1.1204, -2.0566],
        [-1.1357, -2.0221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02945316769182682
RAW KL tensor(0.0488, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 145: train/loss = 0.7443637847900391, train/raw-loss = 0.7174134254455566, train/logprobs = tensor([[-1.5169, -2.0306],
        [-1.5000, -1.9444]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05390075966715813
RAW KL tensor(0.0044, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 146: train/loss = 0.7029982805252075, train/raw-loss = 0.686752200126648, train/logprobs = tensor([[-1.2305, -1.9968],
        [-1.2368, -1.9745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03249222785234451
RAW KL tensor(0.0787, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 147: train/loss = 0.7086063623428345, train/raw-loss = 0.6774764060974121, train/logprobs = tensor([[-1.6738, -1.8114],
        [-1.7254, -1.7878]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.062259774655103683
RAW KL tensor(0.0519, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 148: train/loss = 0.7052717208862305, train/raw-loss = 0.6940677165985107, train/logprobs = tensor([[-0.9212, -1.3137],
        [-0.9359, -1.3196]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022408105432987213
RAW KL tensor(0.0105, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 149: train/loss = 0.7157188057899475, train/raw-loss = 0.693493127822876, train/logprobs = tensor([[-1.2492, -1.9625],
        [-1.2361, -1.9408]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04445139318704605
RAW KL tensor(0.0382, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 150: train/loss = 0.7110912799835205, train/raw-loss = 0.6946554780006409, train/logprobs = tensor([[-1.3653, -1.9754],
        [-1.3587, -1.9287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03287157043814659
RAW KL tensor(0.0401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 151: train/loss = 0.7285417914390564, train/raw-loss = 0.708746612071991, train/logprobs = tensor([[-1.0659, -1.5208],
        [-1.0841, -1.5433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039590224623680115
RAW KL tensor(0.0238, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 152: train/loss = 0.6967376470565796, train/raw-loss = 0.6854485273361206, train/logprobs = tensor([[-1.0887, -1.9725],
        [-1.1004, -1.9499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022577989846467972
RAW KL tensor(0.0394, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 153: train/loss = 0.7145742177963257, train/raw-loss = 0.6995865106582642, train/logprobs = tensor([[-1.1401, -2.2067],
        [-1.1550, -2.2416]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02997545525431633
RAW KL tensor(0.0465, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 154: train/loss = 0.7154361605644226, train/raw-loss = 0.6903640031814575, train/logprobs = tensor([[-1.6322, -2.4762],
        [-1.6492, -2.4609]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05014433711767197
RAW KL tensor(0.0108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 155: train/loss = 0.7096760272979736, train/raw-loss = 0.6946320533752441, train/logprobs = tensor([[-1.0155, -1.8292],
        [-0.9915, -1.8076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030087994411587715
RAW KL tensor(0.0301, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 156: train/loss = 0.704980194568634, train/raw-loss = 0.6908018589019775, train/logprobs = tensor([[-1.1685, -2.3561],
        [-1.1550, -2.3232]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028356662020087242
RAW KL tensor(0.0219, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 157: train/loss = 0.7181792855262756, train/raw-loss = 0.702053427696228, train/logprobs = tensor([[-1.1466, -1.4049],
        [-1.1430, -1.3994]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03225173056125641
RAW KL tensor(0.0101, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 158: train/loss = 0.6888059377670288, train/raw-loss = 0.6827086210250854, train/logprobs = tensor([[-0.9753, -0.9256],
        [-0.9727, -0.8766]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012194797396659851
RAW KL tensor(0.0224, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 159: train/loss = 0.6988812685012817, train/raw-loss = 0.6840364933013916, train/logprobs = tensor([[-0.9970, -1.8240],
        [-1.0230, -1.8027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02968955785036087
RAW KL tensor(0.0064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 160: train/loss = 0.6985074281692505, train/raw-loss = 0.6822465062141418, train/logprobs = tensor([[-0.9816, -1.4733],
        [-0.9677, -1.3998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03252185136079788
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 161: train/loss = 0.7131795287132263, train/raw-loss = 0.6947485208511353, train/logprobs = tensor([[-1.1801, -2.1512],
        [-1.2025, -2.1495]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036862052977085114
RAW KL tensor(0.0298, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 162: train/loss = 0.7058204412460327, train/raw-loss = 0.69282066822052, train/logprobs = tensor([[-0.8674, -2.3897],
        [-0.8311, -2.3212]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025999654084444046
RAW KL tensor(0.0295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 163: train/loss = 0.7302409410476685, train/raw-loss = 0.6997861266136169, train/logprobs = tensor([[-1.5769, -2.2549],
        [-1.5663, -2.1906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06090952455997467
RAW KL tensor(0.0424, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 164: train/loss = 0.7077009677886963, train/raw-loss = 0.6895002126693726, train/logprobs = tensor([[-1.8081, -1.8207],
        [-1.8207, -1.7973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03640163689851761
RAW KL tensor(0.0378, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 165: train/loss = 0.6883811950683594, train/raw-loss = 0.6691206693649292, train/logprobs = tensor([[-1.2086, -2.4105],
        [-1.2706, -2.3710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03852107375860214
RAW KL tensor(0.0295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 166: train/loss = 0.7107299566268921, train/raw-loss = 0.6932286024093628, train/logprobs = tensor([[-1.4476, -2.3440],
        [-1.4208, -2.3079]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03500278666615486
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 167: train/loss = 0.7123175263404846, train/raw-loss = 0.6920074224472046, train/logprobs = tensor([[-1.5438, -1.4613],
        [-1.6003, -1.4766]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04062024503946304
RAW KL tensor(0.0283, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 168: train/loss = 0.7195978164672852, train/raw-loss = 0.6977826952934265, train/logprobs = tensor([[-1.2641, -1.4993],
        [-1.2764, -1.5052]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04363012686371803
RAW KL tensor(0.0306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 169: train/loss = 0.7268739938735962, train/raw-loss = 0.7059722542762756, train/logprobs = tensor([[-1.1020, -1.6068],
        [-1.1145, -1.6215]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04180362448096275
RAW KL tensor(0.0122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 170: train/loss = 0.7101631164550781, train/raw-loss = 0.6994829177856445, train/logprobs = tensor([[-0.7462, -2.1575],
        [-0.7432, -2.1568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021360483020544052
RAW KL tensor(0.0216, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 171: train/loss = 0.701591432094574, train/raw-loss = 0.6888065338134766, train/logprobs = tensor([[-1.4451, -1.7381],
        [-1.4162, -1.6888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025569770485162735
RAW KL tensor(0.0142, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 172: train/loss = 0.7200037240982056, train/raw-loss = 0.6988284587860107, train/logprobs = tensor([[-1.2027, -1.9467],
        [-1.1889, -1.8790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04235054552555084
RAW KL tensor(0.0565, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 173: train/loss = 0.7156456708908081, train/raw-loss = 0.701225757598877, train/logprobs = tensor([[-1.0287, -2.5750],
        [-1.0296, -2.5375]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028839897364377975
RAW KL tensor(0.0527, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 174: train/loss = 0.7280417680740356, train/raw-loss = 0.7001231908798218, train/logprobs = tensor([[-1.3554, -3.0272],
        [-1.3651, -2.9651]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055837132036685944
RAW KL tensor(0.0469, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 175: train/loss = 0.7182837724685669, train/raw-loss = 0.6963642835617065, train/logprobs = tensor([[-1.3508, -1.6231],
        [-1.3263, -1.6014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043839044868946075
RAW KL tensor(0.0373, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 176: train/loss = 0.6988519430160522, train/raw-loss = 0.6835496425628662, train/logprobs = tensor([[-0.9589, -2.0378],
        [-0.9497, -1.9745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030604476109147072
RAW KL tensor(0.0401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 177: train/loss = 0.7003599405288696, train/raw-loss = 0.6779463887214661, train/logprobs = tensor([[-1.1586, -2.2966],
        [-1.1604, -2.2203]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044826943427324295
RAW KL tensor(0.0270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 259])
Epoch 0, Step 178: train/loss = 0.693975567817688, train/raw-loss = 0.6843374967575073, train/logprobs = tensor([[-0.9704, -2.0583],
        [-0.9711, -2.0096]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019276218488812447
RAW KL tensor(0.0199, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 179: train/loss = 0.7149423360824585, train/raw-loss = 0.6987408399581909, train/logprobs = tensor([[-1.5095, -2.0895],
        [-1.5194, -2.1081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032402992248535156
RAW KL tensor(0.0550, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 180: train/loss = 0.7728843092918396, train/raw-loss = 0.6935558915138245, train/logprobs = tensor([[-1.4019, -1.7273],
        [-1.4503, -1.7145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1586567461490631
RAW KL tensor(0.0071, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 326])
Epoch 0, Step 181: train/loss = 0.717759370803833, train/raw-loss = 0.7030057907104492, train/logprobs = tensor([[-1.1461, -1.8053],
        [-1.1448, -1.8269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029507094994187355
RAW KL tensor(0.0356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 182: train/loss = 0.6983977556228638, train/raw-loss = 0.6839159727096558, train/logprobs = tensor([[-1.2828, -1.5860],
        [-1.2926, -1.5529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028963657096028328
RAW KL tensor(0.0045, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 183: train/loss = 0.8670138120651245, train/raw-loss = 0.7974449396133423, train/logprobs = tensor([[-1.3952, -2.3082],
        [-1.4364, -2.0854]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1391378492116928
RAW KL tensor(0.0292, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 184: train/loss = 0.7046117186546326, train/raw-loss = 0.6894667744636536, train/logprobs = tensor([[-1.2255, -2.0667],
        [-1.2203, -2.0271]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03028980828821659
RAW KL tensor(0.0805, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 185: train/loss = 0.7444467544555664, train/raw-loss = 0.7255317568778992, train/logprobs = tensor([[-1.0924, -1.9230],
        [-1.1043, -1.8779]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03782982379198074
RAW KL tensor(0.0210, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 186: train/loss = 0.7151468992233276, train/raw-loss = 0.6983775496482849, train/logprobs = tensor([[-1.0038, -2.1254],
        [-0.9631, -2.0809]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033538658171892166
RAW KL tensor(0.0389, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 187: train/loss = 0.7277261018753052, train/raw-loss = 0.7061160206794739, train/logprobs = tensor([[-1.2965, -1.8272],
        [-1.2705, -1.8300]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043220117688179016
RAW KL tensor(0.0241, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 188: train/loss = 0.7217549085617065, train/raw-loss = 0.7047154903411865, train/logprobs = tensor([[-1.4448, -1.8162],
        [-1.4229, -1.7884]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034078702330589294
RAW KL tensor(0.0980, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 189: train/loss = 0.7258474826812744, train/raw-loss = 0.7079929113388062, train/logprobs = tensor([[-1.1825, -1.1753],
        [-1.1654, -1.1552]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035709068179130554
RAW KL tensor(0.0492, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 190: train/loss = 0.6971632838249207, train/raw-loss = 0.6789069175720215, train/logprobs = tensor([[-1.7798, -1.7964],
        [-1.8367, -1.7529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03651287779211998
RAW KL tensor(0.0408, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 191: train/loss = 0.7297984957695007, train/raw-loss = 0.7046266794204712, train/logprobs = tensor([[-1.2842, -1.9014],
        [-1.2780, -1.8864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05034363269805908
RAW KL tensor(0.0162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 192: train/loss = 0.7016140818595886, train/raw-loss = 0.6811501979827881, train/logprobs = tensor([[-1.1849, -1.6115],
        [-1.1975, -1.5659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040927767753601074
RAW KL tensor(0.0072, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 193: train/loss = 0.7030285596847534, train/raw-loss = 0.6946160197257996, train/logprobs = tensor([[-0.9131, -1.4716],
        [-0.9325, -1.4839]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016825035214424133
RAW KL tensor(0.0059, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 282])
Epoch 0, Step 194: train/loss = 0.7002911567687988, train/raw-loss = 0.6860016584396362, train/logprobs = tensor([[-1.3626, -1.4920],
        [-1.3616, -1.4388]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028578953817486763
RAW KL tensor(0.0066, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 195: train/loss = 0.7023858428001404, train/raw-loss = 0.6916904449462891, train/logprobs = tensor([[-0.7182, -1.5955],
        [-0.7174, -1.5777]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021390806883573532
RAW KL tensor(0.0198, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 196: train/loss = 0.70525062084198, train/raw-loss = 0.6929150223731995, train/logprobs = tensor([[-1.5260, -1.9556],
        [-1.4877, -1.9112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02467118203639984
RAW KL tensor(0.0621, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 197: train/loss = 0.6846059560775757, train/raw-loss = 0.6715816855430603, train/logprobs = tensor([[-1.2095, -1.8239],
        [-1.1971, -1.6880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026048533618450165
RAW KL tensor(0.0329, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 198: train/loss = 0.6874833106994629, train/raw-loss = 0.6728135347366333, train/logprobs = tensor([[-1.0841, -1.2498],
        [-1.0796, -1.1615]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029339591041207314
RAW KL tensor(0.0230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 199: train/loss = 0.6918043494224548, train/raw-loss = 0.6777703166007996, train/logprobs = tensor([[-1.1257, -1.6134],
        [-1.1565, -1.5545]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02806815318763256
RAW KL tensor(0.0101, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 200: train/loss = 0.6757636070251465, train/raw-loss = 0.6579250693321228, train/logprobs = tensor([[-1.4646, -2.7825],
        [-1.4991, -2.6505]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03567708283662796
RAW KL tensor(0.0156, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 201: train/loss = 0.6930469870567322, train/raw-loss = 0.6849583387374878, train/logprobs = tensor([[-0.7768, -1.8837],
        [-0.7717, -1.8233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016177266836166382
RAW KL tensor(0.0343, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 202: train/loss = 0.7064698338508606, train/raw-loss = 0.6907349824905396, train/logprobs = tensor([[-0.8394, -2.0772],
        [-0.8452, -2.0520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03146982565522194
RAW KL tensor(0.0202, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 203: train/loss = 0.6705532073974609, train/raw-loss = 0.645717978477478, train/logprobs = tensor([[-1.2191, -1.0982],
        [-1.3455, -0.9998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0496705137193203
RAW KL tensor(0.0434, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 204: train/loss = 0.697184681892395, train/raw-loss = 0.6855401396751404, train/logprobs = tensor([[-1.5059, -1.4752],
        [-1.5011, -1.4282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02328920178115368
RAW KL tensor(0.0182, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 328])
Epoch 0, Step 205: train/loss = 0.6935579180717468, train/raw-loss = 0.6836488842964172, train/logprobs = tensor([[-0.7725, -1.4896],
        [-0.7634, -1.4311]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019818101078271866
RAW KL tensor(0.0119, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 206: train/loss = 0.6895468235015869, train/raw-loss = 0.6796337366104126, train/logprobs = tensor([[-1.0259, -1.5158],
        [-1.0004, -1.4264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019826194271445274
RAW KL tensor(0.0578, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 207: train/loss = 0.6665815114974976, train/raw-loss = 0.6469281911849976, train/logprobs = tensor([[-1.3697, -2.6511],
        [-1.3916, -2.4654]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03930652141571045
RAW KL tensor(0.1656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 208: train/loss = 0.7050893902778625, train/raw-loss = 0.6712258458137512, train/logprobs = tensor([[-1.5261, -2.3635],
        [-1.5489, -2.2548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06772707402706146
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 209: train/loss = 0.707940936088562, train/raw-loss = 0.6896036863327026, train/logprobs = tensor([[-1.3783, -2.5587],
        [-1.3989, -2.5131]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03667469695210457
RAW KL tensor(0.0114, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 210: train/loss = 0.6969494819641113, train/raw-loss = 0.6855502128601074, train/logprobs = tensor([[-1.5953, -1.6005],
        [-1.6155, -1.5750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022798623889684677
RAW KL tensor(0.0191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 211: train/loss = 0.6915645599365234, train/raw-loss = 0.6786667108535767, train/logprobs = tensor([[-1.3459, -2.3042],
        [-1.3298, -2.2235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02579563856124878
RAW KL tensor(0.0150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 212: train/loss = 0.7035993933677673, train/raw-loss = 0.6922304630279541, train/logprobs = tensor([[-0.9100, -1.3034],
        [-0.8980, -1.2808]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022737689316272736
RAW KL tensor(0.0503, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 213: train/loss = 0.6918184161186218, train/raw-loss = 0.6795514822006226, train/logprobs = tensor([[-1.6352, -2.1261],
        [-1.6360, -2.0665]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02453395165503025
RAW KL tensor(0.0190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 214: train/loss = 0.7053192853927612, train/raw-loss = 0.6949583292007446, train/logprobs = tensor([[-1.0923, -1.3942],
        [-1.0890, -1.3712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020722050219774246
RAW KL tensor(0.0209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 215: train/loss = 0.6977089047431946, train/raw-loss = 0.6751072406768799, train/logprobs = tensor([[-1.1067, -2.2179],
        [-1.0632, -2.0871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04520338773727417
RAW KL tensor(0.0122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 216: train/loss = 0.7096436619758606, train/raw-loss = 0.6916713714599609, train/logprobs = tensor([[-1.4162, -1.4674],
        [-1.3984, -1.4122]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035944487899541855
RAW KL tensor(0.0644, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 217: train/loss = 0.7114136219024658, train/raw-loss = 0.6984138488769531, train/logprobs = tensor([[-1.0449, -1.6492],
        [-1.0597, -1.6712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02599954605102539
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 218: train/loss = 0.7139782309532166, train/raw-loss = 0.6953867673873901, train/logprobs = tensor([[-1.0090, -2.4501],
        [-1.0104, -2.3826]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037182725965976715
RAW KL tensor(0.0047, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 496])
Epoch 0, Step 219: train/loss = 0.708656370639801, train/raw-loss = 0.6974636316299438, train/logprobs = tensor([[-1.3250, -2.0446],
        [-1.2879, -2.0072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022385479882359505
RAW KL tensor(0.0635, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 220: train/loss = 0.7060946822166443, train/raw-loss = 0.6827758550643921, train/logprobs = tensor([[-1.4580, -3.0295],
        [-1.4529, -2.9657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04663773626089096
RAW KL tensor(0.0357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 221: train/loss = 0.7126418352127075, train/raw-loss = 0.6962552070617676, train/logprobs = tensor([[-1.4796, -1.2189],
        [-1.4558, -1.1906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03277334198355675
RAW KL tensor(0.0324, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 222: train/loss = 0.7071528434753418, train/raw-loss = 0.6943429112434387, train/logprobs = tensor([[-1.2867, -1.4437],
        [-1.2871, -1.4050]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025619884952902794
RAW KL tensor(0.0174, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 223: train/loss = 0.6904165744781494, train/raw-loss = 0.6728906035423279, train/logprobs = tensor([[-0.9213, -2.3597],
        [-0.9299, -2.2821]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035051893442869186
RAW KL tensor(0.0529, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 224: train/loss = 0.7011175155639648, train/raw-loss = 0.685649573802948, train/logprobs = tensor([[-1.5380, -2.7656],
        [-1.5306, -2.7004]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03093591332435608
RAW KL tensor(0.0210, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 360])
Epoch 0, Step 225: train/loss = 0.682706356048584, train/raw-loss = 0.666330099105835, train/logprobs = tensor([[-0.9227, -2.1074],
        [-0.9099, -1.9768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03275241702795029
RAW KL tensor(0.0332, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 226: train/loss = 0.6806341409683228, train/raw-loss = 0.6676164269447327, train/logprobs = tensor([[-1.2280, -2.4810],
        [-1.2639, -2.3965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02603546902537346
RAW KL tensor(0.0404, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 227: train/loss = 0.7016395330429077, train/raw-loss = 0.6906155347824097, train/logprobs = tensor([[-0.9468, -1.6647],
        [-0.9063, -1.5990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022048067301511765
RAW KL tensor(0.0373, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 228: train/loss = 0.677782416343689, train/raw-loss = 0.6645593047142029, train/logprobs = tensor([[-1.2646, -2.5976],
        [-1.3054, -2.4801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026446137577295303
RAW KL tensor(0.0183, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 283])
Epoch 0, Step 229: train/loss = 0.6884034276008606, train/raw-loss = 0.6767414212226868, train/logprobs = tensor([[-1.2735, -1.4426],
        [-1.3160, -1.3911]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02332390286028385
RAW KL tensor(0.1339, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 230: train/loss = 0.7136610150337219, train/raw-loss = 0.6799508929252625, train/logprobs = tensor([[-1.3532, -2.6859],
        [-1.3661, -2.6176]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06742021441459656
RAW KL tensor(0.0190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 231: train/loss = 0.7251620888710022, train/raw-loss = 0.689162015914917, train/logprobs = tensor([[-1.2776, -1.5842],
        [-1.2753, -1.4845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0720001757144928
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 232: train/loss = 0.6945562958717346, train/raw-loss = 0.6733745336532593, train/logprobs = tensor([[-1.4607, -2.4134],
        [-1.4514, -2.3080]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042363494634628296
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 233: train/loss = 0.6968883275985718, train/raw-loss = 0.6802486777305603, train/logprobs = tensor([[-1.3943, -2.5451],
        [-1.3745, -2.3599]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03327922895550728
RAW KL tensor(0.0108, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 234: train/loss = 0.6691319942474365, train/raw-loss = 0.6425621509552002, train/logprobs = tensor([[-1.3338, -2.3191],
        [-1.3659, -2.1058]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05313959717750549
RAW KL tensor(0.0485, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 235: train/loss = 0.7091517448425293, train/raw-loss = 0.6921894550323486, train/logprobs = tensor([[-1.3070, -1.8824],
        [-1.2751, -1.8153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03392454609274864
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 236: train/loss = 0.6799693703651428, train/raw-loss = 0.670096755027771, train/logprobs = tensor([[-0.7867, -1.4327],
        [-0.7776, -1.3048]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019745267927646637
RAW KL tensor(0.0559, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 237: train/loss = 0.6980160474777222, train/raw-loss = 0.6871969699859619, train/logprobs = tensor([[-0.9144, -1.1403],
        [-0.9007, -1.0623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021638168022036552
RAW KL tensor(0.0118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 238: train/loss = 0.6934780478477478, train/raw-loss = 0.6840447783470154, train/logprobs = tensor([[-0.8946, -1.7496],
        [-0.9322, -1.7237]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018866535276174545
RAW KL tensor(0.0472, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 239: train/loss = 0.7024053931236267, train/raw-loss = 0.6760560274124146, train/logprobs = tensor([[-2.1383, -1.9407],
        [-2.1248, -1.8429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052698832005262375
RAW KL tensor(0.0276, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 240: train/loss = 0.6856521368026733, train/raw-loss = 0.6746038794517517, train/logprobs = tensor([[-0.7391, -1.4455],
        [-0.7765, -1.3884]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022096645087003708
RAW KL tensor(0.0197, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 241: train/loss = 0.7048401832580566, train/raw-loss = 0.6920763850212097, train/logprobs = tensor([[-1.2399, -2.0240],
        [-1.2521, -1.9826]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025527581572532654
RAW KL tensor(0.0143, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 242: train/loss = 0.679649829864502, train/raw-loss = 0.6606857180595398, train/logprobs = tensor([[-1.1987, -1.8113],
        [-1.2030, -1.6720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0379282683134079
RAW KL tensor(0.0089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 243: train/loss = 0.7099441289901733, train/raw-loss = 0.6896086931228638, train/logprobs = tensor([[-1.5774, -1.7892],
        [-1.5704, -1.7095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04067092761397362
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 244: train/loss = 0.6977832317352295, train/raw-loss = 0.6880747675895691, train/logprobs = tensor([[-1.1840, -1.5862],
        [-1.1166, -1.4949]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01941697858273983
RAW KL tensor(0.0188, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 245: train/loss = 0.697048008441925, train/raw-loss = 0.6844419240951538, train/logprobs = tensor([[-1.3156, -1.8098],
        [-1.3220, -1.7668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02521200105547905
RAW KL tensor(0.0411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 246: train/loss = 0.7158101797103882, train/raw-loss = 0.6948926448822021, train/logprobs = tensor([[-1.5049, -2.0708],
        [-1.5134, -2.0304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041835006326436996
RAW KL tensor(0.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 247: train/loss = 0.6895081400871277, train/raw-loss = 0.679263710975647, train/logprobs = tensor([[-1.1997, -1.8502],
        [-1.1733, -1.7512]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02048879861831665
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 248: train/loss = 0.7209743857383728, train/raw-loss = 0.6936308741569519, train/logprobs = tensor([[-1.2102, -2.4885],
        [-1.2039, -2.4682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05468711256980896
RAW KL tensor(0.0813, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 249: train/loss = 0.7135355472564697, train/raw-loss = 0.6839480400085449, train/logprobs = tensor([[-1.5994, -1.6385],
        [-1.6467, -1.6452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05917510390281677
RAW KL tensor(0.0361, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 250: train/loss = 0.6822689175605774, train/raw-loss = 0.6682558059692383, train/logprobs = tensor([[-0.7148, -2.2708],
        [-0.6921, -2.1282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02802618220448494
RAW KL tensor(0.0226, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 251: train/loss = 0.712941586971283, train/raw-loss = 0.6928391456604004, train/logprobs = tensor([[-1.2233, -2.3822],
        [-1.2709, -2.4222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04020491987466812
RAW KL tensor(0.0215, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 252: train/loss = 0.701974630355835, train/raw-loss = 0.6900198459625244, train/logprobs = tensor([[-1.7050, -1.8474],
        [-1.6886, -1.7765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023909585550427437
RAW KL tensor(0.0450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 253: train/loss = 0.6980383396148682, train/raw-loss = 0.6713707447052002, train/logprobs = tensor([[-1.4662, -2.1926],
        [-1.4801, -2.1115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05333523079752922
RAW KL tensor(0.0311, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 254: train/loss = 0.7000539302825928, train/raw-loss = 0.6912052631378174, train/logprobs = tensor([[-1.1428, -1.1065],
        [-1.1328, -1.0833]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017697278410196304
RAW KL tensor(0.0219, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 255: train/loss = 0.6908438801765442, train/raw-loss = 0.6773938536643982, train/logprobs = tensor([[-1.0111, -2.2912],
        [-1.0312, -2.2182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026900025084614754
RAW KL tensor(0.0149, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 256: train/loss = 0.6826949119567871, train/raw-loss = 0.6737290620803833, train/logprobs = tensor([[-0.8889, -1.9743],
        [-0.8934, -1.8933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01793173886835575
RAW KL tensor(0.0413, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 257: train/loss = 0.7092297077178955, train/raw-loss = 0.6935665011405945, train/logprobs = tensor([[-1.5683, -2.4746],
        [-1.5921, -2.4643]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03132644668221474
RAW KL tensor(0.0434, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 116])
Epoch 0, Step 258: train/loss = 0.7150798439979553, train/raw-loss = 0.6995887160301208, train/logprobs = tensor([[-1.4675, -1.8195],
        [-1.4507, -1.7418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030982300639152527
RAW KL tensor(0.0217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 259: train/loss = 0.6997196078300476, train/raw-loss = 0.6907307505607605, train/logprobs = tensor([[-1.8106, -1.6134],
        [-1.7430, -1.5145]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017977718263864517
RAW KL tensor(0.0772, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 260: train/loss = 0.6980597376823425, train/raw-loss = 0.6775123476982117, train/logprobs = tensor([[-1.3776, -1.4754],
        [-1.3756, -1.3871]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041094791144132614
RAW KL tensor(0.0148, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 261: train/loss = 0.6777541041374207, train/raw-loss = 0.6694110035896301, train/logprobs = tensor([[-0.9384, -2.0128],
        [-0.8815, -1.8398]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016686148941516876
RAW KL tensor(0.0105, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 262: train/loss = 0.7110563516616821, train/raw-loss = 0.6981568932533264, train/logprobs = tensor([[-1.3787, -1.9165],
        [-1.3347, -1.8754]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025798995047807693
RAW KL tensor(0.0098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 263: train/loss = 0.653343915939331, train/raw-loss = 0.643418550491333, train/logprobs = tensor([[-1.7702, -3.1368],
        [-1.7193, -2.8594]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01985064707696438
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 264: train/loss = 0.6987603902816772, train/raw-loss = 0.689638614654541, train/logprobs = tensor([[-1.1215, -2.3331],
        [-1.0741, -2.2484]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018243569880723953
RAW KL tensor(0.0197, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 265: train/loss = 0.7204669117927551, train/raw-loss = 0.7070414423942566, train/logprobs = tensor([[-1.7634, -1.8179],
        [-1.7432, -1.7798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02685101330280304
RAW KL tensor(0.0681, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 120])
Epoch 0, Step 266: train/loss = 0.7262879610061646, train/raw-loss = 0.7016843557357788, train/logprobs = tensor([[-1.4083, -1.7482],
        [-1.4014, -1.7123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04920726269483566
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 267: train/loss = 0.6795819401741028, train/raw-loss = 0.6697548627853394, train/logprobs = tensor([[-1.3898, -1.8505],
        [-1.3538, -1.7067]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01965414546430111
RAW KL tensor(0.0318, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 268: train/loss = 0.6853969693183899, train/raw-loss = 0.6699032783508301, train/logprobs = tensor([[-1.3609, -1.8254],
        [-1.3769, -1.6772]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030987441539764404
RAW KL tensor(0.0166, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 269: train/loss = 0.7010635733604431, train/raw-loss = 0.6379863619804382, train/logprobs = tensor([[-1.3731, -2.0102],
        [-1.4755, -1.8581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12615449726581573
RAW KL tensor(0.0112, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 270: train/loss = 0.7025325298309326, train/raw-loss = 0.6918395161628723, train/logprobs = tensor([[-1.0249, -1.8716],
        [-1.0000, -1.8256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021385982632637024
RAW KL tensor(0.0384, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 271: train/loss = 0.6696386337280273, train/raw-loss = 0.6586219072341919, train/logprobs = tensor([[-0.7981, -1.9767],
        [-0.7758, -1.7845]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022033587098121643
RAW KL tensor(0.0364, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 272: train/loss = 0.7110884785652161, train/raw-loss = 0.6897594332695007, train/logprobs = tensor([[-1.4219, -1.5870],
        [-1.4030, -1.5345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042658157646656036
RAW KL tensor(0.0265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 273: train/loss = 0.6980469226837158, train/raw-loss = 0.6840784549713135, train/logprobs = tensor([[-0.9177, -2.2655],
        [-0.9245, -2.1801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02793707326054573
RAW KL tensor(0.0943, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 274: train/loss = 0.6903954744338989, train/raw-loss = 0.6657133102416992, train/logprobs = tensor([[-1.5738, -1.9586],
        [-1.6554, -1.8989]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049364376813173294
RAW KL tensor(0.0234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 275: train/loss = 0.6839256286621094, train/raw-loss = 0.6657981872558594, train/logprobs = tensor([[-1.2316, -2.4003],
        [-1.2332, -2.2804]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03625485301017761
RAW KL tensor(0.0510, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 276: train/loss = 0.6707413196563721, train/raw-loss = 0.6557456851005554, train/logprobs = tensor([[-0.8047, -2.5058],
        [-0.8060, -2.3034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029991228133440018
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 277: train/loss = 0.6846296787261963, train/raw-loss = 0.662857174873352, train/logprobs = tensor([[-1.8314, -2.2787],
        [-1.8021, -2.0386]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043545033782720566
RAW KL tensor(0.0460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 278: train/loss = 0.7322272062301636, train/raw-loss = 0.7078121900558472, train/logprobs = tensor([[-1.6637, -2.6201],
        [-1.6395, -2.4498]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04883001744747162
RAW KL tensor(0.0293, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 279: train/loss = 0.6853160858154297, train/raw-loss = 0.6684021949768066, train/logprobs = tensor([[-1.6337, -1.8688],
        [-1.6152, -1.7336]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033827830106019974
RAW KL tensor(0.0137, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 280: train/loss = 0.7027900815010071, train/raw-loss = 0.6771618127822876, train/logprobs = tensor([[-1.3299, -1.8269],
        [-1.3369, -1.7002]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05125640705227852
RAW KL tensor(0.0418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 281: train/loss = 0.7069559693336487, train/raw-loss = 0.6836198568344116, train/logprobs = tensor([[-1.7114, -2.2988],
        [-1.6916, -2.1276]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0466722697019577
RAW KL tensor(0.0306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 282: train/loss = 0.6993372440338135, train/raw-loss = 0.6791454553604126, train/logprobs = tensor([[-0.9214, -1.7471],
        [-0.9318, -1.6898]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04038344696164131
RAW KL tensor(0.0430, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 283: train/loss = 0.6724979877471924, train/raw-loss = 0.6559584140777588, train/logprobs = tensor([[-0.9188, -2.4347],
        [-0.9397, -2.2565]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03307922184467316
RAW KL tensor(0.0376, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 284: train/loss = 0.7142673134803772, train/raw-loss = 0.6987840533256531, train/logprobs = tensor([[-1.2468, -1.4775],
        [-1.2664, -1.4217]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03096645697951317
RAW KL tensor(0.0251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 285: train/loss = 0.7038933634757996, train/raw-loss = 0.6820167303085327, train/logprobs = tensor([[-1.4316, -2.5463],
        [-1.4242, -2.4375]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0437532477080822
RAW KL tensor(0.0104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 355])
Epoch 0, Step 286: train/loss = 0.7200441956520081, train/raw-loss = 0.6998831033706665, train/logprobs = tensor([[-1.0621, -1.6161],
        [-1.0462, -1.5503]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040322039276361465
RAW KL tensor(0.0586, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 287: train/loss = 0.704940915107727, train/raw-loss = 0.685927152633667, train/logprobs = tensor([[-1.5112, -1.7638],
        [-1.5306, -1.6888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038027647882699966
RAW KL tensor(0.2788, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 288: train/loss = 0.7240744233131409, train/raw-loss = 0.6740068197250366, train/logprobs = tensor([[-1.8577, -2.9333],
        [-1.9260, -2.5954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10013517737388611
RAW KL tensor(0.0362, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 289: train/loss = 0.6952936053276062, train/raw-loss = 0.6808733940124512, train/logprobs = tensor([[-1.2669, -1.3743],
        [-1.2288, -1.2756]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028840426355600357
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 290: train/loss = 0.6975913047790527, train/raw-loss = 0.6774925589561462, train/logprobs = tensor([[-1.6078, -2.0110],
        [-1.5973, -1.9209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04019749164581299
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 242])
Epoch 0, Step 291: train/loss = 0.7018455266952515, train/raw-loss = 0.6931664943695068, train/logprobs = tensor([[-0.7733, -1.0408],
        [-0.7744, -1.0297]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017357908189296722
RAW KL tensor(0.0412, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 292: train/loss = 0.6695711016654968, train/raw-loss = 0.6552612781524658, train/logprobs = tensor([[-1.1451, -1.9393],
        [-1.1211, -1.7307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02861962467432022
RAW KL tensor(0.0181, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 294])
Epoch 0, Step 293: train/loss = 0.6939389109611511, train/raw-loss = 0.6768922805786133, train/logprobs = tensor([[-1.3104, -2.3662],
        [-1.3271, -2.2429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03409328684210777
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 294: train/loss = 0.682289183139801, train/raw-loss = 0.6735447645187378, train/logprobs = tensor([[-0.9178, -1.9430],
        [-0.8836, -1.8252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017488867044448853
RAW KL tensor(0.0306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 295: train/loss = 0.7039337158203125, train/raw-loss = 0.6884464025497437, train/logprobs = tensor([[-1.4893, -2.1254],
        [-1.4705, -2.0695]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03097452223300934
RAW KL tensor(0.1033, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 296: train/loss = 0.6729562878608704, train/raw-loss = 0.6491215229034424, train/logprobs = tensor([[-1.2556, -2.6023],
        [-1.2932, -2.3387]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047669485211372375
RAW KL tensor(0.0191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 297: train/loss = 0.6734294891357422, train/raw-loss = 0.6565435528755188, train/logprobs = tensor([[-1.3712, -2.3661],
        [-1.3618, -2.1934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03377171978354454
RAW KL tensor(0.0130, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 298: train/loss = 0.6723641157150269, train/raw-loss = 0.6496831774711609, train/logprobs = tensor([[-1.0700, -2.3024],
        [-1.1114, -2.1433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045361943542957306
RAW KL tensor(0.0164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 299: train/loss = 0.6934083104133606, train/raw-loss = 0.6731642484664917, train/logprobs = tensor([[-1.5426, -1.3632],
        [-1.5644, -1.2718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04048824682831764
RAW KL tensor(0.0310, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 300: train/loss = 0.6954474449157715, train/raw-loss = 0.6814901828765869, train/logprobs = tensor([[-1.5523, -1.5508],
        [-1.5397, -1.4800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02791472151875496
RAW KL tensor(0.0032, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 301: train/loss = 0.6897241473197937, train/raw-loss = 0.6824292540550232, train/logprobs = tensor([[-1.0100, -1.3423],
        [-0.9891, -1.2740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014589682221412659
RAW KL tensor(0.0122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 302: train/loss = 0.6538622975349426, train/raw-loss = 0.6402592062950134, train/logprobs = tensor([[-0.9971, -2.4002],
        [-1.0177, -2.0784]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027206256985664368
RAW KL tensor(0.0169, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 303: train/loss = 0.6897931098937988, train/raw-loss = 0.6737481355667114, train/logprobs = tensor([[-1.3116, -1.9650],
        [-1.2580, -1.8064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032090019434690475
RAW KL tensor(0.0211, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 304: train/loss = 0.6781936883926392, train/raw-loss = 0.6654016971588135, train/logprobs = tensor([[-1.3338, -2.0067],
        [-1.2972, -1.8345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025584040209650993
RAW KL tensor(0.0807, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 305: train/loss = 0.7074548006057739, train/raw-loss = 0.6854351162910461, train/logprobs = tensor([[-1.2514, -1.5158],
        [-1.2373, -1.3742]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04403930902481079
RAW KL tensor(0.0633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 306: train/loss = 0.664136528968811, train/raw-loss = 0.649104118347168, train/logprobs = tensor([[-1.0644, -2.1346],
        [-1.0422, -1.9160]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03006478026509285
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 307: train/loss = 0.647587776184082, train/raw-loss = 0.6286436915397644, train/logprobs = tensor([[-1.0447, -2.1382],
        [-1.0967, -1.9131]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037888191640377045
RAW KL tensor(0.0244, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 308: train/loss = 0.702702522277832, train/raw-loss = 0.6804215908050537, train/logprobs = tensor([[-1.4126, -2.1914],
        [-1.3995, -2.0737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044561728835105896
RAW KL tensor(0.0323, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 309: train/loss = 0.6740075349807739, train/raw-loss = 0.6642194986343384, train/logprobs = tensor([[-0.9528, -1.5508],
        [-0.9503, -1.4020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019576139748096466
RAW KL tensor(0.0230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 310: train/loss = 0.6510314345359802, train/raw-loss = 0.6296740770339966, train/logprobs = tensor([[-1.2485, -3.0148],
        [-1.2579, -2.7315]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042714886367321014
RAW KL tensor(0.0156, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 224])
Epoch 0, Step 311: train/loss = 0.6701502203941345, train/raw-loss = 0.6540608406066895, train/logprobs = tensor([[-0.9062, -1.6713],
        [-0.8918, -1.4902]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03217872977256775
RAW KL tensor(0.0386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 312: train/loss = 0.7146533727645874, train/raw-loss = 0.6952571868896484, train/logprobs = tensor([[-1.4294, -1.8454],
        [-1.4436, -1.8249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03879251703619957
RAW KL tensor(0.0239, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 313: train/loss = 0.6713446378707886, train/raw-loss = 0.6588871479034424, train/logprobs = tensor([[-1.0764, -2.7339],
        [-1.0737, -2.5324]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024914942681789398
RAW KL tensor(0.0511, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 447])
Epoch 0, Step 314: train/loss = 0.6953705549240112, train/raw-loss = 0.6742914915084839, train/logprobs = tensor([[-0.9878, -2.7007],
        [-1.0008, -2.5797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042158160358667374
RAW KL tensor(0.0306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 315: train/loss = 0.695429801940918, train/raw-loss = 0.6835673451423645, train/logprobs = tensor([[-0.9019, -2.6156],
        [-0.8593, -2.5251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023724878206849098
RAW KL tensor(0.0166, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 239])
Epoch 0, Step 316: train/loss = 0.6691913604736328, train/raw-loss = 0.6558064222335815, train/logprobs = tensor([[-1.3756, -2.0900],
        [-1.3828, -1.9305]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02676990069448948
RAW KL tensor(0.0086, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 317: train/loss = 0.6948803067207336, train/raw-loss = 0.6747867465019226, train/logprobs = tensor([[-1.0902, -1.5056],
        [-1.0696, -1.3645]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04018707573413849
RAW KL tensor(0.0227, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 318: train/loss = 0.6954665184020996, train/raw-loss = 0.6795979142189026, train/logprobs = tensor([[-1.3099, -2.1421],
        [-1.3035, -2.0453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03173724189400673
RAW KL tensor(0.0350, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 319: train/loss = 0.6757078170776367, train/raw-loss = 0.6625518798828125, train/logprobs = tensor([[-1.4776, -2.1344],
        [-1.4794, -1.9962]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026311984285712242
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 320: train/loss = 0.6581091284751892, train/raw-loss = 0.648559033870697, train/logprobs = tensor([[-0.7085, -1.7343],
        [-0.6907, -1.5235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019100211560726166
RAW KL tensor(0.0316, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 321: train/loss = 0.6782965660095215, train/raw-loss = 0.6656876802444458, train/logprobs = tensor([[-1.6658, -2.4453],
        [-1.6333, -2.2745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02521771751344204
RAW KL tensor(0.0214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 322: train/loss = 0.6749574542045593, train/raw-loss = 0.650069534778595, train/logprobs = tensor([[-1.4497, -2.8047],
        [-1.4504, -2.5201]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04977590590715408
RAW KL tensor(0.0083, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 323: train/loss = 0.6864922642707825, train/raw-loss = 0.6781597137451172, train/logprobs = tensor([[-1.1505, -0.9523],
        [-1.0997, -0.8334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01666523516178131
RAW KL tensor(0.0213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 99])
Epoch 0, Step 324: train/loss = 0.6570925116539001, train/raw-loss = 0.6467885971069336, train/logprobs = tensor([[-1.6358, -2.2634],
        [-1.6549, -2.0631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020607758313417435
RAW KL tensor(0.0210, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 325: train/loss = 0.6938225030899048, train/raw-loss = 0.6366584300994873, train/logprobs = tensor([[-2.1064, -1.9365],
        [-2.0554, -1.5174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11432813107967377
RAW KL tensor(0.0761, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 326: train/loss = 0.6727004051208496, train/raw-loss = 0.6483851671218872, train/logprobs = tensor([[-1.6556, -2.3525],
        [-1.6335, -2.1269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04863046854734421
RAW KL tensor(0.0238, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 327: train/loss = 0.6470929980278015, train/raw-loss = 0.6340325474739075, train/logprobs = tensor([[-1.4700, -2.2442],
        [-1.4369, -1.9541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.026120906695723534
RAW KL tensor(0.0226, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 328: train/loss = 0.6491599082946777, train/raw-loss = 0.6403979063034058, train/logprobs = tensor([[-0.8106, -1.3689],
        [-0.7383, -1.0676]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01752394810318947
RAW KL tensor(0.0197, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 329: train/loss = 0.6526892185211182, train/raw-loss = 0.6411135196685791, train/logprobs = tensor([[-1.1861, -1.4973],
        [-1.2255, -1.3027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023151587694883347
RAW KL tensor(0.0183, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 330: train/loss = 0.6960760354995728, train/raw-loss = 0.6701325178146362, train/logprobs = tensor([[-1.3200, -2.2320],
        [-1.3778, -2.1210]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05188710242509842
RAW KL tensor(0.0134, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 331: train/loss = 0.6856943368911743, train/raw-loss = 0.672873318195343, train/logprobs = tensor([[-1.6516, -1.8674],
        [-1.5115, -1.6017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025641946122050285
RAW KL tensor(0.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 332: train/loss = 0.6887006759643555, train/raw-loss = 0.6772169470787048, train/logprobs = tensor([[-2.0573, -1.5242],
        [-2.0511, -1.4474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0229675080627203
RAW KL tensor(0.0739, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 333: train/loss = 0.6460977792739868, train/raw-loss = 0.624667763710022, train/logprobs = tensor([[-1.5632, -2.3988],
        [-1.5493, -2.0542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042860113084316254
RAW KL tensor(0.0220, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 334: train/loss = 0.6764417290687561, train/raw-loss = 0.6656662225723267, train/logprobs = tensor([[-1.6735, -2.2718],
        [-1.6327, -2.0965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.021551061421632767
RAW KL tensor(0.0254, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 335: train/loss = 0.6715365648269653, train/raw-loss = 0.6593774557113647, train/logprobs = tensor([[-1.0338, -1.6722],
        [-1.0110, -1.4686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024318266659975052
RAW KL tensor(0.0288, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 336: train/loss = 0.6702324748039246, train/raw-loss = 0.6572879552841187, train/logprobs = tensor([[-1.2423, -1.8054],
        [-1.2028, -1.5886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0258890800178051
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 337: train/loss = 0.6682168245315552, train/raw-loss = 0.6528869867324829, train/logprobs = tensor([[-1.3883, -2.7728],
        [-1.3806, -2.5673]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030659547075629234
RAW KL tensor(0.0377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 338: train/loss = 0.6625704765319824, train/raw-loss = 0.6489847898483276, train/logprobs = tensor([[-1.4341, -2.1304],
        [-1.3856, -1.8741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02717130444943905
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 339: train/loss = 0.6543579697608948, train/raw-loss = 0.6427358388900757, train/logprobs = tensor([[-1.6674, -1.7936],
        [-1.6403, -1.5354]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02324433997273445
RAW KL tensor(0.0402, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 340: train/loss = 0.6669744253158569, train/raw-loss = 0.6422659158706665, train/logprobs = tensor([[-1.7131, -1.8422],
        [-1.6455, -1.4865]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049416959285736084
RAW KL tensor(0.0304, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 341: train/loss = 0.66975337266922, train/raw-loss = 0.6574185490608215, train/logprobs = tensor([[-1.4472, -1.8959],
        [-1.4659, -1.7231]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024669665843248367
RAW KL tensor(0.0343, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 342: train/loss = 0.6692396998405457, train/raw-loss = 0.6551745533943176, train/logprobs = tensor([[-1.5082, -1.8598],
        [-1.4815, -1.6397]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028130261227488518
RAW KL tensor(0.0265, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 343: train/loss = 0.6663897037506104, train/raw-loss = 0.6551399230957031, train/logprobs = tensor([[-0.9019, -2.0225],
        [-0.9235, -1.8617]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022499579936265945
RAW KL tensor(0.0640, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 344: train/loss = 0.7054718732833862, train/raw-loss = 0.6858742237091064, train/logprobs = tensor([[-1.8146, -1.7298],
        [-1.7490, -1.6037]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039195310324430466
RAW KL tensor(0.0085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 345: train/loss = 0.6932570338249207, train/raw-loss = 0.6780864000320435, train/logprobs = tensor([[-1.4616, -1.6230],
        [-1.4079, -1.4394]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03034108132123947
RAW KL tensor(0.0487, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 346: train/loss = 0.6567617654800415, train/raw-loss = 0.6306252479553223, train/logprobs = tensor([[-1.3047, -2.5596],
        [-1.2486, -2.2254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0522729828953743
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 347: train/loss = 0.7087620496749878, train/raw-loss = 0.6849958300590515, train/logprobs = tensor([[-1.6771, -1.9235],
        [-1.6342, -1.8007]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0475323460996151
RAW KL tensor(0.0244, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 348: train/loss = 0.6805193424224854, train/raw-loss = 0.6691761016845703, train/logprobs = tensor([[-1.0003, -1.5339],
        [-1.0001, -1.4194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022686515003442764
RAW KL tensor(0.0157, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 349: train/loss = 0.6629711389541626, train/raw-loss = 0.6515356302261353, train/logprobs = tensor([[-1.1664, -1.9591],
        [-1.1424, -1.7488]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022871041670441628
RAW KL tensor(0.0353, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 350: train/loss = 0.6420261263847351, train/raw-loss = 0.6243425011634827, train/logprobs = tensor([[-0.9854, -3.6999],
        [-0.9722, -3.3678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035367317497730255
RAW KL tensor(0.0295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 302])
Epoch 0, Step 351: train/loss = 0.675457775592804, train/raw-loss = 0.6545937061309814, train/logprobs = tensor([[-1.6504, -3.6938],
        [-1.6052, -3.4019]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041728101670742035
RAW KL tensor(0.0327, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 352: train/loss = 0.6863692998886108, train/raw-loss = 0.6691672801971436, train/logprobs = tensor([[-1.3436, -2.3788],
        [-1.4271, -2.2837]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034404002130031586
RAW KL tensor(0.0329, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 353: train/loss = 0.6794198751449585, train/raw-loss = 0.6590763330459595, train/logprobs = tensor([[-1.5035, -1.6447],
        [-1.5915, -1.5303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04068708047270775
RAW KL tensor(0.0450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 354: train/loss = 0.7427780628204346, train/raw-loss = 0.7166800498962402, train/logprobs = tensor([[-0.8314, -2.9904],
        [-0.9152, -3.0905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05219608545303345
RAW KL tensor(0.0903, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 355: train/loss = 0.7232066988945007, train/raw-loss = 0.6972201466560364, train/logprobs = tensor([[-0.8460, -1.6066],
        [-0.9352, -1.6771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05197308212518692
RAW KL tensor(0.0165, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 356: train/loss = 0.6989427804946899, train/raw-loss = 0.6838760375976562, train/logprobs = tensor([[-1.1278, -1.2397],
        [-1.1581, -1.2074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030133545398712158
RAW KL tensor(0.0344, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 345])
Epoch 0, Step 357: train/loss = 0.6887949705123901, train/raw-loss = 0.6730303764343262, train/logprobs = tensor([[-0.9185, -2.4755],
        [-0.9509, -2.3390]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03152914717793465
RAW KL tensor(0.0219, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 358: train/loss = 0.7195340394973755, train/raw-loss = 0.6912593841552734, train/logprobs = tensor([[-1.1502, -1.8300],
        [-1.2921, -1.8581]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05654921010136604
RAW KL tensor(0.0121, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 359: train/loss = 0.6965083479881287, train/raw-loss = 0.6817833185195923, train/logprobs = tensor([[-0.8982, -2.1797],
        [-0.9926, -2.1618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029450198635458946
RAW KL tensor(0.0220, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 249])
Epoch 0, Step 360: train/loss = 0.7306134104728699, train/raw-loss = 0.7108431458473206, train/logprobs = tensor([[-0.8829, -2.9029],
        [-0.9456, -2.8802]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03954050689935684
RAW KL tensor(0.0888, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 361: train/loss = 0.7226879000663757, train/raw-loss = 0.6946016550064087, train/logprobs = tensor([[-0.8074, -2.3317],
        [-0.8979, -2.3303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05617247894406319
RAW KL tensor(0.0281, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 362: train/loss = 0.7163267731666565, train/raw-loss = 0.693920373916626, train/logprobs = tensor([[-1.3684, -1.6959],
        [-1.5028, -1.7927]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04481291398406029
RAW KL tensor(0.0638, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 363: train/loss = 0.690453052520752, train/raw-loss = 0.663115382194519, train/logprobs = tensor([[-1.4092, -2.1777],
        [-1.4981, -2.0879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.054675210267305374
RAW KL tensor(0.0898, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 364: train/loss = 0.7519720196723938, train/raw-loss = 0.7222826480865479, train/logprobs = tensor([[-1.1504, -1.4393],
        [-1.2663, -1.4933]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059378765523433685
RAW KL tensor(0.0289, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 365: train/loss = 0.7282454967498779, train/raw-loss = 0.7105953693389893, train/logprobs = tensor([[-1.0912, -1.1604],
        [-1.2240, -1.1942]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03530025854706764
RAW KL tensor(0.0432, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 366: train/loss = 0.7070505023002625, train/raw-loss = 0.6838333606719971, train/logprobs = tensor([[-1.4260, -1.5683],
        [-1.4940, -1.5003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046434223651885986
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 367: train/loss = 0.6761772632598877, train/raw-loss = 0.6664230823516846, train/logprobs = tensor([[-1.1307, -1.5354],
        [-1.2341, -1.5135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01950829289853573
RAW KL tensor(0.1592, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 368: train/loss = 0.7573401927947998, train/raw-loss = 0.7218797206878662, train/logprobs = tensor([[-1.5143, -2.0620],
        [-1.6120, -2.0354]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07092087715864182
RAW KL tensor(0.0243, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 369: train/loss = 0.7065081000328064, train/raw-loss = 0.6905428171157837, train/logprobs = tensor([[-1.2806, -1.4199],
        [-1.3612, -1.3293]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03193061798810959
RAW KL tensor(0.0258, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 370: train/loss = 0.725021243095398, train/raw-loss = 0.7042261958122253, train/logprobs = tensor([[-1.4797, -1.8270],
        [-1.5512, -1.8226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04159008711576462
RAW KL tensor(0.0178, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 461])
Epoch 0, Step 371: train/loss = 0.7060673236846924, train/raw-loss = 0.6855494976043701, train/logprobs = tensor([[-0.6867, -1.0539],
        [-0.7890, -1.0891]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04103558510541916
RAW KL tensor(0.1052, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 372: train/loss = 0.7168654799461365, train/raw-loss = 0.690772294998169, train/logprobs = tensor([[-1.3119, -1.3636],
        [-1.4067, -1.4474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052186235785484314
RAW KL tensor(0.0379, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 373: train/loss = 0.6906082630157471, train/raw-loss = 0.6723415851593018, train/logprobs = tensor([[-1.0251, -2.4531],
        [-1.0920, -2.3805]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03653338924050331
RAW KL tensor(0.0419, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 374: train/loss = 0.7015752792358398, train/raw-loss = 0.6808310151100159, train/logprobs = tensor([[-0.8427, -1.0106],
        [-0.9573, -1.0426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0414886474609375
RAW KL tensor(0.0454, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 375: train/loss = 0.7215884923934937, train/raw-loss = 0.698900043964386, train/logprobs = tensor([[-1.3236, -1.5636],
        [-1.4067, -1.6064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04537687450647354
RAW KL tensor(0.0432, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 376: train/loss = 0.7174771428108215, train/raw-loss = 0.699427604675293, train/logprobs = tensor([[-1.3443, -1.5392],
        [-1.4066, -1.5636]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03609900549054146
RAW KL tensor(0.0147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 377: train/loss = 0.7060115337371826, train/raw-loss = 0.6942876577377319, train/logprobs = tensor([[-0.7213, -0.7003],
        [-0.8053, -0.7728]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023447701707482338
RAW KL tensor(0.0633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 378: train/loss = 0.7518806457519531, train/raw-loss = 0.7250939607620239, train/logprobs = tensor([[-2.0122, -1.6613],
        [-1.9917, -1.6586]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05357345938682556
RAW KL tensor(0.1019, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 379: train/loss = 0.7527039051055908, train/raw-loss = 0.7261379957199097, train/logprobs = tensor([[-1.5757, -2.3106],
        [-1.6018, -2.1894]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05313171446323395
RAW KL tensor(0.0431, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 380: train/loss = 0.7148900032043457, train/raw-loss = 0.6966652870178223, train/logprobs = tensor([[-1.0709, -1.4773],
        [-1.1670, -1.5130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03644947707653046
RAW KL tensor(0.1467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 381: train/loss = 0.7667296528816223, train/raw-loss = 0.7371168732643127, train/logprobs = tensor([[-1.0799, -1.7020],
        [-1.1641, -1.7280]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05922552943229675
RAW KL tensor(0.0313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 382: train/loss = 0.6763469576835632, train/raw-loss = 0.6560795307159424, train/logprobs = tensor([[-1.2497, -1.6559],
        [-1.2693, -1.4560]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0405348502099514
RAW KL tensor(0.0253, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 383: train/loss = 0.6868592500686646, train/raw-loss = 0.6731147766113281, train/logprobs = tensor([[-1.9247, -1.8084],
        [-1.9531, -1.7319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027488915249705315
RAW KL tensor(0.0339, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 384: train/loss = 0.7137687802314758, train/raw-loss = 0.697566568851471, train/logprobs = tensor([[-0.8766, -1.3394],
        [-0.9755, -1.3974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0324043408036232
RAW KL tensor(0.0598, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 385: train/loss = 0.7015104293823242, train/raw-loss = 0.6788382530212402, train/logprobs = tensor([[-1.5939, -1.5253],
        [-1.5662, -1.4186]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045344289392232895
RAW KL tensor(0.1024, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 386: train/loss = 0.7445082664489746, train/raw-loss = 0.6903223991394043, train/logprobs = tensor([[-1.6549, -1.9631],
        [-1.7220, -1.9430]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10837171226739883
RAW KL tensor(0.4608, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 387: train/loss = 0.7738455533981323, train/raw-loss = 0.7013275623321533, train/logprobs = tensor([[-1.2092, -1.4085],
        [-1.2669, -1.3771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14503596723079681
RAW KL tensor(0.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 388: train/loss = 0.7271814942359924, train/raw-loss = 0.6996058821678162, train/logprobs = tensor([[-1.4619, -1.3786],
        [-1.4449, -1.3163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05515117198228836
RAW KL tensor(0.0566, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 389: train/loss = 0.6919096112251282, train/raw-loss = 0.6667841672897339, train/logprobs = tensor([[-0.9054, -1.5195],
        [-0.9866, -1.4764]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05025096982717514
RAW KL tensor(0.0835, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 390: train/loss = 0.734327495098114, train/raw-loss = 0.7041217684745789, train/logprobs = tensor([[-1.3876, -1.8584],
        [-1.3879, -1.7188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.060411486774683
RAW KL tensor(0.0320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 391: train/loss = 0.7323750257492065, train/raw-loss = 0.6858296394348145, train/logprobs = tensor([[-1.3070, -1.5468],
        [-1.2948, -1.4611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09309066832065582
RAW KL tensor(0.0640, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 392: train/loss = 0.7310009002685547, train/raw-loss = 0.7000483274459839, train/logprobs = tensor([[-1.2243, -1.4596],
        [-1.3318, -1.4625]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061905115842819214
RAW KL tensor(0.0630, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 393: train/loss = 0.7188906669616699, train/raw-loss = 0.6916589736938477, train/logprobs = tensor([[-1.8916, -1.9511],
        [-1.9771, -1.9668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05446340888738632
RAW KL tensor(0.0597, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 394: train/loss = 0.7442905902862549, train/raw-loss = 0.715787410736084, train/logprobs = tensor([[-1.5552, -2.1284],
        [-1.5317, -1.9803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.057006314396858215
RAW KL tensor(0.0330, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 395: train/loss = 0.7112356424331665, train/raw-loss = 0.6908162832260132, train/logprobs = tensor([[-1.2468, -1.5594],
        [-1.3411, -1.5723]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040838584303855896
RAW KL tensor(0.0908, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 396: train/loss = 0.754281759262085, train/raw-loss = 0.7164456844329834, train/logprobs = tensor([[-1.5195, -1.8135],
        [-1.5599, -1.7314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07567216455936432
RAW KL tensor(0.0432, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 397: train/loss = 0.7225756645202637, train/raw-loss = 0.7013959884643555, train/logprobs = tensor([[-1.4572, -0.9782],
        [-1.5262, -1.0282]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042359210550785065
RAW KL tensor(0.1491, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 398: train/loss = 0.7777780890464783, train/raw-loss = 0.7378448843955994, train/logprobs = tensor([[-1.1998, -2.0676],
        [-1.2620, -2.0249]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07986634969711304
RAW KL tensor(0.0455, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 399: train/loss = 0.7257581353187561, train/raw-loss = 0.6999576687812805, train/logprobs = tensor([[-1.0423, -1.9620],
        [-1.1194, -1.9906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05160092934966087
RAW KL tensor(0.0261, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 400: train/loss = 0.7129451036453247, train/raw-loss = 0.6864659786224365, train/logprobs = tensor([[-1.6066, -1.4198],
        [-1.6755, -1.3888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05295836180448532
RAW KL tensor(0.0595, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 401: train/loss = 0.6995060443878174, train/raw-loss = 0.6801139712333679, train/logprobs = tensor([[-1.2891, -1.8697],
        [-1.3606, -1.7563]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03878416121006012
RAW KL tensor(0.0666, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 402: train/loss = 0.702261745929718, train/raw-loss = 0.6763949990272522, train/logprobs = tensor([[-1.2789, -1.7036],
        [-1.3942, -1.7150]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05173346400260925
RAW KL tensor(0.0395, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 403: train/loss = 0.7143468260765076, train/raw-loss = 0.6968631744384766, train/logprobs = tensor([[-1.9433, -1.5753],
        [-1.9599, -1.5115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034967321902513504
RAW KL tensor(0.0619, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 404: train/loss = 0.7025637030601501, train/raw-loss = 0.6749646663665771, train/logprobs = tensor([[-1.6365, -1.9213],
        [-1.7772, -1.9010]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05519804358482361
RAW KL tensor(0.0538, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 405: train/loss = 0.735028088092804, train/raw-loss = 0.7080957889556885, train/logprobs = tensor([[-0.9815, -1.4364],
        [-1.0388, -1.4628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05386459082365036
RAW KL tensor(0.0749, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 406: train/loss = 0.6924831867218018, train/raw-loss = 0.6585627794265747, train/logprobs = tensor([[-1.6693, -2.1099],
        [-1.7451, -1.9295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06784091889858246
RAW KL tensor(0.0884, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 407: train/loss = 0.7207245230674744, train/raw-loss = 0.6834234595298767, train/logprobs = tensor([[-1.7171, -2.0331],
        [-1.8221, -2.0429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07460221648216248
RAW KL tensor(0.0276, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 408: train/loss = 0.7262343764305115, train/raw-loss = 0.7036499977111816, train/logprobs = tensor([[-1.2510, -1.3666],
        [-1.2966, -1.3714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04516886547207832
RAW KL tensor(0.0349, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 409: train/loss = 0.7108089327812195, train/raw-loss = 0.6934943795204163, train/logprobs = tensor([[-0.9445, -1.5084],
        [-1.0268, -1.4767]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03462905064225197
RAW KL tensor(0.0452, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 410: train/loss = 0.793989896774292, train/raw-loss = 0.7004434466362, train/logprobs = tensor([[-1.1329, -1.4322],
        [-1.1622, -1.4068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1870928555727005
RAW KL tensor(0.0087, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 411: train/loss = 0.7069038152694702, train/raw-loss = 0.6900737285614014, train/logprobs = tensor([[-1.4057, -1.2874],
        [-1.3534, -1.2216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03366011008620262
RAW KL tensor(0.0727, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 412: train/loss = 0.7086187601089478, train/raw-loss = 0.6772552132606506, train/logprobs = tensor([[-1.3225, -1.6557],
        [-1.4801, -1.6662]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06272700428962708
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 413: train/loss = 0.7362192869186401, train/raw-loss = 0.7032186985015869, train/logprobs = tensor([[-1.4932, -1.2976],
        [-1.5151, -1.3287]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06600115448236465
RAW KL tensor(0.0509, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 414: train/loss = 0.6950369477272034, train/raw-loss = 0.6757851243019104, train/logprobs = tensor([[-1.1216, -1.3425],
        [-1.1772, -1.2922]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03850356489419937
RAW KL tensor(0.0479, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 263])
Epoch 0, Step 415: train/loss = 0.7196154594421387, train/raw-loss = 0.6967726945877075, train/logprobs = tensor([[-0.7633, -2.0257],
        [-0.8494, -1.9733]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045685552060604095
RAW KL tensor(0.0391, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 416: train/loss = 0.6954065561294556, train/raw-loss = 0.6755355596542358, train/logprobs = tensor([[-1.0525, -2.0199],
        [-1.0984, -1.9622]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03974209353327751
RAW KL tensor(0.0549, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 417: train/loss = 0.6563488245010376, train/raw-loss = 0.6320239305496216, train/logprobs = tensor([[-1.0735, -3.4769],
        [-1.2053, -3.3464]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04864975064992905
RAW KL tensor(0.1039, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 418: train/loss = 0.7204065918922424, train/raw-loss = 0.6914973258972168, train/logprobs = tensor([[-1.7306, -1.3938],
        [-1.7238, -1.3520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05781858414411545
RAW KL tensor(0.0576, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 419: train/loss = 0.7092700004577637, train/raw-loss = 0.6774563193321228, train/logprobs = tensor([[-2.0321, -2.1313],
        [-2.0564, -1.9973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06362725794315338
RAW KL tensor(0.0997, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 420: train/loss = 0.7099891901016235, train/raw-loss = 0.6752877235412598, train/logprobs = tensor([[-1.4768, -2.2858],
        [-1.5145, -2.1756]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06940287351608276
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 421: train/loss = 0.6952594518661499, train/raw-loss = 0.6809375286102295, train/logprobs = tensor([[-0.7378, -1.7055],
        [-0.8246, -1.6935]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028643779456615448
RAW KL tensor(0.0511, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 422: train/loss = 0.6664868593215942, train/raw-loss = 0.645143985748291, train/logprobs = tensor([[-1.1774, -1.4900],
        [-1.2479, -1.3143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042685747146606445
RAW KL tensor(0.0506, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 423: train/loss = 0.7184919714927673, train/raw-loss = 0.6929993033409119, train/logprobs = tensor([[-2.1792, -1.7198],
        [-2.2084, -1.6217]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05098536238074303
RAW KL tensor(0.0526, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 424: train/loss = 0.65289306640625, train/raw-loss = 0.6377423405647278, train/logprobs = tensor([[-0.8438, -1.7596],
        [-0.8984, -1.5332]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030301347374916077
RAW KL tensor(0.0468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 425: train/loss = 0.656969428062439, train/raw-loss = 0.6413987874984741, train/logprobs = tensor([[-1.2428, -2.7784],
        [-1.3559, -2.6515]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031141281127929688
RAW KL tensor(0.0938, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 426: train/loss = 0.7013183236122131, train/raw-loss = 0.6666020750999451, train/logprobs = tensor([[-1.4613, -1.7249],
        [-1.5170, -1.5694]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06943238526582718
RAW KL tensor(0.0293, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 427: train/loss = 0.7083089351654053, train/raw-loss = 0.6909627914428711, train/logprobs = tensor([[-1.0379, -1.2382],
        [-1.0702, -1.2451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03469225764274597
RAW KL tensor(0.0283, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 428: train/loss = 0.68089359998703, train/raw-loss = 0.6621631383895874, train/logprobs = tensor([[-1.6450, -1.5712],
        [-1.7299, -1.4750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03746091574430466
RAW KL tensor(0.0174, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 285])
Epoch 0, Step 429: train/loss = 0.6968334913253784, train/raw-loss = 0.6844974160194397, train/logprobs = tensor([[-1.1239, -1.1285],
        [-1.1299, -1.0880]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024672197178006172
RAW KL tensor(0.0145, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 430: train/loss = 0.6732882857322693, train/raw-loss = 0.6508051753044128, train/logprobs = tensor([[-1.1884, -2.4711],
        [-1.1577, -2.1719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044966328889131546
RAW KL tensor(0.0972, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 431: train/loss = 0.7412921190261841, train/raw-loss = 0.6464537382125854, train/logprobs = tensor([[-1.8928, -2.5510],
        [-1.9047, -2.3435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1896766722202301
RAW KL tensor(0.0530, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 432: train/loss = 0.7162855863571167, train/raw-loss = 0.693002462387085, train/logprobs = tensor([[-1.2333, -1.5487],
        [-1.3116, -1.6032]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046566352248191833
RAW KL tensor(0.0661, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 433: train/loss = 0.6565905809402466, train/raw-loss = 0.6301602721214294, train/logprobs = tensor([[-1.7616, -2.4734],
        [-1.8605, -2.2776]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052860602736473083
RAW KL tensor(0.0881, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 434: train/loss = 0.6993893384933472, train/raw-loss = 0.6745831966400146, train/logprobs = tensor([[-1.4647, -2.4542],
        [-1.5839, -2.4588]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04961240664124489
RAW KL tensor(0.1124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 435: train/loss = 0.743426501750946, train/raw-loss = 0.7118458151817322, train/logprobs = tensor([[-1.9091, -2.2730],
        [-1.9590, -2.3508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06316135823726654
RAW KL tensor(0.0434, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 436: train/loss = 0.7049723863601685, train/raw-loss = 0.6872139573097229, train/logprobs = tensor([[-1.2178, -1.1166],
        [-1.2741, -1.1256]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03551693260669708
RAW KL tensor(0.0847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 437: train/loss = 0.6521614193916321, train/raw-loss = 0.6250332593917847, train/logprobs = tensor([[-1.1951, -2.3351],
        [-1.3215, -2.1397]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05425621569156647
RAW KL tensor(0.0388, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 438: train/loss = 0.700388491153717, train/raw-loss = 0.6786884665489197, train/logprobs = tensor([[-1.4137, -1.9003],
        [-1.3701, -1.7555]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043400056660175323
RAW KL tensor(0.0236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 439: train/loss = 0.687231183052063, train/raw-loss = 0.651153564453125, train/logprobs = tensor([[-1.3053, -1.7677],
        [-1.4209, -1.6647]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07215532660484314
RAW KL tensor(0.0191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 347])
Epoch 0, Step 440: train/loss = 0.6914123892784119, train/raw-loss = 0.6610532999038696, train/logprobs = tensor([[-1.9194, -1.6595],
        [-1.9484, -1.5011]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06071816012263298
RAW KL tensor(0.0491, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 441: train/loss = 0.6589195132255554, train/raw-loss = 0.6302845478057861, train/logprobs = tensor([[-1.2954, -2.5451],
        [-1.4312, -2.3453]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.057269930839538574
RAW KL tensor(0.0583, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 442: train/loss = 0.6984338760375977, train/raw-loss = 0.6673835515975952, train/logprobs = tensor([[-1.9917, -1.8401],
        [-2.0464, -1.6492]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.062100689858198166
RAW KL tensor(0.0685, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 443: train/loss = 0.7005751132965088, train/raw-loss = 0.6714997291564941, train/logprobs = tensor([[-1.5365, -2.2288],
        [-1.6116, -2.0956]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05815063416957855
RAW KL tensor(0.0434, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 444: train/loss = 0.7038310766220093, train/raw-loss = 0.6875936985015869, train/logprobs = tensor([[-0.8590, -1.3616],
        [-0.9443, -1.4216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03247499465942383
RAW KL tensor(0.0328, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 445: train/loss = 0.7004421353340149, train/raw-loss = 0.6768833994865417, train/logprobs = tensor([[-1.9178, -1.4151],
        [-1.9368, -1.2752]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047117479145526886
RAW KL tensor(0.0421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 446: train/loss = 0.6694597601890564, train/raw-loss = 0.6508026719093323, train/logprobs = tensor([[-1.1296, -2.3072],
        [-1.1451, -2.0984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037314191460609436
RAW KL tensor(0.1126, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 447: train/loss = 0.6803383231163025, train/raw-loss = 0.6500541567802429, train/logprobs = tensor([[-1.0975, -2.1005],
        [-1.1398, -1.8968]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06056837737560272
RAW KL tensor(0.0279, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 448: train/loss = 0.7064599990844727, train/raw-loss = 0.680748462677002, train/logprobs = tensor([[-1.4990, -1.9944],
        [-1.4653, -1.7632]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.051423072814941406
RAW KL tensor(0.0708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 449: train/loss = 0.6714712381362915, train/raw-loss = 0.6548817157745361, train/logprobs = tensor([[-1.5378, -1.5364],
        [-1.4923, -1.3270]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033179134130477905
RAW KL tensor(0.0331, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 450: train/loss = 0.6795128583908081, train/raw-loss = 0.6697880029678345, train/logprobs = tensor([[-0.7459, -1.4375],
        [-0.7326, -1.3106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01944991573691368
RAW KL tensor(0.0158, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 451: train/loss = 0.6617183089256287, train/raw-loss = 0.6514858603477478, train/logprobs = tensor([[-1.7767, -2.1961],
        [-1.6480, -1.8902]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020464899018406868
RAW KL tensor(0.0298, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 452: train/loss = 0.6062865257263184, train/raw-loss = 0.5857724547386169, train/logprobs = tensor([[-1.4254, -2.0414],
        [-1.4589, -1.6088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04102807119488716
RAW KL tensor(0.0246, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 453: train/loss = 0.6061719059944153, train/raw-loss = 0.5843029618263245, train/logprobs = tensor([[-1.2783, -2.4931],
        [-1.3063, -1.9988]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043737925589084625
RAW KL tensor(0.0680, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 454: train/loss = 0.6835232377052307, train/raw-loss = 0.6604408025741577, train/logprobs = tensor([[-1.6835, -1.7200],
        [-1.6849, -1.5540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04616490751504898
RAW KL tensor(0.0470, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 374])
Epoch 0, Step 455: train/loss = 0.6427844762802124, train/raw-loss = 0.6272794008255005, train/logprobs = tensor([[-0.9039, -1.8714],
        [-0.9652, -1.6195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031010203063488007
RAW KL tensor(0.0128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 423])
Epoch 0, Step 456: train/loss = 0.6264845132827759, train/raw-loss = 0.6123300790786743, train/logprobs = tensor([[-0.9434, -1.9862],
        [-0.9762, -1.5828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02830888144671917
RAW KL tensor(0.0243, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 457: train/loss = 0.6488159894943237, train/raw-loss = 0.6308959722518921, train/logprobs = tensor([[-0.7769, -1.6383],
        [-0.8019, -1.3927]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03584008291363716
RAW KL tensor(0.0187, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 118])
Epoch 0, Step 458: train/loss = 0.6426435112953186, train/raw-loss = 0.6290115714073181, train/logprobs = tensor([[-1.6874, -1.8433],
        [-1.6541, -1.5152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027263889089226723
RAW KL tensor(0.0280, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 459: train/loss = 0.6654361486434937, train/raw-loss = 0.641364574432373, train/logprobs = tensor([[-0.9145, -2.0213],
        [-0.9337, -1.8077]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04814313352108002
RAW KL tensor(0.0103, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 452])
Epoch 0, Step 460: train/loss = 0.6581430435180664, train/raw-loss = 0.6397705078125, train/logprobs = tensor([[-0.9873, -1.4933],
        [-1.0487, -1.2864]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03674506023526192
RAW KL tensor(0.0325, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 461: train/loss = 0.6521593332290649, train/raw-loss = 0.6327468752861023, train/logprobs = tensor([[-1.6214, -1.7773],
        [-1.5721, -1.4320]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03882501646876335
RAW KL tensor(0.0406, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 361])
Epoch 0, Step 462: train/loss = 0.6614797711372375, train/raw-loss = 0.6401394009590149, train/logprobs = tensor([[-0.9627, -2.5028],
        [-0.9704, -2.2491]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.042680658400058746
RAW KL tensor(0.0149, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 462])
Epoch 0, Step 463: train/loss = 0.6459408402442932, train/raw-loss = 0.6323647499084473, train/logprobs = tensor([[-1.0039, -1.8656],
        [-1.0497, -1.6281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027152255177497864
RAW KL tensor(0.0580, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 464: train/loss = 0.6581629514694214, train/raw-loss = 0.630934476852417, train/logprobs = tensor([[-1.1760, -3.1829],
        [-1.1741, -2.9019]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05445689335465431
RAW KL tensor(0.0331, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 465: train/loss = 0.7013571262359619, train/raw-loss = 0.6788617372512817, train/logprobs = tensor([[-1.1492, -1.4100],
        [-1.1771, -1.3247]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0449906662106514
RAW KL tensor(0.0423, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 466: train/loss = 0.6137452721595764, train/raw-loss = 0.5897892713546753, train/logprobs = tensor([[-1.6573, -2.4390],
        [-1.6684, -1.9681]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047911979258060455
RAW KL tensor(0.0160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 343])
Epoch 0, Step 467: train/loss = 0.6723736524581909, train/raw-loss = 0.6519513726234436, train/logprobs = tensor([[-1.5684, -1.6485],
        [-1.5470, -1.4435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04084467142820358
RAW KL tensor(0.0214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 468: train/loss = 0.6782563328742981, train/raw-loss = 0.6605072617530823, train/logprobs = tensor([[-1.3973, -1.8475],
        [-1.3222, -1.6345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03549806773662567
RAW KL tensor(0.0347, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 469: train/loss = 0.6486318707466125, train/raw-loss = 0.636807918548584, train/logprobs = tensor([[-1.0528, -1.4608],
        [-1.1083, -1.2682]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02364806830883026
RAW KL tensor(0.0527, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 470: train/loss = 0.6543316841125488, train/raw-loss = 0.6346824169158936, train/logprobs = tensor([[-1.0075, -2.5453],
        [-1.0026, -2.2680]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03929852694272995
RAW KL tensor(0.0837, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 471: train/loss = 0.681371808052063, train/raw-loss = 0.6338645219802856, train/logprobs = tensor([[-1.2739, -2.0572],
        [-1.2840, -1.8093]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09501459449529648
RAW KL tensor(0.0102, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 472: train/loss = 0.5391503572463989, train/raw-loss = 0.5228409171104431, train/logprobs = tensor([[-1.8569, -4.0177],
        [-1.8839, -3.1381]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03261885792016983
RAW KL tensor(0.0309, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 473: train/loss = 0.6805455684661865, train/raw-loss = 0.6578162908554077, train/logprobs = tensor([[-1.7451, -1.8398],
        [-1.6659, -1.5426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045458726584911346
RAW KL tensor(0.0471, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 474: train/loss = 0.6313838958740234, train/raw-loss = 0.6167407035827637, train/logprobs = tensor([[-0.8096, -2.3030],
        [-0.8158, -1.9810]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02928638458251953
RAW KL tensor(0.0270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 475: train/loss = 0.6539514064788818, train/raw-loss = 0.6406712532043457, train/logprobs = tensor([[-1.0158, -1.9743],
        [-1.1120, -1.8314]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02656024694442749
RAW KL tensor(0.0310, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 476: train/loss = 0.6971577405929565, train/raw-loss = 0.684714674949646, train/logprobs = tensor([[-1.3946, -1.5060],
        [-1.2333, -1.3042]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024886026978492737
RAW KL tensor(0.0769, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 477: train/loss = 0.6219512224197388, train/raw-loss = 0.5973985195159912, train/logprobs = tensor([[-2.1453, -2.1893],
        [-2.1756, -1.7627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04910548776388168
RAW KL tensor(0.0460, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 478: train/loss = 0.69870525598526, train/raw-loss = 0.6759476661682129, train/logprobs = tensor([[-1.8122, -1.8716],
        [-1.7822, -1.7176]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04551518335938454
RAW KL tensor(0.0489, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 479: train/loss = 0.634611964225769, train/raw-loss = 0.5896651744842529, train/logprobs = tensor([[-1.4851, -2.7034],
        [-1.4461, -2.0023]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08989362418651581
RAW KL tensor(0.0648, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 480: train/loss = 0.6713455319404602, train/raw-loss = 0.6489160060882568, train/logprobs = tensor([[-1.1861, -1.8276],
        [-1.1847, -1.6251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044859059154987335
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 481: train/loss = 0.6404861211776733, train/raw-loss = 0.630071759223938, train/logprobs = tensor([[-1.3789, -2.0619],
        [-1.3936, -1.7730]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020828556269407272
RAW KL tensor(0.0209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 278])
Epoch 0, Step 482: train/loss = 0.6541410088539124, train/raw-loss = 0.6374375820159912, train/logprobs = tensor([[-1.1072, -2.6579],
        [-1.1272, -2.4051]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03340693563222885
RAW KL tensor(0.0655, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 483: train/loss = 0.6620196104049683, train/raw-loss = 0.6405783891677856, train/logprobs = tensor([[-1.5206, -2.4606],
        [-1.5019, -2.1745]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04288236051797867
RAW KL tensor(0.0369, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 484: train/loss = 0.6679106950759888, train/raw-loss = 0.6385906934738159, train/logprobs = tensor([[-1.6715, -3.4768],
        [-1.6178, -3.0975]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05863980948925018
RAW KL tensor(0.0172, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 485: train/loss = 0.6393729448318481, train/raw-loss = 0.6229586005210876, train/logprobs = tensor([[-1.4894, -1.8500],
        [-1.4543, -1.5010]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03282865509390831
RAW KL tensor(0.0139, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 486: train/loss = 0.6872367262840271, train/raw-loss = 0.6753226518630981, train/logprobs = tensor([[-0.8052, -1.2809],
        [-0.7846, -1.1820]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02382810413837433
RAW KL tensor(0.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 487: train/loss = 0.6532760262489319, train/raw-loss = 0.6375541687011719, train/logprobs = tensor([[-1.1650, -3.6821],
        [-1.0736, -3.3456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031443770974874496
RAW KL tensor(0.0353, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 488: train/loss = 0.6617670655250549, train/raw-loss = 0.6413254737854004, train/logprobs = tensor([[-1.9951, -2.4304],
        [-1.9613, -2.1613]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04088316112756729
RAW KL tensor(0.0212, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 489: train/loss = 0.652478814125061, train/raw-loss = 0.6339612007141113, train/logprobs = tensor([[-1.5491, -2.1064],
        [-1.5296, -1.8155]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03703535348176956
RAW KL tensor(0.0121, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 321])
Epoch 0, Step 490: train/loss = 0.6964550018310547, train/raw-loss = 0.6850985288619995, train/logprobs = tensor([[-0.6698, -1.4857],
        [-0.6644, -1.4472]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.022712893784046173
RAW KL tensor(0.0195, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 491: train/loss = 0.6512146592140198, train/raw-loss = 0.6322011351585388, train/logprobs = tensor([[-1.2777, -2.2115],
        [-1.2792, -1.9226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03802704066038132
RAW KL tensor(0.0751, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 492: train/loss = 0.664061427116394, train/raw-loss = 0.6471156477928162, train/logprobs = tensor([[-0.7587, -1.7993],
        [-0.7569, -1.5720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03389156237244606
RAW KL tensor(0.0405, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 493: train/loss = 0.6625240445137024, train/raw-loss = 0.6362979412078857, train/logprobs = tensor([[-1.2267, -2.2347],
        [-1.1690, -1.9308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05245215818285942
RAW KL tensor(0.0304, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 494: train/loss = 0.6785715222358704, train/raw-loss = 0.6613434553146362, train/logprobs = tensor([[-1.3880, -1.9818],
        [-1.3540, -1.7490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03445606678724289
RAW KL tensor(0.0548, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 495: train/loss = 0.6605499386787415, train/raw-loss = 0.6387426853179932, train/logprobs = tensor([[-2.1305, -2.6487],
        [-2.0776, -2.3319]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04361443221569061
RAW KL tensor(0.0596, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 496: train/loss = 0.653583288192749, train/raw-loss = 0.6286575794219971, train/logprobs = tensor([[-1.6524, -2.8261],
        [-1.6593, -2.4604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049851372838020325
RAW KL tensor(0.0385, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 497: train/loss = 0.6518518924713135, train/raw-loss = 0.6263018846511841, train/logprobs = tensor([[-1.2209, -3.1353],
        [-1.2585, -2.8598]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05110013112425804
RAW KL tensor(0.0257, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 498: train/loss = 0.6597025990486145, train/raw-loss = 0.6433393955230713, train/logprobs = tensor([[-1.8018, -2.3096],
        [-1.7392, -1.9765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.032726407051086426
RAW KL tensor(0.0435, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 499: train/loss = 0.6779078245162964, train/raw-loss = 0.6533483266830444, train/logprobs = tensor([[-1.3561, -2.3247],
        [-1.3381, -2.1258]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04911899194121361
RAW KL tensor(0.0464, device='cuda:0')
SHAPES: torch.Size([4, 122])
RAW KL tensor(0.0302, device='cuda:0')
SHAPES: torch.Size([4, 145])
RAW KL tensor(0.0782, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0334, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0303, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0299, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.0795, device='cuda:0')
SHAPES: torch.Size([4, 168])
RAW KL tensor(0.0281, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.0380, device='cuda:0')
SHAPES: torch.Size([4, 184])
RAW KL tensor(0.0450, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0398, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.0245, device='cuda:0')
SHAPES: torch.Size([4, 189])
RAW KL tensor(0.0496, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0662, device='cuda:0')
SHAPES: torch.Size([4, 169])
RAW KL tensor(0.0698, device='cuda:0')
SHAPES: torch.Size([4, 239])
RAW KL tensor(0.0099, device='cuda:0')
SHAPES: torch.Size([4, 182])
RAW KL tensor(0.1114, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0339, device='cuda:0')
SHAPES: torch.Size([4, 170])
RAW KL tensor(0.0703, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0488, device='cuda:0')
SHAPES: torch.Size([4, 159])
RAW KL tensor(0.0445, device='cuda:0')
SHAPES: torch.Size([4, 158])
RAW KL tensor(0.0151, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.0377, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0211, device='cuda:0')
SHAPES: torch.Size([4, 201])
RAW KL tensor(0.0343, device='cuda:0')
SHAPES: torch.Size([4, 187])
RAW KL tensor(0.0268, device='cuda:0')
SHAPES: torch.Size([4, 317])
RAW KL tensor(0.0450, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0228, device='cuda:0')
SHAPES: torch.Size([4, 185])
RAW KL tensor(0.0165, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0248, device='cuda:0')
SHAPES: torch.Size([4, 106])
RAW KL tensor(0.0124, device='cuda:0')
SHAPES: torch.Size([4, 173])
RAW KL tensor(0.0613, device='cuda:0')
SHAPES: torch.Size([4, 455])
RAW KL tensor(0.0275, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.0097, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0486, device='cuda:0')
SHAPES: torch.Size([4, 263])
RAW KL tensor(0.0199, device='cuda:0')
SHAPES: torch.Size([4, 261])
RAW KL tensor(0.0505, device='cuda:0')
SHAPES: torch.Size([4, 155])
RAW KL tensor(0.0504, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0755, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0229, device='cuda:0')
SHAPES: torch.Size([4, 146])
RAW KL tensor(0.0230, device='cuda:0')
SHAPES: torch.Size([4, 226])
RAW KL tensor(0.0200, device='cuda:0')
SHAPES: torch.Size([4, 213])
RAW KL tensor(0.0559, device='cuda:0')
SHAPES: torch.Size([4, 204])
RAW KL tensor(0.0298, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0647, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0160, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0158, device='cuda:0')
SHAPES: torch.Size([4, 130])
RAW KL tensor(0.0234, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0292, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.0212, device='cuda:0')
SHAPES: torch.Size([4, 175])
RAW KL tensor(0.0143, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.1008, device='cuda:0')
SHAPES: torch.Size([4, 143])
RAW KL tensor(0.0398, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.0208, device='cuda:0')
SHAPES: torch.Size([4, 208])
RAW KL tensor(0.0480, device='cuda:0')
SHAPES: torch.Size([4, 177])
RAW KL tensor(0.0640, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0370, device='cuda:0')
SHAPES: torch.Size([4, 164])
RAW KL tensor(0.0256, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.0530, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.0447, device='cuda:0')
SHAPES: torch.Size([4, 141])
RAW KL tensor(0.0191, device='cuda:0')
SHAPES: torch.Size([4, 186])
RAW KL tensor(0.0227, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0227, device='cuda:0')
SHAPES: torch.Size([4, 194])
RAW KL tensor(0.0252, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.5601, device='cuda:0')
SHAPES: torch.Size([4, 125])
RAW KL tensor(0.0479, device='cuda:0')
SHAPES: torch.Size([4, 179])
RAW KL tensor(0.0161, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0239, device='cuda:0')
SHAPES: torch.Size([4, 199])
RAW KL tensor(0.0326, device='cuda:0')
SHAPES: torch.Size([4, 136])
RAW KL tensor(0.0264, device='cuda:0')
SHAPES: torch.Size([4, 229])
RAW KL tensor(0.0169, device='cuda:0')
SHAPES: torch.Size([4, 341])
RAW KL tensor(0.0207, device='cuda:0')
SHAPES: torch.Size([4, 237])
RAW KL tensor(0.0780, device='cuda:0')
SHAPES: torch.Size([4, 162])
RAW KL tensor(0.0319, device='cuda:0')
SHAPES: torch.Size([4, 118])
RAW KL tensor(0.0519, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0096, device='cuda:0')
SHAPES: torch.Size([4, 210])
RAW KL tensor(0.0584, device='cuda:0')
SHAPES: torch.Size([4, 216])
RAW KL tensor(0.0415, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.0694, device='cuda:0')
SHAPES: torch.Size([4, 191])
RAW KL tensor(0.0220, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.0252, device='cuda:0')
SHAPES: torch.Size([4, 157])
RAW KL tensor(0.0310, device='cuda:0')
SHAPES: torch.Size([4, 171])
RAW KL tensor(0.0264, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.0244, device='cuda:0')
SHAPES: torch.Size([4, 148])
RAW KL tensor(0.0269, device='cuda:0')
SHAPES: torch.Size([4, 195])
RAW KL tensor(0.0212, device='cuda:0')
SHAPES: torch.Size([4, 212])
RAW KL tensor(0.0397, device='cuda:0')
SHAPES: torch.Size([4, 147])
RAW KL tensor(0.0266, device='cuda:0')
SHAPES: torch.Size([4, 163])
RAW KL tensor(0.0163, device='cuda:0')
SHAPES: torch.Size([4, 198])
RAW KL tensor(0.0041, device='cuda:0')
SHAPES: torch.Size([4, 450])
RAW KL tensor(0.0552, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0397, device='cuda:0')
SHAPES: torch.Size([4, 452])
RAW KL tensor(0.0440, device='cuda:0')
SHAPES: torch.Size([4, 149])
RAW KL tensor(0.0139, device='cuda:0')
SHAPES: torch.Size([4, 188])
RAW KL tensor(0.0292, device='cuda:0')
SHAPES: torch.Size([4, 161])
RAW KL tensor(0.0264, device='cuda:0')
SHAPES: torch.Size([4, 174])
RAW KL tensor(0.0466, device='cuda:0')
SHAPES: torch.Size([4, 137])
RAW KL tensor(0.0152, device='cuda:0')
SHAPES: torch.Size([4, 219])
RAW KL tensor(0.0680, device='cuda:0')
SHAPES: torch.Size([4, 140])
RAW KL tensor(0.0563, device='cuda:0')
SHAPES: torch.Size([4, 154])
RAW KL tensor(0.0259, device='cuda:0')
SHAPES: torch.Size([4, 410])
RAW KL tensor(0.0228, device='cuda:0')
SHAPES: torch.Size([4, 214])
RAW KL tensor(0.0139, device='cuda:0')
SHAPES: torch.Size([4, 467])
RAW KL tensor(0.0493, device='cuda:0')
SHAPES: torch.Size([4, 144])
RAW KL tensor(0.0228, device='cuda:0')
SHAPES: torch.Size([4, 205])
RAW KL tensor(0.0474, device='cuda:0')
SHAPES: torch.Size([4, 190])
RAW KL tensor(0.0257, device='cuda:0')
SHAPES: torch.Size([4, 178])
RAW KL tensor(0.0286, device='cuda:0')
SHAPES: torch.Size([4, 206])
RAW KL tensor(0.0134, device='cuda:0')
SHAPES: torch.Size([4, 235])
RAW KL tensor(0.0303, device='cuda:0')
SHAPES: torch.Size([4, 152])
RAW KL tensor(0.0198, device='cuda:0')
SHAPES: torch.Size([4, 227])
RAW KL tensor(0.0337, device='cuda:0')
SHAPES: torch.Size([4, 217])
RAW KL tensor(0.0294, device='cuda:0')
SHAPES: torch.Size([4, 142])
RAW KL tensor(0.0402, device='cuda:0')
SHAPES: torch.Size([4, 192])
RAW KL tensor(0.0305, device='cuda:0')
SHAPES: torch.Size([4, 181])
RAW KL tensor(0.0257, device='cuda:0')
SHAPES: torch.Size([4, 183])
RAW KL tensor(0.0399, device='cuda:0')
SHAPES: torch.Size([4, 222])
RAW KL tensor(0.0555, device='cuda:0')
SHAPES: torch.Size([4, 126])
RAW KL tensor(0.0116, device='cuda:0')
SHAPES: torch.Size([4, 207])
RAW KL tensor(0.0564, device='cuda:0')
SHAPES: torch.Size([4, 121])
RAW KL tensor(0.0188, device='cuda:0')
SHAPES: torch.Size([4, 167])
RAW KL tensor(0.0259, device='cuda:0')
SHAPES: torch.Size([4, 236])
RAW KL tensor(0.0193, device='cuda:0')
SHAPES: torch.Size([4, 123])
RAW KL tensor(0.0522, device='cuda:0')
SHAPES: torch.Size([4, 128])
RAW KL tensor(0.0410, device='cuda:0')
SHAPES: torch.Size([4, 217])
eval/loss: 0.6544443368911743
RAW KL tensor(0.0170, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 500: train/loss = 0.6489386558532715, train/raw-loss = 0.6323094367980957, train/logprobs = tensor([[-1.2390, -2.7651],
        [-1.2063, -2.4624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03325836360454559
RAW KL tensor(0.0310, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 501: train/loss = 0.6631773114204407, train/raw-loss = 0.6372740268707275, train/logprobs = tensor([[-1.4374, -2.2724],
        [-1.4083, -1.9877]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05180658400058746
RAW KL tensor(0.0232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 502: train/loss = 0.6640748977661133, train/raw-loss = 0.6481446027755737, train/logprobs = tensor([[-0.9102, -2.6797],
        [-0.8971, -2.3719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031860560178756714
RAW KL tensor(0.0248, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 503: train/loss = 0.6492741107940674, train/raw-loss = 0.6114379167556763, train/logprobs = tensor([[-1.5924, -2.4031],
        [-1.5813, -2.0128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07567235827445984
RAW KL tensor(0.0150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 504: train/loss = 0.6759155988693237, train/raw-loss = 0.6610291004180908, train/logprobs = tensor([[-1.3049, -1.6310],
        [-1.1828, -1.3732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029773056507110596
RAW KL tensor(0.0352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 505: train/loss = 0.6502887010574341, train/raw-loss = 0.6321369409561157, train/logprobs = tensor([[-1.2936, -2.1125],
        [-1.3235, -1.8818]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03630353510379791
RAW KL tensor(0.0533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 506: train/loss = 0.6476817727088928, train/raw-loss = 0.6239498853683472, train/logprobs = tensor([[-1.4596, -2.6577],
        [-1.4071, -2.3048]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047463804483413696
RAW KL tensor(0.0273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 507: train/loss = 0.645541250705719, train/raw-loss = 0.6267147660255432, train/logprobs = tensor([[-1.0075, -2.3321],
        [-1.0241, -2.0151]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03765301778912544
RAW KL tensor(0.0528, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 508: train/loss = 0.6276084184646606, train/raw-loss = 0.6091479063034058, train/logprobs = tensor([[-0.8678, -2.2997],
        [-0.8953, -1.8887]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036920953541994095
RAW KL tensor(0.0044, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 509: train/loss = 0.662994384765625, train/raw-loss = 0.6443619728088379, train/logprobs = tensor([[-1.5070, -2.0791],
        [-1.2883, -1.6548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037264756858348846
RAW KL tensor(0.0768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 510: train/loss = 0.642719566822052, train/raw-loss = 0.6177253723144531, train/logprobs = tensor([[-1.0896, -2.3784],
        [-1.0391, -1.9710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04998837411403656
RAW KL tensor(0.0615, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 511: train/loss = 0.6886627078056335, train/raw-loss = 0.6703387498855591, train/logprobs = tensor([[-1.1935, -2.7982],
        [-1.1331, -2.4648]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03664779290556908
RAW KL tensor(0.0350, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 217])
Epoch 0, Step 512: train/loss = 0.5977058410644531, train/raw-loss = 0.5735459327697754, train/logprobs = tensor([[-1.3818, -3.0341],
        [-1.3811, -2.4721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04831980913877487
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 513: train/loss = 0.6030930280685425, train/raw-loss = 0.5889644622802734, train/logprobs = tensor([[-1.3558, -2.3632],
        [-1.3067, -1.8428]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028257226571440697
RAW KL tensor(0.0467, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 228])
Epoch 0, Step 514: train/loss = 0.7626579403877258, train/raw-loss = 0.668822169303894, train/logprobs = tensor([[-2.1323, -2.2099],
        [-2.0932, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18767161667346954
RAW KL tensor(0.0673, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 515: train/loss = 0.6367541551589966, train/raw-loss = 0.6136472225189209, train/logprobs = tensor([[-0.9888, -2.8930],
        [-0.9848, -2.5229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04621392488479614
RAW KL tensor(0.0836, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 516: train/loss = 0.6858005523681641, train/raw-loss = 0.643612802028656, train/logprobs = tensor([[-2.2571, -2.7525],
        [-2.0866, -2.2715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08437541872262955
RAW KL tensor(0.0123, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 307])
Epoch 0, Step 517: train/loss = 0.6820146441459656, train/raw-loss = 0.6654081344604492, train/logprobs = tensor([[-0.9443, -1.9271],
        [-0.9466, -1.7798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.033212922513484955
RAW KL tensor(0.1428, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 518: train/loss = 0.6670185327529907, train/raw-loss = 0.6404786109924316, train/logprobs = tensor([[-1.4369, -2.3480],
        [-1.4959, -2.1782]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05307987332344055
RAW KL tensor(0.0173, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 519: train/loss = 0.6286548376083374, train/raw-loss = 0.6183369755744934, train/logprobs = tensor([[-0.8608, -2.1876],
        [-0.8362, -1.8331]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02063574641942978
RAW KL tensor(0.0090, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 520: train/loss = 0.6425382494926453, train/raw-loss = 0.6255868673324585, train/logprobs = tensor([[-1.3818, -3.0306],
        [-1.3119, -2.4830]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03390270844101906
RAW KL tensor(0.0373, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 521: train/loss = 0.5583654642105103, train/raw-loss = 0.5401507616043091, train/logprobs = tensor([[-1.4501, -3.3529],
        [-1.4326, -2.6248]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036429423838853836
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 522: train/loss = 0.5988162755966187, train/raw-loss = 0.5751559734344482, train/logprobs = tensor([[-1.8902, -3.3653],
        [-1.7897, -2.7187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04732050001621246
RAW KL tensor(0.0183, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 253])
Epoch 0, Step 523: train/loss = 0.647119402885437, train/raw-loss = 0.6315559148788452, train/logprobs = tensor([[-0.9519, -1.9398],
        [-0.8528, -1.5369]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031126948073506355
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 524: train/loss = 0.6884426474571228, train/raw-loss = 0.6738737225532532, train/logprobs = tensor([[-1.1332, -1.4663],
        [-1.1084, -1.3523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029137862846255302
RAW KL tensor(0.0417, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 238])
Epoch 0, Step 525: train/loss = 0.6488635540008545, train/raw-loss = 0.6242151260375977, train/logprobs = tensor([[-1.5619, -2.7447],
        [-1.3646, -2.2029]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049296785145998
RAW KL tensor(0.0548, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 526: train/loss = 0.6248517632484436, train/raw-loss = 0.604128897190094, train/logprobs = tensor([[-1.8333, -2.9012],
        [-1.8038, -2.3489]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.041445665061473846
RAW KL tensor(0.0463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 527: train/loss = 0.6245155334472656, train/raw-loss = 0.6002686023712158, train/logprobs = tensor([[-1.2688, -3.3936],
        [-1.2707, -2.8694]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04849380999803543
RAW KL tensor(0.0127, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 528: train/loss = 0.668597400188446, train/raw-loss = 0.6531845331192017, train/logprobs = tensor([[-1.2591, -1.9415],
        [-1.2731, -1.7334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030825776979327202
RAW KL tensor(0.0541, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 529: train/loss = 0.6319762468338013, train/raw-loss = 0.6006953716278076, train/logprobs = tensor([[-0.7812, -4.0997],
        [-0.7756, -3.4299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06256179511547089
RAW KL tensor(0.0281, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 530: train/loss = 0.6366137862205505, train/raw-loss = 0.6181888580322266, train/logprobs = tensor([[-1.5243, -2.6387],
        [-1.3467, -2.1306]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03684978559613228
RAW KL tensor(0.0290, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 531: train/loss = 0.6898223757743835, train/raw-loss = 0.6738263964653015, train/logprobs = tensor([[-1.5047, -1.8472],
        [-1.3016, -1.5339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03199189528822899
RAW KL tensor(0.0424, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 532: train/loss = 0.6291075348854065, train/raw-loss = 0.6101241111755371, train/logprobs = tensor([[-1.5498, -2.5409],
        [-1.5391, -2.1296]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03796686977148056
RAW KL tensor(0.0161, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 419])
Epoch 0, Step 533: train/loss = 0.6533218622207642, train/raw-loss = 0.6358246803283691, train/logprobs = tensor([[-0.7923, -2.7317],
        [-0.7565, -2.3690]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034994322806596756
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 114])
Epoch 0, Step 534: train/loss = 0.6472788453102112, train/raw-loss = 0.6305902600288391, train/logprobs = tensor([[-1.4077, -2.1029],
        [-1.3079, -1.7370]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03337709978222847
RAW KL tensor(0.0229, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 535: train/loss = 0.673210859298706, train/raw-loss = 0.6571407914161682, train/logprobs = tensor([[-1.2083, -1.2701],
        [-1.2065, -1.0808]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03214017301797867
RAW KL tensor(0.0204, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 536: train/loss = 0.6215114593505859, train/raw-loss = 0.609144926071167, train/logprobs = tensor([[-1.4512, -2.8843],
        [-1.4717, -2.5262]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.024733033031225204
RAW KL tensor(0.0344, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 537: train/loss = 0.6921327114105225, train/raw-loss = 0.6766859889030457, train/logprobs = tensor([[-1.4170, -1.6851],
        [-1.3259, -1.5131]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0308934785425663
RAW KL tensor(0.0405, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 538: train/loss = 0.6438250541687012, train/raw-loss = 0.6240824460983276, train/logprobs = tensor([[-1.5536, -2.5374],
        [-1.5508, -2.2159]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.039485082030296326
RAW KL tensor(0.0483, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 199])
Epoch 0, Step 539: train/loss = 0.5919130444526672, train/raw-loss = 0.5720647573471069, train/logprobs = tensor([[-1.2371, -3.3687],
        [-1.2692, -2.8191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03969653695821762
RAW KL tensor(0.2028, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 540: train/loss = 0.6886397004127502, train/raw-loss = 0.6538428664207458, train/logprobs = tensor([[-1.3801, -1.9463],
        [-1.2170, -1.5886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06959374248981476
RAW KL tensor(0.0514, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 541: train/loss = 0.6080719232559204, train/raw-loss = 0.5889236330986023, train/logprobs = tensor([[-1.8380, -2.6204],
        [-1.8178, -2.1451]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038296520709991455
RAW KL tensor(0.0605, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 542: train/loss = 0.6755180358886719, train/raw-loss = 0.6457256078720093, train/logprobs = tensor([[-1.5784, -1.8524],
        [-1.5705, -1.6331]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059584859758615494
RAW KL tensor(0.0847, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 543: train/loss = 0.6288875937461853, train/raw-loss = 0.6077699661254883, train/logprobs = tensor([[-1.1997, -2.5865],
        [-1.1743, -2.1534]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04223528504371643
RAW KL tensor(0.0282, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 544: train/loss = 0.6454944610595703, train/raw-loss = 0.6288390159606934, train/logprobs = tensor([[-0.9968, -1.7633],
        [-1.0086, -1.4937]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03331094607710838
RAW KL tensor(0.0054, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 545: train/loss = 0.6224986910820007, train/raw-loss = 0.6111761927604675, train/logprobs = tensor([[-0.8444, -1.9646],
        [-0.8147, -1.5505]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02264498546719551
RAW KL tensor(0.0418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 546: train/loss = 0.6093977689743042, train/raw-loss = 0.5854160189628601, train/logprobs = tensor([[-1.8582, -1.8833],
        [-1.8093, -1.3327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04796344041824341
RAW KL tensor(0.0231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 201])
Epoch 0, Step 547: train/loss = 0.6093422174453735, train/raw-loss = 0.5937371850013733, train/logprobs = tensor([[-0.7852, -2.1056],
        [-0.7707, -1.6647]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031210053712129593
RAW KL tensor(0.0233, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 548: train/loss = 0.6817638278007507, train/raw-loss = 0.6657629013061523, train/logprobs = tensor([[-1.3748, -1.5718],
        [-1.2617, -1.3438]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03200187906622887
RAW KL tensor(0.0186, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 549: train/loss = 0.617879331111908, train/raw-loss = 0.5911657810211182, train/logprobs = tensor([[-1.2558, -2.1520],
        [-1.2893, -1.7364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053426988422870636
RAW KL tensor(0.0351, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 550: train/loss = 0.6148474216461182, train/raw-loss = 0.5962454676628113, train/logprobs = tensor([[-1.4386, -1.7236],
        [-1.3317, -1.1666]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03720390424132347
RAW KL tensor(0.0289, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 551: train/loss = 0.6157730221748352, train/raw-loss = 0.6019828915596008, train/logprobs = tensor([[-0.8495, -1.8471],
        [-0.8055, -1.3828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.027580276131629944
RAW KL tensor(0.0372, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 552: train/loss = 0.4547189772129059, train/raw-loss = 0.4191383123397827, train/logprobs = tensor([[-1.2875, -2.9122],
        [-1.1172, -1.3400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07116135954856873
RAW KL tensor(0.0095, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 553: train/loss = 0.6504032015800476, train/raw-loss = 0.6327863931655884, train/logprobs = tensor([[-1.3556, -2.0518],
        [-1.3354, -1.7602]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0352337472140789
RAW KL tensor(0.0269, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 554: train/loss = 0.5959267020225525, train/raw-loss = 0.5729416012763977, train/logprobs = tensor([[-1.2660, -2.0996],
        [-1.1765, -1.3919]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04597017168998718
RAW KL tensor(0.1636, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 555: train/loss = 0.5029215812683105, train/raw-loss = 0.46627604961395264, train/logprobs = tensor([[-1.9517, -3.4575],
        [-1.9155, -2.2199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07329107075929642
RAW KL tensor(0.0306, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 556: train/loss = 0.5459063053131104, train/raw-loss = 0.52309250831604, train/logprobs = tensor([[-1.4411, -2.8884],
        [-1.4194, -2.0566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.045627571642398834
RAW KL tensor(0.0478, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 557: train/loss = 0.6183571219444275, train/raw-loss = 0.6053941249847412, train/logprobs = tensor([[-0.8930, -2.0291],
        [-0.8293, -1.4969]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02592599391937256
RAW KL tensor(0.0533, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 558: train/loss = 0.6618905067443848, train/raw-loss = 0.6425683498382568, train/logprobs = tensor([[-1.4377, -1.8605],
        [-1.3950, -1.5954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038644276559352875
RAW KL tensor(0.0446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 559: train/loss = 0.5980411767959595, train/raw-loss = 0.5826334953308105, train/logprobs = tensor([[-1.1201, -2.6709],
        [-1.1123, -2.1550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030815351754426956
RAW KL tensor(0.0376, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 560: train/loss = 0.6365962028503418, train/raw-loss = 0.6220276951789856, train/logprobs = tensor([[-1.3308, -2.1063],
        [-1.2623, -1.7310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02913692034780979
RAW KL tensor(0.0532, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 561: train/loss = 0.6242846250534058, train/raw-loss = 0.6011075377464294, train/logprobs = tensor([[-1.5862, -2.3577],
        [-1.4651, -1.8168]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04635419324040413
RAW KL tensor(0.0611, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 562: train/loss = 0.664961576461792, train/raw-loss = 0.6475813984870911, train/logprobs = tensor([[-1.3061, -2.2276],
        [-1.2496, -1.9616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034760378301143646
RAW KL tensor(0.0289, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 563: train/loss = 0.6037788391113281, train/raw-loss = 0.5742294788360596, train/logprobs = tensor([[-1.6118, -2.5296],
        [-1.5551, -1.8499]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05909866839647293
RAW KL tensor(0.0399, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 564: train/loss = 0.6540482044219971, train/raw-loss = 0.636725902557373, train/logprobs = tensor([[-1.4250, -1.5487],
        [-1.2704, -1.1567]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034644655883312225
RAW KL tensor(0.0509, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 565: train/loss = 0.5797778367996216, train/raw-loss = 0.5662552118301392, train/logprobs = tensor([[-1.2037, -2.2206],
        [-1.0792, -1.4831]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02704527974128723
RAW KL tensor(0.0356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 566: train/loss = 0.585580587387085, train/raw-loss = 0.5641211271286011, train/logprobs = tensor([[-1.2236, -2.6256],
        [-1.1783, -1.9860]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04291893169283867
RAW KL tensor(0.0253, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 567: train/loss = 0.6053385734558105, train/raw-loss = 0.5958365201950073, train/logprobs = tensor([[-1.0006, -2.8123],
        [-0.8343, -2.2005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.019004197791218758
RAW KL tensor(0.0589, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 568: train/loss = 0.5950508117675781, train/raw-loss = 0.576340913772583, train/logprobs = tensor([[-1.4335, -2.3486],
        [-1.2840, -1.6641]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037419918924570084
RAW KL tensor(0.0299, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 569: train/loss = 0.5524078011512756, train/raw-loss = 0.5280709266662598, train/logprobs = tensor([[-1.4563, -3.4165],
        [-1.3753, -2.5423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04867377132177353
RAW KL tensor(0.0338, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 570: train/loss = 0.6294291019439697, train/raw-loss = 0.6081407070159912, train/logprobs = tensor([[-1.2882, -2.3120],
        [-1.1409, -1.7759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04257674515247345
RAW KL tensor(0.0422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 571: train/loss = 0.6324735283851624, train/raw-loss = 0.6181700229644775, train/logprobs = tensor([[-1.5041, -2.8105],
        [-1.4057, -2.3233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028606997802853584
RAW KL tensor(0.0225, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 572: train/loss = 0.571992039680481, train/raw-loss = 0.5542300939559937, train/logprobs = tensor([[-1.1653, -2.7847],
        [-1.1247, -1.9855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03552379086613655
RAW KL tensor(0.0616, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 573: train/loss = 0.6527327299118042, train/raw-loss = 0.6270301342010498, train/logprobs = tensor([[-1.3799, -2.4020],
        [-1.2057, -1.8952]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05140504986047745
RAW KL tensor(0.0297, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 574: train/loss = 0.6041749715805054, train/raw-loss = 0.5721744894981384, train/logprobs = tensor([[-1.2320, -1.9724],
        [-1.1467, -1.2510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0640009343624115
RAW KL tensor(0.0430, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 575: train/loss = 0.5531026124954224, train/raw-loss = 0.5296711921691895, train/logprobs = tensor([[-1.3215, -2.8766],
        [-1.2106, -1.9363]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04686294123530388
RAW KL tensor(0.0404, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 576: train/loss = 0.5231983661651611, train/raw-loss = 0.5047905445098877, train/logprobs = tensor([[-1.0445, -2.0478],
        [-0.9936, -1.1206]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.036815643310546875
RAW KL tensor(0.0636, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 577: train/loss = 0.4662606716156006, train/raw-loss = 0.4442894458770752, train/logprobs = tensor([[-2.0221, -3.0577],
        [-1.9012, -1.6668]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04394252225756645
RAW KL tensor(0.0081, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 578: train/loss = 0.5470715761184692, train/raw-loss = 0.5212199687957764, train/logprobs = tensor([[-1.6122, -2.4883],
        [-1.3266, -1.2737]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05170315504074097
RAW KL tensor(0.0257, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 579: train/loss = 0.5918139219284058, train/raw-loss = 0.5777292847633362, train/logprobs = tensor([[-0.9429, -1.9385],
        [-0.8088, -1.2936]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028169328346848488
RAW KL tensor(0.0272, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 580: train/loss = 0.6517534255981445, train/raw-loss = 0.6339828968048096, train/logprobs = tensor([[-1.5319, -1.8128],
        [-1.4201, -1.3910]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035540953278541565
RAW KL tensor(0.0271, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 581: train/loss = 0.48966261744499207, train/raw-loss = 0.4738803505897522, train/logprobs = tensor([[-0.6754, -3.7601],
        [-0.6131, -2.6716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03156439587473869
RAW KL tensor(0.0388, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 582: train/loss = 0.5872183442115784, train/raw-loss = 0.5757505893707275, train/logprobs = tensor([[-1.2166, -1.9778],
        [-1.0943, -1.3474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02293551154434681
RAW KL tensor(0.0356, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 583: train/loss = 0.5174142122268677, train/raw-loss = 0.49003565311431885, train/logprobs = tensor([[-1.1587, -3.3783],
        [-1.1079, -2.3721]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.054757095873355865
RAW KL tensor(0.0216, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 584: train/loss = 0.6260258555412292, train/raw-loss = 0.6065481305122375, train/logprobs = tensor([[-1.2135, -1.7879],
        [-1.0751, -1.2191]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038955479860305786
RAW KL tensor(0.0567, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 585: train/loss = 0.575637936592102, train/raw-loss = 0.5562387704849243, train/logprobs = tensor([[-0.7869, -2.6328],
        [-0.7577, -1.9017]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038798216730356216
RAW KL tensor(0.0568, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 586: train/loss = 0.6436463594436646, train/raw-loss = 0.5993102788925171, train/logprobs = tensor([[-2.1924, -2.4343],
        [-1.9561, -1.6125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08867219090461731
RAW KL tensor(0.0230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 587: train/loss = 0.6174476146697998, train/raw-loss = 0.605664849281311, train/logprobs = tensor([[-0.7627, -1.5845],
        [-0.7127, -1.1374]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02356555312871933
RAW KL tensor(0.0490, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 588: train/loss = 0.5719141364097595, train/raw-loss = 0.5544462203979492, train/logprobs = tensor([[-2.1491, -2.0318],
        [-1.9600, -1.1513]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03493586555123329
RAW KL tensor(0.0513, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 589: train/loss = 0.6882442235946655, train/raw-loss = 0.6243737936019897, train/logprobs = tensor([[-2.2962, -2.8510],
        [-1.3499, -1.6021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1277407705783844
RAW KL tensor(0.0214, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 296])
Epoch 0, Step 590: train/loss = 0.5276342034339905, train/raw-loss = 0.4969232678413391, train/logprobs = tensor([[-1.3815, -2.6754],
        [-1.2018, -1.5311]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06142178922891617
RAW KL tensor(0.0574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 591: train/loss = 0.5825833082199097, train/raw-loss = 0.5608041286468506, train/logprobs = tensor([[-1.3079, -2.1113],
        [-1.2210, -1.4146]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043558359146118164
RAW KL tensor(0.0771, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 592: train/loss = 0.5471291542053223, train/raw-loss = 0.5285031199455261, train/logprobs = tensor([[-1.8282, -2.1730],
        [-1.6332, -1.1794]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03725210949778557
RAW KL tensor(0.0326, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 593: train/loss = 0.48129087686538696, train/raw-loss = 0.43930232524871826, train/logprobs = tensor([[-1.5055, -3.1753],
        [-1.4388, -1.7897]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0839770957827568
RAW KL tensor(0.0336, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 594: train/loss = 0.6361714601516724, train/raw-loss = 0.6169428825378418, train/logprobs = tensor([[-1.1512, -1.9377],
        [-1.0676, -1.5114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038457226008176804
RAW KL tensor(0.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 595: train/loss = 0.5764833688735962, train/raw-loss = 0.5525190234184265, train/logprobs = tensor([[-1.3572, -2.2886],
        [-1.2179, -1.5062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04792860895395279
RAW KL tensor(0.0515, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 596: train/loss = 0.6070848107337952, train/raw-loss = 0.5907180905342102, train/logprobs = tensor([[-1.0025, -1.7836],
        [-0.8996, -1.2263]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0327335000038147
RAW KL tensor(0.0408, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 597: train/loss = 0.4955163300037384, train/raw-loss = 0.4767339825630188, train/logprobs = tensor([[-0.8742, -2.7188],
        [-0.7609, -1.5707]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03756464272737503
RAW KL tensor(0.0140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 444])
Epoch 0, Step 598: train/loss = 0.5862855315208435, train/raw-loss = 0.5638556480407715, train/logprobs = tensor([[-1.0273, -2.0896],
        [-1.0243, -1.4639]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04485980048775673
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 599: train/loss = 0.5909070372581482, train/raw-loss = 0.5581954717636108, train/logprobs = tensor([[-1.5757, -2.6455],
        [-1.4870, -1.8712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06542299687862396
RAW KL tensor(0.0431, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 600: train/loss = 0.6446826457977295, train/raw-loss = 0.6206310987472534, train/logprobs = tensor([[-1.4781, -1.7467],
        [-1.3551, -1.2895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048103123903274536
RAW KL tensor(0.0551, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 601: train/loss = 0.6280503273010254, train/raw-loss = 0.6059912443161011, train/logprobs = tensor([[-1.3779, -2.1497],
        [-1.1887, -1.5853]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04411810636520386
RAW KL tensor(0.0773, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 602: train/loss = 0.5853409767150879, train/raw-loss = 0.5662361979484558, train/logprobs = tensor([[-1.4252, -1.9185],
        [-1.2301, -1.1484]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03820953518152237
RAW KL tensor(0.0148, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 603: train/loss = 0.5612052083015442, train/raw-loss = 0.5453979969024658, train/logprobs = tensor([[-1.0245, -2.9087],
        [-0.9360, -2.1250]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03161439299583435
RAW KL tensor(0.0309, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 604: train/loss = 0.5456146001815796, train/raw-loss = 0.5281817317008972, train/logprobs = tensor([[-0.8964, -2.4536],
        [-0.9043, -1.6433]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03486575931310654
RAW KL tensor(0.0266, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 605: train/loss = 0.5692875981330872, train/raw-loss = 0.5522777438163757, train/logprobs = tensor([[-1.2291, -2.3186],
        [-1.0962, -1.5308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.034019678831100464
RAW KL tensor(0.0770, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 606: train/loss = 0.605145275592804, train/raw-loss = 0.5760576725006104, train/logprobs = tensor([[-1.6242, -2.4689],
        [-1.5422, -1.7778]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.058175209909677505
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 607: train/loss = 0.52866131067276, train/raw-loss = 0.4986129105091095, train/logprobs = tensor([[-1.6685, -2.6279],
        [-1.5673, -1.5711]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06009681895375252
RAW KL tensor(0.0375, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 608: train/loss = 0.49045273661613464, train/raw-loss = 0.4642542898654938, train/logprobs = tensor([[-1.6195, -3.7478],
        [-1.4326, -2.2130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05239686369895935
RAW KL tensor(0.0427, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 609: train/loss = 0.5415343046188354, train/raw-loss = 0.526702880859375, train/logprobs = tensor([[-1.2383, -2.3063],
        [-1.1676, -1.4802]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02966286614537239
RAW KL tensor(0.0461, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 251])
Epoch 0, Step 610: train/loss = 0.5421807169914246, train/raw-loss = 0.5195858478546143, train/logprobs = tensor([[-1.2958, -3.0740],
        [-1.0053, -1.8149]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04518982768058777
RAW KL tensor(0.0313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 611: train/loss = 0.5708164572715759, train/raw-loss = 0.5538796782493591, train/logprobs = tensor([[-1.3498, -2.7684],
        [-1.2486, -2.0102]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03387349098920822
RAW KL tensor(0.0425, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 612: train/loss = 0.5346671342849731, train/raw-loss = 0.5111337900161743, train/logprobs = tensor([[-1.5486, -2.6532],
        [-1.2876, -1.5456]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04706656187772751
RAW KL tensor(0.0465, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 613: train/loss = 0.4874347448348999, train/raw-loss = 0.4673421382904053, train/logprobs = tensor([[-0.9928, -3.5612],
        [-0.8923, -2.2596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04018525406718254
RAW KL tensor(0.0511, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 614: train/loss = 0.48947763442993164, train/raw-loss = 0.4589005708694458, train/logprobs = tensor([[-1.5068, -3.8183],
        [-1.2221, -2.4026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061154093593358994
RAW KL tensor(0.0193, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 615: train/loss = 0.5203036069869995, train/raw-loss = 0.5012137293815613, train/logprobs = tensor([[-0.7841, -2.1920],
        [-0.6787, -1.1154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03817978873848915
RAW KL tensor(0.0221, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 616: train/loss = 0.548845648765564, train/raw-loss = 0.5257349014282227, train/logprobs = tensor([[-2.0359, -3.4972],
        [-1.5895, -2.2070]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04622149094939232
RAW KL tensor(0.0333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 617: train/loss = 0.5379135608673096, train/raw-loss = 0.516010582447052, train/logprobs = tensor([[-1.2270, -2.2816],
        [-1.0787, -1.3349]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043806079775094986
RAW KL tensor(0.0621, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 618: train/loss = 0.536697506904602, train/raw-loss = 0.5088387131690979, train/logprobs = tensor([[-1.2253, -2.7447],
        [-0.9861, -1.5315]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05571749433875084
RAW KL tensor(0.0793, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 619: train/loss = 0.6271117329597473, train/raw-loss = 0.555014431476593, train/logprobs = tensor([[-2.6892, -3.4276],
        [-1.3657, -1.4015]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14419463276863098
RAW KL tensor(0.0444, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 620: train/loss = 0.5379024147987366, train/raw-loss = 0.5132439136505127, train/logprobs = tensor([[-1.2725, -2.6900],
        [-1.1304, -1.7272]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04931708425283432
RAW KL tensor(0.0669, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 621: train/loss = 0.520343005657196, train/raw-loss = 0.492756187915802, train/logprobs = tensor([[-2.6799, -3.9529],
        [-1.5650, -1.6199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055173613131046295
RAW KL tensor(0.0418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 622: train/loss = 0.482937753200531, train/raw-loss = 0.45819008350372314, train/logprobs = tensor([[-1.2773, -3.2751],
        [-1.2381, -1.9762]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04949532449245453
RAW KL tensor(0.0933, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 623: train/loss = 0.44218993186950684, train/raw-loss = 0.4132866859436035, train/logprobs = tensor([[-2.8147, -4.6656],
        [-2.0520, -2.1765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.057806529104709625
RAW KL tensor(0.0491, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 624: train/loss = 0.5768090486526489, train/raw-loss = 0.5471125841140747, train/logprobs = tensor([[-1.6002, -1.9784],
        [-1.3958, -1.0748]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05939286947250366
RAW KL tensor(0.0200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 625: train/loss = 0.6137885451316833, train/raw-loss = 0.5626217126846313, train/logprobs = tensor([[-1.5256, -2.3285],
        [-1.5472, -1.5680]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10233361274003983
RAW KL tensor(0.0447, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 626: train/loss = 0.5222431421279907, train/raw-loss = 0.49656495451927185, train/logprobs = tensor([[-1.8385, -3.1914],
        [-1.5217, -1.9575]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05135645344853401
RAW KL tensor(0.0401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 627: train/loss = 0.5530925393104553, train/raw-loss = 0.5393356084823608, train/logprobs = tensor([[-1.3478, -2.5656],
        [-0.9377, -1.3791]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0275137759745121
RAW KL tensor(0.0332, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 628: train/loss = 0.5293092727661133, train/raw-loss = 0.5002241134643555, train/logprobs = tensor([[-1.3174, -3.0949],
        [-1.0388, -1.8928]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05817025527358055
RAW KL tensor(0.1062, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 629: train/loss = 0.5704898238182068, train/raw-loss = 0.5349440574645996, train/logprobs = tensor([[-1.4668, -2.2897],
        [-1.3319, -1.3905]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.071091428399086
RAW KL tensor(0.0207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 630: train/loss = 0.4909958243370056, train/raw-loss = 0.4733670949935913, train/logprobs = tensor([[-1.1904, -2.9144],
        [-1.1305, -1.7068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03525741025805473
RAW KL tensor(0.0494, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 631: train/loss = 0.6250470876693726, train/raw-loss = 0.605653703212738, train/logprobs = tensor([[-1.0792, -1.5114],
        [-0.9087, -0.9554]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0387866385281086
RAW KL tensor(0.0564, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 632: train/loss = 0.5370723009109497, train/raw-loss = 0.5128387212753296, train/logprobs = tensor([[-1.1815, -2.5053],
        [-1.1120, -1.5455]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04846714809536934
RAW KL tensor(0.0638, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 633: train/loss = 0.48159682750701904, train/raw-loss = 0.4592343270778656, train/logprobs = tensor([[-1.0561, -2.9661],
        [-0.9673, -1.7130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04472501575946808
RAW KL tensor(0.0577, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 634: train/loss = 0.5256870985031128, train/raw-loss = 0.5032358765602112, train/logprobs = tensor([[-1.7568, -3.5246],
        [-1.6213, -2.4513]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044902440160512924
RAW KL tensor(0.0234, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 635: train/loss = 0.6176549792289734, train/raw-loss = 0.5911788940429688, train/logprobs = tensor([[-1.5539, -2.4461],
        [-1.2413, -1.5404]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05295215919613838
RAW KL tensor(0.1034, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 636: train/loss = 0.5721914768218994, train/raw-loss = 0.5375516414642334, train/logprobs = tensor([[-1.8662, -2.5112],
        [-1.5804, -1.4895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06927971541881561
RAW KL tensor(0.0654, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 637: train/loss = 0.6022433042526245, train/raw-loss = 0.5755212903022766, train/logprobs = tensor([[-1.5172, -1.7875],
        [-1.3130, -1.0225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05344393476843834
RAW KL tensor(0.0315, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 638: train/loss = 0.6221579313278198, train/raw-loss = 0.6070724725723267, train/logprobs = tensor([[-1.4804, -1.7454],
        [-1.1979, -1.0833]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.030170904472470284
RAW KL tensor(0.0116, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 305])
Epoch 0, Step 639: train/loss = 0.618604838848114, train/raw-loss = 0.5965946316719055, train/logprobs = tensor([[-1.5609, -1.9003],
        [-1.2088, -1.0979]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04402044788002968
RAW KL tensor(0.0252, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 640: train/loss = 0.45959922671318054, train/raw-loss = 0.43711328506469727, train/logprobs = tensor([[-1.0521, -3.1303],
        [-0.8340, -1.4812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04497189074754715
RAW KL tensor(0.0560, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 641: train/loss = 0.5423511266708374, train/raw-loss = 0.5251872539520264, train/logprobs = tensor([[-1.2048, -2.4780],
        [-1.2067, -1.4147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03432774916291237
RAW KL tensor(0.0342, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 642: train/loss = 0.613521933555603, train/raw-loss = 0.5979200601577759, train/logprobs = tensor([[-1.0184, -1.7707],
        [-0.7903, -1.0957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031203676015138626
RAW KL tensor(0.0741, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 643: train/loss = 0.6089016199111938, train/raw-loss = 0.5865447521209717, train/logprobs = tensor([[-1.2040, -1.9535],
        [-0.9741, -1.2150]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04471374303102493
RAW KL tensor(0.0526, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 644: train/loss = 0.4650336802005768, train/raw-loss = 0.42539340257644653, train/logprobs = tensor([[-1.5370, -3.9186],
        [-1.3257, -2.2148]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0792805626988411
RAW KL tensor(0.0333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 645: train/loss = 0.6022586822509766, train/raw-loss = 0.5857729315757751, train/logprobs = tensor([[-1.0702, -1.6748],
        [-0.9100, -1.0305]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03297139331698418
RAW KL tensor(0.0405, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 646: train/loss = 0.5040087699890137, train/raw-loss = 0.48496323823928833, train/logprobs = tensor([[-1.5040, -3.2454],
        [-1.2311, -1.9425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.038091085851192474
RAW KL tensor(0.0682, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 647: train/loss = 0.5038770437240601, train/raw-loss = 0.4764680862426758, train/logprobs = tensor([[-1.7481, -2.7723],
        [-1.5709, -1.5125]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05481795221567154
RAW KL tensor(0.0521, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 648: train/loss = 0.47674402594566345, train/raw-loss = 0.4517757296562195, train/logprobs = tensor([[-1.1257, -2.8856],
        [-0.8329, -1.4264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.049936581403017044
RAW KL tensor(0.0454, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 649: train/loss = 0.6207888722419739, train/raw-loss = 0.5950772762298584, train/logprobs = tensor([[-1.0620, -1.8654],
        [-0.9181, -1.1436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05142321437597275
RAW KL tensor(0.1593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 650: train/loss = 0.5904099941253662, train/raw-loss = 0.5458313226699829, train/logprobs = tensor([[-1.9581, -3.1072],
        [-1.4182, -1.7940]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08915743976831436
RAW KL tensor(0.0360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 651: train/loss = 0.5583308935165405, train/raw-loss = 0.5328826904296875, train/logprobs = tensor([[-1.5254, -2.0653],
        [-1.3946, -1.1209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05089631676673889
RAW KL tensor(0.0811, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 652: train/loss = 0.5130810141563416, train/raw-loss = 0.4830515384674072, train/logprobs = tensor([[-1.0320, -2.6260],
        [-0.9243, -1.4743]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06005903333425522
RAW KL tensor(0.0699, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 653: train/loss = 0.6336219310760498, train/raw-loss = 0.6078217625617981, train/logprobs = tensor([[-1.5006, -2.0591],
        [-1.1135, -1.2731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05160033702850342
RAW KL tensor(0.0278, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 246])
Epoch 0, Step 654: train/loss = 0.4987316131591797, train/raw-loss = 0.4668295681476593, train/logprobs = tensor([[-1.2409, -2.1669],
        [-1.2885, -1.0998]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06380403786897659
RAW KL tensor(0.0524, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 655: train/loss = 0.4511375427246094, train/raw-loss = 0.4220106601715088, train/logprobs = tensor([[-1.4176, -3.3534],
        [-1.3038, -1.8644]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05825382471084595
RAW KL tensor(0.0260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 656: train/loss = 0.5793901085853577, train/raw-loss = 0.5635006427764893, train/logprobs = tensor([[-0.9089, -2.1796],
        [-0.8458, -1.4936]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03177891671657562
RAW KL tensor(0.0410, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 657: train/loss = 0.5830751061439514, train/raw-loss = 0.5616215467453003, train/logprobs = tensor([[-1.3813, -2.0555],
        [-1.0384, -1.0114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04290715977549553
RAW KL tensor(0.0168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 658: train/loss = 0.6054039001464844, train/raw-loss = 0.5876849889755249, train/logprobs = tensor([[-1.4271, -1.6808],
        [-1.2878, -1.0135]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.035437777638435364
RAW KL tensor(0.0832, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 659: train/loss = 0.4296764135360718, train/raw-loss = 0.37426891922950745, train/logprobs = tensor([[-1.3021, -4.6666],
        [-1.1084, -2.2446]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11081498861312866
RAW KL tensor(0.0626, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 660: train/loss = 0.5570134520530701, train/raw-loss = 0.523999035358429, train/logprobs = tensor([[-1.3168, -3.0119],
        [-1.1095, -1.7764]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06602884829044342
RAW KL tensor(0.0978, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 661: train/loss = 0.512953519821167, train/raw-loss = 0.4689605236053467, train/logprobs = tensor([[-1.6836, -3.2317],
        [-1.6796, -2.0688]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08798591047525406
RAW KL tensor(0.0698, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 662: train/loss = 0.5411967635154724, train/raw-loss = 0.508853554725647, train/logprobs = tensor([[-1.2421, -2.2430],
        [-1.1071, -1.0974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06468638777732849
RAW KL tensor(0.0421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 663: train/loss = 0.4755723476409912, train/raw-loss = 0.45073240995407104, train/logprobs = tensor([[-1.4275, -2.5844],
        [-1.1365, -1.0057]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04967978596687317
RAW KL tensor(0.0437, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 664: train/loss = 0.5311663150787354, train/raw-loss = 0.4879721403121948, train/logprobs = tensor([[-1.8619, -3.4479],
        [-1.7195, -2.2755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08638826757669449
RAW KL tensor(0.0825, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 665: train/loss = 0.4781278967857361, train/raw-loss = 0.45409560203552246, train/logprobs = tensor([[-1.2491, -3.4022],
        [-1.1036, -2.0932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.048064492642879486
RAW KL tensor(0.0299, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 666: train/loss = 0.537880003452301, train/raw-loss = 0.5192272663116455, train/logprobs = tensor([[-1.5259, -2.1581],
        [-1.3252, -1.1417]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037305496633052826
RAW KL tensor(0.0323, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 667: train/loss = 0.6270973682403564, train/raw-loss = 0.5993236303329468, train/logprobs = tensor([[-1.4134, -1.7188],
        [-1.1704, -1.0119]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05554739385843277
RAW KL tensor(0.0314, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 668: train/loss = 0.4495571255683899, train/raw-loss = 0.42181408405303955, train/logprobs = tensor([[-0.9095, -3.0035],
        [-0.8670, -1.6323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.055486105382442474
RAW KL tensor(0.0547, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 669: train/loss = 0.5319924354553223, train/raw-loss = 0.5082485675811768, train/logprobs = tensor([[-1.2843, -2.8789],
        [-1.0620, -1.7669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047487761825323105
RAW KL tensor(0.0201, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 670: train/loss = 0.6916368007659912, train/raw-loss = 0.6655715107917786, train/logprobs = tensor([[-1.3438, -1.7863],
        [-1.0594, -1.3512]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05213059484958649
RAW KL tensor(0.0100, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 671: train/loss = 0.6862987279891968, train/raw-loss = 0.6721148490905762, train/logprobs = tensor([[-1.4340, -1.6398],
        [-1.1046, -1.2216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02836776338517666
RAW KL tensor(0.0521, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 672: train/loss = 0.44869494438171387, train/raw-loss = 0.41480082273483276, train/logprobs = tensor([[-1.7075, -3.3104],
        [-1.2854, -1.3981]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06778820604085922
RAW KL tensor(0.1113, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 117])
Epoch 0, Step 673: train/loss = 0.484794020652771, train/raw-loss = 0.44473516941070557, train/logprobs = tensor([[-1.5641, -3.3074],
        [-1.1660, -1.6618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08011771738529205
RAW KL tensor(0.0368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 674: train/loss = 0.5591937899589539, train/raw-loss = 0.5356545448303223, train/logprobs = tensor([[-1.3074, -2.4731],
        [-1.1650, -1.6063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04707860201597214
RAW KL tensor(0.0423, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 675: train/loss = 0.48804813623428345, train/raw-loss = 0.4676772952079773, train/logprobs = tensor([[-1.0915, -3.0457],
        [-0.8651, -1.6872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04074173420667648
RAW KL tensor(0.0642, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 676: train/loss = 0.5494645237922668, train/raw-loss = 0.510444164276123, train/logprobs = tensor([[-1.6576, -2.8715],
        [-1.1725, -1.4508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07804075628519058
RAW KL tensor(0.0162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 677: train/loss = 0.4243682026863098, train/raw-loss = 0.4001844823360443, train/logprobs = tensor([[-1.5526, -3.7228],
        [-1.3350, -1.8931]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04836741462349892
RAW KL tensor(0.0728, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 678: train/loss = 0.4464978575706482, train/raw-loss = 0.415950208902359, train/logprobs = tensor([[-1.7680, -3.8467],
        [-1.3570, -2.0278]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06109531223773956
RAW KL tensor(0.0325, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 679: train/loss = 0.6352609395980835, train/raw-loss = 0.6151069402694702, train/logprobs = tensor([[-1.5691, -1.7517],
        [-1.2329, -1.0009]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040307942777872086
RAW KL tensor(0.0474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 680: train/loss = 0.5999623537063599, train/raw-loss = 0.5723835229873657, train/logprobs = tensor([[-1.9301, -2.3867],
        [-1.7865, -1.6508]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05515769496560097
RAW KL tensor(0.0650, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 681: train/loss = 0.554249107837677, train/raw-loss = 0.5250632166862488, train/logprobs = tensor([[-1.0123, -2.9359],
        [-0.8267, -1.7023]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05837170034646988
RAW KL tensor(0.1018, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 682: train/loss = 0.4125731289386749, train/raw-loss = 0.3797664940357208, train/logprobs = tensor([[-1.6990, -3.7925],
        [-1.5100, -1.8711]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06561324000358582
RAW KL tensor(0.0694, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 683: train/loss = 0.5370234847068787, train/raw-loss = 0.508222222328186, train/logprobs = tensor([[-2.2079, -2.8000],
        [-1.7874, -1.3481]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05760243535041809
RAW KL tensor(0.0645, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 684: train/loss = 0.6071556210517883, train/raw-loss = 0.5642549991607666, train/logprobs = tensor([[-1.3424, -2.3024],
        [-1.1362, -1.4229]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08580120652914047
RAW KL tensor(0.0428, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 685: train/loss = 0.4932975471019745, train/raw-loss = 0.4710198938846588, train/logprobs = tensor([[-1.4744, -3.1559],
        [-1.1616, -1.8078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04455532878637314
RAW KL tensor(0.0272, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 686: train/loss = 0.5378416776657104, train/raw-loss = 0.508962094783783, train/logprobs = tensor([[-0.9601, -2.5435],
        [-0.8132, -1.4471]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05775916576385498
RAW KL tensor(0.0736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 687: train/loss = 0.5345635414123535, train/raw-loss = 0.5028864145278931, train/logprobs = tensor([[-1.6998, -2.8308],
        [-1.5725, -1.5879]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06335421651601791
RAW KL tensor(0.0462, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 688: train/loss = 0.462382972240448, train/raw-loss = 0.42630326747894287, train/logprobs = tensor([[-1.6077, -3.9295],
        [-1.2726, -2.0895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07215944677591324
RAW KL tensor(0.0233, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 689: train/loss = 0.42883971333503723, train/raw-loss = 0.40860530734062195, train/logprobs = tensor([[-0.8640, -2.8857],
        [-0.6728, -1.2657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04046878591179848
RAW KL tensor(0.0207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 690: train/loss = 0.5014216899871826, train/raw-loss = 0.47609400749206543, train/logprobs = tensor([[-1.3726, -3.1078],
        [-1.1274, -1.7474]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05065544322133064
RAW KL tensor(0.0654, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 691: train/loss = 0.5467519760131836, train/raw-loss = 0.5252717733383179, train/logprobs = tensor([[-0.9565, -2.1958],
        [-0.8024, -1.2731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04296032339334488
RAW KL tensor(0.0888, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 692: train/loss = 0.5140460133552551, train/raw-loss = 0.487261027097702, train/logprobs = tensor([[-1.3227, -2.8317],
        [-1.2081, -1.7206]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05356994643807411
RAW KL tensor(0.0690, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 693: train/loss = 0.4925287961959839, train/raw-loss = 0.4674491584300995, train/logprobs = tensor([[-1.4746, -3.0168],
        [-1.2786, -1.4793]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05015937238931656
RAW KL tensor(0.0619, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 694: train/loss = 0.5241562128067017, train/raw-loss = 0.5022690892219543, train/logprobs = tensor([[-1.5190, -2.8334],
        [-1.2116, -1.5954]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.043774303048849106
RAW KL tensor(0.0653, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 695: train/loss = 0.41658324003219604, train/raw-loss = 0.3667602241039276, train/logprobs = tensor([[-1.5518, -3.6675],
        [-1.2267, -1.6065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09964604675769806
RAW KL tensor(0.0841, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 696: train/loss = 0.4307829737663269, train/raw-loss = 0.401064395904541, train/logprobs = tensor([[-1.3132, -3.6023],
        [-1.1238, -1.8188]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05943715572357178
RAW KL tensor(0.0670, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 697: train/loss = 0.454517662525177, train/raw-loss = 0.41561076045036316, train/logprobs = tensor([[-1.4900, -3.3332],
        [-1.3125, -1.6275]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07781380414962769
RAW KL tensor(0.1088, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 698: train/loss = 0.481850266456604, train/raw-loss = 0.44568145275115967, train/logprobs = tensor([[-1.2722, -3.3895],
        [-0.9710, -1.7612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07233758270740509
RAW KL tensor(0.0387, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 699: train/loss = 0.546568751335144, train/raw-loss = 0.5113487839698792, train/logprobs = tensor([[-1.5319, -2.6696],
        [-1.1322, -1.3727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0704399049282074
RAW KL tensor(0.0137, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 700: train/loss = 0.49869227409362793, train/raw-loss = 0.4840065836906433, train/logprobs = tensor([[-0.7502, -2.7083],
        [-0.6542, -1.5083]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.029371388256549835
RAW KL tensor(0.0968, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 701: train/loss = 0.5498127341270447, train/raw-loss = 0.5231172442436218, train/logprobs = tensor([[-1.3202, -2.4773],
        [-1.1466, -1.4524]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.053390972316265106
RAW KL tensor(0.0597, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 289])
Epoch 0, Step 702: train/loss = 0.5333788990974426, train/raw-loss = 0.49546241760253906, train/logprobs = tensor([[-2.3691, -2.9561],
        [-1.9707, -1.5242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07583296298980713
RAW KL tensor(0.0404, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 703: train/loss = 0.42075827717781067, train/raw-loss = 0.3863837718963623, train/logprobs = tensor([[-1.8398, -3.9801],
        [-1.3051, -1.7983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06874902546405792
RAW KL tensor(0.0425, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 704: train/loss = 0.46957194805145264, train/raw-loss = 0.4445546865463257, train/logprobs = tensor([[-1.4993, -3.4248],
        [-1.3864, -2.0608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05003451183438301
RAW KL tensor(0.0749, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 705: train/loss = 0.5742651224136353, train/raw-loss = 0.539409875869751, train/logprobs = tensor([[-1.5747, -2.9914],
        [-1.1750, -1.8437]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06971044838428497
RAW KL tensor(0.0890, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 706: train/loss = 0.4561753273010254, train/raw-loss = 0.42504823207855225, train/logprobs = tensor([[-1.4689, -3.6439],
        [-0.9242, -1.6279]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06225421652197838
RAW KL tensor(0.0895, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 707: train/loss = 0.5828201174736023, train/raw-loss = 0.5487322211265564, train/logprobs = tensor([[-1.7891, -2.6783],
        [-1.1467, -1.3559]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06817580759525299
RAW KL tensor(0.1031, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 708: train/loss = 0.6363039016723633, train/raw-loss = 0.6138226985931396, train/logprobs = tensor([[-1.1977, -1.6286],
        [-1.1114, -1.1869]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04496241360902786
RAW KL tensor(0.0928, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 709: train/loss = 0.5647172927856445, train/raw-loss = 0.5383783578872681, train/logprobs = tensor([[-0.9439, -2.2701],
        [-0.7751, -1.3610]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0526779368519783
RAW KL tensor(0.0260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 710: train/loss = 0.4868094325065613, train/raw-loss = 0.4524811804294586, train/logprobs = tensor([[-0.8392, -3.1389],
        [-0.6073, -1.4635]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06865646690130234
RAW KL tensor(0.0738, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 711: train/loss = 0.4221898317337036, train/raw-loss = 0.38736844062805176, train/logprobs = tensor([[-1.6400, -4.0432],
        [-1.4304, -2.1432]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0696428120136261
RAW KL tensor(0.0465, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 712: train/loss = 0.4717904329299927, train/raw-loss = 0.43780866265296936, train/logprobs = tensor([[-1.1101, -3.2037],
        [-0.8831, -1.6628]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06796353310346603
RAW KL tensor(0.0708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 713: train/loss = 0.337503582239151, train/raw-loss = 0.29478585720062256, train/logprobs = tensor([[-1.4826, -4.4925],
        [-1.2611, -1.7402]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08543548732995987
RAW KL tensor(0.0512, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 714: train/loss = 0.5837898850440979, train/raw-loss = 0.5629476308822632, train/logprobs = tensor([[-1.1340, -1.6181],
        [-0.9750, -0.8486]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04168446362018585
RAW KL tensor(0.0578, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 715: train/loss = 0.6624264717102051, train/raw-loss = 0.6416981816291809, train/logprobs = tensor([[-1.0214, -1.2700],
        [-0.8402, -0.8484]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04145659878849983
RAW KL tensor(0.0810, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 716: train/loss = 0.47261252999305725, train/raw-loss = 0.4295614957809448, train/logprobs = tensor([[-1.7684, -3.7412],
        [-1.4791, -1.6717]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08610201627016068
RAW KL tensor(0.0481, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 717: train/loss = 0.5572749376296997, train/raw-loss = 0.523001492023468, train/logprobs = tensor([[-1.7760, -3.2602],
        [-1.4943, -1.8997]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06854688376188278
RAW KL tensor(0.7202, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 108])
Epoch 0, Step 718: train/loss = 0.5419549942016602, train/raw-loss = 0.4165542423725128, train/logprobs = tensor([[-2.6290, -5.6372],
        [-1.2348, -2.1710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2508014440536499
RAW KL tensor(0.0115, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 719: train/loss = 0.5840210914611816, train/raw-loss = 0.5635538101196289, train/logprobs = tensor([[-1.4436, -1.8813],
        [-1.3300, -1.0309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.040934666991233826
RAW KL tensor(0.0302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 720: train/loss = 0.5395374298095703, train/raw-loss = 0.520811140537262, train/logprobs = tensor([[-1.0541, -2.3399],
        [-0.8557, -1.2792]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03745265305042267
RAW KL tensor(0.0782, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 721: train/loss = 0.4802666902542114, train/raw-loss = 0.438077449798584, train/logprobs = tensor([[-1.9017, -3.5001],
        [-1.3166, -1.4440]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08437839150428772
RAW KL tensor(0.0227, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 722: train/loss = 0.4630351662635803, train/raw-loss = 0.4315873384475708, train/logprobs = tensor([[-1.1357, -3.3619],
        [-0.9035, -1.4463]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06289571523666382
RAW KL tensor(0.0348, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 723: train/loss = 0.5368868112564087, train/raw-loss = 0.5015404224395752, train/logprobs = tensor([[-1.8879, -3.5794],
        [-1.2648, -2.0436]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07069283723831177
RAW KL tensor(0.0431, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 724: train/loss = 0.536853015422821, train/raw-loss = 0.5106325149536133, train/logprobs = tensor([[-1.1267, -2.8616],
        [-0.9133, -1.6071]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.052440959960222244
RAW KL tensor(0.0719, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 725: train/loss = 0.540793776512146, train/raw-loss = 0.5140870213508606, train/logprobs = tensor([[-1.6930, -2.3172],
        [-1.4152, -1.1707]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05341336503624916
RAW KL tensor(0.0206, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 358])
Epoch 0, Step 726: train/loss = 0.465605765581131, train/raw-loss = 0.43156740069389343, train/logprobs = tensor([[-1.3618, -2.6898],
        [-1.1730, -1.0779]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0680767297744751
RAW KL tensor(0.0275, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 727: train/loss = 0.5344775915145874, train/raw-loss = 0.5132288932800293, train/logprobs = tensor([[-0.9484, -2.1483],
        [-0.8109, -1.1194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04249735176563263
RAW KL tensor(0.1167, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 728: train/loss = 0.5040045976638794, train/raw-loss = 0.4701026380062103, train/logprobs = tensor([[-1.9851, -3.0148],
        [-1.5385, -1.5097]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06780402362346649
RAW KL tensor(0.0781, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 323])
Epoch 0, Step 729: train/loss = 0.4915210008621216, train/raw-loss = 0.4579702913761139, train/logprobs = tensor([[-1.1153, -2.8280],
        [-0.9673, -1.4773]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06710142642259598
RAW KL tensor(0.0667, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 730: train/loss = 0.6277223825454712, train/raw-loss = 0.5993765592575073, train/logprobs = tensor([[-2.0257, -2.0249],
        [-1.7496, -1.3057]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05669168010354042
RAW KL tensor(0.0601, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 731: train/loss = 0.5460308194160461, train/raw-loss = 0.5224441885948181, train/logprobs = tensor([[-2.0404, -2.6341],
        [-1.6072, -1.3718]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.047173283994197845
RAW KL tensor(0.0979, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 732: train/loss = 0.484578400850296, train/raw-loss = 0.44305238127708435, train/logprobs = tensor([[-2.0421, -3.1348],
        [-1.8088, -1.5136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08305206894874573
RAW KL tensor(0.0546, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 733: train/loss = 0.4354132413864136, train/raw-loss = 0.4080672562122345, train/logprobs = tensor([[-1.2949, -3.3704],
        [-1.1957, -1.7817]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05469197779893875
RAW KL tensor(0.0238, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 734: train/loss = 0.6239453554153442, train/raw-loss = 0.6059554815292358, train/logprobs = tensor([[-1.3005, -1.8091],
        [-0.7400, -0.8334]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03597988188266754
RAW KL tensor(0.1681, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 735: train/loss = 0.6710690855979919, train/raw-loss = 0.6371275186538696, train/logprobs = tensor([[-2.2496, -2.0343],
        [-1.4665, -0.9901]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06788308918476105
RAW KL tensor(0.0409, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 736: train/loss = 0.4197712540626526, train/raw-loss = 0.3964795470237732, train/logprobs = tensor([[-1.8519, -3.5880],
        [-1.5035, -1.4293]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.046583518385887146
RAW KL tensor(0.0514, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 737: train/loss = 0.5909518003463745, train/raw-loss = 0.553932249546051, train/logprobs = tensor([[-1.6195, -2.3499],
        [-1.3031, -1.1855]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07403907924890518
RAW KL tensor(0.2995, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 738: train/loss = 0.45934444665908813, train/raw-loss = 0.4036492705345154, train/logprobs = tensor([[-1.4489, -4.7460],
        [-1.3600, -2.5923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11139041930437088
RAW KL tensor(0.0736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 739: train/loss = 0.34508442878723145, train/raw-loss = 0.2807495594024658, train/logprobs = tensor([[-1.7034, -4.7226],
        [-1.1488, -1.3771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12866972386837006
RAW KL tensor(0.0276, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 740: train/loss = 0.5071721076965332, train/raw-loss = 0.48132434487342834, train/logprobs = tensor([[-0.8194, -2.7517],
        [-0.6993, -1.4541]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05169546604156494
RAW KL tensor(0.1014, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 741: train/loss = 0.4794640839099884, train/raw-loss = 0.4351521134376526, train/logprobs = tensor([[-2.0692, -3.2362],
        [-1.5024, -1.3424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08862389624118805
RAW KL tensor(0.0527, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 742: train/loss = 0.4836735725402832, train/raw-loss = 0.46505895256996155, train/logprobs = tensor([[-0.9195, -2.6966],
        [-0.7007, -1.1534]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.037229228764772415
RAW KL tensor(0.0550, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 743: train/loss = 0.5328748822212219, train/raw-loss = 0.5141711235046387, train/logprobs = tensor([[-1.0930, -3.1170],
        [-0.8333, -1.5873]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03740746155381203
RAW KL tensor(0.0151, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 744: train/loss = 0.4845001995563507, train/raw-loss = 0.45411333441734314, train/logprobs = tensor([[-1.3507, -3.7150],
        [-1.1853, -2.2360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06077377498149872
RAW KL tensor(0.0562, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 745: train/loss = 0.460991770029068, train/raw-loss = 0.4216424822807312, train/logprobs = tensor([[-1.2020, -3.7794],
        [-1.0673, -2.0330]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07869866490364075
RAW KL tensor(0.0475, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 746: train/loss = 0.48039138317108154, train/raw-loss = 0.4581713378429413, train/logprobs = tensor([[-0.8075, -3.5096],
        [-0.6813, -2.0066]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.044440027326345444
RAW KL tensor(0.0660, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 747: train/loss = 0.6565643548965454, train/raw-loss = 0.6227855682373047, train/logprobs = tensor([[-1.6828, -1.8486],
        [-1.3626, -1.1886]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0675574541091919
RAW KL tensor(0.0523, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 748: train/loss = 0.5485783815383911, train/raw-loss = 0.5180071592330933, train/logprobs = tensor([[-1.7220, -3.5518],
        [-1.4407, -2.2707]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.061142366379499435
RAW KL tensor(0.0485, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 749: train/loss = 0.5351737141609192, train/raw-loss = 0.5054351091384888, train/logprobs = tensor([[-1.9883, -2.7980],
        [-1.4176, -1.3211]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.059477195143699646
RAW KL tensor(0.0215, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 750: train/loss = 0.48993536829948425, train/raw-loss = 0.4530671238899231, train/logprobs = tensor([[-1.6058, -3.0074],
        [-1.1328, -1.3417]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07373642176389694
RAW KL tensor(0.0426, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 751: train/loss = 0.4827783703804016, train/raw-loss = 0.45757192373275757, train/logprobs = tensor([[-1.3811, -2.7145],
        [-1.2505, -1.3739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05041283369064331
RAW KL tensor(0.0917, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 752: train/loss = 0.5600863695144653, train/raw-loss = 0.5411016345024109, train/logprobs = tensor([[-0.8151, -1.9082],
        [-0.6656, -0.9966]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03796938806772232
RAW KL tensor(0.0711, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 753: train/loss = 0.46396225690841675, train/raw-loss = 0.44040608406066895, train/logprobs = tensor([[-1.0955, -2.8726],
        [-1.0025, -1.4796]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04711231216788292
RAW KL tensor(0.0730, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 754: train/loss = 0.4679340720176697, train/raw-loss = 0.4307134747505188, train/logprobs = tensor([[-1.3829, -4.2952],
        [-1.2312, -2.6490]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07444118708372116
RAW KL tensor(0.0894, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 755: train/loss = 0.3430357575416565, train/raw-loss = 0.30226317048072815, train/logprobs = tensor([[-1.8138, -4.7205],
        [-1.4757, -1.9226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08154520392417908
RAW KL tensor(0.0957, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 756: train/loss = 0.46915268898010254, train/raw-loss = 0.44547298550605774, train/logprobs = tensor([[-1.1557, -3.2349],
        [-0.9622, -1.5797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04735937714576721
RAW KL tensor(0.0573, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 757: train/loss = 0.4505895972251892, train/raw-loss = 0.408828467130661, train/logprobs = tensor([[-1.7284, -4.3003],
        [-1.5692, -2.5596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08352221548557281
RAW KL tensor(0.0586, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 758: train/loss = 0.4767370820045471, train/raw-loss = 0.45748844742774963, train/logprobs = tensor([[-1.3808, -3.1742],
        [-0.8955, -1.4487]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03849722445011139
RAW KL tensor(0.0638, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 759: train/loss = 0.4119170904159546, train/raw-loss = 0.37127113342285156, train/logprobs = tensor([[-1.9159, -3.3231],
        [-1.6804, -1.4091]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08129193633794785
RAW KL tensor(0.0715, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 760: train/loss = 0.399438738822937, train/raw-loss = 0.3287765085697174, train/logprobs = tensor([[-2.1641, -4.3062],
        [-1.8736, -2.0225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14132444560527802
RAW KL tensor(0.0768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 761: train/loss = 0.4212019145488739, train/raw-loss = 0.3835928738117218, train/logprobs = tensor([[-1.6010, -3.0002],
        [-1.3389, -1.1722]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07521798461675644
RAW KL tensor(0.0549, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 762: train/loss = 0.49440711736679077, train/raw-loss = 0.4737064838409424, train/logprobs = tensor([[-1.8843, -3.5620],
        [-1.3234, -1.8185]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04140126332640648
RAW KL tensor(0.0453, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 221])
Epoch 0, Step 763: train/loss = 0.5710010528564453, train/raw-loss = 0.5496221780776978, train/logprobs = tensor([[-1.2035, -1.8419],
        [-0.9609, -0.9182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04275774210691452
RAW KL tensor(0.1195, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 764: train/loss = 0.45547181367874146, train/raw-loss = 0.41945987939834595, train/logprobs = tensor([[-1.3885, -3.3745],
        [-1.1397, -1.7183]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07202381640672684
RAW KL tensor(0.0580, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 765: train/loss = 0.3829663097858429, train/raw-loss = 0.33875781297683716, train/logprobs = tensor([[-1.3395, -3.8998],
        [-1.1147, -1.5117]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08841699361801147
RAW KL tensor(0.0588, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 766: train/loss = 0.5569915175437927, train/raw-loss = 0.5343891382217407, train/logprobs = tensor([[-1.4794, -3.2428],
        [-1.3453, -2.0614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0452047735452652
RAW KL tensor(0.0551, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 767: train/loss = 0.5554673671722412, train/raw-loss = 0.5238978862762451, train/logprobs = tensor([[-1.5048, -2.6575],
        [-1.1837, -1.3844]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06313885748386383
RAW KL tensor(0.0637, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 768: train/loss = 0.49936991930007935, train/raw-loss = 0.47236916422843933, train/logprobs = tensor([[-1.4910, -2.1422],
        [-1.4291, -0.9806]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05400153249502182
RAW KL tensor(0.0867, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 769: train/loss = 0.5008121132850647, train/raw-loss = 0.47402840852737427, train/logprobs = tensor([[-1.0439, -2.6508],
        [-0.8129, -1.1663]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05356743559241295
RAW KL tensor(0.0205, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 770: train/loss = 0.6035547852516174, train/raw-loss = 0.5715002417564392, train/logprobs = tensor([[-1.2510, -2.0192],
        [-1.0364, -1.2067]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06410905718803406
RAW KL tensor(0.0568, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 264])
Epoch 0, Step 771: train/loss = 0.3427128195762634, train/raw-loss = 0.3028959631919861, train/logprobs = tensor([[-2.0892, -4.5700],
        [-1.5611, -1.8276]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07963375002145767
RAW KL tensor(0.1090, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 772: train/loss = 0.5624803900718689, train/raw-loss = 0.5215564370155334, train/logprobs = tensor([[-1.5538, -2.3227],
        [-1.2471, -0.9976]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08184792101383209
RAW KL tensor(0.0506, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 773: train/loss = 0.6035839319229126, train/raw-loss = 0.5660726428031921, train/logprobs = tensor([[-2.1143, -2.4287],
        [-1.3241, -1.0157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07502248883247375
RAW KL tensor(0.0552, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 774: train/loss = 0.4610390365123749, train/raw-loss = 0.42582207918167114, train/logprobs = tensor([[-1.8700, -3.6466],
        [-1.5371, -1.6656]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07043395936489105
RAW KL tensor(0.1604, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 775: train/loss = 0.452028751373291, train/raw-loss = 0.4091830253601074, train/logprobs = tensor([[-1.3832, -3.2374],
        [-1.2879, -1.6527]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08569145202636719
RAW KL tensor(0.0397, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 776: train/loss = 0.3774447441101074, train/raw-loss = 0.3295401930809021, train/logprobs = tensor([[-1.5662, -4.6675],
        [-1.3395, -1.5548]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09580911695957184
RAW KL tensor(0.1118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 777: train/loss = 0.5189128518104553, train/raw-loss = 0.4599309265613556, train/logprobs = tensor([[-1.6893, -3.3496],
        [-1.5360, -1.6014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11796391755342484
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 778: train/loss = 0.4155310094356537, train/raw-loss = 0.37606292963027954, train/logprobs = tensor([[-1.3226, -3.5001],
        [-1.1744, -1.3764]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07893620431423187
RAW KL tensor(0.0449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 779: train/loss = 0.4597787857055664, train/raw-loss = 0.400091290473938, train/logprobs = tensor([[-1.9965, -3.5453],
        [-1.4971, -1.2734]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11937505006790161
RAW KL tensor(0.0451, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 780: train/loss = 0.49018174409866333, train/raw-loss = 0.45798933506011963, train/logprobs = tensor([[-1.3594, -2.7546],
        [-0.9973, -1.1380]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06438487023115158
RAW KL tensor(0.1021, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 781: train/loss = 0.53372722864151, train/raw-loss = 0.46063435077667236, train/logprobs = tensor([[-2.1234, -3.5520],
        [-1.5269, -1.5350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.14618578553199768
RAW KL tensor(0.1034, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 782: train/loss = 0.3979238271713257, train/raw-loss = 0.35389289259910583, train/logprobs = tensor([[-1.2625, -4.0890],
        [-0.8245, -1.5908]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08806192874908447
RAW KL tensor(0.0472, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 234])
Epoch 0, Step 783: train/loss = 0.49915245175361633, train/raw-loss = 0.4710330367088318, train/logprobs = tensor([[-1.2342, -2.3476],
        [-0.8742, -0.9072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.056238796561956406
RAW KL tensor(0.0389, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 784: train/loss = 0.38022032380104065, train/raw-loss = 0.3446640372276306, train/logprobs = tensor([[-1.2049, -3.7153],
        [-0.9520, -1.6276]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07111259549856186
RAW KL tensor(0.0625, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 785: train/loss = 0.3117868900299072, train/raw-loss = 0.26579567790031433, train/logprobs = tensor([[-1.5046, -4.5157],
        [-1.1903, -1.6530]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0919824168086052
RAW KL tensor(0.0856, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 786: train/loss = 0.43705374002456665, train/raw-loss = 0.3999178409576416, train/logprobs = tensor([[-1.5040, -3.7528],
        [-1.3955, -1.9235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07427185028791428
RAW KL tensor(0.0724, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 255])
Epoch 0, Step 787: train/loss = 0.3816682696342468, train/raw-loss = 0.346197247505188, train/logprobs = tensor([[-1.4324, -3.8814],
        [-1.2994, -1.9027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07094195485115051
RAW KL tensor(0.0598, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 788: train/loss = 0.3314647972583771, train/raw-loss = 0.29093867540359497, train/logprobs = tensor([[-0.9779, -4.4223],
        [-0.8675, -1.7106]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08105221390724182
RAW KL tensor(0.1051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 789: train/loss = 0.5734660625457764, train/raw-loss = 0.5263521075248718, train/logprobs = tensor([[-1.6780, -2.2721],
        [-1.5115, -1.3079]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09422788769006729
RAW KL tensor(0.0568, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 250])
Epoch 0, Step 790: train/loss = 0.5314425230026245, train/raw-loss = 0.4899534285068512, train/logprobs = tensor([[-1.4205, -2.6042],
        [-1.0400, -1.1901]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0829782485961914
RAW KL tensor(0.0337, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 280])
Epoch 0, Step 791: train/loss = 0.48090052604675293, train/raw-loss = 0.45575541257858276, train/logprobs = tensor([[-1.4968, -2.6977],
        [-1.2154, -1.0452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05029027909040451
RAW KL tensor(0.1130, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 792: train/loss = 0.4862421751022339, train/raw-loss = 0.44459086656570435, train/logprobs = tensor([[-1.8370, -3.1395],
        [-1.6343, -1.6684]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08330269157886505
RAW KL tensor(0.0858, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 793: train/loss = 0.38483303785324097, train/raw-loss = 0.34519055485725403, train/logprobs = tensor([[-1.5155, -4.2501],
        [-1.3323, -2.0003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07928498089313507
RAW KL tensor(0.1295, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 794: train/loss = 0.5473183393478394, train/raw-loss = 0.5059217214584351, train/logprobs = tensor([[-1.4138, -2.8445],
        [-0.9724, -1.3371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08279326558113098
RAW KL tensor(0.0906, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 795: train/loss = 0.5410320162773132, train/raw-loss = 0.498852014541626, train/logprobs = tensor([[-1.2570, -2.7717],
        [-0.9183, -1.2510]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08435998857021332
RAW KL tensor(0.0267, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 300])
Epoch 0, Step 796: train/loss = 0.46231502294540405, train/raw-loss = 0.4350653886795044, train/logprobs = tensor([[-1.1288, -2.5691],
        [-1.0399, -1.0925]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05449916049838066
RAW KL tensor(0.0660, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 797: train/loss = 0.5302254557609558, train/raw-loss = 0.4925512373447418, train/logprobs = tensor([[-1.8189, -2.3014],
        [-1.5723, -1.1309]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07534842938184738
RAW KL tensor(0.0251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 798: train/loss = 0.5014674663543701, train/raw-loss = 0.4699014127254486, train/logprobs = tensor([[-1.3580, -2.4173],
        [-1.0861, -0.9230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06313210725784302
RAW KL tensor(0.0540, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 799: train/loss = 0.4470655918121338, train/raw-loss = 0.40147727727890015, train/logprobs = tensor([[-1.3046, -3.5546],
        [-0.8499, -1.4747]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0911765992641449
RAW KL tensor(0.0593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 800: train/loss = 0.5573370456695557, train/raw-loss = 0.5099705457687378, train/logprobs = tensor([[-1.6087, -1.9777],
        [-1.3784, -0.8076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09473305195569992
RAW KL tensor(0.1692, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 801: train/loss = 0.3501851260662079, train/raw-loss = 0.2986879050731659, train/logprobs = tensor([[-1.2399, -4.3547],
        [-0.9363, -1.7942]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10299447178840637
RAW KL tensor(0.1012, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 802: train/loss = 0.46478360891342163, train/raw-loss = 0.4081345200538635, train/logprobs = tensor([[-1.9231, -3.6547],
        [-1.5445, -1.6707]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11329823732376099
RAW KL tensor(0.0762, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 803: train/loss = 0.48089802265167236, train/raw-loss = 0.4505470395088196, train/logprobs = tensor([[-0.8880, -2.7999],
        [-0.7524, -1.4026]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06070190295577049
RAW KL tensor(0.0473, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 804: train/loss = 0.4583805501461029, train/raw-loss = 0.41211143136024475, train/logprobs = tensor([[-1.2567, -3.8392],
        [-1.0777, -1.8034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0925382599234581
RAW KL tensor(0.0626, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 805: train/loss = 0.39886003732681274, train/raw-loss = 0.35059085488319397, train/logprobs = tensor([[-1.5977, -4.2020],
        [-1.2387, -1.7974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09653834253549576
RAW KL tensor(0.0826, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 806: train/loss = 0.5639060139656067, train/raw-loss = 0.5065133571624756, train/logprobs = tensor([[-1.7094, -3.3934],
        [-1.2022, -1.4206]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11478530615568161
RAW KL tensor(0.0740, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 127])
Epoch 0, Step 807: train/loss = 0.4408506155014038, train/raw-loss = 0.40030500292778015, train/logprobs = tensor([[-1.8455, -3.9518],
        [-1.2888, -1.3800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08109122514724731
RAW KL tensor(0.0819, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 220])
Epoch 0, Step 808: train/loss = 0.4983316659927368, train/raw-loss = 0.45949673652648926, train/logprobs = tensor([[-1.7462, -3.3167],
        [-1.2723, -1.6001]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07766985148191452
RAW KL tensor(0.0988, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 809: train/loss = 0.4732409715652466, train/raw-loss = 0.4341149926185608, train/logprobs = tensor([[-1.2923, -3.3891],
        [-0.8312, -1.4378]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07825198024511337
RAW KL tensor(0.0880, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 810: train/loss = 0.3883872926235199, train/raw-loss = 0.345093309879303, train/logprobs = tensor([[-1.0454, -3.9968],
        [-0.9058, -1.7123]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08658798038959503
RAW KL tensor(0.1623, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 811: train/loss = 0.3561670780181885, train/raw-loss = 0.30213671922683716, train/logprobs = tensor([[-1.4042, -4.6338],
        [-1.1429, -1.9255]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10806074738502502
RAW KL tensor(0.0961, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 812: train/loss = 0.48104530572891235, train/raw-loss = 0.4302480220794678, train/logprobs = tensor([[-1.6800, -4.3749],
        [-1.2518, -2.3221]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10159452259540558
RAW KL tensor(0.0446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 813: train/loss = 0.48081499338150024, train/raw-loss = 0.42423972487449646, train/logprobs = tensor([[-1.8654, -4.3323],
        [-1.2084, -1.9268]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1131504699587822
RAW KL tensor(0.0657, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 814: train/loss = 0.4659743905067444, train/raw-loss = 0.43146345019340515, train/logprobs = tensor([[-1.9031, -3.4879],
        [-1.3995, -1.4265]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06902188062667847
RAW KL tensor(0.0910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 815: train/loss = 0.3439013659954071, train/raw-loss = 0.2991521954536438, train/logprobs = tensor([[-1.7213, -5.2604],
        [-1.2079, -2.0525]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08949838578701019
RAW KL tensor(0.1882, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 816: train/loss = 0.5247703790664673, train/raw-loss = 0.47876453399658203, train/logprobs = tensor([[-1.5974, -2.9074],
        [-1.0358, -1.3008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09201173484325409
RAW KL tensor(0.0597, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 817: train/loss = 0.45621275901794434, train/raw-loss = 0.3780317008495331, train/logprobs = tensor([[-1.9783, -5.0533],
        [-1.6468, -1.0152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1563621461391449
RAW KL tensor(0.0372, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 818: train/loss = 0.4511684775352478, train/raw-loss = 0.41066795587539673, train/logprobs = tensor([[-1.1144, -3.1462],
        [-0.8788, -1.2350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08100097626447678
RAW KL tensor(0.0942, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 819: train/loss = 0.5514489412307739, train/raw-loss = 0.4964420199394226, train/logprobs = tensor([[-2.0675, -3.1121],
        [-1.4916, -1.3857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11001379042863846
RAW KL tensor(0.0688, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 820: train/loss = 0.2898538112640381, train/raw-loss = 0.24237249791622162, train/logprobs = tensor([[-1.7737, -5.5629],
        [-1.3626, -2.2391]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09496262669563293
RAW KL tensor(0.1312, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 821: train/loss = 0.38946330547332764, train/raw-loss = 0.32833176851272583, train/logprobs = tensor([[-1.8520, -4.4483],
        [-1.4143, -1.9112]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12226314842700958
RAW KL tensor(0.0269, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 822: train/loss = 0.3336838483810425, train/raw-loss = 0.29880350828170776, train/logprobs = tensor([[-1.2407, -4.5631],
        [-1.0495, -1.3461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06976071000099182
RAW KL tensor(0.0961, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 823: train/loss = 0.5420957803726196, train/raw-loss = 0.47551149129867554, train/logprobs = tensor([[-1.3687, -2.8485],
        [-1.1404, -1.3932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13316866755485535
RAW KL tensor(0.0718, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 824: train/loss = 0.44200921058654785, train/raw-loss = 0.4086097180843353, train/logprobs = tensor([[-1.5891, -3.5809],
        [-1.2629, -1.0689]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06679889559745789
RAW KL tensor(0.1104, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 825: train/loss = 0.6187219619750977, train/raw-loss = 0.5797119140625, train/logprobs = tensor([[-2.0359, -2.2283],
        [-1.2738, -0.8409]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07801993191242218
RAW KL tensor(0.0883, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 826: train/loss = 0.4101913869380951, train/raw-loss = 0.3656538128852844, train/logprobs = tensor([[-1.8367, -3.5674],
        [-1.3277, -1.2828]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08907514810562134
RAW KL tensor(0.0955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 827: train/loss = 0.39932990074157715, train/raw-loss = 0.3413216173648834, train/logprobs = tensor([[-2.1134, -4.3023],
        [-1.5988, -1.4113]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11601658165454865
RAW KL tensor(0.0995, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 828: train/loss = 0.4209449887275696, train/raw-loss = 0.3721159100532532, train/logprobs = tensor([[-1.9172, -3.4983],
        [-1.4101, -1.2462]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09765823185443878
RAW KL tensor(0.0805, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 829: train/loss = 0.5313032269477844, train/raw-loss = 0.5036745667457581, train/logprobs = tensor([[-1.4610, -2.2885],
        [-1.2637, -1.1521]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05525729060173035
RAW KL tensor(0.0649, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 830: train/loss = 0.47036027908325195, train/raw-loss = 0.44369038939476013, train/logprobs = tensor([[-1.2269, -2.9869],
        [-0.9624, -1.2957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05333976820111275
RAW KL tensor(0.0623, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 831: train/loss = 0.45965778827667236, train/raw-loss = 0.43149346113204956, train/logprobs = tensor([[-1.6014, -3.0901],
        [-1.2445, -1.3081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0563286654651165
RAW KL tensor(0.1713, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 832: train/loss = 0.44596630334854126, train/raw-loss = 0.3612028658390045, train/logprobs = tensor([[-2.6298, -4.7507],
        [-1.3768, -1.5706]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1695268750190735
RAW KL tensor(0.1213, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 833: train/loss = 0.49982136487960815, train/raw-loss = 0.46520835161209106, train/logprobs = tensor([[-1.9792, -2.7836],
        [-1.6120, -1.2870]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06922604143619537
RAW KL tensor(0.0900, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 834: train/loss = 0.4437423050403595, train/raw-loss = 0.40824687480926514, train/logprobs = tensor([[-1.5295, -4.2279],
        [-1.1882, -1.9348]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07099084556102753
RAW KL tensor(0.1222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 835: train/loss = 0.44609954953193665, train/raw-loss = 0.3990015387535095, train/logprobs = tensor([[-1.7483, -3.4024],
        [-1.4394, -1.1720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09419605135917664
RAW KL tensor(0.0629, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 836: train/loss = 0.40473926067352295, train/raw-loss = 0.337238609790802, train/logprobs = tensor([[-1.6381, -4.3277],
        [-1.3565, -1.7623]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13500124216079712
RAW KL tensor(0.0868, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 837: train/loss = 0.4170052707195282, train/raw-loss = 0.37232375144958496, train/logprobs = tensor([[-1.2066, -3.8100],
        [-0.9160, -1.2425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08936303853988647
RAW KL tensor(0.0841, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 838: train/loss = 0.3030439019203186, train/raw-loss = 0.2537394165992737, train/logprobs = tensor([[-2.0563, -5.8359],
        [-1.3757, -1.5997]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09860902279615402
RAW KL tensor(0.0893, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 839: train/loss = 0.6990633010864258, train/raw-loss = 0.6617140769958496, train/logprobs = tensor([[-2.2621, -2.4690],
        [-1.8772, -1.6888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07469835132360458
RAW KL tensor(0.1046, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 840: train/loss = 0.3515389561653137, train/raw-loss = 0.2556309103965759, train/logprobs = tensor([[-2.1242, -6.3646],
        [-1.7871, -1.5156]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.19181615114212036
RAW KL tensor(0.1809, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 841: train/loss = 0.31796973943710327, train/raw-loss = 0.2595359981060028, train/logprobs = tensor([[-0.9859, -5.2000],
        [-0.7736, -1.9085]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11686750501394272
RAW KL tensor(0.0723, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 842: train/loss = 0.43987715244293213, train/raw-loss = 0.3959614932537079, train/logprobs = tensor([[-2.1015, -3.9387],
        [-1.6422, -1.6722]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0878312736749649
RAW KL tensor(0.0709, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 384])
Epoch 0, Step 843: train/loss = 0.4426391124725342, train/raw-loss = 0.37531018257141113, train/logprobs = tensor([[-1.2006, -4.4384],
        [-1.0957, -2.0283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.13465794920921326
RAW KL tensor(0.1401, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 844: train/loss = 0.44389310479164124, train/raw-loss = 0.4029286503791809, train/logprobs = tensor([[-1.7895, -3.1706],
        [-1.2756, -1.0801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08192893117666245
RAW KL tensor(0.0328, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 451])
Epoch 0, Step 845: train/loss = 0.6192237138748169, train/raw-loss = 0.54827880859375, train/logprobs = tensor([[-1.8802, -3.1134],
        [-1.5464, -0.9304]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1418897658586502
RAW KL tensor(0.1384, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 846: train/loss = 0.5291643738746643, train/raw-loss = 0.48186588287353516, train/logprobs = tensor([[-1.9370, -3.3899],
        [-1.2753, -1.1197]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0945969894528389
RAW KL tensor(0.2133, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 847: train/loss = 0.32897523045539856, train/raw-loss = 0.28084415197372437, train/logprobs = tensor([[-1.2042, -3.7562],
        [-1.0387, -1.0699]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09626218676567078
RAW KL tensor(0.0986, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 848: train/loss = 0.361163467168808, train/raw-loss = 0.32000061869621277, train/logprobs = tensor([[-1.7568, -4.0152],
        [-1.2687, -1.3768]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08232572674751282
RAW KL tensor(0.0640, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 849: train/loss = 0.6646750569343567, train/raw-loss = 0.6327379941940308, train/logprobs = tensor([[-1.8404, -2.4220],
        [-1.0278, -1.0375]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06387405097484589
RAW KL tensor(0.0843, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 850: train/loss = 0.4336864948272705, train/raw-loss = 0.3547629714012146, train/logprobs = tensor([[-2.1126, -4.3468],
        [-1.7425, -1.4814]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.157847061753273
RAW KL tensor(0.1160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 851: train/loss = 0.4466243386268616, train/raw-loss = 0.41027122735977173, train/logprobs = tensor([[-1.6247, -3.8265],
        [-1.1425, -1.5307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0727061852812767
RAW KL tensor(0.1386, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 852: train/loss = 0.42090755701065063, train/raw-loss = 0.2840898931026459, train/logprobs = tensor([[-1.7966, -4.6173],
        [-1.2966, -1.4109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2736353278160095
RAW KL tensor(0.0268, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 853: train/loss = 0.619692325592041, train/raw-loss = 0.5761759281158447, train/logprobs = tensor([[-1.3981, -1.9634],
        [-0.9721, -0.8537]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08703284710645676
RAW KL tensor(0.0572, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 854: train/loss = 0.518304705619812, train/raw-loss = 0.46869635581970215, train/logprobs = tensor([[-1.5153, -4.0133],
        [-1.0082, -1.9731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09921670705080032
RAW KL tensor(0.0828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 855: train/loss = 0.5376973152160645, train/raw-loss = 0.4791739583015442, train/logprobs = tensor([[-2.1836, -3.1460],
        [-1.4908, -1.3775]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11704665422439575
RAW KL tensor(0.0921, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 856: train/loss = 0.4347352981567383, train/raw-loss = 0.3862656354904175, train/logprobs = tensor([[-1.6143, -3.6533],
        [-1.1896, -1.5686]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09693926572799683
RAW KL tensor(0.1631, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 857: train/loss = 0.32087865471839905, train/raw-loss = 0.2600502371788025, train/logprobs = tensor([[-1.8809, -5.1812],
        [-1.5132, -2.0244]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1216568797826767
RAW KL tensor(0.0639, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 858: train/loss = 0.5932561755180359, train/raw-loss = 0.5480588674545288, train/logprobs = tensor([[-2.2535, -3.6447],
        [-1.1496, -1.2035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09039463102817535
RAW KL tensor(0.0785, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 859: train/loss = 0.4736848771572113, train/raw-loss = 0.42823514342308044, train/logprobs = tensor([[-1.9003, -5.6929],
        [-1.2226, -2.6049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09089946746826172
RAW KL tensor(0.0150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 860: train/loss = 0.5532398223876953, train/raw-loss = 0.5276229977607727, train/logprobs = tensor([[-1.1694, -1.6838],
        [-1.1120, -0.8102]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05123358964920044
RAW KL tensor(0.0563, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 861: train/loss = 0.4234738051891327, train/raw-loss = 0.3861428201198578, train/logprobs = tensor([[-1.3373, -4.1335],
        [-1.0852, -1.5740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07466195523738861
RAW KL tensor(0.0411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 862: train/loss = 0.504409670829773, train/raw-loss = 0.4794759750366211, train/logprobs = tensor([[-1.5890, -2.7392],
        [-1.2370, -1.2049]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04986731708049774
RAW KL tensor(0.0504, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 448])
Epoch 0, Step 863: train/loss = 0.38837751746177673, train/raw-loss = 0.3501828908920288, train/logprobs = tensor([[-1.5984, -4.2654],
        [-0.9547, -1.5184]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07638927549123764
RAW KL tensor(0.1147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 864: train/loss = 0.4412264823913574, train/raw-loss = 0.3968636095523834, train/logprobs = tensor([[-1.6791, -3.1507],
        [-1.4067, -1.3140]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0887257531285286
RAW KL tensor(0.0601, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 865: train/loss = 0.5065988302230835, train/raw-loss = 0.4304918646812439, train/logprobs = tensor([[-1.5716, -4.5836],
        [-1.0654, -1.4016]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15221385657787323
RAW KL tensor(0.0497, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 196])
Epoch 0, Step 866: train/loss = 0.46549999713897705, train/raw-loss = 0.42061007022857666, train/logprobs = tensor([[-2.3017, -3.3307],
        [-1.6901, -1.2976]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08977991342544556
RAW KL tensor(0.1244, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 867: train/loss = 0.39669162034988403, train/raw-loss = 0.319049596786499, train/logprobs = tensor([[-2.7960, -5.1313],
        [-1.8701, -1.5553]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.15528397262096405
