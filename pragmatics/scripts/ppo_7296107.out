[2024-02-19 09:07:08,373][root][INFO] - beta: 0.01
[2024-02-19 09:07:08,373][root][INFO] - temperature: 1
[2024-02-19 09:07:08,373][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 10000 training examples...
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Epoch 0, Step 15: loss/train = 0.04351605474948883, logprobs/train = tensor([[-0.8196, -0.9555],
        [-0.8241, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.04332902282476425, logprobs/train = tensor([[-1.2200, -1.2428],
        [-1.2477, -1.1750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.046370428055524826, logprobs/train = tensor([[-1.3692, -1.1574],
        [-1.3665, -1.1021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.213958163745701e-06
Epoch 0, Step 63: loss/train = 0.04506981372833252, logprobs/train = tensor([[-1.1974, -1.2022],
        [-1.1672, -1.1073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9732027542195283e-06
Epoch 0, Step 79: loss/train = 0.04342862218618393, logprobs/train = tensor([[-0.8091, -0.8658],
        [-0.8188, -0.7790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.431782821891829e-07
Epoch 0, Step 95: loss/train = 0.04331377148628235, logprobs/train = tensor([[-0.8973, -1.0712],
        [-0.9348, -0.9600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.62507773085963e-07
Epoch 0, Step 111: loss/train = 0.04382738471031189, logprobs/train = tensor([[-0.8006, -0.7956],
        [-0.8196, -0.7799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.3851287045981735e-07
Epoch 0, Step 127: loss/train = 0.04379041865468025, logprobs/train = tensor([[-1.0690, -0.9253],
        [-1.1216, -0.8727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7339407349936664e-06
Epoch 0, Step 143: loss/train = 0.043622054159641266, logprobs/train = tensor([[-0.8526, -0.9075],
        [-0.8004, -0.8727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.356107870582491e-06
Epoch 0, Step 159: loss/train = 0.04395968094468117, logprobs/train = tensor([[-1.0039, -1.0479],
        [-1.0371, -0.9219]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.503799573285505e-06
Epoch 0, Step 175: loss/train = 0.050062380731105804, logprobs/train = tensor([[-0.9703, -1.5940],
        [-0.9376, -1.5111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.689966878388077e-06
Epoch 0, Step 191: loss/train = 0.046107035130262375, logprobs/train = tensor([[-1.0624, -1.2333],
        [-1.0951, -1.2225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.292219717986882e-06
Epoch 0, Step 207: loss/train = 0.04429211467504501, logprobs/train = tensor([[-0.9504, -1.0472],
        [-0.9334, -0.9984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.6765883376356214e-07
Epoch 0, Step 223: loss/train = 0.04493569955229759, logprobs/train = tensor([[-0.7742, -1.1023],
        [-0.8063, -1.0429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.29634552448988e-07
Epoch 0, Step 239: loss/train = 0.04397689923644066, logprobs/train = tensor([[-0.7242, -0.9555],
        [-0.7412, -0.9308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.5374062058981508e-06
Epoch 0, Step 255: loss/train = 0.04419127106666565, logprobs/train = tensor([[-0.9922, -0.7770],
        [-0.9913, -0.7529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3639019016409293e-06
Epoch 0, Step 271: loss/train = 0.04660039767622948, logprobs/train = tensor([[-0.8872, -1.3766],
        [-0.9020, -1.2732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.32057446334511e-06
Epoch 0, Step 287: loss/train = 0.043914664536714554, logprobs/train = tensor([[-0.7373, -0.7948],
        [-0.7510, -0.7286]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.913039108738303e-06
Epoch 0, Step 303: loss/train = 0.04841790348291397, logprobs/train = tensor([[-1.2598, -1.6923],
        [-1.2312, -1.4699]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.106783646624535e-05
Epoch 0, Step 319: loss/train = 0.0477026104927063, logprobs/train = tensor([[-0.8828, -1.5934],
        [-0.9003, -1.4396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0313757229596376e-05
Epoch 0, Step 335: loss/train = 0.043833281844854355, logprobs/train = tensor([[-0.7569, -0.8107],
        [-0.7557, -0.7790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.1888245100853965e-06
Epoch 0, Step 351: loss/train = 0.04432158172130585, logprobs/train = tensor([[-0.8732, -0.8952],
        [-0.8981, -0.8616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.280072506750003e-06
Epoch 0, Step 367: loss/train = 0.04860381782054901, logprobs/train = tensor([[-0.8201, -1.4909],
        [-0.8703, -1.3640]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.073965389281511e-05
Epoch 0, Step 383: loss/train = 0.04782244563102722, logprobs/train = tensor([[-1.0667, -1.5267],
        [-1.1366, -1.4098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.759008879773319e-05
Epoch 0, Step 399: loss/train = 0.04394064471125603, logprobs/train = tensor([[-1.2477, -1.4569],
        [-1.2875, -1.3895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.7256915422622114e-05
Epoch 0, Step 415: loss/train = 0.04744197800755501, logprobs/train = tensor([[-0.8801, -1.5151],
        [-0.9147, -1.5064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002542404690757394
Epoch 0, Step 431: loss/train = 0.043197087943553925, logprobs/train = tensor([[-0.8582, -0.9558],
        [-1.1127, -0.9105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00024108632351271808
Epoch 0, Step 447: loss/train = 0.044269755482673645, logprobs/train = tensor([[-0.9183, -1.4790],
        [-1.3293, -1.4254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005169439828023314
Epoch 0, Step 463: loss/train = 0.04457007721066475, logprobs/train = tensor([[-0.8142, -1.2128],
        [-0.8156, -1.1284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011767108226194978
Epoch 0, Step 479: loss/train = 0.04564119130373001, logprobs/train = tensor([[-1.1430, -0.8393],
        [-1.1675, -0.8038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002929977490566671
Epoch 0, Step 495: loss/train = 0.0460568368434906, logprobs/train = tensor([[-1.0004, -0.6349],
        [-1.0638, -0.6520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.096780559048057e-05
Epoch 0, Step 511: loss/train = 0.049113910645246506, logprobs/train = tensor([[-0.6530, -1.2181],
        [-0.6961, -1.1290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00038531579775735736
Epoch 0, Step 527: loss/train = 0.04643087461590767, logprobs/train = tensor([[-1.1864, -0.9269],
        [-1.2128, -0.9536]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.725054794922471e-05
Epoch 0, Step 543: loss/train = 0.044356100261211395, logprobs/train = tensor([[-1.0917, -0.8021],
        [-1.1737, -0.8021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.427447417285293e-05
Epoch 0, Step 559: loss/train = 0.04438400641083717, logprobs/train = tensor([[-0.8325, -0.8457],
        [-0.9474, -0.8405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0001269119675271213
Epoch 0, Step 575: loss/train = 0.043467823415994644, logprobs/train = tensor([[-0.7248, -0.7607],
        [-0.7425, -0.7487]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.0432863645255566e-05
Epoch 0, Step 591: loss/train = 0.04387292638421059, logprobs/train = tensor([[-0.8247, -0.9647],
        [-0.9860, -1.0780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007471595890820026
Epoch 0, Step 607: loss/train = 0.04391903802752495, logprobs/train = tensor([[-0.9390, -0.8021],
        [-0.9903, -0.7973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7223599570570514e-05
Epoch 0, Step 623: loss/train = 0.043245747685432434, logprobs/train = tensor([[-0.7396, -0.7722],
        [-0.8595, -0.7771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00023253611288964748
Epoch 0, Step 639: loss/train = 0.04624714329838753, logprobs/train = tensor([[-1.3112, -1.0091],
        [-1.4313, -1.0104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002860166714526713
Epoch 0, Step 655: loss/train = 0.04337500408291817, logprobs/train = tensor([[-0.7705, -0.7309],
        [-0.8237, -0.7719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00013307509652804583
Epoch 0, Step 671: loss/train = 0.04408919811248779, logprobs/train = tensor([[-0.7173, -0.8219],
        [-0.8063, -0.9141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004744370002299547
Epoch 0, Step 687: loss/train = 0.04349697381258011, logprobs/train = tensor([[-0.9867, -1.0688],
        [-1.0798, -1.0335]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007562743849121034
Epoch 0, Step 703: loss/train = 0.04402050003409386, logprobs/train = tensor([[-0.7929, -0.6503],
        [-0.9090, -0.6401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003906132187694311
Epoch 0, Step 719: loss/train = 0.04683390632271767, logprobs/train = tensor([[-0.6722, -0.9998],
        [-0.7043, -1.0098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018993265985045582
Epoch 0, Step 735: loss/train = 0.04344428330659866, logprobs/train = tensor([[-0.9179, -0.8665],
        [-0.9750, -0.8752]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00015146506484597921
Epoch 0, Step 751: loss/train = 0.04367232322692871, logprobs/train = tensor([[-0.6388, -0.6137],
        [-0.7496, -0.6339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0010214272188022733
Epoch 0, Step 767: loss/train = 0.04592973366379738, logprobs/train = tensor([[-0.8174, -0.5715],
        [-1.0345, -0.6230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014878642978146672
Epoch 0, Step 783: loss/train = 0.04390903189778328, logprobs/train = tensor([[-1.0200, -0.8345],
        [-1.1404, -0.8848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011043376289308071
Epoch 0, Step 799: loss/train = 0.043938763439655304, logprobs/train = tensor([[-0.8576, -0.8869],
        [-0.8573, -0.8672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.412425965303555e-05
Epoch 0, Step 815: loss/train = 0.043492402881383896, logprobs/train = tensor([[-0.6963, -0.8768],
        [-0.7391, -0.8523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015581815969198942
Epoch 0, Step 831: loss/train = 0.044038720428943634, logprobs/train = tensor([[-0.7518, -0.6589],
        [-0.8336, -0.7183]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009786905720829964
Epoch 0, Step 847: loss/train = 0.04394690319895744, logprobs/train = tensor([[-0.7520, -0.6208],
        [-0.8129, -0.6564]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.765938840340823e-05
Epoch 0, Step 863: loss/train = 0.04385599493980408, logprobs/train = tensor([[-0.9177, -0.8526],
        [-1.0147, -0.8519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.300319076515734e-05
Epoch 0, Step 879: loss/train = 0.044145166873931885, logprobs/train = tensor([[-0.5502, -0.8169],
        [-0.6132, -0.8182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00022103480296209455
Epoch 0, Step 895: loss/train = 0.04340603947639465, logprobs/train = tensor([[-0.6684, -0.7325],
        [-0.7364, -0.7660]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014937245287001133
Epoch 0, Step 911: loss/train = 0.04339715465903282, logprobs/train = tensor([[-0.6684, -0.6380],
        [-0.7101, -0.6269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.890596316428855e-05
Epoch 0, Step 927: loss/train = 0.04374519735574722, logprobs/train = tensor([[-0.7759, -0.7105],
        [-0.8096, -0.7136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007732221274636686
Epoch 0, Step 943: loss/train = 0.044181421399116516, logprobs/train = tensor([[-0.6880, -0.6520],
        [-0.8157, -0.6600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011025492567569017
Epoch 0, Step 959: loss/train = 0.04373984411358833, logprobs/train = tensor([[-0.6496, -0.5531],
        [-0.7622, -0.5164]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005111973732709885
Epoch 0, Step 975: loss/train = 0.043648071587085724, logprobs/train = tensor([[-0.7924, -0.8769],
        [-0.9142, -0.9157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004002406494691968
Epoch 0, Step 991: loss/train = 0.04340934753417969, logprobs/train = tensor([[-0.4787, -0.5500],
        [-0.5446, -0.6021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.309010596945882e-05
Epoch 0, Step 1007: loss/train = 0.045385897159576416, logprobs/train = tensor([[-0.6456, -1.0310],
        [-0.6985, -0.8105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0034261862747371197
Epoch 0, Step 1023: loss/train = 0.04418521001935005, logprobs/train = tensor([[-0.4881, -1.0072],
        [-0.5680, -0.7566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004478796385228634
Epoch 0, Step 1039: loss/train = 0.043371353298425674, logprobs/train = tensor([[-0.5914, -0.5775],
        [-0.6762, -0.6105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007537479395978153
Epoch 0, Step 1055: loss/train = 0.043410468846559525, logprobs/train = tensor([[-0.4929, -0.7424],
        [-0.5816, -0.6614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0025484745856374502
Epoch 0, Step 1071: loss/train = 0.04334547743201256, logprobs/train = tensor([[-0.5530, -0.6722],
        [-0.6466, -0.6584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0008082896238192916
Epoch 0, Step 1087: loss/train = 0.04329961538314819, logprobs/train = tensor([[-0.5773, -0.6482],
        [-0.6534, -0.5350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009840696584433317
Epoch 0, Step 1103: loss/train = 0.04335004463791847, logprobs/train = tensor([[-0.4773, -0.5542],
        [-0.5005, -0.5187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004071089788340032
Epoch 0, Step 1119: loss/train = 0.04362577572464943, logprobs/train = tensor([[-0.5730, -0.6421],
        [-0.6696, -0.6242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009205423993989825
Epoch 0, Step 1135: loss/train = 0.04333759844303131, logprobs/train = tensor([[-0.5253, -0.5735],
        [-0.5544, -0.5396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018022849690169096
Epoch 0, Step 1151: loss/train = 0.043949104845523834, logprobs/train = tensor([[-0.4627, -0.8020],
        [-0.6753, -0.8307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0019854879938066006
Epoch 0, Step 1167: loss/train = 0.04347147420048714, logprobs/train = tensor([[-0.5563, -0.7487],
        [-0.5950, -0.6550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004540008958429098
Epoch 0, Step 1183: loss/train = 0.0559147372841835, logprobs/train = tensor([[-0.6675, -1.5179],
        [-0.7733, -1.2874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005206423811614513
Epoch 0, Step 1199: loss/train = 0.04353444278240204, logprobs/train = tensor([[-0.6543, -0.6665],
        [-0.8847, -0.6109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001798069104552269
Epoch 0, Step 1215: loss/train = 0.042907994240522385, logprobs/train = tensor([[-0.5536, -0.8039],
        [-0.6105, -0.5266]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015060587786138058
Epoch 0, Step 1231: loss/train = 0.043438512831926346, logprobs/train = tensor([[-0.5253, -0.5137],
        [-0.6063, -0.4066]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00424539390951395
Epoch 0, Step 1247: loss/train = 0.04356589540839195, logprobs/train = tensor([[-0.5209, -0.5018],
        [-0.6891, -0.3546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006878140848129988
Epoch 0, Step 1263: loss/train = 0.04355041682720184, logprobs/train = tensor([[-0.6732, -0.6528],
        [-0.8183, -0.6288]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0022725805174559355
Epoch 0, Step 1279: loss/train = 0.04445584863424301, logprobs/train = tensor([[-0.6964, -1.0263],
        [-0.7773, -0.5812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016584178432822227
Epoch 0, Step 1295: loss/train = 0.04325968772172928, logprobs/train = tensor([[-0.6020, -0.6531],
        [-0.8723, -0.6929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0033525151666253805
Epoch 0, Step 1311: loss/train = 0.043162934482097626, logprobs/train = tensor([[-0.4988, -0.7973],
        [-0.5718, -0.5608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007154223509132862
Epoch 0, Step 1327: loss/train = 0.042736031115055084, logprobs/train = tensor([[-0.7327, -0.9467],
        [-0.9782, -0.6000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023320749402046204
Epoch 0, Step 1343: loss/train = 0.043124135583639145, logprobs/train = tensor([[-0.5860, -0.6421],
        [-0.8879, -0.7105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.011629627086222172
Epoch 0, Step 1359: loss/train = 0.04151241108775139, logprobs/train = tensor([[-0.5373, -1.1981],
        [-0.8067, -0.5128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04409007355570793
Epoch 0, Step 1375: loss/train = 0.04238357022404671, logprobs/train = tensor([[-0.7780, -1.2066],
        [-0.8466, -0.5904]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028779173269867897
Epoch 0, Step 1391: loss/train = 0.04232830926775932, logprobs/train = tensor([[-0.6202, -0.9886],
        [-0.7861, -0.6014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04012206569314003
Epoch 0, Step 1407: loss/train = 0.03919078782200813, logprobs/train = tensor([[-0.6993, -1.6952],
        [-1.0182, -0.6078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.142377108335495
Epoch 0, Step 1423: loss/train = 0.0434531606733799, logprobs/train = tensor([[-0.9017, -0.9168],
        [-0.8526, -0.8121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006178163457661867
Epoch 0, Step 1439: loss/train = 0.040402933955192566, logprobs/train = tensor([[-0.8394, -1.4271],
        [-1.2543, -1.0692]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10642562061548233
Epoch 0, Step 1455: loss/train = 0.03469999134540558, logprobs/train = tensor([[-0.9452, -1.9725],
        [-2.1339, -0.8152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2682877480983734
Epoch 0, Step 1471: loss/train = 0.03805042430758476, logprobs/train = tensor([[-1.0062, -1.8301],
        [-1.5603, -0.8664]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16512593626976013
Epoch 0, Step 1487: loss/train = 0.036261312663555145, logprobs/train = tensor([[-1.1157, -2.2070],
        [-1.8444, -1.2426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2615106701850891
Epoch 0, Step 1503: loss/train = 0.03949498385190964, logprobs/train = tensor([[-1.2924, -2.6420],
        [-1.4369, -0.8678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24664458632469177
Epoch 0, Step 1519: loss/train = 0.04119711369276047, logprobs/train = tensor([[-2.1514, -5.0735],
        [-1.1813, -1.2353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3504452109336853
Epoch 0, Step 1535: loss/train = 0.04399678111076355, logprobs/train = tensor([[-1.1232, -1.0655],
        [-4.7379, -5.6868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11203674972057343
Epoch 0, Step 1551: loss/train = 0.02221357449889183, logprobs/train = tensor([[-1.5136, -4.8489],
        [-4.5074, -2.2744]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7053160667419434
Epoch 0, Step 1567: loss/train = 0.022932039573788643, logprobs/train = tensor([[-2.2123, -3.7737],
        [-4.8262, -1.8662]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6924181580543518
Epoch 0, Step 1583: loss/train = 0.08056686818599701, logprobs/train = tensor([[-2.6646, -7.6698],
        [-5.4942, -5.0364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6058638691902161
Epoch 0, Step 1599: loss/train = 0.02576947584748268, logprobs/train = tensor([[-1.9104, -6.3742],
        [-3.3608, -1.8443]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7950295209884644
Epoch 0, Step 1615: loss/train = 0.035007722675800323, logprobs/train = tensor([[-4.5071, -6.4968],
        [-3.9574, -3.1324]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3553313612937927
Epoch 0, Step 1631: loss/train = 0.0564592145383358, logprobs/train = tensor([[-3.3077, -4.8507],
        [-3.8394, -4.3669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09507817029953003
Epoch 0, Step 1647: loss/train = 0.03450257331132889, logprobs/train = tensor([[-5.0480, -5.9358],
        [-4.8329, -2.6498]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5587279200553894
Epoch 0, Step 1663: loss/train = 0.019783763214945793, logprobs/train = tensor([[-3.4405, -7.3940],
        [-8.0908, -3.7181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9873623847961426
Epoch 0, Step 1679: loss/train = 0.01788349077105522, logprobs/train = tensor([[ -6.7632, -14.0096],
        [ -5.3685,  -2.0034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9170956611633301
Epoch 0, Step 1695: loss/train = 0.009136220440268517, logprobs/train = tensor([[ -3.6389, -12.5152],
        [ -6.7216,  -2.3157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.110370397567749
Epoch 0, Step 1711: loss/train = 0.013640428893268108, logprobs/train = tensor([[-3.0903, -8.4490],
        [-6.1815, -3.2542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8784236907958984
Epoch 0, Step 1727: loss/train = 0.04479299485683441, logprobs/train = tensor([[ -3.2642, -11.8340],
        [ -6.0460,  -5.5062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6524990797042847
Epoch 0, Step 1743: loss/train = 0.03705977648496628, logprobs/train = tensor([[-6.1503, -8.1957],
        [-4.0438, -3.2153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.38542646169662476
Epoch 0, Step 1759: loss/train = 0.0128700602799654, logprobs/train = tensor([[ -8.1665, -13.6463],
        [ -7.6484,  -3.5414]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9646425247192383
Epoch 0, Step 1775: loss/train = 0.03391483426094055, logprobs/train = tensor([[-11.9906, -16.9092],
        [ -7.6939,  -4.4128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6110473871231079
Epoch 0, Step 1791: loss/train = 0.0012148653622716665, logprobs/train = tensor([[ -5.7870, -20.8697],
        [-11.9053,  -3.6739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.238376259803772
Epoch 0, Step 1807: loss/train = 0.022186536341905594, logprobs/train = tensor([[ -8.2864, -11.5424],
        [-11.2938,  -7.5041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6959711313247681
Epoch 0, Step 1823: loss/train = 0.008897489868104458, logprobs/train = tensor([[ -3.9270, -14.0333],
        [-13.4202,  -8.0233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0848606824874878
Epoch 0, Step 1839: loss/train = 0.019330032169818878, logprobs/train = tensor([[ -4.8887,  -9.0747],
        [-12.9285,  -9.4418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8603125810623169
Epoch 0, Step 1855: loss/train = 0.044332414865493774, logprobs/train = tensor([[ -9.5655,  -9.3910],
        [-11.1522,  -7.2741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7633429765701294
Epoch 0, Step 1871: loss/train = 0.037261951714754105, logprobs/train = tensor([[ -5.0854,  -9.8120],
        [-13.8188, -12.0448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6029924750328064
Epoch 0, Step 1887: loss/train = 0.002080146688967943, logprobs/train = tensor([[ -7.1875, -18.4859],
        [-11.4805,  -4.9629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2784416675567627
Epoch 0, Step 1903: loss/train = 0.010332334786653519, logprobs/train = tensor([[ -6.1323,  -9.2932],
        [-10.2634, -12.3851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.145312786102295
Epoch 0, Step 1919: loss/train = 0.019358467310667038, logprobs/train = tensor([[-11.6122, -17.8860],
        [ -8.3055,  -5.0119]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8551235198974609
Epoch 0, Step 1935: loss/train = 0.033005088567733765, logprobs/train = tensor([[ -6.8365, -12.9874],
        [ -9.6550,  -6.3264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8618890047073364
Epoch 0, Step 1951: loss/train = 0.01456376537680626, logprobs/train = tensor([[-11.5408, -18.4336],
        [ -7.7405,  -3.4761]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8860299587249756
Epoch 0, Step 1967: loss/train = 0.02955631911754608, logprobs/train = tensor([[-12.2566, -16.7020],
        [-10.5648,  -5.7611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5948206186294556
Epoch 0, Step 1983: loss/train = 0.007218886166810989, logprobs/train = tensor([[ -8.4473, -20.7905],
        [-14.4266,  -7.1803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1923463344573975
Epoch 0, Step 1999: loss/train = 0.012004535645246506, logprobs/train = tensor([[-10.8936, -18.1710],
        [-14.4668,  -9.8500]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9355384111404419
Epoch 0, Step 2015: loss/train = 0.03564350679516792, logprobs/train = tensor([[-15.4809, -15.3496],
        [ -9.9453,  -7.9377]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6286916136741638
Epoch 0, Step 2031: loss/train = 0.04892786964774132, logprobs/train = tensor([[-10.9736, -17.5533],
        [-16.1935, -12.1005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7318522334098816
Epoch 0, Step 2047: loss/train = 0.007880925200879574, logprobs/train = tensor([[ -9.6096, -13.3099],
        [-19.1175, -14.2359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.153550624847412
Epoch 0, Step 2063: loss/train = 0.01922621577978134, logprobs/train = tensor([[-16.6199, -22.8743],
        [-10.8743,  -5.5267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7241653203964233
Epoch 0, Step 2079: loss/train = 0.02517794817686081, logprobs/train = tensor([[-11.7146, -13.4030],
        [-12.2099, -10.1539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.525536298751831
Epoch 0, Step 2095: loss/train = 0.011832434684038162, logprobs/train = tensor([[-13.6410, -25.0767],
        [-12.1964,  -3.9797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9940956830978394
Epoch 0, Step 2111: loss/train = 0.009402568452060223, logprobs/train = tensor([[-12.3753, -21.8954],
        [ -8.8967,  -3.3659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0548179149627686
Epoch 0, Step 2127: loss/train = 0.03839918226003647, logprobs/train = tensor([[-11.7172, -12.7712],
        [-13.7926, -14.2147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.33645719289779663
Epoch 0, Step 2143: loss/train = 0.025192543864250183, logprobs/train = tensor([[-14.9269, -19.0384],
        [-15.9314, -11.3060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8219487071037292
Epoch 0, Step 2159: loss/train = 0.019683677703142166, logprobs/train = tensor([[-12.8839, -19.2918],
        [-25.7005, -18.2861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7459852695465088
Epoch 0, Step 2175: loss/train = 0.03419485688209534, logprobs/train = tensor([[-14.0202, -17.2752],
        [-28.1342, -26.5157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5059844255447388
Epoch 0, Step 2191: loss/train = 0.030454915016889572, logprobs/train = tensor([[-17.8469, -21.5425],
        [-27.2497, -25.6843]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.34687021374702454
Epoch 0, Step 2207: loss/train = 0.07067329436540604, logprobs/train = tensor([[-23.9970, -24.7965],
        [-20.1432, -17.2379]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3400135040283203
Epoch 0, Step 2223: loss/train = 0.011726190336048603, logprobs/train = tensor([[-19.4558, -26.9295],
        [-26.6940, -16.5712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9516243934631348
Epoch 0, Step 2239: loss/train = 0.00108197215013206, logprobs/train = tensor([[-15.0308, -26.6591],
        [-27.3890,  -7.9914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0981791019439697
Epoch 0, Step 2255: loss/train = 0.01204830314964056, logprobs/train = tensor([[-14.6885, -21.6717],
        [-24.8513, -14.4173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9758792519569397
Epoch 0, Step 2271: loss/train = 0.011835048906505108, logprobs/train = tensor([[-10.5323, -18.1898],
        [-31.6117, -18.4199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0064971446990967
Epoch 0, Step 2287: loss/train = 0.007112852763384581, logprobs/train = tensor([[-11.7319, -18.0018],
        [-24.5618,  -8.9267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.263463020324707
Epoch 0, Step 2303: loss/train = 0.010186741128563881, logprobs/train = tensor([[-11.2375, -16.4382],
        [-30.3466, -14.0003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0661964416503906
Epoch 0, Step 2319: loss/train = 0.025075826793909073, logprobs/train = tensor([[-14.4877, -16.1985],
        [-28.9484, -12.9151]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2727396488189697
Epoch 0, Step 2335: loss/train = 0.0011129657505080104, logprobs/train = tensor([[-13.3342, -22.2684],
        [-33.9542, -13.0227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3438862562179565
Epoch 0, Step 2351: loss/train = 0.01243430096656084, logprobs/train = tensor([[-13.8238, -17.8836],
        [-31.9708, -22.4208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.000694751739502
Epoch 0, Step 2367: loss/train = 0.009880821220576763, logprobs/train = tensor([[-22.7770, -31.8519],
        [-33.2205, -15.7195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2721006870269775
Epoch 0, Step 2374: loss/train = 0.06779179722070694, logprobs/train = tensor([[-37.2196, -37.8185],
        [-27.0337, -20.2731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7964086532592773
loss/eval: 0.3403623402118683
[2024-02-19 10:17:23,591][root][INFO] - beta: 0.1
[2024-02-19 10:17:23,592][root][INFO] - temperature: 1
[2024-02-19 10:17:23,592][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 10000 training examples...
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
Epoch 0, Step 15: loss/train = 0.04351605474948883, logprobs/train = tensor([[-0.8196, -0.9555],
        [-0.8241, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.04332902282476425, logprobs/train = tensor([[-1.2200, -1.2428],
        [-1.2477, -1.1750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.04636668041348457, logprobs/train = tensor([[-1.3679, -1.1554],
        [-1.3646, -1.1008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.236746124457568e-06
Epoch 0, Step 63: loss/train = 0.04504905268549919, logprobs/train = tensor([[-1.1951, -1.1993],
        [-1.1687, -1.1047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9223338060546666e-06
Epoch 0, Step 79: loss/train = 0.04342884197831154, logprobs/train = tensor([[-0.8095, -0.8663],
        [-0.8176, -0.7785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.556999091524631e-07
Epoch 0, Step 95: loss/train = 0.04330907016992569, logprobs/train = tensor([[-0.8993, -1.0760],
        [-0.9345, -0.9582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3206627045292407e-06
Epoch 0, Step 111: loss/train = 0.04381747171282768, logprobs/train = tensor([[-0.7995, -0.7953],
        [-0.8197, -0.7801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3239828149380628e-06
Epoch 0, Step 127: loss/train = 0.04379782825708389, logprobs/train = tensor([[-1.0678, -0.9248],
        [-1.1233, -0.8715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1430474842200056e-06
Epoch 0, Step 143: loss/train = 0.04362579435110092, logprobs/train = tensor([[-0.8528, -0.9069],
        [-0.7997, -0.8715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.485732567147352e-08
Epoch 0, Step 159: loss/train = 0.04394681379199028, logprobs/train = tensor([[-1.0082, -1.0505],
        [-1.0329, -0.9194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.128013875335455e-06
Epoch 0, Step 175: loss/train = 0.050094615668058395, logprobs/train = tensor([[-0.9673, -1.5874],
        [-0.9331, -1.5099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.354953024536371e-06
Epoch 0, Step 191: loss/train = 0.04608536511659622, logprobs/train = tensor([[-1.0607, -1.2305],
        [-1.0933, -1.2237]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.435417627566494e-06
Epoch 0, Step 207: loss/train = 0.04428188502788544, logprobs/train = tensor([[-0.9511, -1.0441],
        [-0.9360, -0.9995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.560003384947777e-07
Epoch 0, Step 223: loss/train = 0.04496470093727112, logprobs/train = tensor([[-0.7752, -1.1053],
        [-0.8065, -1.0448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.522556653479114e-06
Epoch 0, Step 239: loss/train = 0.04396941885352135, logprobs/train = tensor([[-0.7240, -0.9552],
        [-0.7416, -0.9299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.8217598355695372e-06
Epoch 0, Step 255: loss/train = 0.044189855456352234, logprobs/train = tensor([[-0.9914, -0.7768],
        [-0.9895, -0.7509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0075746104121208e-07
Epoch 0, Step 271: loss/train = 0.04662768915295601, logprobs/train = tensor([[-0.8880, -1.3784],
        [-0.9022, -1.2765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.119560566730797e-06
Epoch 0, Step 287: loss/train = 0.04391949623823166, logprobs/train = tensor([[-0.7379, -0.7937],
        [-0.7509, -0.7284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.135952960699797e-07
Epoch 0, Step 303: loss/train = 0.04843025654554367, logprobs/train = tensor([[-1.2621, -1.6911],
        [-1.2277, -1.4719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.5089080231264234e-05
Epoch 0, Step 319: loss/train = 0.04771144315600395, logprobs/train = tensor([[-0.8812, -1.5909],
        [-0.9004, -1.4382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.6441263142041862e-05
Epoch 0, Step 335: loss/train = 0.043836429715156555, logprobs/train = tensor([[-0.7557, -0.8111],
        [-0.7571, -0.7809]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.580212357221171e-07
Epoch 0, Step 351: loss/train = 0.04431260749697685, logprobs/train = tensor([[-0.8770, -0.8966],
        [-0.8961, -0.8618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.668694600695744e-06
Epoch 0, Step 367: loss/train = 0.04862580448389053, logprobs/train = tensor([[-0.8198, -1.4883],
        [-0.8676, -1.3649]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00011171546066179872
Epoch 0, Step 383: loss/train = 0.04778234288096428, logprobs/train = tensor([[-1.0660, -1.5237],
        [-1.1410, -1.4110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.016970681026578e-05
Epoch 0, Step 399: loss/train = 0.0439346581697464, logprobs/train = tensor([[-1.2457, -1.4549],
        [-1.2911, -1.3884]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.981331484392285e-05
Epoch 0, Step 415: loss/train = 0.04746757447719574, logprobs/train = tensor([[-0.8806, -1.5208],
        [-0.9159, -1.5095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002597944112494588
Epoch 0, Step 431: loss/train = 0.043205391615629196, logprobs/train = tensor([[-0.8618, -0.9569],
        [-1.1164, -0.9128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00024195674632210284
Epoch 0, Step 447: loss/train = 0.04422440379858017, logprobs/train = tensor([[-0.9163, -1.4765],
        [-1.3306, -1.4222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005015234346501529
Epoch 0, Step 463: loss/train = 0.04458817094564438, logprobs/train = tensor([[-0.8139, -1.2177],
        [-0.8157, -1.1316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0012423035223037004
Epoch 0, Step 479: loss/train = 0.045653361827135086, logprobs/train = tensor([[-1.1476, -0.8398],
        [-1.1674, -0.8081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00021133312839083374
Epoch 0, Step 495: loss/train = 0.046047452837228775, logprobs/train = tensor([[-1.0012, -0.6350],
        [-1.0634, -0.6517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.756035145372152e-05
Epoch 0, Step 511: loss/train = 0.04917535558342934, logprobs/train = tensor([[-0.6533, -1.2210],
        [-0.6963, -1.1345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003502399194985628
Epoch 0, Step 527: loss/train = 0.046433549374341965, logprobs/train = tensor([[-1.1873, -0.9275],
        [-1.2082, -0.9540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.843113467562944e-05
Epoch 0, Step 543: loss/train = 0.044348571449518204, logprobs/train = tensor([[-1.0908, -0.8035],
        [-1.1737, -0.8024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.620406151749194e-05
Epoch 0, Step 559: loss/train = 0.044390350580215454, logprobs/train = tensor([[-0.8324, -0.8428],
        [-0.9468, -0.8406]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00014036393258720636
Epoch 0, Step 575: loss/train = 0.04346732422709465, logprobs/train = tensor([[-0.7258, -0.7606],
        [-0.7409, -0.7514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.429964221548289e-05
Epoch 0, Step 591: loss/train = 0.04387086257338524, logprobs/train = tensor([[-0.8238, -0.9619],
        [-0.9860, -1.0780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007733374368399382
Epoch 0, Step 607: loss/train = 0.04391108825802803, logprobs/train = tensor([[-0.9370, -0.8002],
        [-0.9867, -0.7977]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1922107660211623e-05
Epoch 0, Step 623: loss/train = 0.04324132949113846, logprobs/train = tensor([[-0.7386, -0.7721],
        [-0.8599, -0.7736]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018182030180469155
Epoch 0, Step 639: loss/train = 0.04627781733870506, logprobs/train = tensor([[-1.3093, -1.0072],
        [-1.4338, -1.0060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002066769520752132
Epoch 0, Step 655: loss/train = 0.04337708652019501, logprobs/train = tensor([[-0.7710, -0.7313],
        [-0.8223, -0.7704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00011461914982646704
Epoch 0, Step 671: loss/train = 0.04411274194717407, logprobs/train = tensor([[-0.7157, -0.8216],
        [-0.8082, -0.9147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004588114097714424
Epoch 0, Step 687: loss/train = 0.04349204897880554, logprobs/train = tensor([[-0.9880, -1.0723],
        [-1.0759, -1.0283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007966295816004276
Epoch 0, Step 703: loss/train = 0.044004663825035095, logprobs/train = tensor([[-0.7898, -0.6499],
        [-0.9058, -0.6401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00036483712028712034
Epoch 0, Step 719: loss/train = 0.0468500554561615, logprobs/train = tensor([[-0.6700, -0.9985],
        [-0.7040, -1.0068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018586315854918212
Epoch 0, Step 735: loss/train = 0.04344214126467705, logprobs/train = tensor([[-0.9185, -0.8647],
        [-0.9744, -0.8720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00014830310828983784
Epoch 0, Step 751: loss/train = 0.04368920996785164, logprobs/train = tensor([[-0.6398, -0.6133],
        [-0.7499, -0.6310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009641089127399027
Epoch 0, Step 767: loss/train = 0.045937083661556244, logprobs/train = tensor([[-0.8161, -0.5703],
        [-1.0335, -0.6252]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0017345541855320334
Epoch 0, Step 783: loss/train = 0.0439244881272316, logprobs/train = tensor([[-1.0207, -0.8294],
        [-1.1380, -0.8812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011816311161965132
Epoch 0, Step 799: loss/train = 0.04392743110656738, logprobs/train = tensor([[-0.8582, -0.8853],
        [-0.8592, -0.8685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.2108390112407506e-05
Epoch 0, Step 815: loss/train = 0.04349060356616974, logprobs/train = tensor([[-0.6957, -0.8753],
        [-0.7405, -0.8476]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015991399995982647
Epoch 0, Step 831: loss/train = 0.04404299333691597, logprobs/train = tensor([[-0.7506, -0.6594],
        [-0.8315, -0.7167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009643837111070752
Epoch 0, Step 847: loss/train = 0.04394734650850296, logprobs/train = tensor([[-0.7467, -0.6191],
        [-0.8134, -0.6547]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.651826409623027e-05
Epoch 0, Step 863: loss/train = 0.04385833814740181, logprobs/train = tensor([[-0.9179, -0.8539],
        [-1.0156, -0.8494]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9641931178048253e-05
Epoch 0, Step 879: loss/train = 0.044143348932266235, logprobs/train = tensor([[-0.5512, -0.8172],
        [-0.6126, -0.8142]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00021383684361353517
Epoch 0, Step 895: loss/train = 0.0434206984937191, logprobs/train = tensor([[-0.6631, -0.7293],
        [-0.7335, -0.7631]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014214543625712395
Epoch 0, Step 911: loss/train = 0.04339270293712616, logprobs/train = tensor([[-0.6689, -0.6406],
        [-0.7073, -0.6247]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.135166328633204e-05
Epoch 0, Step 927: loss/train = 0.04374638572335243, logprobs/train = tensor([[-0.7731, -0.7122],
        [-0.8059, -0.7136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0008037831867113709
Epoch 0, Step 943: loss/train = 0.044181305915117264, logprobs/train = tensor([[-0.6879, -0.6496],
        [-0.8124, -0.6596]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001141824061051011
Epoch 0, Step 959: loss/train = 0.04373285174369812, logprobs/train = tensor([[-0.6500, -0.5546],
        [-0.7582, -0.5170]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000543294008821249
Epoch 0, Step 975: loss/train = 0.04365374892950058, logprobs/train = tensor([[-0.7906, -0.8751],
        [-0.9086, -0.9153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003881179145537317
Epoch 0, Step 991: loss/train = 0.04341154173016548, logprobs/train = tensor([[-0.4774, -0.5485],
        [-0.5431, -0.5996]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.0797306206077337e-05
Epoch 0, Step 1007: loss/train = 0.04552311822772026, logprobs/train = tensor([[-0.6406, -1.0399],
        [-0.6947, -0.8138]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003557251999154687
Epoch 0, Step 1023: loss/train = 0.04413750767707825, logprobs/train = tensor([[-0.4895, -1.0066],
        [-0.5674, -0.7484]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004947438836097717
Epoch 0, Step 1039: loss/train = 0.04336927458643913, logprobs/train = tensor([[-0.5931, -0.5779],
        [-0.6712, -0.6033]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000779734633397311
Epoch 0, Step 1055: loss/train = 0.04340985417366028, logprobs/train = tensor([[-0.4933, -0.7440],
        [-0.5832, -0.6557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0027129140216857195
Epoch 0, Step 1071: loss/train = 0.04335649311542511, logprobs/train = tensor([[-0.5508, -0.6739],
        [-0.6441, -0.6592]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007401725742965937
Epoch 0, Step 1087: loss/train = 0.04329601302742958, logprobs/train = tensor([[-0.5762, -0.6486],
        [-0.6519, -0.5312]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0010339785367250443
Epoch 0, Step 1103: loss/train = 0.043349489569664, logprobs/train = tensor([[-0.4742, -0.5536],
        [-0.5004, -0.5168]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000320263730827719
Epoch 0, Step 1119: loss/train = 0.04364655539393425, logprobs/train = tensor([[-0.5747, -0.6411],
        [-0.6675, -0.6232]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0008286620723083615
Epoch 0, Step 1135: loss/train = 0.0433373898267746, logprobs/train = tensor([[-0.5234, -0.5713],
        [-0.5548, -0.5399]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002080327831208706
Epoch 0, Step 1151: loss/train = 0.04396643489599228, logprobs/train = tensor([[-0.4643, -0.8050],
        [-0.6733, -0.8267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0020284357015043497
Epoch 0, Step 1167: loss/train = 0.0434749573469162, logprobs/train = tensor([[-0.5530, -0.7484],
        [-0.5915, -0.6557]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00047398952301591635
Epoch 0, Step 1183: loss/train = 0.055840954184532166, logprobs/train = tensor([[-0.6662, -1.5150],
        [-0.7716, -1.2839]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0054950653575360775
Epoch 0, Step 1199: loss/train = 0.0435708612203598, logprobs/train = tensor([[-0.6534, -0.6583],
        [-0.8809, -0.6098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016108187846839428
Epoch 0, Step 1215: loss/train = 0.04302113503217697, logprobs/train = tensor([[-0.5511, -0.7823],
        [-0.6090, -0.5226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.012998414225876331
Epoch 0, Step 1231: loss/train = 0.043485160917043686, logprobs/train = tensor([[-0.5248, -0.5013],
        [-0.6061, -0.4034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003806225024163723
Epoch 0, Step 1247: loss/train = 0.043623656034469604, logprobs/train = tensor([[-0.5169, -0.4883],
        [-0.6847, -0.3522]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006322544068098068
Epoch 0, Step 1263: loss/train = 0.0435599759221077, logprobs/train = tensor([[-0.6713, -0.6487],
        [-0.8126, -0.6273]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0023067337460815907
Epoch 0, Step 1279: loss/train = 0.04462830349802971, logprobs/train = tensor([[-0.6901, -0.9985],
        [-0.7696, -0.5901]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013530964031815529
Epoch 0, Step 1295: loss/train = 0.043324604630470276, logprobs/train = tensor([[-0.6025, -0.6369],
        [-0.8550, -0.6823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0024502617307007313
Epoch 0, Step 1311: loss/train = 0.043241627514362335, logprobs/train = tensor([[-0.4951, -0.7598],
        [-0.5639, -0.5592]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005361589603126049
Epoch 0, Step 1327: loss/train = 0.04301493242383003, logprobs/train = tensor([[-0.7119, -0.8934],
        [-0.9514, -0.6034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018060047179460526
Epoch 0, Step 1343: loss/train = 0.043268829584121704, logprobs/train = tensor([[-0.5811, -0.6112],
        [-0.8634, -0.6832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009105421602725983
Epoch 0, Step 1359: loss/train = 0.04230576008558273, logprobs/train = tensor([[-0.5341, -1.0019],
        [-0.7842, -0.5126]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023320145905017853
Epoch 0, Step 1375: loss/train = 0.042743947356939316, logprobs/train = tensor([[-0.7401, -1.0645],
        [-0.8255, -0.5906]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.018453333526849747
Epoch 0, Step 1391: loss/train = 0.04293685406446457, logprobs/train = tensor([[-0.5986, -0.8497],
        [-0.7281, -0.5790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020306112244725227
Epoch 0, Step 1407: loss/train = 0.04141562804579735, logprobs/train = tensor([[-0.6559, -1.3767],
        [-0.9386, -0.5996]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.08905646204948425
Epoch 0, Step 1423: loss/train = 0.0434328131377697, logprobs/train = tensor([[-0.7793, -0.8021],
        [-0.7658, -0.7271]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00039202088373713195
Epoch 0, Step 1439: loss/train = 0.042007606476545334, logprobs/train = tensor([[-0.7498, -1.2225],
        [-0.9487, -0.7844]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06344921141862869
Epoch 0, Step 1455: loss/train = 0.0396483838558197, logprobs/train = tensor([[-0.8326, -1.5361],
        [-1.5263, -0.7450]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.12982623279094696
Epoch 0, Step 1471: loss/train = 0.04032748192548752, logprobs/train = tensor([[-0.8054, -1.4634],
        [-1.1808, -0.7086]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09942015260457993
Epoch 0, Step 1487: loss/train = 0.03904741257429123, logprobs/train = tensor([[-0.9130, -1.7124],
        [-1.4287, -0.9226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18399673700332642
Epoch 0, Step 1503: loss/train = 0.04257781058549881, logprobs/train = tensor([[-1.0129, -1.9791],
        [-1.0793, -0.6788]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1377156674861908
Epoch 0, Step 1519: loss/train = 0.04290253296494484, logprobs/train = tensor([[-1.6542, -3.9400],
        [-0.9644, -1.0425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2713547646999359
Epoch 0, Step 1535: loss/train = 0.04393627867102623, logprobs/train = tensor([[-0.9332, -0.8742],
        [-3.3364, -4.0088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06565184891223907
Epoch 0, Step 1551: loss/train = 0.02975221537053585, logprobs/train = tensor([[-1.1947, -3.8525],
        [-3.6619, -1.8781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6049063801765442
Epoch 0, Step 1567: loss/train = 0.029709629714488983, logprobs/train = tensor([[-1.6170, -2.9944],
        [-3.7673, -1.4955]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5810508728027344
Epoch 0, Step 1583: loss/train = 0.0710887610912323, logprobs/train = tensor([[-2.1881, -5.8630],
        [-4.4578, -4.1965]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5240755081176758
Epoch 0, Step 1599: loss/train = 0.034971192479133606, logprobs/train = tensor([[-1.5732, -4.7328],
        [-2.4350, -1.4681]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5484974384307861
Epoch 0, Step 1615: loss/train = 0.0382499024271965, logprobs/train = tensor([[-3.5117, -5.0434],
        [-3.0797, -2.4089]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2792537808418274
Epoch 0, Step 1631: loss/train = 0.05441686511039734, logprobs/train = tensor([[-2.8882, -4.1487],
        [-3.1805, -3.7327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0473790280520916
Epoch 0, Step 1647: loss/train = 0.032645583152770996, logprobs/train = tensor([[-3.5709, -4.6606],
        [-3.8569, -2.1320]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4841550290584564
Epoch 0, Step 1663: loss/train = 0.0287018995732069, logprobs/train = tensor([[-3.3057, -5.9763],
        [-7.2280, -3.4784]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8119953274726868
Epoch 0, Step 1679: loss/train = 0.018882740288972855, logprobs/train = tensor([[ -5.3634, -10.9367],
        [ -5.2961,  -1.4202]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9316133260726929
Epoch 0, Step 1695: loss/train = 0.015843389555811882, logprobs/train = tensor([[ -3.2323, -10.9333],
        [ -6.2977,  -1.1914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0365525484085083
Epoch 0, Step 1711: loss/train = 0.01988879218697548, logprobs/train = tensor([[-2.7842, -7.2834],
        [-5.4500, -2.9134]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8362826108932495
Epoch 0, Step 1727: loss/train = 0.04580297693610191, logprobs/train = tensor([[ -3.2378, -10.7721],
        [ -5.6433,  -4.8307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5974512100219727
Epoch 0, Step 1743: loss/train = 0.03333083167672157, logprobs/train = tensor([[-5.9509, -7.8600],
        [-3.4721, -2.3486]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.47286278009414673
Epoch 0, Step 1759: loss/train = 0.01686973124742508, logprobs/train = tensor([[ -7.1853, -12.3288],
        [ -6.3853,  -1.9602]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9952688217163086
Epoch 0, Step 1775: loss/train = 0.02391245774924755, logprobs/train = tensor([[ -8.5116, -13.8603],
        [ -5.9178,  -2.7431]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7436239719390869
Epoch 0, Step 1791: loss/train = 0.008203871548175812, logprobs/train = tensor([[ -4.0330, -18.4818],
        [-10.5292,  -3.6888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2394777536392212
Epoch 0, Step 1807: loss/train = 0.0330716110765934, logprobs/train = tensor([[ -7.2414, -10.1033],
        [-10.0152,  -6.0990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6507218480110168
Epoch 0, Step 1823: loss/train = 0.01594245433807373, logprobs/train = tensor([[ -3.7401, -11.3495],
        [-12.1436,  -5.7043]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0164453983306885
Epoch 0, Step 1839: loss/train = 0.021334465593099594, logprobs/train = tensor([[ -3.9490,  -8.9315],
        [-10.7223,  -7.0781]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.846436083316803
Epoch 0, Step 1855: loss/train = 0.03728260099887848, logprobs/train = tensor([[-6.0573, -5.9675],
        [-9.1652, -5.9235]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6851105690002441
Epoch 0, Step 1871: loss/train = 0.034395214170217514, logprobs/train = tensor([[ -3.4969,  -6.4524],
        [-12.4195, -11.0326]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5598463416099548
Epoch 0, Step 1887: loss/train = 0.009179532527923584, logprobs/train = tensor([[ -5.2636, -15.8375],
        [-10.3179,  -3.9672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2847330570220947
Epoch 0, Step 1903: loss/train = 0.04045293480157852, logprobs/train = tensor([[-3.8105, -7.7541],
        [-9.2462, -9.4971]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.856160581111908
Epoch 0, Step 1919: loss/train = 0.025427252054214478, logprobs/train = tensor([[ -8.6998, -14.0243],
        [ -7.8195,  -4.6378]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7053662538528442
Epoch 0, Step 1935: loss/train = 0.027584848925471306, logprobs/train = tensor([[ -5.4317, -11.0424],
        [ -9.5912,  -6.0259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8674410581588745
Epoch 0, Step 1951: loss/train = 0.01734846830368042, logprobs/train = tensor([[ -8.7116, -15.4814],
        [ -8.9908,  -2.7352]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.996577799320221
Epoch 0, Step 1967: loss/train = 0.024153675884008408, logprobs/train = tensor([[-10.1741, -13.6316],
        [-12.1684,  -8.2700]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8948121666908264
Epoch 0, Step 1983: loss/train = 0.010048182681202888, logprobs/train = tensor([[ -6.6111, -17.5923],
        [-14.8764,  -6.8842]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2230859994888306
Epoch 0, Step 1999: loss/train = 0.017969371750950813, logprobs/train = tensor([[-10.4791, -18.5526],
        [-11.3849,  -6.3957]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9113591909408569
Epoch 0, Step 2015: loss/train = 0.053008630871772766, logprobs/train = tensor([[-17.1977, -17.9704],
        [-14.9636, -12.9961]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3812776207923889
Epoch 0, Step 2031: loss/train = 0.01776643469929695, logprobs/train = tensor([[-11.8583, -18.6040],
        [-21.5528, -17.3727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.127723217010498
Epoch 0, Step 2047: loss/train = 0.040059786289930344, logprobs/train = tensor([[ -9.7076, -14.5991],
        [-21.4458, -16.6358]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8062761425971985
Epoch 0, Step 2063: loss/train = 0.02030711993575096, logprobs/train = tensor([[-17.0145, -26.9860],
        [-20.4563, -13.3844]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8462201952934265
Epoch 0, Step 2079: loss/train = 0.04679558053612709, logprobs/train = tensor([[-14.4860, -14.3693],
        [-19.7501, -15.6066]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6012189388275146
Epoch 0, Step 2095: loss/train = 0.017592476680874825, logprobs/train = tensor([[-12.7638, -24.5593],
        [-18.6425, -13.1500]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9939731955528259
Epoch 0, Step 2111: loss/train = 0.017799777910113335, logprobs/train = tensor([[-10.3091, -24.3141],
        [-18.0663, -11.0572]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9889246821403503
Epoch 0, Step 2127: loss/train = 0.037736471742391586, logprobs/train = tensor([[ -7.4420,  -9.3617],
        [-17.5366, -13.9072]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4683411419391632
Epoch 0, Step 2143: loss/train = 0.0393465980887413, logprobs/train = tensor([[ -7.9522, -14.8392],
        [-14.0228, -10.4173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8122361302375793
Epoch 0, Step 2159: loss/train = 0.027922041714191437, logprobs/train = tensor([[ -4.4808, -12.2377],
        [-16.9262, -12.6701]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8746447563171387
Epoch 0, Step 2175: loss/train = 0.03489620238542557, logprobs/train = tensor([[ -4.1497,  -6.4991],
        [-14.7729, -11.7923]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4035276174545288
Epoch 0, Step 2191: loss/train = 0.03636154904961586, logprobs/train = tensor([[ -8.3716, -13.9382],
        [-13.6058,  -9.9549]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3343992829322815
Epoch 0, Step 2207: loss/train = 0.04357899725437164, logprobs/train = tensor([[ -6.2968,  -7.2166],
        [-11.2824,  -8.9081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.34382355213165283
Epoch 0, Step 2223: loss/train = 0.018735593184828758, logprobs/train = tensor([[ -4.8840, -10.0777],
        [-11.1178,  -7.0664]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9010222554206848
Epoch 0, Step 2239: loss/train = 0.02393846958875656, logprobs/train = tensor([[ -2.5726, -11.7528],
        [-12.1296,  -7.3105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7629036903381348
Epoch 0, Step 2255: loss/train = 0.020909784361720085, logprobs/train = tensor([[ -4.0265,  -9.9038],
        [-13.1164,  -7.8400]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8456243276596069
Epoch 0, Step 2271: loss/train = 0.01975453458726406, logprobs/train = tensor([[ -1.5024,  -5.7440],
        [-18.0866, -10.4254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0176875591278076
Epoch 0, Step 2287: loss/train = 0.015750382095575333, logprobs/train = tensor([[ -2.1315,  -8.6899],
        [-11.8879,  -3.5685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0996853113174438
Epoch 0, Step 2303: loss/train = 0.01763603650033474, logprobs/train = tensor([[ -2.3588, -10.5296],
        [-12.9246,  -5.5461]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9464588165283203
Epoch 0, Step 2319: loss/train = 0.015767337754368782, logprobs/train = tensor([[ -1.9959,  -8.4878],
        [-12.0118,  -3.8074]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1123360395431519
Epoch 0, Step 2335: loss/train = 0.009141344577074051, logprobs/train = tensor([[ -2.2330, -10.5574],
        [-15.6604,  -3.3195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.336013913154602
Epoch 0, Step 2351: loss/train = 0.02460271678864956, logprobs/train = tensor([[ -4.3440,  -8.7452],
        [-16.1017,  -9.9832]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7670050859451294
Epoch 0, Step 2367: loss/train = 0.01735386997461319, logprobs/train = tensor([[ -3.7622, -14.9914],
        [-18.2128,  -5.2353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0519740581512451
Epoch 0, Step 2374: loss/train = 0.023371953517198563, logprobs/train = tensor([[ -5.5980, -10.9292],
        [-19.9519, -14.9986]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8407309651374817
loss/eval: 0.34556326270103455
[2024-02-19 11:27:29,270][root][INFO] - beta: 0.3
[2024-02-19 11:27:29,270][root][INFO] - temperature: 1
[2024-02-19 11:27:29,270][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-temp-1
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 10000 training examples...
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-temp-1 after each epoch.
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-temp-1 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-temp-1 after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.3-temp-1 after each epoch.
Epoch 0, Step 15: loss/train = 0.04351605474948883, logprobs/train = tensor([[-0.8196, -0.9555],
        [-0.8241, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.04332902282476425, logprobs/train = tensor([[-1.2200, -1.2428],
        [-1.2477, -1.1750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.046375710517168045, logprobs/train = tensor([[-1.3721, -1.1582],
        [-1.3641, -1.1025]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.964989107567817e-06
Epoch 0, Step 63: loss/train = 0.04505981132388115, logprobs/train = tensor([[-1.1952, -1.2003],
        [-1.1697, -1.1035]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.1981613826937973e-06
Epoch 0, Step 79: loss/train = 0.04342583939433098, logprobs/train = tensor([[-0.8098, -0.8647],
        [-0.8207, -0.7796]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.054508175002411e-06
Epoch 0, Step 95: loss/train = 0.04331028461456299, logprobs/train = tensor([[-0.8973, -1.0742],
        [-0.9343, -0.9590]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.526905392296612e-07
Epoch 0, Step 111: loss/train = 0.043830566108226776, logprobs/train = tensor([[-0.8026, -0.7944],
        [-0.8203, -0.7800]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.469069143757224e-07
Epoch 0, Step 127: loss/train = 0.04378943890333176, logprobs/train = tensor([[-1.0685, -0.9249],
        [-1.1228, -0.8726]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.514237611554563e-06
Epoch 0, Step 143: loss/train = 0.04362219199538231, logprobs/train = tensor([[-0.8516, -0.9097],
        [-0.8013, -0.8714]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.98616645927541e-06
Epoch 0, Step 159: loss/train = 0.043950535356998444, logprobs/train = tensor([[-1.0065, -1.0504],
        [-1.0341, -0.9189]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9213117613835493e-06
Epoch 0, Step 175: loss/train = 0.0500912219285965, logprobs/train = tensor([[-0.9672, -1.5907],
        [-0.9391, -1.5132]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.931695679668337e-06
Epoch 0, Step 191: loss/train = 0.0461185947060585, logprobs/train = tensor([[-1.0600, -1.2311],
        [-1.0966, -1.2251]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.561517122667283e-06
Epoch 0, Step 207: loss/train = 0.04429071024060249, logprobs/train = tensor([[-0.9501, -1.0444],
        [-0.9354, -1.0014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.6073081496870145e-06
Epoch 0, Step 223: loss/train = 0.04494203254580498, logprobs/train = tensor([[-0.7761, -1.1049],
        [-0.8064, -1.0420]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.01741305924952e-06
Epoch 0, Step 239: loss/train = 0.04396801441907883, logprobs/train = tensor([[-0.7225, -0.9549],
        [-0.7418, -0.9295]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.2360912882722914e-07
Epoch 0, Step 255: loss/train = 0.04420250281691551, logprobs/train = tensor([[-0.9937, -0.7777],
        [-0.9925, -0.7525]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.413356120698154e-06
Epoch 0, Step 271: loss/train = 0.046608369797468185, logprobs/train = tensor([[-0.8876, -1.3770],
        [-0.9027, -1.2763]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.511450581252575e-06
Epoch 0, Step 287: loss/train = 0.04390423744916916, logprobs/train = tensor([[-0.7359, -0.7918],
        [-0.7523, -0.7301]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.742854121606797e-07
Epoch 0, Step 303: loss/train = 0.048463739454746246, logprobs/train = tensor([[-1.2642, -1.6980],
        [-1.2281, -1.4727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.660741589963436e-05
Epoch 0, Step 319: loss/train = 0.047713443636894226, logprobs/train = tensor([[-0.8791, -1.5910],
        [-0.8999, -1.4387]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.575010876171291e-05
Epoch 0, Step 335: loss/train = 0.0438389852643013, logprobs/train = tensor([[-0.7585, -0.8099],
        [-0.7554, -0.7803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.886736860498786e-06
Epoch 0, Step 351: loss/train = 0.04431837052106857, logprobs/train = tensor([[-0.8741, -0.8965],
        [-0.8986, -0.8612]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.455862942151725e-06
Epoch 0, Step 367: loss/train = 0.04861301928758621, logprobs/train = tensor([[-0.8192, -1.4912],
        [-0.8693, -1.3607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.03269974514842e-05
Epoch 0, Step 383: loss/train = 0.04776281863451004, logprobs/train = tensor([[-1.0661, -1.5242],
        [-1.1427, -1.4117]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.600725125987083e-05
Epoch 0, Step 399: loss/train = 0.04393501579761505, logprobs/train = tensor([[-1.2483, -1.4521],
        [-1.2922, -1.3914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.384102744050324e-05
Epoch 0, Step 415: loss/train = 0.047481559216976166, logprobs/train = tensor([[-0.8786, -1.5142],
        [-0.9138, -1.5115]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000326602254062891
Epoch 0, Step 431: loss/train = 0.043209124356508255, logprobs/train = tensor([[-0.8575, -0.9574],
        [-1.1140, -0.9114]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002810565638355911
Epoch 0, Step 447: loss/train = 0.044277485460042953, logprobs/train = tensor([[-0.9153, -1.4780],
        [-1.3299, -1.4263]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005554026574827731
Epoch 0, Step 463: loss/train = 0.044602956622838974, logprobs/train = tensor([[-0.8137, -1.2170],
        [-0.8176, -1.1323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0012713274918496609
Epoch 0, Step 479: loss/train = 0.04563820734620094, logprobs/train = tensor([[-1.1419, -0.8422],
        [-1.1693, -0.8076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00029352615820243955
Epoch 0, Step 495: loss/train = 0.0460839569568634, logprobs/train = tensor([[-1.0018, -0.6344],
        [-1.0662, -0.6524]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.537571946159005e-05
Epoch 0, Step 511: loss/train = 0.049217209219932556, logprobs/train = tensor([[-0.6539, -1.2226],
        [-0.6954, -1.1371]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003149410476908088
Epoch 0, Step 527: loss/train = 0.046428121626377106, logprobs/train = tensor([[-1.1855, -0.9283],
        [-1.2110, -0.9540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.152178346179426e-05
Epoch 0, Step 543: loss/train = 0.0443396158516407, logprobs/train = tensor([[-1.0867, -0.8032],
        [-1.1702, -0.7990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.165834540501237e-05
Epoch 0, Step 559: loss/train = 0.04438289999961853, logprobs/train = tensor([[-0.8315, -0.8432],
        [-0.9452, -0.8384]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00017236388521268964
Epoch 0, Step 575: loss/train = 0.04346710443496704, logprobs/train = tensor([[-0.7227, -0.7607],
        [-0.7386, -0.7479]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.974389932816848e-05
Epoch 0, Step 591: loss/train = 0.043882738798856735, logprobs/train = tensor([[-0.8225, -0.9610],
        [-0.9813, -1.0759]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007903536316007376
Epoch 0, Step 607: loss/train = 0.043907541781663895, logprobs/train = tensor([[-0.9337, -0.7988],
        [-0.9888, -0.7958]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.0424069589353167e-05
Epoch 0, Step 623: loss/train = 0.04325195029377937, logprobs/train = tensor([[-0.7389, -0.7676],
        [-0.8567, -0.7720]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002454610075801611
Epoch 0, Step 639: loss/train = 0.046301864087581635, logprobs/train = tensor([[-1.3091, -1.0002],
        [-1.4316, -1.0069]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002903289860114455
Epoch 0, Step 655: loss/train = 0.04337994009256363, logprobs/train = tensor([[-0.7736, -0.7310],
        [-0.8210, -0.7696]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00015442071889992803
Epoch 0, Step 671: loss/train = 0.04417659342288971, logprobs/train = tensor([[-0.7134, -0.8211],
        [-0.8045, -0.9094]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00446537509560585
Epoch 0, Step 687: loss/train = 0.0435100719332695, logprobs/train = tensor([[-0.9853, -1.0688],
        [-1.0744, -1.0321]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006945249624550343
Epoch 0, Step 703: loss/train = 0.04401898384094238, logprobs/train = tensor([[-0.7928, -0.6485],
        [-0.9016, -0.6377]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000350729504134506
Epoch 0, Step 719: loss/train = 0.04683034121990204, logprobs/train = tensor([[-0.6686, -0.9965],
        [-0.7006, -1.0040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00020707212388515472
Epoch 0, Step 735: loss/train = 0.043441105633974075, logprobs/train = tensor([[-0.9136, -0.8626],
        [-0.9706, -0.8679]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000140884323627688
Epoch 0, Step 751: loss/train = 0.04369106888771057, logprobs/train = tensor([[-0.6401, -0.6126],
        [-0.7450, -0.6303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009214833262376487
Epoch 0, Step 767: loss/train = 0.04595037177205086, logprobs/train = tensor([[-0.8183, -0.5667],
        [-1.0322, -0.6230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001725355046801269
Epoch 0, Step 783: loss/train = 0.04393212869763374, logprobs/train = tensor([[-1.0213, -0.8328],
        [-1.1336, -0.8755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001118253218010068
Epoch 0, Step 799: loss/train = 0.043928418308496475, logprobs/train = tensor([[-0.8546, -0.8773],
        [-0.8552, -0.8633]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.0075217839330435e-05
Epoch 0, Step 815: loss/train = 0.043512433767318726, logprobs/train = tensor([[-0.6983, -0.8685],
        [-0.7351, -0.8485]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0013957407791167498
Epoch 0, Step 831: loss/train = 0.04405013844370842, logprobs/train = tensor([[-0.7501, -0.6570],
        [-0.8247, -0.7154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009403980802744627
Epoch 0, Step 847: loss/train = 0.043968088924884796, logprobs/train = tensor([[-0.7511, -0.6195],
        [-0.8118, -0.6531]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.166631697444245e-05
Epoch 0, Step 863: loss/train = 0.043861083686351776, logprobs/train = tensor([[-0.9142, -0.8516],
        [-1.0103, -0.8435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.3931679606903344e-05
Epoch 0, Step 879: loss/train = 0.044137973338365555, logprobs/train = tensor([[-0.5536, -0.8188],
        [-0.6132, -0.8136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00022201781393960118
Epoch 0, Step 895: loss/train = 0.043433938175439835, logprobs/train = tensor([[-0.6634, -0.7291],
        [-0.7304, -0.7570]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001346350647509098
Epoch 0, Step 911: loss/train = 0.043404050171375275, logprobs/train = tensor([[-0.6707, -0.6377],
        [-0.7047, -0.6200]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.411606929963455e-05
Epoch 0, Step 927: loss/train = 0.043763890862464905, logprobs/train = tensor([[-0.7739, -0.7112],
        [-0.8038, -0.7107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000695374677889049
Epoch 0, Step 943: loss/train = 0.04420855641365051, logprobs/train = tensor([[-0.6892, -0.6510],
        [-0.8072, -0.6520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.000983807840384543
Epoch 0, Step 959: loss/train = 0.043757107108831406, logprobs/train = tensor([[-0.6513, -0.5521],
        [-0.7573, -0.5130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004184289136901498
Epoch 0, Step 975: loss/train = 0.043655067682266235, logprobs/train = tensor([[-0.7917, -0.8788],
        [-0.9037, -0.9063]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00045508702169172466
Epoch 0, Step 991: loss/train = 0.043409064412117004, logprobs/train = tensor([[-0.4792, -0.5514],
        [-0.5347, -0.5888]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1833966709673405e-05
Epoch 0, Step 1007: loss/train = 0.0455845408141613, logprobs/train = tensor([[-0.6460, -1.0336],
        [-0.6917, -0.8167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0029509267769753933
Epoch 0, Step 1023: loss/train = 0.04433152824640274, logprobs/train = tensor([[-0.4936, -1.0081],
        [-0.5629, -0.7559]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003898218274116516
Epoch 0, Step 1039: loss/train = 0.04338567703962326, logprobs/train = tensor([[-0.5968, -0.5827],
        [-0.6626, -0.5983]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006042779423296452
Epoch 0, Step 1055: loss/train = 0.04347230866551399, logprobs/train = tensor([[-0.4942, -0.7447],
        [-0.5794, -0.6568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0020494412165135145
Epoch 0, Step 1071: loss/train = 0.043370772153139114, logprobs/train = tensor([[-0.5523, -0.6778],
        [-0.6382, -0.6593]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007350108935497701
Epoch 0, Step 1087: loss/train = 0.04332933947443962, logprobs/train = tensor([[-0.5842, -0.6431],
        [-0.6499, -0.5369]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006126211956143379
Epoch 0, Step 1103: loss/train = 0.04336167499423027, logprobs/train = tensor([[-0.4840, -0.5571],
        [-0.5034, -0.5236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00048761320067569613
Epoch 0, Step 1119: loss/train = 0.0437161810696125, logprobs/train = tensor([[-0.5890, -0.6322],
        [-0.6732, -0.6266]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005892246263101697
Epoch 0, Step 1135: loss/train = 0.04335545375943184, logprobs/train = tensor([[-0.5286, -0.5744],
        [-0.5542, -0.5424]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.782083073630929e-05
Epoch 0, Step 1151: loss/train = 0.04397577792406082, logprobs/train = tensor([[-0.4760, -0.8033],
        [-0.6602, -0.8199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001511367503553629
Epoch 0, Step 1167: loss/train = 0.04345199838280678, logprobs/train = tensor([[-0.5563, -0.7426],
        [-0.5988, -0.6575]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004085865803062916
Epoch 0, Step 1183: loss/train = 0.05581095069646835, logprobs/train = tensor([[-0.6680, -1.4908],
        [-0.7696, -1.2872]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00423022173345089
Epoch 0, Step 1199: loss/train = 0.04366089403629303, logprobs/train = tensor([[-0.6681, -0.6421],
        [-0.8723, -0.6143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007808145601302385
Epoch 0, Step 1215: loss/train = 0.043253131210803986, logprobs/train = tensor([[-0.5417, -0.7151],
        [-0.6001, -0.5345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007289508357644081
Epoch 0, Step 1231: loss/train = 0.04358714073896408, logprobs/train = tensor([[-0.5310, -0.4698],
        [-0.6029, -0.4040]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0021715769544243813
Epoch 0, Step 1247: loss/train = 0.043714165687561035, logprobs/train = tensor([[-0.5025, -0.4454],
        [-0.6639, -0.3410]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004760564770549536
Epoch 0, Step 1263: loss/train = 0.04359816014766693, logprobs/train = tensor([[-0.6667, -0.6375],
        [-0.8017, -0.6192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.002143711084499955
Epoch 0, Step 1279: loss/train = 0.04480280727148056, logprobs/train = tensor([[-0.6720, -0.8786],
        [-0.7606, -0.6130]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004671896807849407
Epoch 0, Step 1295: loss/train = 0.04341079294681549, logprobs/train = tensor([[-0.6045, -0.6017],
        [-0.8020, -0.6452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011691437102854252
Epoch 0, Step 1311: loss/train = 0.04336769878864288, logprobs/train = tensor([[-0.4886, -0.6706],
        [-0.5388, -0.5627]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016859895549714565
Epoch 0, Step 1327: loss/train = 0.043390966951847076, logprobs/train = tensor([[-0.6847, -0.7961],
        [-0.8996, -0.6228]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00877760536968708
Epoch 0, Step 1343: loss/train = 0.04345610737800598, logprobs/train = tensor([[-0.5805, -0.5445],
        [-0.7981, -0.6255]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004479495342820883
Epoch 0, Step 1359: loss/train = 0.043168969452381134, logprobs/train = tensor([[-0.5434, -0.7575],
        [-0.7239, -0.5167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003902251133695245
Epoch 0, Step 1375: loss/train = 0.04326695576310158, logprobs/train = tensor([[-0.6833, -0.8304],
        [-0.7813, -0.6086]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004498237743973732
Epoch 0, Step 1391: loss/train = 0.0433514304459095, logprobs/train = tensor([[-0.5575, -0.6506],
        [-0.6391, -0.5423]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0033429544419050217
Epoch 0, Step 1407: loss/train = 0.04354849457740784, logprobs/train = tensor([[-0.6225, -0.9590],
        [-0.8356, -0.5990]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.031639423221349716
Epoch 0, Step 1423: loss/train = 0.04339989274740219, logprobs/train = tensor([[-0.6447, -0.6543],
        [-0.6532, -0.6127]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00021776731591671705
Epoch 0, Step 1439: loss/train = 0.043300729244947433, logprobs/train = tensor([[-0.6517, -0.9030],
        [-0.6730, -0.6020]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010997127741575241
Epoch 0, Step 1455: loss/train = 0.04304037615656853, logprobs/train = tensor([[-0.6927, -0.9293],
        [-0.9728, -0.7174]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.010272900573909283
Epoch 0, Step 1471: loss/train = 0.042893439531326294, logprobs/train = tensor([[-0.6488, -1.0070],
        [-0.7973, -0.6292]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.01899101957678795
Epoch 0, Step 1487: loss/train = 0.0433010496199131, logprobs/train = tensor([[-0.5914, -0.7602],
        [-0.8437, -0.6230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015605656430125237
Epoch 0, Step 1503: loss/train = 0.04364211857318878, logprobs/train = tensor([[-0.5391, -0.6915],
        [-0.6459, -0.5143]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004057449288666248
Epoch 0, Step 1519: loss/train = 0.04428728297352791, logprobs/train = tensor([[-0.6050, -1.4337],
        [-0.5980, -0.6310]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.05495128780603409
Epoch 0, Step 1535: loss/train = 0.043410077691078186, logprobs/train = tensor([[-0.6421, -0.6280],
        [-0.9723, -1.0250]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0018292750464752316
Epoch 0, Step 1551: loss/train = 0.04231492429971695, logprobs/train = tensor([[-0.7523, -1.5487],
        [-1.1960, -0.7930]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.07338067889213562
Epoch 0, Step 1567: loss/train = 0.04300803318619728, logprobs/train = tensor([[-0.8217, -1.0573],
        [-1.0607, -0.6477]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.02627892605960369
Epoch 0, Step 1583: loss/train = 0.041771553456783295, logprobs/train = tensor([[-0.7198, -2.2888],
        [-1.1733, -0.7281]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2074587345123291
Epoch 0, Step 1599: loss/train = 0.043865662068128586, logprobs/train = tensor([[-0.6485, -1.7908],
        [-0.7668, -0.5716]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0939360111951828
Epoch 0, Step 1615: loss/train = 0.04275576025247574, logprobs/train = tensor([[-0.9132, -1.3113],
        [-1.0805, -0.7589]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.03346423804759979
Epoch 0, Step 1631: loss/train = 0.04436580091714859, logprobs/train = tensor([[-0.7997, -1.0101],
        [-0.9784, -1.2816]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0029723644256591797
Epoch 0, Step 1647: loss/train = 0.04235170781612396, logprobs/train = tensor([[-0.6488, -0.9719],
        [-0.9342, -0.4867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.050800297409296036
Epoch 0, Step 1663: loss/train = 0.042634595185518265, logprobs/train = tensor([[-0.9728, -1.5367],
        [-1.5874, -1.0470]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.06909363716840744
Epoch 0, Step 1679: loss/train = 0.04037001356482506, logprobs/train = tensor([[-1.5700, -2.5253],
        [-1.5646, -0.6156]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.1729951947927475
Epoch 0, Step 1695: loss/train = 0.037978410720825195, logprobs/train = tensor([[-0.8059, -2.9036],
        [-1.8314, -0.7366]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.33933836221694946
Epoch 0, Step 1711: loss/train = 0.040042899549007416, logprobs/train = tensor([[-0.8541, -1.7286],
        [-1.9065, -1.0331]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16358864307403564
Epoch 0, Step 1727: loss/train = 0.04120013862848282, logprobs/train = tensor([[-0.9037, -2.7871],
        [-1.9845, -1.4503]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.32002103328704834
Epoch 0, Step 1743: loss/train = 0.03929018974304199, logprobs/train = tensor([[-1.5964, -2.4397],
        [-1.9482, -1.3209]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.26126354932785034
Epoch 0, Step 1759: loss/train = 0.038140758872032166, logprobs/train = tensor([[-1.5695, -3.0275],
        [-2.7208, -1.0857]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.42545560002326965
Epoch 0, Step 1775: loss/train = 0.03766684979200363, logprobs/train = tensor([[-2.5011, -3.8983],
        [-3.0867, -1.2068]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.46408361196517944
Epoch 0, Step 1791: loss/train = 0.030603263527154922, logprobs/train = tensor([[-1.2782, -7.2403],
        [-5.3691, -2.1259]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0325262546539307
Epoch 0, Step 1807: loss/train = 0.038673121482133865, logprobs/train = tensor([[-2.9569, -4.7832],
        [-5.7538, -3.4183]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5878391265869141
Epoch 0, Step 1823: loss/train = 0.02960200421512127, logprobs/train = tensor([[-1.1269, -4.9130],
        [-7.7016, -3.0863]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9404367208480835
Epoch 0, Step 1839: loss/train = 0.039565250277519226, logprobs/train = tensor([[-1.7000, -4.0672],
        [-5.9915, -4.6267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.48567453026771545
Epoch 0, Step 1855: loss/train = 0.0487002395093441, logprobs/train = tensor([[-4.4928, -3.8520],
        [-5.5223, -3.4539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.39106810092926025
Epoch 0, Step 1871: loss/train = 0.04005003720521927, logprobs/train = tensor([[-2.3306, -4.2543],
        [-7.0539, -5.7900]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4322359263896942
Epoch 0, Step 1887: loss/train = 0.032846931368112564, logprobs/train = tensor([[ -4.0437, -10.1120],
        [ -5.9456,  -2.6141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0608537197113037
Epoch 0, Step 1903: loss/train = 0.03719199448823929, logprobs/train = tensor([[-2.8716, -3.9700],
        [-5.4142, -5.3934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6498112678527832
Epoch 0, Step 1919: loss/train = 0.036413971334695816, logprobs/train = tensor([[-5.1081, -8.8503],
        [-4.7066, -2.5434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.660701334476471
Epoch 0, Step 1935: loss/train = 0.03388625755906105, logprobs/train = tensor([[-2.9261, -6.4060],
        [-5.2084, -2.8088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8062777519226074
Epoch 0, Step 1951: loss/train = 0.034477755427360535, logprobs/train = tensor([[-5.5745, -8.8033],
        [-3.4534, -1.1154]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6244320273399353
Epoch 0, Step 1967: loss/train = 0.04106714576482773, logprobs/train = tensor([[-5.7126, -9.1065],
        [-3.7587, -2.9861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.4776899814605713
Epoch 0, Step 1983: loss/train = 0.02733796089887619, logprobs/train = tensor([[ -4.1057, -11.0497],
        [ -6.7733,  -1.9755]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0326365232467651
Epoch 0, Step 1999: loss/train = 0.03329715132713318, logprobs/train = tensor([[-4.5559, -8.9519],
        [-7.3413, -4.8517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.719789981842041
Epoch 0, Step 2015: loss/train = 0.03810630738735199, logprobs/train = tensor([[ -7.9599, -10.3639],
        [ -6.0429,  -4.6434]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.44614386558532715
Epoch 0, Step 2031: loss/train = 0.0325089767575264, logprobs/train = tensor([[ -4.4217,  -9.7284],
        [-10.0581,  -7.2088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0439339876174927
Epoch 0, Step 2047: loss/train = 0.03630397468805313, logprobs/train = tensor([[-3.7799, -8.2373],
        [-9.3707, -7.3082]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.790775716304779
Epoch 0, Step 2063: loss/train = 0.030012276023626328, logprobs/train = tensor([[ -8.8010, -15.7603],
        [ -8.0022,  -3.3156]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8529719114303589
Epoch 0, Step 2079: loss/train = 0.05517963320016861, logprobs/train = tensor([[-8.6671, -8.3698],
        [-8.3184, -6.7723]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.18696275353431702
Epoch 0, Step 2095: loss/train = 0.02983146905899048, logprobs/train = tensor([[ -8.5453, -17.7603],
        [ -8.3225,  -2.1084]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9900051355361938
Epoch 0, Step 2111: loss/train = 0.029676172882318497, logprobs/train = tensor([[ -9.2364, -20.6398],
        [ -7.0059,  -2.2909]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.009856939315796
Epoch 0, Step 2127: loss/train = 0.03695724159479141, logprobs/train = tensor([[ -8.3919, -10.0332],
        [ -8.1666,  -5.9656]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.47759681940078735
Epoch 0, Step 2143: loss/train = 0.04414262995123863, logprobs/train = tensor([[-10.0142, -16.6468],
        [ -7.1257,  -4.4168]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7238707542419434
Epoch 0, Step 2159: loss/train = 0.03573184832930565, logprobs/train = tensor([[ -5.5281, -15.6559],
        [-12.6353,  -7.1341]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9352298378944397
Epoch 0, Step 2175: loss/train = 0.03502524644136429, logprobs/train = tensor([[ -5.9811, -11.2760],
        [ -9.9123,  -7.5263]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.48185813426971436
Epoch 0, Step 2191: loss/train = 0.03790121525526047, logprobs/train = tensor([[-11.4454, -16.8415],
        [ -8.2709,  -6.0760]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2542704939842224
Epoch 0, Step 2207: loss/train = 0.03522975742816925, logprobs/train = tensor([[-12.1717, -14.4323],
        [ -6.9201,  -4.6611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5939756631851196
Epoch 0, Step 2223: loss/train = 0.030169181525707245, logprobs/train = tensor([[-10.8385, -18.2385],
        [ -9.2074,  -2.5856]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9254014492034912
Epoch 0, Step 2239: loss/train = 0.02240893244743347, logprobs/train = tensor([[-10.6525, -24.3230],
        [ -9.5363,  -1.1815]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0241421461105347
Epoch 0, Step 2255: loss/train = 0.03468114882707596, logprobs/train = tensor([[-14.8762, -24.0399],
        [ -8.7167,  -4.6604]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7396299242973328
Epoch 0, Step 2271: loss/train = 0.03126435726881027, logprobs/train = tensor([[ -4.8908, -20.1742],
        [-12.2747,  -6.4185]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1277927160263062
Epoch 0, Step 2287: loss/train = 0.028713317587971687, logprobs/train = tensor([[ -8.5846, -27.0465],
        [-12.3475,  -3.7932]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1591415405273438
Epoch 0, Step 2303: loss/train = 0.02845758944749832, logprobs/train = tensor([[ -8.4003, -21.2375],
        [-10.4437,  -5.2002]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0040932893753052
Epoch 0, Step 2319: loss/train = 0.02861350029706955, logprobs/train = tensor([[ -6.7844, -14.5443],
        [ -9.9206,  -3.0238]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1780259609222412
Epoch 0, Step 2335: loss/train = 0.02839725837111473, logprobs/train = tensor([[ -5.8405, -21.6096],
        [-13.5272,  -4.3358]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2703936100006104
Epoch 0, Step 2351: loss/train = 0.03452648967504501, logprobs/train = tensor([[-10.0897, -17.1142],
        [ -9.8773,  -5.3750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9600662589073181
Epoch 0, Step 2367: loss/train = 0.03292300924658775, logprobs/train = tensor([[ -5.9744, -24.0468],
        [ -8.6642,  -2.0945]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0321369171142578
