[2024-02-19 09:07:08,373][root][INFO] - beta: 0.01
[2024-02-19 09:07:08,373][root][INFO] - temperature: 1
[2024-02-19 09:07:08,373][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 10000 training examples...
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Epoch 0, Step 15: loss/train = 0.04351605474948883, logprobs/train = tensor([[-0.8196, -0.9555],
        [-0.8241, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.04332902282476425, logprobs/train = tensor([[-1.2200, -1.2428],
        [-1.2477, -1.1750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.046370428055524826, logprobs/train = tensor([[-1.3692, -1.1574],
        [-1.3665, -1.1021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.213958163745701e-06
Epoch 0, Step 63: loss/train = 0.04506981372833252, logprobs/train = tensor([[-1.1974, -1.2022],
        [-1.1672, -1.1073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9732027542195283e-06
Epoch 0, Step 79: loss/train = 0.04342862218618393, logprobs/train = tensor([[-0.8091, -0.8658],
        [-0.8188, -0.7790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.431782821891829e-07
Epoch 0, Step 95: loss/train = 0.04331377148628235, logprobs/train = tensor([[-0.8973, -1.0712],
        [-0.9348, -0.9600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.62507773085963e-07
Epoch 0, Step 111: loss/train = 0.04382738471031189, logprobs/train = tensor([[-0.8006, -0.7956],
        [-0.8196, -0.7799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.3851287045981735e-07
Epoch 0, Step 127: loss/train = 0.04379041865468025, logprobs/train = tensor([[-1.0690, -0.9253],
        [-1.1216, -0.8727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7339407349936664e-06
Epoch 0, Step 143: loss/train = 0.043622054159641266, logprobs/train = tensor([[-0.8526, -0.9075],
        [-0.8004, -0.8727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.356107870582491e-06
Epoch 0, Step 159: loss/train = 0.04395968094468117, logprobs/train = tensor([[-1.0039, -1.0479],
        [-1.0371, -0.9219]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.503799573285505e-06
Epoch 0, Step 175: loss/train = 0.050062380731105804, logprobs/train = tensor([[-0.9703, -1.5940],
        [-0.9376, -1.5111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.689966878388077e-06
Epoch 0, Step 191: loss/train = 0.046107035130262375, logprobs/train = tensor([[-1.0624, -1.2333],
        [-1.0951, -1.2225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.292219717986882e-06
