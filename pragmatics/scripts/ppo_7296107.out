[2024-02-19 09:07:08,373][root][INFO] - beta: 0.01
[2024-02-19 09:07:08,373][root][INFO] - temperature: 1
[2024-02-19 09:07:08,373][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 10000 training examples...
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.01-temp-1 after each epoch.
Epoch 0, Step 15: loss/train = 0.04351605474948883, logprobs/train = tensor([[-0.8196, -0.9555],
        [-0.8241, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.04332902282476425, logprobs/train = tensor([[-1.2200, -1.2428],
        [-1.2477, -1.1750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.046370428055524826, logprobs/train = tensor([[-1.3692, -1.1574],
        [-1.3665, -1.1021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.213958163745701e-06
Epoch 0, Step 63: loss/train = 0.04506981372833252, logprobs/train = tensor([[-1.1974, -1.2022],
        [-1.1672, -1.1073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9732027542195283e-06
Epoch 0, Step 79: loss/train = 0.04342862218618393, logprobs/train = tensor([[-0.8091, -0.8658],
        [-0.8188, -0.7790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.431782821891829e-07
Epoch 0, Step 95: loss/train = 0.04331377148628235, logprobs/train = tensor([[-0.8973, -1.0712],
        [-0.9348, -0.9600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.62507773085963e-07
Epoch 0, Step 111: loss/train = 0.04382738471031189, logprobs/train = tensor([[-0.8006, -0.7956],
        [-0.8196, -0.7799]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.3851287045981735e-07
Epoch 0, Step 127: loss/train = 0.04379041865468025, logprobs/train = tensor([[-1.0690, -0.9253],
        [-1.1216, -0.8727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7339407349936664e-06
Epoch 0, Step 143: loss/train = 0.043622054159641266, logprobs/train = tensor([[-0.8526, -0.9075],
        [-0.8004, -0.8727]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.356107870582491e-06
Epoch 0, Step 159: loss/train = 0.04395968094468117, logprobs/train = tensor([[-1.0039, -1.0479],
        [-1.0371, -0.9219]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.503799573285505e-06
Epoch 0, Step 175: loss/train = 0.050062380731105804, logprobs/train = tensor([[-0.9703, -1.5940],
        [-0.9376, -1.5111]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.689966878388077e-06
Epoch 0, Step 191: loss/train = 0.046107035130262375, logprobs/train = tensor([[-1.0624, -1.2333],
        [-1.0951, -1.2225]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.292219717986882e-06
Epoch 0, Step 207: loss/train = 0.04429211467504501, logprobs/train = tensor([[-0.9504, -1.0472],
        [-0.9334, -0.9984]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.6765883376356214e-07
Epoch 0, Step 223: loss/train = 0.04493569955229759, logprobs/train = tensor([[-0.7742, -1.1023],
        [-0.8063, -1.0429]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.29634552448988e-07
Epoch 0, Step 239: loss/train = 0.04397689923644066, logprobs/train = tensor([[-0.7242, -0.9555],
        [-0.7412, -0.9308]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.5374062058981508e-06
Epoch 0, Step 255: loss/train = 0.04419127106666565, logprobs/train = tensor([[-0.9922, -0.7770],
        [-0.9913, -0.7529]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3639019016409293e-06
Epoch 0, Step 271: loss/train = 0.04660039767622948, logprobs/train = tensor([[-0.8872, -1.3766],
        [-0.9020, -1.2732]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.32057446334511e-06
Epoch 0, Step 287: loss/train = 0.043914664536714554, logprobs/train = tensor([[-0.7373, -0.7948],
        [-0.7510, -0.7286]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.913039108738303e-06
Epoch 0, Step 303: loss/train = 0.04841790348291397, logprobs/train = tensor([[-1.2598, -1.6923],
        [-1.2312, -1.4699]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.106783646624535e-05
Epoch 0, Step 319: loss/train = 0.0477026104927063, logprobs/train = tensor([[-0.8828, -1.5934],
        [-0.9003, -1.4396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0313757229596376e-05
Epoch 0, Step 335: loss/train = 0.043833281844854355, logprobs/train = tensor([[-0.7569, -0.8107],
        [-0.7557, -0.7790]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.1888245100853965e-06
Epoch 0, Step 351: loss/train = 0.04432158172130585, logprobs/train = tensor([[-0.8732, -0.8952],
        [-0.8981, -0.8616]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.280072506750003e-06
Epoch 0, Step 367: loss/train = 0.04860381782054901, logprobs/train = tensor([[-0.8201, -1.4909],
        [-0.8703, -1.3640]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.073965389281511e-05
Epoch 0, Step 383: loss/train = 0.04782244563102722, logprobs/train = tensor([[-1.0667, -1.5267],
        [-1.1366, -1.4098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.759008879773319e-05
Epoch 0, Step 399: loss/train = 0.04394064471125603, logprobs/train = tensor([[-1.2477, -1.4569],
        [-1.2875, -1.3895]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.7256915422622114e-05
Epoch 0, Step 415: loss/train = 0.04744197800755501, logprobs/train = tensor([[-0.8801, -1.5151],
        [-0.9147, -1.5064]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002542404690757394
Epoch 0, Step 431: loss/train = 0.043197087943553925, logprobs/train = tensor([[-0.8582, -0.9558],
        [-1.1127, -0.9105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00024108632351271808
Epoch 0, Step 447: loss/train = 0.044269755482673645, logprobs/train = tensor([[-0.9183, -1.4790],
        [-1.3293, -1.4254]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005169439828023314
Epoch 0, Step 463: loss/train = 0.04457007721066475, logprobs/train = tensor([[-0.8142, -1.2128],
        [-0.8156, -1.1284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011767108226194978
Epoch 0, Step 479: loss/train = 0.04564119130373001, logprobs/train = tensor([[-1.1430, -0.8393],
        [-1.1675, -0.8038]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002929977490566671
Epoch 0, Step 495: loss/train = 0.0460568368434906, logprobs/train = tensor([[-1.0004, -0.6349],
        [-1.0638, -0.6520]], device='cuda:0', grad_fn=<DivBackward0>), KL = 9.096780559048057e-05
Epoch 0, Step 511: loss/train = 0.049113910645246506, logprobs/train = tensor([[-0.6530, -1.2181],
        [-0.6961, -1.1290]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00038531579775735736
Epoch 0, Step 527: loss/train = 0.04643087461590767, logprobs/train = tensor([[-1.1864, -0.9269],
        [-1.2128, -0.9536]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.725054794922471e-05
Epoch 0, Step 543: loss/train = 0.044356100261211395, logprobs/train = tensor([[-1.0917, -0.8021],
        [-1.1737, -0.8021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.427447417285293e-05
Epoch 0, Step 559: loss/train = 0.04438400641083717, logprobs/train = tensor([[-0.8325, -0.8457],
        [-0.9474, -0.8405]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0001269119675271213
Epoch 0, Step 575: loss/train = 0.043467823415994644, logprobs/train = tensor([[-0.7248, -0.7607],
        [-0.7425, -0.7487]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.0432863645255566e-05
Epoch 0, Step 591: loss/train = 0.04387292638421059, logprobs/train = tensor([[-0.8247, -0.9647],
        [-0.9860, -1.0780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007471595890820026
Epoch 0, Step 607: loss/train = 0.04391903802752495, logprobs/train = tensor([[-0.9390, -0.8021],
        [-0.9903, -0.7973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.7223599570570514e-05
Epoch 0, Step 623: loss/train = 0.043245747685432434, logprobs/train = tensor([[-0.7396, -0.7722],
        [-0.8595, -0.7771]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00023253611288964748
Epoch 0, Step 639: loss/train = 0.04624714329838753, logprobs/train = tensor([[-1.3112, -1.0091],
        [-1.4313, -1.0104]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002860166714526713
Epoch 0, Step 655: loss/train = 0.04337500408291817, logprobs/train = tensor([[-0.7705, -0.7309],
        [-0.8237, -0.7719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00013307509652804583
Epoch 0, Step 671: loss/train = 0.04408919811248779, logprobs/train = tensor([[-0.7173, -0.8219],
        [-0.8063, -0.9141]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004744370002299547
Epoch 0, Step 687: loss/train = 0.04349697381258011, logprobs/train = tensor([[-0.9867, -1.0688],
        [-1.0798, -1.0335]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007562743849121034
Epoch 0, Step 703: loss/train = 0.04402050003409386, logprobs/train = tensor([[-0.7929, -0.6503],
        [-0.9090, -0.6401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003906132187694311
Epoch 0, Step 719: loss/train = 0.04683390632271767, logprobs/train = tensor([[-0.6722, -0.9998],
        [-0.7043, -1.0098]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018993265985045582
Epoch 0, Step 735: loss/train = 0.04344428330659866, logprobs/train = tensor([[-0.9179, -0.8665],
        [-0.9750, -0.8752]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00015146506484597921
Epoch 0, Step 751: loss/train = 0.04367232322692871, logprobs/train = tensor([[-0.6388, -0.6137],
        [-0.7496, -0.6339]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0010214272188022733
Epoch 0, Step 767: loss/train = 0.04592973366379738, logprobs/train = tensor([[-0.8174, -0.5715],
        [-1.0345, -0.6230]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014878642978146672
Epoch 0, Step 783: loss/train = 0.04390903189778328, logprobs/train = tensor([[-1.0200, -0.8345],
        [-1.1404, -0.8848]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011043376289308071
Epoch 0, Step 799: loss/train = 0.043938763439655304, logprobs/train = tensor([[-0.8576, -0.8869],
        [-0.8573, -0.8672]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.412425965303555e-05
Epoch 0, Step 815: loss/train = 0.043492402881383896, logprobs/train = tensor([[-0.6963, -0.8768],
        [-0.7391, -0.8523]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015581815969198942
Epoch 0, Step 831: loss/train = 0.044038720428943634, logprobs/train = tensor([[-0.7518, -0.6589],
        [-0.8336, -0.7183]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009786905720829964
Epoch 0, Step 847: loss/train = 0.04394690319895744, logprobs/train = tensor([[-0.7520, -0.6208],
        [-0.8129, -0.6564]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.765938840340823e-05
Epoch 0, Step 863: loss/train = 0.04385599493980408, logprobs/train = tensor([[-0.9177, -0.8526],
        [-1.0147, -0.8519]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.300319076515734e-05
Epoch 0, Step 879: loss/train = 0.044145166873931885, logprobs/train = tensor([[-0.5502, -0.8169],
        [-0.6132, -0.8182]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00022103480296209455
Epoch 0, Step 895: loss/train = 0.04340603947639465, logprobs/train = tensor([[-0.6684, -0.7325],
        [-0.7364, -0.7660]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014937245287001133
Epoch 0, Step 911: loss/train = 0.04339715465903282, logprobs/train = tensor([[-0.6684, -0.6380],
        [-0.7101, -0.6269]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.890596316428855e-05
Epoch 0, Step 927: loss/train = 0.04374519735574722, logprobs/train = tensor([[-0.7759, -0.7105],
        [-0.8096, -0.7136]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007732221274636686
Epoch 0, Step 943: loss/train = 0.044181421399116516, logprobs/train = tensor([[-0.6880, -0.6520],
        [-0.8157, -0.6600]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0011025492567569017
Epoch 0, Step 959: loss/train = 0.04373984411358833, logprobs/train = tensor([[-0.6496, -0.5531],
        [-0.7622, -0.5164]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005111973732709885
Epoch 0, Step 975: loss/train = 0.043648071587085724, logprobs/train = tensor([[-0.7924, -0.8769],
        [-0.9142, -0.9157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004002406494691968
Epoch 0, Step 991: loss/train = 0.04340934753417969, logprobs/train = tensor([[-0.4787, -0.5500],
        [-0.5446, -0.6021]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.309010596945882e-05
Epoch 0, Step 1007: loss/train = 0.045385897159576416, logprobs/train = tensor([[-0.6456, -1.0310],
        [-0.6985, -0.8105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0034261862747371197
Epoch 0, Step 1023: loss/train = 0.04418521001935005, logprobs/train = tensor([[-0.4881, -1.0072],
        [-0.5680, -0.7566]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004478796385228634
Epoch 0, Step 1039: loss/train = 0.043371353298425674, logprobs/train = tensor([[-0.5914, -0.5775],
        [-0.6762, -0.6105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007537479395978153
Epoch 0, Step 1055: loss/train = 0.043410468846559525, logprobs/train = tensor([[-0.4929, -0.7424],
        [-0.5816, -0.6614]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0025484745856374502
Epoch 0, Step 1071: loss/train = 0.04334547743201256, logprobs/train = tensor([[-0.5530, -0.6722],
        [-0.6466, -0.6584]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0008082896238192916
Epoch 0, Step 1087: loss/train = 0.04329961538314819, logprobs/train = tensor([[-0.5773, -0.6482],
        [-0.6534, -0.5350]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009840696584433317
Epoch 0, Step 1103: loss/train = 0.04335004463791847, logprobs/train = tensor([[-0.4773, -0.5542],
        [-0.5005, -0.5187]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004071089788340032
Epoch 0, Step 1119: loss/train = 0.04362577572464943, logprobs/train = tensor([[-0.5730, -0.6421],
        [-0.6696, -0.6242]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009205423993989825
Epoch 0, Step 1135: loss/train = 0.04333759844303131, logprobs/train = tensor([[-0.5253, -0.5735],
        [-0.5544, -0.5396]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018022849690169096
Epoch 0, Step 1151: loss/train = 0.043949104845523834, logprobs/train = tensor([[-0.4627, -0.8020],
        [-0.6753, -0.8307]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0019854879938066006
Epoch 0, Step 1167: loss/train = 0.04347147420048714, logprobs/train = tensor([[-0.5563, -0.7487],
        [-0.5950, -0.6550]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0004540008958429098
Epoch 0, Step 1183: loss/train = 0.0559147372841835, logprobs/train = tensor([[-0.6675, -1.5179],
        [-0.7733, -1.2874]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005206423811614513
Epoch 0, Step 1199: loss/train = 0.04353444278240204, logprobs/train = tensor([[-0.6543, -0.6665],
        [-0.8847, -0.6109]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001798069104552269
Epoch 0, Step 1215: loss/train = 0.042907994240522385, logprobs/train = tensor([[-0.5536, -0.8039],
        [-0.6105, -0.5266]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015060587786138058
Epoch 0, Step 1231: loss/train = 0.043438512831926346, logprobs/train = tensor([[-0.5253, -0.5137],
        [-0.6063, -0.4066]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00424539390951395
Epoch 0, Step 1247: loss/train = 0.04356589540839195, logprobs/train = tensor([[-0.5209, -0.5018],
        [-0.6891, -0.3546]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006878140848129988
Epoch 0, Step 1263: loss/train = 0.04355041682720184, logprobs/train = tensor([[-0.6732, -0.6528],
        [-0.8183, -0.6288]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0022725805174559355
Epoch 0, Step 1279: loss/train = 0.04445584863424301, logprobs/train = tensor([[-0.6964, -1.0263],
        [-0.7773, -0.5812]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.016584178432822227
Epoch 0, Step 1295: loss/train = 0.04325968772172928, logprobs/train = tensor([[-0.6020, -0.6531],
        [-0.8723, -0.6929]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0033525151666253805
Epoch 0, Step 1311: loss/train = 0.043162934482097626, logprobs/train = tensor([[-0.4988, -0.7973],
        [-0.5718, -0.5608]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007154223509132862
Epoch 0, Step 1327: loss/train = 0.042736031115055084, logprobs/train = tensor([[-0.7327, -0.9467],
        [-0.9782, -0.6000]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.023320749402046204
Epoch 0, Step 1343: loss/train = 0.043124135583639145, logprobs/train = tensor([[-0.5860, -0.6421],
        [-0.8879, -0.7105]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.011629627086222172
Epoch 0, Step 1359: loss/train = 0.04151241108775139, logprobs/train = tensor([[-0.5373, -1.1981],
        [-0.8067, -0.5128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04409007355570793
Epoch 0, Step 1375: loss/train = 0.04238357022404671, logprobs/train = tensor([[-0.7780, -1.2066],
        [-0.8466, -0.5904]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.028779173269867897
Epoch 0, Step 1391: loss/train = 0.04232830926775932, logprobs/train = tensor([[-0.6202, -0.9886],
        [-0.7861, -0.6014]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.04012206569314003
Epoch 0, Step 1407: loss/train = 0.03919078782200813, logprobs/train = tensor([[-0.6993, -1.6952],
        [-1.0182, -0.6078]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.142377108335495
Epoch 0, Step 1423: loss/train = 0.0434531606733799, logprobs/train = tensor([[-0.9017, -0.9168],
        [-0.8526, -0.8121]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006178163457661867
Epoch 0, Step 1439: loss/train = 0.040402933955192566, logprobs/train = tensor([[-0.8394, -1.4271],
        [-1.2543, -1.0692]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.10642562061548233
Epoch 0, Step 1455: loss/train = 0.03469999134540558, logprobs/train = tensor([[-0.9452, -1.9725],
        [-2.1339, -0.8152]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2682877480983734
Epoch 0, Step 1471: loss/train = 0.03805042430758476, logprobs/train = tensor([[-1.0062, -1.8301],
        [-1.5603, -0.8664]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.16512593626976013
Epoch 0, Step 1487: loss/train = 0.036261312663555145, logprobs/train = tensor([[-1.1157, -2.2070],
        [-1.8444, -1.2426]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.2615106701850891
Epoch 0, Step 1503: loss/train = 0.03949498385190964, logprobs/train = tensor([[-1.2924, -2.6420],
        [-1.4369, -0.8678]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.24664458632469177
Epoch 0, Step 1519: loss/train = 0.04119711369276047, logprobs/train = tensor([[-2.1514, -5.0735],
        [-1.1813, -1.2353]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3504452109336853
Epoch 0, Step 1535: loss/train = 0.04399678111076355, logprobs/train = tensor([[-1.1232, -1.0655],
        [-4.7379, -5.6868]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.11203674972057343
Epoch 0, Step 1551: loss/train = 0.02221357449889183, logprobs/train = tensor([[-1.5136, -4.8489],
        [-4.5074, -2.2744]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7053160667419434
Epoch 0, Step 1567: loss/train = 0.022932039573788643, logprobs/train = tensor([[-2.2123, -3.7737],
        [-4.8262, -1.8662]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6924181580543518
Epoch 0, Step 1583: loss/train = 0.08056686818599701, logprobs/train = tensor([[-2.6646, -7.6698],
        [-5.4942, -5.0364]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6058638691902161
Epoch 0, Step 1599: loss/train = 0.02576947584748268, logprobs/train = tensor([[-1.9104, -6.3742],
        [-3.3608, -1.8443]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7950295209884644
Epoch 0, Step 1615: loss/train = 0.035007722675800323, logprobs/train = tensor([[-4.5071, -6.4968],
        [-3.9574, -3.1324]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3553313612937927
Epoch 0, Step 1631: loss/train = 0.0564592145383358, logprobs/train = tensor([[-3.3077, -4.8507],
        [-3.8394, -4.3669]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.09507817029953003
Epoch 0, Step 1647: loss/train = 0.03450257331132889, logprobs/train = tensor([[-5.0480, -5.9358],
        [-4.8329, -2.6498]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5587279200553894
Epoch 0, Step 1663: loss/train = 0.019783763214945793, logprobs/train = tensor([[-3.4405, -7.3940],
        [-8.0908, -3.7181]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9873623847961426
Epoch 0, Step 1679: loss/train = 0.01788349077105522, logprobs/train = tensor([[ -6.7632, -14.0096],
        [ -5.3685,  -2.0034]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9170956611633301
Epoch 0, Step 1695: loss/train = 0.009136220440268517, logprobs/train = tensor([[ -3.6389, -12.5152],
        [ -6.7216,  -2.3157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.110370397567749
Epoch 0, Step 1711: loss/train = 0.013640428893268108, logprobs/train = tensor([[-3.0903, -8.4490],
        [-6.1815, -3.2542]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8784236907958984
Epoch 0, Step 1727: loss/train = 0.04479299485683441, logprobs/train = tensor([[ -3.2642, -11.8340],
        [ -6.0460,  -5.5062]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6524990797042847
Epoch 0, Step 1743: loss/train = 0.03705977648496628, logprobs/train = tensor([[-6.1503, -8.1957],
        [-4.0438, -3.2153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.38542646169662476
Epoch 0, Step 1759: loss/train = 0.0128700602799654, logprobs/train = tensor([[ -8.1665, -13.6463],
        [ -7.6484,  -3.5414]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9646425247192383
Epoch 0, Step 1775: loss/train = 0.03391483426094055, logprobs/train = tensor([[-11.9906, -16.9092],
        [ -7.6939,  -4.4128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6110473871231079
Epoch 0, Step 1791: loss/train = 0.0012148653622716665, logprobs/train = tensor([[ -5.7870, -20.8697],
        [-11.9053,  -3.6739]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.238376259803772
Epoch 0, Step 1807: loss/train = 0.022186536341905594, logprobs/train = tensor([[ -8.2864, -11.5424],
        [-11.2938,  -7.5041]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6959711313247681
Epoch 0, Step 1823: loss/train = 0.008897489868104458, logprobs/train = tensor([[ -3.9270, -14.0333],
        [-13.4202,  -8.0233]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0848606824874878
Epoch 0, Step 1839: loss/train = 0.019330032169818878, logprobs/train = tensor([[ -4.8887,  -9.0747],
        [-12.9285,  -9.4418]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8603125810623169
Epoch 0, Step 1855: loss/train = 0.044332414865493774, logprobs/train = tensor([[ -9.5655,  -9.3910],
        [-11.1522,  -7.2741]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7633429765701294
Epoch 0, Step 1871: loss/train = 0.037261951714754105, logprobs/train = tensor([[ -5.0854,  -9.8120],
        [-13.8188, -12.0448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6029924750328064
Epoch 0, Step 1887: loss/train = 0.002080146688967943, logprobs/train = tensor([[ -7.1875, -18.4859],
        [-11.4805,  -4.9629]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2784416675567627
Epoch 0, Step 1903: loss/train = 0.010332334786653519, logprobs/train = tensor([[ -6.1323,  -9.2932],
        [-10.2634, -12.3851]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.145312786102295
Epoch 0, Step 1919: loss/train = 0.019358467310667038, logprobs/train = tensor([[-11.6122, -17.8860],
        [ -8.3055,  -5.0119]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8551235198974609
Epoch 0, Step 1935: loss/train = 0.033005088567733765, logprobs/train = tensor([[ -6.8365, -12.9874],
        [ -9.6550,  -6.3264]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8618890047073364
Epoch 0, Step 1951: loss/train = 0.01456376537680626, logprobs/train = tensor([[-11.5408, -18.4336],
        [ -7.7405,  -3.4761]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8860299587249756
Epoch 0, Step 1967: loss/train = 0.02955631911754608, logprobs/train = tensor([[-12.2566, -16.7020],
        [-10.5648,  -5.7611]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5948206186294556
Epoch 0, Step 1983: loss/train = 0.007218886166810989, logprobs/train = tensor([[ -8.4473, -20.7905],
        [-14.4266,  -7.1803]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1923463344573975
Epoch 0, Step 1999: loss/train = 0.012004535645246506, logprobs/train = tensor([[-10.8936, -18.1710],
        [-14.4668,  -9.8500]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9355384111404419
Epoch 0, Step 2015: loss/train = 0.03564350679516792, logprobs/train = tensor([[-15.4809, -15.3496],
        [ -9.9453,  -7.9377]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.6286916136741638
Epoch 0, Step 2031: loss/train = 0.04892786964774132, logprobs/train = tensor([[-10.9736, -17.5533],
        [-16.1935, -12.1005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7318522334098816
Epoch 0, Step 2047: loss/train = 0.007880925200879574, logprobs/train = tensor([[ -9.6096, -13.3099],
        [-19.1175, -14.2359]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.153550624847412
Epoch 0, Step 2063: loss/train = 0.01922621577978134, logprobs/train = tensor([[-16.6199, -22.8743],
        [-10.8743,  -5.5267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7241653203964233
Epoch 0, Step 2079: loss/train = 0.02517794817686081, logprobs/train = tensor([[-11.7146, -13.4030],
        [-12.2099, -10.1539]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.525536298751831
Epoch 0, Step 2095: loss/train = 0.011832434684038162, logprobs/train = tensor([[-13.6410, -25.0767],
        [-12.1964,  -3.9797]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9940956830978394
Epoch 0, Step 2111: loss/train = 0.009402568452060223, logprobs/train = tensor([[-12.3753, -21.8954],
        [ -8.8967,  -3.3659]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0548179149627686
Epoch 0, Step 2127: loss/train = 0.03839918226003647, logprobs/train = tensor([[-11.7172, -12.7712],
        [-13.7926, -14.2147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.33645719289779663
Epoch 0, Step 2143: loss/train = 0.025192543864250183, logprobs/train = tensor([[-14.9269, -19.0384],
        [-15.9314, -11.3060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.8219487071037292
Epoch 0, Step 2159: loss/train = 0.019683677703142166, logprobs/train = tensor([[-12.8839, -19.2918],
        [-25.7005, -18.2861]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7459852695465088
Epoch 0, Step 2175: loss/train = 0.03419485688209534, logprobs/train = tensor([[-14.0202, -17.2752],
        [-28.1342, -26.5157]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.5059844255447388
Epoch 0, Step 2191: loss/train = 0.030454915016889572, logprobs/train = tensor([[-17.8469, -21.5425],
        [-27.2497, -25.6843]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.34687021374702454
Epoch 0, Step 2207: loss/train = 0.07067329436540604, logprobs/train = tensor([[-23.9970, -24.7965],
        [-20.1432, -17.2379]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.3400135040283203
Epoch 0, Step 2223: loss/train = 0.011726190336048603, logprobs/train = tensor([[-19.4558, -26.9295],
        [-26.6940, -16.5712]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9516243934631348
Epoch 0, Step 2239: loss/train = 0.00108197215013206, logprobs/train = tensor([[-15.0308, -26.6591],
        [-27.3890,  -7.9914]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0981791019439697
Epoch 0, Step 2255: loss/train = 0.01204830314964056, logprobs/train = tensor([[-14.6885, -21.6717],
        [-24.8513, -14.4173]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.9758792519569397
Epoch 0, Step 2271: loss/train = 0.011835048906505108, logprobs/train = tensor([[-10.5323, -18.1898],
        [-31.6117, -18.4199]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0064971446990967
Epoch 0, Step 2287: loss/train = 0.007112852763384581, logprobs/train = tensor([[-11.7319, -18.0018],
        [-24.5618,  -8.9267]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.263463020324707
Epoch 0, Step 2303: loss/train = 0.010186741128563881, logprobs/train = tensor([[-11.2375, -16.4382],
        [-30.3466, -14.0003]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0661964416503906
Epoch 0, Step 2319: loss/train = 0.025075826793909073, logprobs/train = tensor([[-14.4877, -16.1985],
        [-28.9484, -12.9151]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2727396488189697
Epoch 0, Step 2335: loss/train = 0.0011129657505080104, logprobs/train = tensor([[-13.3342, -22.2684],
        [-33.9542, -13.0227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3438862562179565
Epoch 0, Step 2351: loss/train = 0.01243430096656084, logprobs/train = tensor([[-13.8238, -17.8836],
        [-31.9708, -22.4208]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.000694751739502
Epoch 0, Step 2367: loss/train = 0.009880821220576763, logprobs/train = tensor([[-22.7770, -31.8519],
        [-33.2205, -15.7195]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.2721006870269775
Epoch 0, Step 2374: loss/train = 0.06779179722070694, logprobs/train = tensor([[-37.2196, -37.8185],
        [-27.0337, -20.2731]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.7964086532592773
loss/eval: 0.3403623402118683
[2024-02-19 10:17:23,591][root][INFO] - beta: 0.1
[2024-02-19 10:17:23,592][root][INFO] - temperature: 1
[2024-02-19 10:17:23,592][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
tokenizing 10000 training examples...
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-temp-1 after each epoch.
Epoch 0, Step 15: loss/train = 0.04351605474948883, logprobs/train = tensor([[-0.8196, -0.9555],
        [-0.8241, -0.9065]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 31: loss/train = 0.04332902282476425, logprobs/train = tensor([[-1.2200, -1.2428],
        [-1.2477, -1.1750]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
Epoch 0, Step 47: loss/train = 0.04636668041348457, logprobs/train = tensor([[-1.3679, -1.1554],
        [-1.3646, -1.1008]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.236746124457568e-06
Epoch 0, Step 63: loss/train = 0.04504905268549919, logprobs/train = tensor([[-1.1951, -1.1993],
        [-1.1687, -1.1047]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.9223338060546666e-06
Epoch 0, Step 79: loss/train = 0.04342884197831154, logprobs/train = tensor([[-0.8095, -0.8663],
        [-0.8176, -0.7785]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.556999091524631e-07
Epoch 0, Step 95: loss/train = 0.04330907016992569, logprobs/train = tensor([[-0.8993, -1.0760],
        [-0.9345, -0.9582]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3206627045292407e-06
Epoch 0, Step 111: loss/train = 0.04381747171282768, logprobs/train = tensor([[-0.7995, -0.7953],
        [-0.8197, -0.7801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.3239828149380628e-06
Epoch 0, Step 127: loss/train = 0.04379782825708389, logprobs/train = tensor([[-1.0678, -0.9248],
        [-1.1233, -0.8715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1430474842200056e-06
Epoch 0, Step 143: loss/train = 0.04362579435110092, logprobs/train = tensor([[-0.8528, -0.9069],
        [-0.7997, -0.8715]], device='cuda:0', grad_fn=<DivBackward0>), KL = 7.485732567147352e-08
Epoch 0, Step 159: loss/train = 0.04394681379199028, logprobs/train = tensor([[-1.0082, -1.0505],
        [-1.0329, -0.9194]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.128013875335455e-06
Epoch 0, Step 175: loss/train = 0.050094615668058395, logprobs/train = tensor([[-0.9673, -1.5874],
        [-0.9331, -1.5099]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.354953024536371e-06
Epoch 0, Step 191: loss/train = 0.04608536511659622, logprobs/train = tensor([[-1.0607, -1.2305],
        [-1.0933, -1.2237]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.435417627566494e-06
Epoch 0, Step 207: loss/train = 0.04428188502788544, logprobs/train = tensor([[-0.9511, -1.0441],
        [-0.9360, -0.9995]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.560003384947777e-07
Epoch 0, Step 223: loss/train = 0.04496470093727112, logprobs/train = tensor([[-0.7752, -1.1053],
        [-0.8065, -1.0448]], device='cuda:0', grad_fn=<DivBackward0>), KL = 3.522556653479114e-06
Epoch 0, Step 239: loss/train = 0.04396941885352135, logprobs/train = tensor([[-0.7240, -0.9552],
        [-0.7416, -0.9299]], device='cuda:0', grad_fn=<DivBackward0>), KL = 2.8217598355695372e-06
Epoch 0, Step 255: loss/train = 0.044189855456352234, logprobs/train = tensor([[-0.9914, -0.7768],
        [-0.9895, -0.7509]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.0075746104121208e-07
Epoch 0, Step 271: loss/train = 0.04662768915295601, logprobs/train = tensor([[-0.8880, -1.3784],
        [-0.9022, -1.2765]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.119560566730797e-06
Epoch 0, Step 287: loss/train = 0.04391949623823166, logprobs/train = tensor([[-0.7379, -0.7937],
        [-0.7509, -0.7284]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.135952960699797e-07
Epoch 0, Step 303: loss/train = 0.04843025654554367, logprobs/train = tensor([[-1.2621, -1.6911],
        [-1.2277, -1.4719]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.5089080231264234e-05
Epoch 0, Step 319: loss/train = 0.04771144315600395, logprobs/train = tensor([[-0.8812, -1.5909],
        [-0.9004, -1.4382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.6441263142041862e-05
Epoch 0, Step 335: loss/train = 0.043836429715156555, logprobs/train = tensor([[-0.7557, -0.8111],
        [-0.7571, -0.7809]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.580212357221171e-07
Epoch 0, Step 351: loss/train = 0.04431260749697685, logprobs/train = tensor([[-0.8770, -0.8966],
        [-0.8961, -0.8618]], device='cuda:0', grad_fn=<DivBackward0>), KL = 4.668694600695744e-06
Epoch 0, Step 367: loss/train = 0.04862580448389053, logprobs/train = tensor([[-0.8198, -1.4883],
        [-0.8676, -1.3649]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00011171546066179872
Epoch 0, Step 383: loss/train = 0.04778234288096428, logprobs/train = tensor([[-1.0660, -1.5237],
        [-1.1410, -1.4110]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.016970681026578e-05
Epoch 0, Step 399: loss/train = 0.0439346581697464, logprobs/train = tensor([[-1.2457, -1.4549],
        [-1.2911, -1.3884]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.981331484392285e-05
Epoch 0, Step 415: loss/train = 0.04746757447719574, logprobs/train = tensor([[-0.8806, -1.5208],
        [-0.9159, -1.5095]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002597944112494588
Epoch 0, Step 431: loss/train = 0.043205391615629196, logprobs/train = tensor([[-0.8618, -0.9569],
        [-1.1164, -0.9128]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00024195674632210284
Epoch 0, Step 447: loss/train = 0.04422440379858017, logprobs/train = tensor([[-0.9163, -1.4765],
        [-1.3306, -1.4222]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005015234346501529
Epoch 0, Step 463: loss/train = 0.04458817094564438, logprobs/train = tensor([[-0.8139, -1.2177],
        [-0.8157, -1.1316]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0012423035223037004
Epoch 0, Step 479: loss/train = 0.045653361827135086, logprobs/train = tensor([[-1.1476, -0.8398],
        [-1.1674, -0.8081]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00021133312839083374
Epoch 0, Step 495: loss/train = 0.046047452837228775, logprobs/train = tensor([[-1.0012, -0.6350],
        [-1.0634, -0.6517]], device='cuda:0', grad_fn=<DivBackward0>), KL = 6.756035145372152e-05
Epoch 0, Step 511: loss/train = 0.04917535558342934, logprobs/train = tensor([[-0.6533, -1.2210],
        [-0.6963, -1.1345]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0003502399194985628
Epoch 0, Step 527: loss/train = 0.046433549374341965, logprobs/train = tensor([[-1.1873, -0.9275],
        [-1.2082, -0.9540]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.843113467562944e-05
Epoch 0, Step 543: loss/train = 0.044348571449518204, logprobs/train = tensor([[-1.0908, -0.8035],
        [-1.1737, -0.8024]], device='cuda:0', grad_fn=<DivBackward0>), KL = 8.620406151749194e-05
Epoch 0, Step 559: loss/train = 0.044390350580215454, logprobs/train = tensor([[-0.8324, -0.8428],
        [-0.9468, -0.8406]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00014036393258720636
Epoch 0, Step 575: loss/train = 0.04346732422709465, logprobs/train = tensor([[-0.7258, -0.7606],
        [-0.7409, -0.7514]], device='cuda:0', grad_fn=<DivBackward0>), KL = 5.429964221548289e-05
Epoch 0, Step 591: loss/train = 0.04387086257338524, logprobs/train = tensor([[-0.8238, -0.9619],
        [-0.9860, -1.0780]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007733374368399382
Epoch 0, Step 607: loss/train = 0.04391108825802803, logprobs/train = tensor([[-0.9370, -0.8002],
        [-0.9867, -0.7977]], device='cuda:0', grad_fn=<DivBackward0>), KL = 1.1922107660211623e-05
Epoch 0, Step 623: loss/train = 0.04324132949113846, logprobs/train = tensor([[-0.7386, -0.7721],
        [-0.8599, -0.7736]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00018182030180469155
Epoch 0, Step 639: loss/train = 0.04627781733870506, logprobs/train = tensor([[-1.3093, -1.0072],
        [-1.4338, -1.0060]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0002066769520752132
Epoch 0, Step 655: loss/train = 0.04337708652019501, logprobs/train = tensor([[-0.7710, -0.7313],
        [-0.8223, -0.7704]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00011461914982646704
Epoch 0, Step 671: loss/train = 0.04411274194717407, logprobs/train = tensor([[-0.7157, -0.8216],
        [-0.8082, -0.9147]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004588114097714424
Epoch 0, Step 687: loss/train = 0.04349204897880554, logprobs/train = tensor([[-0.9880, -1.0723],
        [-1.0759, -1.0283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0007966295816004276
Epoch 0, Step 703: loss/train = 0.044004663825035095, logprobs/train = tensor([[-0.7898, -0.6499],
        [-0.9058, -0.6401]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00036483712028712034
