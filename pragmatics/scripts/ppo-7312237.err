[2024-02-27 14:02:26,459] torch.distributed.run: [WARNING] 
[2024-02-27 14:02:26,459] torch.distributed.run: [WARNING] *****************************************
[2024-02-27 14:02:26,459] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-02-27 14:02:26,459] torch.distributed.run: [WARNING] *****************************************
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'config': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: janphilipp-franken. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240227_140230-4army51a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/4army51a
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240227_140230-r333m4bn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/r333m4bn
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240227_140230-i9c1bcgs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/i9c1bcgs
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /sailhome/jphilipp/research_projects/scai-tuning/pragmatics/wandb/run-20240227_140231-4n35pet3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run beta-0.5-per-token-kl
wandb: ‚≠êÔ∏è View project at https://wandb.ai/janphilipp-franken/scai-with-labels
wandb: üöÄ View run at https://wandb.ai/janphilipp-franken/scai-with-labels/runs/4n35pet3
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.85s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.79s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.76s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:05<00:05,  5.80s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.14s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.40s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.39s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.13s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.37s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.16s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.40s/it]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.18s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.12s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.75s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.11s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.06s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.63s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.99s/it]
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  4.69s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.04s/it]
Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:00, ?it/s]Running epoch: 0: 0it [00:01, ?it/s]
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl']
Running epoch: 0: 0it [00:01, ?it/s]
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl']
Running epoch: 0: 0it [00:01, ?it/s]
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl']
Running epoch: 0: 0it [00:01, ?it/s]
Error executing job with overrides: ['ppo.beta=0.5', 'ppo.max_iter=0', 'wandb.name=beta-0.5-per-token-kl', 'training.checkpoint_dir=/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl']
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 428, in _run_batch
    kl_div = kl_divergence_from_logits_per_token(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 504, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 436, in _run_batch
    kl_div = torch.FloatTensor(1.0)
TypeError: new(): data must be a sequence (got float)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 428, in _run_batch
    kl_div = kl_divergence_from_logits_per_token(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 504, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 436, in _run_batch
    kl_div = torch.FloatTensor(1.0)
TypeError: new(): data must be a sequence (got float)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 428, in _run_batch
    kl_div = kl_divergence_from_logits_per_token(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 504, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 436, in _run_batch
    kl_div = torch.FloatTensor(1.0)
TypeError: new(): data must be a sequence (got float)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 428, in _run_batch
    kl_div = kl_divergence_from_logits_per_token(
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/loss_functions.py", line 106, in kl_divergence_from_logits_per_token
    for i in range(logprobs_policy_logits.shape[0]):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 90, in trace_dispatch
    return self.dispatch_line(frame)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/bdb.py", line 115, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/train.py", line 186, in main
    trainer.train()
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 504, in train
    raw_loss, loss, batch_logprobs, kl_div = self._run_batch(batch)
  File "/sailhome/jphilipp/research_projects/scai-tuning/pragmatics/trainers.py", line 436, in _run_batch
    kl_div = torch.FloatTensor(1.0)
TypeError: new(): data must be a sequence (got float)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: - 0.015 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.027 MB uploadedwandb: | 0.031 MB of 0.031 MB uploadedwandb: üöÄ View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/r333m4bn
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzYwNDQ3MQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_140230-r333m4bn/logs
wandb: üöÄ View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/4army51a
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzYwNDQ3MQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_140230-4army51a/logs
wandb: üöÄ View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/i9c1bcgs
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzYwNDQ3MQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_140230-i9c1bcgs/logs
wandb: üöÄ View run beta-0.5-per-token-kl at: https://wandb.ai/janphilipp-franken/scai-with-labels/runs/4n35pet3
wandb: Ô∏è‚ö° View job at https://wandb.ai/janphilipp-franken/scai-with-labels/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MzYwNDQ3MQ==/version_details/v0
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240227_140231-4n35pet3/logs
[2024-02-27 14:04:01,550] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 3985283) of binary: /scr/jphilipp/miniconda3/envs/scai-tuning/bin/python
Traceback (most recent call last):
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scr/jphilipp/miniconda3/envs/scai-tuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-02-27_14:04:01
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3985284)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-02-27_14:04:01
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3985285)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-02-27_14:04:01
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3985286)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-02-27_14:04:01
  host      : cocoflops-hgx-1.stanford.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3985283)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
