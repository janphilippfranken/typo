[2024-02-24 13:05:22,177][root][INFO] - beta: 0.1
[2024-02-24 13:05:22,177][root][INFO] - loss with_labels
[2024-02-24 13:05:22,178][root][INFO] - max_iter: 0
[2024-02-24 13:05:22,178][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-with-labels
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-with-labels after each epoch.
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-with-labels after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-with-labels after each epoch.
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.1-with-labels after each epoch.
Epoch 0, Step 0: train/loss = 0.7008420825004578, train/raw-loss = 0.700703501701355, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001385421957820654
Epoch 0, Step 1: train/loss = 0.6948565244674683, train/raw-loss = 0.6946866512298584, train/logprobs = tensor([[-1.2483, -1.6199],
        [-1.2440, -1.6139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016988443676382303
Epoch 0, Step 2: train/loss = 0.6923662424087524, train/raw-loss = 0.6923112869262695, train/logprobs = tensor([[-1.0892, -2.0869],
        [-1.1206, -2.1103]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005500263068825006
Epoch 0, Step 3: train/loss = 0.6866117715835571, train/raw-loss = 0.6862989068031311, train/logprobs = tensor([[-0.5922, -1.4420],
        [-0.5997, -1.4088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0031291749328374863
Epoch 0, Step 4: train/loss = 0.6822781562805176, train/raw-loss = 0.6821818947792053, train/logprobs = tensor([[-0.9682, -2.2517],
        [-1.0031, -2.2372]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0009627505205571651
Epoch 0, Step 5: train/loss = 0.6994282007217407, train/raw-loss = 0.6991856098175049, train/logprobs = tensor([[-1.2526, -1.8470],
        [-1.2661, -1.8698]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0024260750506073236
Epoch 0, Step 6: train/loss = 0.6909489631652832, train/raw-loss = 0.6907837390899658, train/logprobs = tensor([[-0.9823, -1.5991],
        [-1.0011, -1.5974]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016527380794286728
Epoch 0, Step 7: train/loss = 0.7021964192390442, train/raw-loss = 0.7020244598388672, train/logprobs = tensor([[-1.0630, -1.5507],
        [-1.1077, -1.6226]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0017196990083903074
Epoch 0, Step 8: train/loss = 0.6932111978530884, train/raw-loss = 0.6930682063102722, train/logprobs = tensor([[-0.8023, -1.7333],
        [-0.7871, -1.7088]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014302500057965517
Epoch 0, Step 9: train/loss = 0.6969835758209229, train/raw-loss = 0.6962793469429016, train/logprobs = tensor([[-1.0835, -1.7181],
        [-1.0813, -1.6973]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007042370270937681
Epoch 0, Step 10: train/loss = 0.6985957622528076, train/raw-loss = 0.6965494155883789, train/logprobs = tensor([[-1.1971, -2.5060],
        [-1.2764, -2.5027]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020463809370994568
Epoch 0, Step 11: train/loss = 0.6863366961479187, train/raw-loss = 0.6862969398498535, train/logprobs = tensor([[-0.7466, -1.5640],
        [-0.7228, -1.5107]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00039786301204003394
Epoch 0, Step 12: train/loss = 0.7163760662078857, train/raw-loss = 0.7148674726486206, train/logprobs = tensor([[-0.7433, -1.8789],
        [-0.7825, -1.8867]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015085747465491295
Epoch 0, Step 13: train/loss = 0.6872075796127319, train/raw-loss = 0.6869643926620483, train/logprobs = tensor([[-0.7519, -1.6611],
        [-0.7745, -1.6382]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.002432220848277211
Epoch 0, Step 14: train/loss = 0.6850913763046265, train/raw-loss = 0.684917688369751, train/logprobs = tensor([[-1.1222, -1.7514],
        [-1.1230, -1.7108]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0017367040272802114
Epoch 0, Step 15: train/loss = 0.7039209008216858, train/raw-loss = 0.7035808563232422, train/logprobs = tensor([[-1.0901, -1.5558],
        [-1.0901, -1.5823]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0034001865424215794
Epoch 0, Step 16: train/loss = 0.7082311511039734, train/raw-loss = 0.705668568611145, train/logprobs = tensor([[-0.9616, -2.0296],
        [-0.9940, -1.9801]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.025626003742218018
Epoch 0, Step 17: train/loss = 0.6926672458648682, train/raw-loss = 0.6924867033958435, train/logprobs = tensor([[-1.0904, -1.5065],
        [-1.0878, -1.4924]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0018046624027192593
Epoch 0, Step 18: train/loss = 0.6931636929512024, train/raw-loss = 0.6928467154502869, train/logprobs = tensor([[-1.1498, -1.6923],
        [-1.1412, -1.6685]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0031702592968940735
Epoch 0, Step 19: train/loss = 0.6977421045303345, train/raw-loss = 0.6973589658737183, train/logprobs = tensor([[-0.7575, -1.3558],
        [-0.7511, -1.3457]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0038300659507513046
Epoch 0, Step 20: train/loss = 0.7534458637237549, train/raw-loss = 0.751427173614502, train/logprobs = tensor([[-0.6982, -1.6722],
        [-0.7384, -1.6710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.020186442881822586
Epoch 0, Step 21: train/loss = 0.6943658590316772, train/raw-loss = 0.6941919326782227, train/logprobs = tensor([[-0.9935, -1.2997],
        [-0.9919, -1.2949]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0017392875161021948
Epoch 0, Step 22: train/loss = 0.6882961988449097, train/raw-loss = 0.6880555152893066, train/logprobs = tensor([[-1.1048, -1.2115],
        [-1.1166, -1.1903]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.002407266292721033
Epoch 0, Step 23: train/loss = 0.7264970541000366, train/raw-loss = 0.7249387502670288, train/logprobs = tensor([[-1.1101, -1.9440],
        [-1.1279, -2.0090]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.015583477914333344
Epoch 0, Step 24: train/loss = 0.6901059150695801, train/raw-loss = 0.6900442838668823, train/logprobs = tensor([[-0.9510, -2.1814],
        [-0.9490, -2.1607]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006165502709336579
Epoch 0, Step 25: train/loss = 0.6854063272476196, train/raw-loss = 0.6853428483009338, train/logprobs = tensor([[-0.8277, -2.2563],
        [-0.8247, -2.2167]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006352229975163937
Epoch 0, Step 26: train/loss = 0.6983336210250854, train/raw-loss = 0.6981753706932068, train/logprobs = tensor([[-0.5759, -2.1066],
        [-0.6087, -2.1473]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0015831212513148785
Epoch 0, Step 27: train/loss = 0.6974491477012634, train/raw-loss = 0.6973809003829956, train/logprobs = tensor([[-1.0402, -2.5564],
        [-1.0804, -2.5702]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006825224263593554
Epoch 0, Step 28: train/loss = 0.6982411742210388, train/raw-loss = 0.6975324153900146, train/logprobs = tensor([[-0.8392, -1.7205],
        [-0.8369, -1.6912]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007087648846209049
Epoch 0, Step 29: train/loss = 0.6935246586799622, train/raw-loss = 0.6934842467308044, train/logprobs = tensor([[-0.5879, -1.8914],
        [-0.5988, -1.9005]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.00040351462666876614
Epoch 0, Step 30: train/loss = 0.6920292973518372, train/raw-loss = 0.6918355822563171, train/logprobs = tensor([[-0.8561, -1.1920],
        [-0.8388, -1.1585]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001937028020620346
Epoch 0, Step 31: train/loss = 0.6731337904930115, train/raw-loss = 0.672492265701294, train/logprobs = tensor([[-1.0550, -2.2670],
        [-1.0818, -2.1798]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006415537558495998
Epoch 0, Step 32: train/loss = 0.68923020362854, train/raw-loss = 0.6886965036392212, train/logprobs = tensor([[-0.9240, -1.9170],
        [-0.9467, -1.8934]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005336553789675236
Epoch 0, Step 33: train/loss = 0.7014134526252747, train/raw-loss = 0.7009933590888977, train/logprobs = tensor([[-0.8779, -1.8828],
        [-0.8573, -1.8740]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.004200874827802181
Epoch 0, Step 34: train/loss = 0.6970118284225464, train/raw-loss = 0.6958221793174744, train/logprobs = tensor([[-1.0511, -1.5128],
        [-1.0826, -1.5036]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.011896110139787197
Epoch 0, Step 35: train/loss = 0.7053303122520447, train/raw-loss = 0.7038596868515015, train/logprobs = tensor([[-0.9902, -1.8427],
        [-0.9916, -1.8163]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.014705857262015343
Epoch 0, Step 36: train/loss = 0.7010126113891602, train/raw-loss = 0.6996639966964722, train/logprobs = tensor([[-1.2729, -1.4568],
        [-1.2776, -1.4303]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.013486114330589771
Epoch 0, Step 37: train/loss = 0.6882570385932922, train/raw-loss = 0.6881989240646362, train/logprobs = tensor([[-0.6132, -1.4763],
        [-0.6105, -1.4501]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005811454029753804
Epoch 0, Step 38: train/loss = 0.6924845576286316, train/raw-loss = 0.6917117834091187, train/logprobs = tensor([[-0.8017, -1.5622],
        [-0.8207, -1.5360]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007727716118097305
Epoch 0, Step 39: train/loss = 0.6977053880691528, train/raw-loss = 0.6974338889122009, train/logprobs = tensor([[-0.6779, -1.3967],
        [-0.6694, -1.3926]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0027145948261022568
Epoch 0, Step 40: train/loss = 0.6941096782684326, train/raw-loss = 0.6939688920974731, train/logprobs = tensor([[-0.7632, -1.1195],
        [-0.7654, -1.1192]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0014077682280912995
Epoch 0, Step 41: train/loss = 0.6798899173736572, train/raw-loss = 0.6795097589492798, train/logprobs = tensor([[-0.8216, -1.7078],
        [-0.8400, -1.6511]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.003801777958869934
Epoch 0, Step 42: train/loss = 0.694694459438324, train/raw-loss = 0.694367527961731, train/logprobs = tensor([[-0.6560, -1.7110],
        [-0.6654, -1.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0032684793695807457
Epoch 0, Step 43: train/loss = 0.6867050528526306, train/raw-loss = 0.6866177916526794, train/logprobs = tensor([[-1.0934, -2.2801],
        [-1.0771, -2.2323]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0008726525702513754
Epoch 0, Step 44: train/loss = 0.6904557347297668, train/raw-loss = 0.6903976798057556, train/logprobs = tensor([[-0.9415, -1.1740],
        [-0.9243, -1.1435]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0005802650121040642
Epoch 0, Step 45: train/loss = 0.7049940228462219, train/raw-loss = 0.7042211294174194, train/logprobs = tensor([[-0.8766, -1.8393],
        [-0.8959, -1.8327]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.007728645112365484
Epoch 0, Step 46: train/loss = 0.7236996293067932, train/raw-loss = 0.7230007648468018, train/logprobs = tensor([[-1.1262, -2.6505],
        [-1.1680, -2.7227]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.006988189183175564
Epoch 0, Step 47: train/loss = 0.6939867734909058, train/raw-loss = 0.6937203407287598, train/logprobs = tensor([[-0.7199, -1.4910],
        [-0.7111, -1.4710]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0026645706966519356
Epoch 0, Step 48: train/loss = 0.7075241804122925, train/raw-loss = 0.7070211172103882, train/logprobs = tensor([[-1.3350, -1.4101],
        [-1.3527, -1.4624]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005030433181673288
Epoch 0, Step 49: train/loss = 0.6919335126876831, train/raw-loss = 0.6914311051368713, train/logprobs = tensor([[-0.8395, -1.8763],
        [-0.8370, -1.8425]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005024871788918972
Epoch 0, Step 50: train/loss = 0.6909795999526978, train/raw-loss = 0.6909155249595642, train/logprobs = tensor([[-1.0832, -1.3937],
        [-1.0782, -1.3770]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006405673339031637
Epoch 0, Step 51: train/loss = 0.6861550807952881, train/raw-loss = 0.6857802867889404, train/logprobs = tensor([[-0.9913, -1.7751],
        [-0.9906, -1.7283]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0037480113096535206
Epoch 0, Step 52: train/loss = 0.6875786781311035, train/raw-loss = 0.687508761882782, train/logprobs = tensor([[-0.7081, -0.7210],
        [-0.7282, -0.7153]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0006987795932218432
Epoch 0, Step 53: train/loss = 0.7094684839248657, train/raw-loss = 0.7088227868080139, train/logprobs = tensor([[-0.6088, -1.0795],
        [-0.5906, -1.0657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0064574312418699265
Epoch 0, Step 54: train/loss = 0.6830158233642578, train/raw-loss = 0.682507336139679, train/logprobs = tensor([[-1.2230, -1.6885],
        [-1.2433, -1.6452]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.005085480399429798
Epoch 0, Step 55: train/loss = 0.7045963406562805, train/raw-loss = 0.7028936743736267, train/logprobs = tensor([[-1.3000, -1.1052],
        [-1.3418, -1.1073]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.017026763409376144
Epoch 0, Step 56: train/loss = 0.682129979133606, train/raw-loss = 0.6819606423377991, train/logprobs = tensor([[-0.8773, -1.4545],
        [-0.8995, -1.4236]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0016936644678935409
Epoch 0, Step 57: train/loss = 0.6860986948013306, train/raw-loss = 0.6851555109024048, train/logprobs = tensor([[-0.6412, -1.4265],
        [-0.6492, -1.3568]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.009431957267224789
Epoch 0, Step 58: train/loss = 0.6736461520195007, train/raw-loss = 0.6735333204269409, train/logprobs = tensor([[-0.6794, -1.6542],
        [-0.6772, -1.5657]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.001128867152146995
