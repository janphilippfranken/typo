[2024-02-27 13:40:16,883][root][INFO] - beta: 0.5
[2024-02-27 13:40:16,883][root][INFO] - loss with_labels
[2024-02-27 13:40:16,883][root][INFO] - max_iter: 0
[2024-02-27 13:40:16,883][root][INFO] - writing checkpoints to: /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl
clearing gpu cache for all ranks
Model with 7241.732096M params prepared
n helpful: 5000
n harmless: 5000
Loaded model on rank 1
Loaded reference model on rank 1
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 2
Loaded reference model on rank 2
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
tokenized 9500 training examples...
train dataset has 9500 examples.
eval dataset has 500 examples.
Loaded model on rank 0
Loaded reference model on rank 0
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
Loaded model on rank 3
Loaded reference model on rank 3
Writing checkpoints to /scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/checkpoints/ppo-beta-0.5-per-token-kl after each epoch.
RAW KL tensor(0., device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 0: train/loss = 0.6982459425926208, train/raw-loss = 0.6982459425926208, train/logprobs = tensor([[-0.8431, -2.6125],
        [-0.8326, -2.6216]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
RAW KL tensor(0., device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 1: train/loss = 0.6927576065063477, train/raw-loss = 0.6927576065063477, train/logprobs = tensor([[-1.2483, -1.6199],
        [-1.2440, -1.6139]], device='cuda:0', grad_fn=<DivBackward0>), KL = 0.0
RAW KL tensor(-0.0546, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 2: train/loss = 0.6712270975112915, train/raw-loss = 0.6905900239944458, train/logprobs = tensor([[-1.1643, -2.1437],
        [-1.1882, -2.1573]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.03872578218579292
RAW KL tensor(-0.0421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 3: train/loss = 0.6525788903236389, train/raw-loss = 0.6883082389831543, train/logprobs = tensor([[-0.6786, -1.6377],
        [-0.6832, -1.6227]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.07145869731903076
RAW KL tensor(-0.2232, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 4: train/loss = 0.5907104015350342, train/raw-loss = 0.6821411848068237, train/logprobs = tensor([[-1.3914, -2.5365],
        [-1.3950, -2.4957]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.18286170065402985
RAW KL tensor(-0.7355, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 111])
Epoch 0, Step 5: train/loss = 0.5326955914497375, train/raw-loss = 0.706889271736145, train/logprobs = tensor([[-1.9769, -2.6199],
        [-1.9152, -2.6127]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.3483874201774597
RAW KL tensor(-0.3188, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 121])
Epoch 0, Step 6: train/loss = 0.5412850379943848, train/raw-loss = 0.6913240551948547, train/logprobs = tensor([[-1.7153, -2.3901],
        [-1.6992, -2.3667]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.30007806420326233
RAW KL tensor(-0.3859, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 7: train/loss = 0.5416867136955261, train/raw-loss = 0.6918110251426697, train/logprobs = tensor([[-2.0239, -2.5655],
        [-2.0187, -2.5549]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.3002485930919647
RAW KL tensor(-0.3955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 8: train/loss = 0.42725396156311035, train/raw-loss = 0.6685856580734253, train/logprobs = tensor([[-1.6916, -2.8041],
        [-1.6854, -2.6972]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.48266348242759705
RAW KL tensor(-0.3534, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 9: train/loss = 0.4755435287952423, train/raw-loss = 0.6755210757255554, train/logprobs = tensor([[-1.9187, -2.7092],
        [-1.9257, -2.6450]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.399955153465271
RAW KL tensor(-0.4891, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 10: train/loss = 0.45759668946266174, train/raw-loss = 0.6780622601509094, train/logprobs = tensor([[-2.5122, -3.6574],
        [-2.4715, -3.5555]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.4409312605857849
RAW KL tensor(-0.8517, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 11: train/loss = 0.3115842342376709, train/raw-loss = 0.6751865148544312, train/logprobs = tensor([[-1.7920, -3.0108],
        [-1.7773, -2.9227]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.7272045612335205
RAW KL tensor(-0.7393, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 12: train/loss = 0.3997412323951721, train/raw-loss = 0.6663269996643066, train/logprobs = tensor([[-2.2294, -3.1372],
        [-2.2468, -3.0451]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.5331714749336243
RAW KL tensor(-1.1230, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 13: train/loss = 0.24627554416656494, train/raw-loss = 0.668156087398529, train/logprobs = tensor([[-2.3932, -3.4123],
        [-2.3910, -3.3080]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.8437610864639282
RAW KL tensor(-1.1832, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 14: train/loss = 0.08937576413154602, train/raw-loss = 0.6680772304534912, train/logprobs = tensor([[-3.0683, -3.8095],
        [-2.9974, -3.6333]], device='cuda:0', grad_fn=<DivBackward0>), KL = -1.1574029922485352
RAW KL tensor(-0.8464, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 15: train/loss = 0.24578481912612915, train/raw-loss = 0.6803056001663208, train/logprobs = tensor([[-3.3501, -3.7435],
        [-3.2711, -3.6109]], device='cuda:0', grad_fn=<DivBackward0>), KL = -0.8690416216850281
RAW KL tensor(-2.8128, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 16: train/loss = -0.2054441273212433, train/raw-loss = 0.6483664512634277, train/logprobs = tensor([[-4.0043, -5.0463],
        [-3.9559, -4.8118]], device='cuda:0', grad_fn=<DivBackward0>), KL = -1.7076213359832764
RAW KL tensor(-1.5160, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 17: train/loss = -0.24720174074172974, train/raw-loss = 0.6756352186203003, train/logprobs = tensor([[-4.5421, -4.4958],
        [-4.4666, -4.3495]], device='cuda:0', grad_fn=<DivBackward0>), KL = -1.84567391872406
RAW KL tensor(-2.9509, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 18: train/loss = -0.9236634373664856, train/raw-loss = 0.6395220756530762, train/logprobs = tensor([[-6.4851, -6.6541],
        [-6.2994, -6.2387]], device='cuda:0', grad_fn=<DivBackward0>), KL = -3.126370906829834
RAW KL tensor(-5.0085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 19: train/loss = -1.7146556377410889, train/raw-loss = 0.6059942245483398, train/logprobs = tensor([[-7.8670, -8.5344],
        [-7.6134, -7.9041]], device='cuda:0', grad_fn=<DivBackward0>), KL = -4.641299724578857
RAW KL tensor(-7.6581, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 20: train/loss = -2.337456703186035, train/raw-loss = 0.6587644815444946, train/logprobs = tensor([[-11.1766, -11.2461],
        [-12.3298, -12.2375]], device='cuda:0', grad_fn=<DivBackward0>), KL = -5.992441654205322
RAW KL tensor(-11.9486, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 21: train/loss = -5.120709419250488, train/raw-loss = 0.6200777888298035, train/logprobs = tensor([[-15.7486, -15.6823],
        [-16.1969, -15.8150]], device='cuda:0', grad_fn=<DivBackward0>), KL = -11.481575012207031
RAW KL tensor(-7.2393, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 22: train/loss = -6.323071479797363, train/raw-loss = 0.6641513109207153, train/logprobs = tensor([[-17.9140, -18.2546],
        [-17.8007, -18.0105]], device='cuda:0', grad_fn=<DivBackward0>), KL = -13.974445343017578
RAW KL tensor(-23.0286, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 23: train/loss = -10.442593574523926, train/raw-loss = 0.6139294505119324, train/logprobs = tensor([[-24.7234, -25.1796],
        [-26.3441, -26.4254]], device='cuda:0', grad_fn=<DivBackward0>), KL = -22.11304473876953
RAW KL tensor(-27.5195, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 24: train/loss = -14.241327285766602, train/raw-loss = 0.5753846764564514, train/logprobs = tensor([[-33.2451, -33.1367],
        [-34.0240, -33.3517]], device='cuda:0', grad_fn=<DivBackward0>), KL = -29.633426666259766
RAW KL tensor(-38.1668, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 25: train/loss = -15.5304536819458, train/raw-loss = 0.6633286476135254, train/logprobs = tensor([[-44.1463, -43.8106],
        [-44.3708, -43.9122]], device='cuda:0', grad_fn=<DivBackward0>), KL = -32.38756561279297
RAW KL tensor(-45.7155, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 26: train/loss = -17.05589485168457, train/raw-loss = 0.5735220909118652, train/logprobs = tensor([[-44.8620, -49.6435],
        [-44.9910, -49.2297]], device='cuda:0', grad_fn=<DivBackward0>), KL = -35.25883483886719
RAW KL tensor(-48.0661, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 27: train/loss = -23.086101531982422, train/raw-loss = 0.6822505593299866, train/logprobs = tensor([[-52.7349, -52.5088],
        [-52.7039, -52.4326]], device='cuda:0', grad_fn=<DivBackward0>), KL = -47.536705017089844
RAW KL tensor(-52.7983, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 28: train/loss = -25.369705200195312, train/raw-loss = 0.6719459295272827, train/logprobs = tensor([[-55.2259, -56.2734],
        [-55.6297, -56.5894]], device='cuda:0', grad_fn=<DivBackward0>), KL = -52.08330535888672
RAW KL tensor(-56.2004, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 362])
Epoch 0, Step 29: train/loss = -27.945796966552734, train/raw-loss = 0.6856294870376587, train/logprobs = tensor([[-59.0194, -59.6781],
        [-59.3267, -59.9547]], device='cuda:0', grad_fn=<DivBackward0>), KL = -57.262847900390625
RAW KL tensor(-61.2449, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 219])
Epoch 0, Step 30: train/loss = -29.872241973876953, train/raw-loss = 0.6921914219856262, train/logprobs = tensor([[-62.8184, -62.7751],
        [-62.8455, -62.7981]], device='cuda:0', grad_fn=<DivBackward0>), KL = -61.12886428833008
RAW KL tensor(-63.9346, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 31: train/loss = -31.018718719482422, train/raw-loss = 0.6697261333465576, train/logprobs = tensor([[-65.8429, -65.5568],
        [-65.8024, -65.4200]], device='cuda:0', grad_fn=<DivBackward0>), KL = -63.376895904541016
RAW KL tensor(-62.6660, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 32: train/loss = -31.059539794921875, train/raw-loss = 0.6782246232032776, train/logprobs = tensor([[-66.9628, -66.8720],
        [-67.1539, -67.0012]], device='cuda:0', grad_fn=<DivBackward0>), KL = -63.47553253173828
RAW KL tensor(-66.5867, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 33: train/loss = -32.40957260131836, train/raw-loss = 0.6878921985626221, train/logprobs = tensor([[-68.9079, -69.2803],
        [-69.0478, -69.3987]], device='cuda:0', grad_fn=<DivBackward0>), KL = -66.19493103027344
RAW KL tensor(-68.5574, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 34: train/loss = -33.51603698730469, train/raw-loss = 0.6832247972488403, train/logprobs = tensor([[-70.4833, -70.8963],
        [-70.4927, -70.8657]], device='cuda:0', grad_fn=<DivBackward0>), KL = -68.39851379394531
RAW KL tensor(-68.8382, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 35: train/loss = -34.44629669189453, train/raw-loss = 0.6791335344314575, train/logprobs = tensor([[-72.5170, -72.7099],
        [-72.4995, -72.6354]], device='cuda:0', grad_fn=<DivBackward0>), KL = -70.25086975097656
RAW KL tensor(-72.2779, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 36: train/loss = -35.62529754638672, train/raw-loss = 0.6945238709449768, train/logprobs = tensor([[-74.4150, -74.6042],
        [-74.4027, -74.5974]], device='cuda:0', grad_fn=<DivBackward0>), KL = -72.6396484375
RAW KL tensor(-72.4904, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 235])
Epoch 0, Step 37: train/loss = -35.89208984375, train/raw-loss = 0.6744962930679321, train/logprobs = tensor([[-74.8269, -75.4634],
        [-75.0161, -75.5769]], device='cuda:0', grad_fn=<DivBackward0>), KL = -73.1331787109375
RAW KL tensor(-74.4993, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 292])
Epoch 0, Step 38: train/loss = -36.2546501159668, train/raw-loss = 0.6792535781860352, train/logprobs = tensor([[-76.8228, -77.3744],
        [-76.8054, -77.3006]], device='cuda:0', grad_fn=<DivBackward0>), KL = -73.8677978515625
RAW KL tensor(-75.2742, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 39: train/loss = -37.50286102294922, train/raw-loss = 0.6853212714195251, train/logprobs = tensor([[-77.8755, -78.1618],
        [-77.9302, -78.1850]], device='cuda:0', grad_fn=<DivBackward0>), KL = -76.37637329101562
RAW KL tensor(-76.2140, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 226])
Epoch 0, Step 40: train/loss = -37.72364807128906, train/raw-loss = 0.6900511980056763, train/logprobs = tensor([[-78.3108, -78.5401],
        [-78.3419, -78.5587]], device='cuda:0', grad_fn=<DivBackward0>), KL = -76.827392578125
RAW KL tensor(-78.1208, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 329])
Epoch 0, Step 41: train/loss = -38.451263427734375, train/raw-loss = 0.6924675703048706, train/logprobs = tensor([[-79.5938, -79.4903],
        [-79.5939, -79.4877]], device='cuda:0', grad_fn=<DivBackward0>), KL = -78.28746032714844
RAW KL tensor(-77.6468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 42: train/loss = -38.251888275146484, train/raw-loss = 0.694778561592102, train/logprobs = tensor([[-80.7022, -81.0007],
        [-80.7239, -81.0288]], device='cuda:0', grad_fn=<DivBackward0>), KL = -77.8933334350586
RAW KL tensor(-79.9745, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 43: train/loss = -38.73674774169922, train/raw-loss = 0.6883583068847656, train/logprobs = tensor([[-81.6851, -81.5278],
        [-81.7514, -81.5749]], device='cuda:0', grad_fn=<DivBackward0>), KL = -78.8502197265625
RAW KL tensor(-79.7575, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 44: train/loss = -38.98063659667969, train/raw-loss = 0.6876708269119263, train/logprobs = tensor([[-81.8356, -82.0038],
        [-81.8960, -82.0421]], device='cuda:0', grad_fn=<DivBackward0>), KL = -79.33661651611328
RAW KL tensor(-80.2064, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 45: train/loss = -38.82935333251953, train/raw-loss = 0.684149444103241, train/logprobs = tensor([[-82.0686, -81.7934],
        [-82.0791, -81.7675]], device='cuda:0', grad_fn=<DivBackward0>), KL = -79.02700805664062
RAW KL tensor(-80.4911, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 46: train/loss = -39.27677917480469, train/raw-loss = 0.6935076117515564, train/logprobs = tensor([[-82.1819, -81.7435],
        [-82.1880, -81.7510]], device='cuda:0', grad_fn=<DivBackward0>), KL = -79.94056701660156
RAW KL tensor(-80.7092, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 47: train/loss = -39.3170166015625, train/raw-loss = 0.6891782283782959, train/logprobs = tensor([[-82.0809, -82.3597],
        [-82.1064, -82.3692]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.01239013671875
RAW KL tensor(-80.2403, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 48: train/loss = -39.37835693359375, train/raw-loss = 0.6978400945663452, train/logprobs = tensor([[-82.5930, -82.0202],
        [-82.5787, -82.0246]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.15238952636719
RAW KL tensor(-81.4191, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 49: train/loss = -39.832794189453125, train/raw-loss = 0.6934701204299927, train/logprobs = tensor([[-82.6536, -82.7352],
        [-82.6716, -82.7543]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.05252075195312
RAW KL tensor(-80.8270, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 50: train/loss = -39.781494140625, train/raw-loss = 0.6913684010505676, train/logprobs = tensor([[-82.5323, -82.4792],
        [-82.5397, -82.4795]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.94571685791016
RAW KL tensor(-81.1269, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 51: train/loss = -39.425357818603516, train/raw-loss = 0.6871970891952515, train/logprobs = tensor([[-82.5965, -82.9635],
        [-82.6569, -83.0001]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.22511291503906
RAW KL tensor(-80.8955, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 191])
Epoch 0, Step 52: train/loss = -39.66905212402344, train/raw-loss = 0.692817211151123, train/logprobs = tensor([[-82.7390, -82.9673],
        [-82.7671, -82.9940]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.72373962402344
RAW KL tensor(-80.6883, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 459])
Epoch 0, Step 53: train/loss = -39.82061767578125, train/raw-loss = 0.6981477737426758, train/logprobs = tensor([[-82.4681, -82.3852],
        [-82.4982, -82.4352]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.03753662109375
RAW KL tensor(-80.9603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 54: train/loss = -39.53608703613281, train/raw-loss = 0.6918827295303345, train/logprobs = tensor([[-82.7787, -82.7897],
        [-82.7987, -82.8044]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.45594787597656
RAW KL tensor(-80.8519, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 55: train/loss = -40.02816390991211, train/raw-loss = 0.6951485872268677, train/logprobs = tensor([[-83.1136, -83.2854],
        [-83.1152, -83.2948]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.44662475585938
RAW KL tensor(-80.5810, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 56: train/loss = -40.0601921081543, train/raw-loss = 0.6855661869049072, train/logprobs = tensor([[-83.1738, -83.2295],
        [-83.2044, -83.2294]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.49151611328125
RAW KL tensor(-81.5219, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 57: train/loss = -39.996734619140625, train/raw-loss = 0.6917359828948975, train/logprobs = tensor([[-83.0729, -82.9251],
        [-83.0910, -82.9375]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.37694549560547
RAW KL tensor(-81.4845, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 335])
Epoch 0, Step 58: train/loss = -40.05170440673828, train/raw-loss = 0.6934866309165955, train/logprobs = tensor([[-82.9141, -83.0153],
        [-82.9349, -83.0375]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.49037170410156
RAW KL tensor(-77.8166, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 364])
Epoch 0, Step 59: train/loss = -39.631595611572266, train/raw-loss = 0.6930009722709656, train/logprobs = tensor([[-82.8315, -82.8846],
        [-82.8472, -82.8997]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.6491928100586
RAW KL tensor(-81.1436, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 180])
Epoch 0, Step 60: train/loss = -40.072723388671875, train/raw-loss = 0.6851717233657837, train/logprobs = tensor([[-83.4416, -83.6608],
        [-83.5144, -83.7014]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.51579284667969
RAW KL tensor(-81.8394, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 61: train/loss = -39.88777542114258, train/raw-loss = 0.688439667224884, train/logprobs = tensor([[-83.2379, -83.2989],
        [-83.2659, -83.3078]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.15243530273438
RAW KL tensor(-81.0887, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 62: train/loss = -40.246726989746094, train/raw-loss = 0.6900057792663574, train/logprobs = tensor([[-83.7287, -83.9146],
        [-83.7661, -83.9394]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.87345886230469
RAW KL tensor(-81.3874, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 63: train/loss = -40.097843170166016, train/raw-loss = 0.6959909200668335, train/logprobs = tensor([[-83.4707, -83.5409],
        [-83.4639, -83.5454]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.58766174316406
RAW KL tensor(-82.3659, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 193])
Epoch 0, Step 64: train/loss = -40.100772857666016, train/raw-loss = 0.6899663209915161, train/logprobs = tensor([[-83.4724, -83.8239],
        [-83.4957, -83.8342]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.58147430419922
RAW KL tensor(-81.7365, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 65: train/loss = -40.293670654296875, train/raw-loss = 0.6874990463256836, train/logprobs = tensor([[-83.4721, -83.7257],
        [-83.5037, -83.7344]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.96234130859375
RAW KL tensor(-82.4877, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 66: train/loss = -40.309173583984375, train/raw-loss = 0.6868632435798645, train/logprobs = tensor([[-83.4258, -83.6523],
        [-83.4618, -83.6629]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.9920654296875
RAW KL tensor(-81.4988, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 67: train/loss = -40.315155029296875, train/raw-loss = 0.675413966178894, train/logprobs = tensor([[-83.5920, -83.8630],
        [-83.6585, -83.8578]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.98113250732422
RAW KL tensor(-81.2552, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 258])
Epoch 0, Step 68: train/loss = -40.247520446777344, train/raw-loss = 0.6888901591300964, train/logprobs = tensor([[-83.8169, -83.8435],
        [-83.8376, -83.8471]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.8728256225586
RAW KL tensor(-82.5578, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 69: train/loss = -40.531822204589844, train/raw-loss = 0.689713716506958, train/logprobs = tensor([[-84.2171, -84.3485],
        [-84.2258, -84.3434]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.44306945800781
RAW KL tensor(-81.6803, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 70: train/loss = -40.218414306640625, train/raw-loss = 0.6887146234512329, train/logprobs = tensor([[-83.7403, -84.2761],
        [-83.7489, -84.2669]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.81425476074219
RAW KL tensor(-82.4867, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 71: train/loss = -40.39739227294922, train/raw-loss = 0.6909264326095581, train/logprobs = tensor([[-84.0886, -84.2201],
        [-84.0884, -84.2110]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.1766357421875
RAW KL tensor(-82.1105, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 261])
Epoch 0, Step 72: train/loss = -40.41420364379883, train/raw-loss = 0.6899374723434448, train/logprobs = tensor([[-83.4714, -84.1552],
        [-83.5043, -84.1753]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.20828247070312
RAW KL tensor(-82.7870, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 73: train/loss = -40.313236236572266, train/raw-loss = 0.691643238067627, train/logprobs = tensor([[-83.7659, -83.7272],
        [-83.7802, -83.7355]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.00975799560547
RAW KL tensor(-82.4352, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 74: train/loss = -40.411903381347656, train/raw-loss = 0.6908532381057739, train/logprobs = tensor([[-83.9429, -84.0332],
        [-83.9484, -84.0294]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.20551300048828
RAW KL tensor(-80.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 254])
Epoch 0, Step 75: train/loss = -40.19975280761719, train/raw-loss = 0.6901997327804565, train/logprobs = tensor([[-83.3707, -84.1317],
        [-83.3687, -84.1178]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.7799072265625
RAW KL tensor(-81.6412, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 76: train/loss = -40.35795593261719, train/raw-loss = 0.6788801550865173, train/logprobs = tensor([[-84.1518, -84.3126],
        [-84.1262, -84.2291]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.07366943359375
RAW KL tensor(-81.4735, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 77: train/loss = -40.37629699707031, train/raw-loss = 0.6970953345298767, train/logprobs = tensor([[-84.4670, -84.4175],
        [-84.4549, -84.4211]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.14678192138672
RAW KL tensor(-83.1730, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 78: train/loss = -40.72053146362305, train/raw-loss = 0.6869946122169495, train/logprobs = tensor([[-84.5022, -84.4335],
        [-84.5037, -84.4102]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.81504821777344
RAW KL tensor(-82.0914, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 257])
Epoch 0, Step 79: train/loss = -39.75676345825195, train/raw-loss = 0.685890793800354, train/logprobs = tensor([[-83.9219, -84.1344],
        [-83.9463, -84.1296]], device='cuda:0', grad_fn=<DivBackward0>), KL = -80.88529968261719
RAW KL tensor(-82.3302, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 80: train/loss = -40.271331787109375, train/raw-loss = 0.7012483477592468, train/logprobs = tensor([[-84.0561, -84.0160],
        [-84.0836, -84.0757]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.94515991210938
RAW KL tensor(-82.4888, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 81: train/loss = -40.62213134765625, train/raw-loss = 0.6808291077613831, train/logprobs = tensor([[-83.9939, -84.2394],
        [-84.0211, -84.2169]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.60591125488281
RAW KL tensor(-83.2961, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 82: train/loss = -40.76097869873047, train/raw-loss = 0.6910823583602905, train/logprobs = tensor([[-84.3429, -84.0374],
        [-84.3488, -84.0350]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.90412139892578
RAW KL tensor(-82.1461, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 83: train/loss = -40.615108489990234, train/raw-loss = 0.6907269954681396, train/logprobs = tensor([[-84.0997, -84.4308],
        [-84.1003, -84.4216]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.61166381835938
RAW KL tensor(-82.4619, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 344])
Epoch 0, Step 84: train/loss = -40.57451629638672, train/raw-loss = 0.6840329766273499, train/logprobs = tensor([[-84.2443, -84.5462],
        [-84.2525, -84.5178]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.51710510253906
RAW KL tensor(-82.8856, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 85: train/loss = -40.736244201660156, train/raw-loss = 0.6940900087356567, train/logprobs = tensor([[-84.6838, -84.9062],
        [-84.6744, -84.8998]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.86067962646484
RAW KL tensor(-83.0564, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 86: train/loss = -40.69105529785156, train/raw-loss = 0.6945748329162598, train/logprobs = tensor([[-84.3584, -84.7047],
        [-84.3743, -84.7260]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.77125549316406
RAW KL tensor(-83.3935, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 87: train/loss = -40.539127349853516, train/raw-loss = 0.6891337633132935, train/logprobs = tensor([[-84.8181, -84.4266],
        [-84.8488, -84.4412]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.45652770996094
RAW KL tensor(-82.4069, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 88: train/loss = -40.72026824951172, train/raw-loss = 0.6923868656158447, train/logprobs = tensor([[-84.2026, -84.7413],
        [-84.1981, -84.7335]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.82530975341797
RAW KL tensor(-82.9887, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 89: train/loss = -40.815574645996094, train/raw-loss = 0.6958332061767578, train/logprobs = tensor([[-84.4396, -84.5920],
        [-84.4269, -84.5900]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.02281951904297
RAW KL tensor(-81.9484, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 90: train/loss = -40.6157112121582, train/raw-loss = 0.6927969455718994, train/logprobs = tensor([[-84.4290, -84.5201],
        [-84.4082, -84.4978]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.61701202392578
RAW KL tensor(-83.7121, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 91: train/loss = -40.918617248535156, train/raw-loss = 0.6942426562309265, train/logprobs = tensor([[-84.9256, -84.6020],
        [-84.9204, -84.6012]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.22572326660156
RAW KL tensor(-81.8471, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 92: train/loss = -40.81711959838867, train/raw-loss = 0.6890445947647095, train/logprobs = tensor([[-84.9692, -85.0269],
        [-84.9751, -85.0164]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.0123291015625
RAW KL tensor(-83.0005, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 93: train/loss = -40.94477844238281, train/raw-loss = 0.6945868730545044, train/logprobs = tensor([[-85.1747, -85.0274],
        [-85.1796, -85.0381]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.27873229980469
RAW KL tensor(-82.6046, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 94: train/loss = -40.81584548950195, train/raw-loss = 0.689348578453064, train/logprobs = tensor([[-85.1700, -85.2902],
        [-85.1734, -85.2778]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.01039123535156
RAW KL tensor(-82.8506, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 95: train/loss = -40.695457458496094, train/raw-loss = 0.6896865367889404, train/logprobs = tensor([[-85.2104, -85.1795],
        [-85.2152, -85.1703]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.77027893066406
RAW KL tensor(-83.3471, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 96: train/loss = -40.88812255859375, train/raw-loss = 0.6862667202949524, train/logprobs = tensor([[-84.9348, -85.0878],
        [-84.9630, -85.0882]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.1487808227539
RAW KL tensor(-83.4582, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 204])
Epoch 0, Step 97: train/loss = -39.93421936035156, train/raw-loss = 0.6875815987586975, train/logprobs = tensor([[-84.1754, -84.8515],
        [-84.1900, -84.8436]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.24359893798828
RAW KL tensor(-83.0482, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 98: train/loss = -41.008636474609375, train/raw-loss = 0.6878635883331299, train/logprobs = tensor([[-85.0050, -84.8689],
        [-85.0132, -84.8558]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.39300537109375
RAW KL tensor(-83.9176, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 99: train/loss = -40.98472213745117, train/raw-loss = 0.6869999766349792, train/logprobs = tensor([[-84.8142, -85.0242],
        [-84.8179, -85.0031]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.34344482421875
RAW KL tensor(-83.5054, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 100: train/loss = -41.184181213378906, train/raw-loss = 0.693558931350708, train/logprobs = tensor([[-85.0328, -85.1461],
        [-85.0395, -85.1544]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.75547790527344
RAW KL tensor(-82.8331, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 101: train/loss = -40.561317443847656, train/raw-loss = 0.689691424369812, train/logprobs = tensor([[-84.8740, -85.0978],
        [-84.8709, -85.0808]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.50202178955078
RAW KL tensor(-83.2686, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 102: train/loss = -40.903831481933594, train/raw-loss = 0.6879047155380249, train/logprobs = tensor([[-84.8450, -85.6713],
        [-84.8680, -85.6732]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.1834716796875
RAW KL tensor(-82.4828, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 103: train/loss = -40.912879943847656, train/raw-loss = 0.684733510017395, train/logprobs = tensor([[-84.8520, -85.2849],
        [-84.8685, -85.2675]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.19522094726562
RAW KL tensor(-83.7543, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 104: train/loss = -40.993282318115234, train/raw-loss = 0.6907283067703247, train/logprobs = tensor([[-84.9115, -85.1820],
        [-84.9290, -85.1897]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.36802673339844
RAW KL tensor(-83.6426, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 105: train/loss = -40.97624588012695, train/raw-loss = 0.6826248168945312, train/logprobs = tensor([[-84.8999, -85.4932],
        [-84.9259, -85.4766]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.3177490234375
RAW KL tensor(-83.5102, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 106: train/loss = -41.14784240722656, train/raw-loss = 0.6862037181854248, train/logprobs = tensor([[-84.9629, -85.3489],
        [-84.9961, -85.3540]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.66808319091797
RAW KL tensor(-83.0804, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 107: train/loss = -41.172401428222656, train/raw-loss = 0.6921480894088745, train/logprobs = tensor([[-85.0812, -85.1517],
        [-85.0867, -85.1532]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.7291030883789
RAW KL tensor(-82.4841, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 108: train/loss = -40.85892105102539, train/raw-loss = 0.6939057111740112, train/logprobs = tensor([[-84.6475, -84.5651],
        [-84.6567, -84.5773]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.10565185546875
RAW KL tensor(-84.4921, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 109: train/loss = -41.21815872192383, train/raw-loss = 0.6857056617736816, train/logprobs = tensor([[-85.1796, -85.5580],
        [-85.1767, -85.5250]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.80773162841797
RAW KL tensor(-83.4588, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 110: train/loss = -41.15191650390625, train/raw-loss = 0.6951898336410522, train/logprobs = tensor([[-85.3625, -85.3777],
        [-85.3652, -85.3881]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.6942138671875
RAW KL tensor(-83.7722, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 111: train/loss = -40.864768981933594, train/raw-loss = 0.6891840696334839, train/logprobs = tensor([[-85.3349, -85.3540],
        [-85.3772, -85.3802]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.10790252685547
RAW KL tensor(-83.9656, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 109])
Epoch 0, Step 112: train/loss = -41.26658630371094, train/raw-loss = 0.6959155797958374, train/logprobs = tensor([[-85.7011, -85.6915],
        [-85.7025, -85.7037]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.92500305175781
RAW KL tensor(-84.1022, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 113: train/loss = -41.23484802246094, train/raw-loss = 0.6922295093536377, train/logprobs = tensor([[-85.4015, -85.4359],
        [-85.4395, -85.4702]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.85415649414062
RAW KL tensor(-84.4603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 353])
Epoch 0, Step 114: train/loss = -41.401737213134766, train/raw-loss = 0.6856915950775146, train/logprobs = tensor([[-85.1236, -85.6777],
        [-85.1487, -85.6728]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.17485809326172
RAW KL tensor(-84.5476, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 115: train/loss = -40.780059814453125, train/raw-loss = 0.6835142374038696, train/logprobs = tensor([[-85.2439, -85.7339],
        [-85.2600, -85.7109]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.9271469116211
RAW KL tensor(-84.9563, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 116: train/loss = -41.23134994506836, train/raw-loss = 0.6875199675559998, train/logprobs = tensor([[-85.3636, -85.6793],
        [-85.3917, -85.6846]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.83773803710938
RAW KL tensor(-83.8019, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 117: train/loss = -41.33050537109375, train/raw-loss = 0.6911219954490662, train/logprobs = tensor([[-85.6790, -86.1540],
        [-85.6859, -86.1525]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.04325866699219
RAW KL tensor(-84.1730, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 118: train/loss = -40.8294677734375, train/raw-loss = 0.6889265775680542, train/logprobs = tensor([[-85.7954, -85.9521],
        [-85.7979, -85.9376]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.03678894042969
RAW KL tensor(-84.4009, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 119: train/loss = -41.5235481262207, train/raw-loss = 0.6934144496917725, train/logprobs = tensor([[-85.8685, -86.3341],
        [-85.8636, -86.3302]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.43392181396484
RAW KL tensor(-83.9951, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 120: train/loss = -41.42377471923828, train/raw-loss = 0.6897202730178833, train/logprobs = tensor([[-85.6489, -85.9699],
        [-85.6543, -85.9614]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.22698974609375
RAW KL tensor(-84.2383, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 121: train/loss = -41.30196762084961, train/raw-loss = 0.6797103881835938, train/logprobs = tensor([[-85.2776, -85.3660],
        [-85.3105, -85.3442]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.9633560180664
RAW KL tensor(-83.9541, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 132])
Epoch 0, Step 122: train/loss = -41.18916702270508, train/raw-loss = 0.6833213567733765, train/logprobs = tensor([[-85.3591, -86.2127],
        [-85.3881, -86.2019]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.74497985839844
RAW KL tensor(-83.7695, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 123: train/loss = -41.602638244628906, train/raw-loss = 0.687648355960846, train/logprobs = tensor([[-85.7574, -86.3646],
        [-85.7837, -86.3688]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.58057403564453
RAW KL tensor(-81.9534, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 197])
Epoch 0, Step 124: train/loss = -40.75254440307617, train/raw-loss = 0.6852834224700928, train/logprobs = tensor([[-85.3739, -85.9911],
        [-85.3919, -85.9775]], device='cuda:0', grad_fn=<DivBackward0>), KL = -82.87565612792969
RAW KL tensor(-84.8776, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 125: train/loss = -41.50623321533203, train/raw-loss = 0.6827796697616577, train/logprobs = tensor([[-85.8101, -86.6232],
        [-85.8034, -86.5745]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.37802124023438
RAW KL tensor(-80.5834, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 136])
Epoch 0, Step 126: train/loss = -40.27629852294922, train/raw-loss = 0.6859102249145508, train/logprobs = tensor([[-85.4708, -85.9418],
        [-85.4837, -85.9257]], device='cuda:0', grad_fn=<DivBackward0>), KL = -81.9244155883789
RAW KL tensor(-84.2776, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 127: train/loss = -41.307090759277344, train/raw-loss = 0.6892027854919434, train/logprobs = tensor([[-85.8348, -86.0106],
        [-85.8310, -85.9909]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.99258422851562
RAW KL tensor(-82.5002, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 128: train/loss = -41.11744689941406, train/raw-loss = 0.6880862712860107, train/logprobs = tensor([[-85.6368, -85.5068],
        [-85.6554, -85.5051]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.61106872558594
RAW KL tensor(-82.9916, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 129: train/loss = -41.078880310058594, train/raw-loss = 0.6970522403717041, train/logprobs = tensor([[-85.5977, -85.6363],
        [-85.6164, -85.6702]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.55186462402344
RAW KL tensor(-82.9190, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 430])
Epoch 0, Step 130: train/loss = -41.037471771240234, train/raw-loss = 0.6653608083724976, train/logprobs = tensor([[-85.3729, -86.2694],
        [-85.3661, -86.1491]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.4056625366211
RAW KL tensor(-83.9968, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 131: train/loss = -41.25067138671875, train/raw-loss = 0.6932461857795715, train/logprobs = tensor([[-85.7769, -85.7526],
        [-85.7841, -85.7601]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.88783264160156
RAW KL tensor(-84.3645, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 132: train/loss = -41.24317932128906, train/raw-loss = 0.6778140068054199, train/logprobs = tensor([[-85.5593, -86.0424],
        [-85.5941, -86.0150]], device='cuda:0', grad_fn=<DivBackward0>), KL = -83.84198760986328
RAW KL tensor(-84.4243, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 133: train/loss = -41.5576171875, train/raw-loss = 0.677120566368103, train/logprobs = tensor([[-86.1458, -86.7996],
        [-86.1854, -86.7739]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.46947479248047
RAW KL tensor(-84.5422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 134: train/loss = -41.51854705810547, train/raw-loss = 0.6735231280326843, train/logprobs = tensor([[-86.1221, -86.6912],
        [-86.1785, -86.6674]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.38414001464844
RAW KL tensor(-84.1827, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 135: train/loss = -41.73176574707031, train/raw-loss = 0.6947755813598633, train/logprobs = tensor([[-86.8135, -86.4246],
        [-86.8078, -86.4252]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.85308074951172
RAW KL tensor(-85.0995, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 136: train/loss = -41.70154571533203, train/raw-loss = 0.675422191619873, train/logprobs = tensor([[-86.5351, -86.8722],
        [-86.5552, -86.8188]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.75393676757812
RAW KL tensor(-84.2890, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 137: train/loss = -41.81140899658203, train/raw-loss = 0.6961228847503662, train/logprobs = tensor([[-86.6731, -86.5399],
        [-86.6802, -86.5589]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.01506805419922
RAW KL tensor(-85.3722, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 138: train/loss = -41.85309982299805, train/raw-loss = 0.669843316078186, train/logprobs = tensor([[-86.3800, -86.7677],
        [-86.4145, -86.7076]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.0458755493164
RAW KL tensor(-84.2911, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 139: train/loss = -41.43996047973633, train/raw-loss = 0.6819384098052979, train/logprobs = tensor([[-86.1590, -86.4154],
        [-86.1985, -86.4098]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.2437973022461
RAW KL tensor(-84.4822, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 140: train/loss = -41.700504302978516, train/raw-loss = 0.6799319982528687, train/logprobs = tensor([[-86.4238, -86.9377],
        [-86.4350, -86.8939]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.76087951660156
RAW KL tensor(-84.6944, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 141: train/loss = -41.45318603515625, train/raw-loss = 0.6817857027053833, train/logprobs = tensor([[-86.3805, -86.6405],
        [-86.3996, -86.6128]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.26994323730469
RAW KL tensor(-84.2427, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 142: train/loss = -41.520694732666016, train/raw-loss = 0.6876605749130249, train/logprobs = tensor([[-86.2852, -86.5799],
        [-86.2872, -86.5598]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.41671752929688
RAW KL tensor(-85.1548, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 143: train/loss = -41.6629524230957, train/raw-loss = 0.6868019104003906, train/logprobs = tensor([[-86.9056, -86.0149],
        [-86.9065, -85.9901]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.69950866699219
RAW KL tensor(-84.7366, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 144: train/loss = -41.79326629638672, train/raw-loss = 0.6667079925537109, train/logprobs = tensor([[-86.5850, -87.0593],
        [-86.6049, -86.9709]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.9199447631836
RAW KL tensor(-82.7228, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 122])
Epoch 0, Step 145: train/loss = -41.4337272644043, train/raw-loss = 0.6960437297821045, train/logprobs = tensor([[-86.6582, -86.2821],
        [-86.6390, -86.2744]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.25953674316406
RAW KL tensor(-84.2903, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 146: train/loss = -41.41893005371094, train/raw-loss = 0.6830703020095825, train/logprobs = tensor([[-86.1382, -86.4143],
        [-86.1662, -86.4009]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.2040023803711
RAW KL tensor(-85.1865, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 163])
Epoch 0, Step 147: train/loss = -41.718284606933594, train/raw-loss = 0.6709378957748413, train/logprobs = tensor([[-86.4431, -86.9412],
        [-86.4414, -86.8487]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.7784423828125
RAW KL tensor(-85.4682, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 148: train/loss = -41.924503326416016, train/raw-loss = 0.6774649620056152, train/logprobs = tensor([[-86.4125, -86.7674],
        [-86.4580, -86.7487]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.20393371582031
RAW KL tensor(-83.6416, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 149: train/loss = -41.715431213378906, train/raw-loss = 0.6893138885498047, train/logprobs = tensor([[-86.6887, -87.0740],
        [-86.6893, -87.0590]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.80948638916016
RAW KL tensor(-83.5067, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 150: train/loss = -41.51783752441406, train/raw-loss = 0.687780499458313, train/logprobs = tensor([[-86.8387, -86.7225],
        [-86.8756, -86.7378]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.41123962402344
RAW KL tensor(-85.8525, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 151: train/loss = -41.79863739013672, train/raw-loss = 0.6692580580711365, train/logprobs = tensor([[-86.9680, -87.1415],
        [-86.9892, -87.0613]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.935791015625
RAW KL tensor(-85.2743, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 152: train/loss = -41.99661636352539, train/raw-loss = 0.6489177942276001, train/logprobs = tensor([[-86.8609, -87.5277],
        [-86.9238, -87.4066]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.29106903076172
RAW KL tensor(-85.1858, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 153: train/loss = -41.66510009765625, train/raw-loss = 0.6784183382987976, train/logprobs = tensor([[-86.8746, -86.9583],
        [-86.8695, -86.8918]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.6870346069336
RAW KL tensor(-84.9987, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 203])
Epoch 0, Step 154: train/loss = -41.93082046508789, train/raw-loss = 0.6652242541313171, train/logprobs = tensor([[-86.9450, -86.9738],
        [-86.9956, -86.9066]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.19208526611328
RAW KL tensor(-84.1076, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 155: train/loss = -41.633056640625, train/raw-loss = 0.6783431768417358, train/logprobs = tensor([[-86.0442, -86.3664],
        [-86.0994, -86.3601]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.622802734375
RAW KL tensor(-84.5477, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 156: train/loss = -41.558013916015625, train/raw-loss = 0.6470992565155029, train/logprobs = tensor([[-86.2199, -87.1680],
        [-86.2316, -86.9902]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.41023254394531
RAW KL tensor(-84.8150, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 157: train/loss = -41.94557189941406, train/raw-loss = 0.6779299378395081, train/logprobs = tensor([[-86.5664, -86.6066],
        [-86.5990, -86.5772]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.24700927734375
RAW KL tensor(-85.2033, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 158: train/loss = -41.809608459472656, train/raw-loss = 0.6918206214904785, train/logprobs = tensor([[-86.5958, -86.4638],
        [-86.5672, -86.4289]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.00286102294922
RAW KL tensor(-83.9752, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 159: train/loss = -41.66598892211914, train/raw-loss = 0.688859224319458, train/logprobs = tensor([[-86.9689, -86.7716],
        [-86.9879, -86.7733]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.70970153808594
RAW KL tensor(-85.2251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 160: train/loss = -41.80506134033203, train/raw-loss = 0.6740602254867554, train/logprobs = tensor([[-86.6188, -86.9111],
        [-86.6757, -86.8886]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.95824432373047
RAW KL tensor(-86.0158, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 161: train/loss = -41.9769172668457, train/raw-loss = 0.6665058732032776, train/logprobs = tensor([[-86.6892, -87.4517],
        [-86.7642, -87.4161]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.2868423461914
RAW KL tensor(-84.9798, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 162: train/loss = -41.98381805419922, train/raw-loss = 0.604304313659668, train/logprobs = tensor([[-86.6034, -88.1294],
        [-86.7348, -87.8856]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.1762466430664
RAW KL tensor(-84.4717, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 163: train/loss = -41.94407653808594, train/raw-loss = 0.6544450521469116, train/logprobs = tensor([[-87.2012, -87.3947],
        [-87.2449, -87.2789]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.19705200195312
RAW KL tensor(-82.6617, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 164: train/loss = -41.543785095214844, train/raw-loss = 0.6769865155220032, train/logprobs = tensor([[-87.1416, -87.1194],
        [-87.2054, -87.1172]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.44154357910156
RAW KL tensor(-85.6537, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 165: train/loss = -41.95051193237305, train/raw-loss = 0.6601694822311401, train/logprobs = tensor([[-87.2767, -87.4225],
        [-87.2890, -87.2968]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.22135925292969
RAW KL tensor(-84.6494, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 166: train/loss = -41.84195327758789, train/raw-loss = 0.6053833961486816, train/logprobs = tensor([[-86.5653, -87.3394],
        [-86.6722, -87.0721]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.89466857910156
RAW KL tensor(-85.8273, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 167: train/loss = -42.23228454589844, train/raw-loss = 0.6656533479690552, train/logprobs = tensor([[-87.0269, -87.0448],
        [-87.0898, -86.9881]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.79586791992188
RAW KL tensor(-84.5721, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 214])
Epoch 0, Step 168: train/loss = -41.71258544921875, train/raw-loss = 0.6625471115112305, train/logprobs = tensor([[-86.6539, -86.6181],
        [-86.7410, -86.5784]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.75025939941406
RAW KL tensor(-84.4368, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 169: train/loss = -42.06952667236328, train/raw-loss = 0.6419895887374878, train/logprobs = tensor([[-86.7960, -87.6630],
        [-86.9002, -87.5536]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.42302703857422
RAW KL tensor(-84.1474, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 216])
Epoch 0, Step 170: train/loss = -41.99605178833008, train/raw-loss = 0.6224039793014526, train/logprobs = tensor([[-86.6577, -87.5818],
        [-86.8506, -87.4739]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.23690795898438
RAW KL tensor(-83.9418, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 171: train/loss = -41.94682312011719, train/raw-loss = 0.6719685196876526, train/logprobs = tensor([[-87.1834, -87.3272],
        [-87.2526, -87.3074]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.23757934570312
RAW KL tensor(-85.1734, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 172: train/loss = -42.150848388671875, train/raw-loss = 0.6355685591697693, train/logprobs = tensor([[-86.7954, -87.3481],
        [-86.9213, -87.2176]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.57283020019531
RAW KL tensor(-85.9175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 173: train/loss = -42.07360076904297, train/raw-loss = 0.6274545192718506, train/logprobs = tensor([[-86.9469, -87.3626],
        [-87.0562, -87.1961]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.40210723876953
RAW KL tensor(-85.4863, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 174: train/loss = -42.272212982177734, train/raw-loss = 0.586819589138031, train/logprobs = tensor([[-87.4058, -87.8055],
        [-87.3753, -87.2710]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.71806335449219
RAW KL tensor(-85.3168, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 175: train/loss = -42.035362243652344, train/raw-loss = 0.6512378454208374, train/logprobs = tensor([[-87.1892, -87.4605],
        [-87.2367, -87.3151]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.3731918334961
RAW KL tensor(-86.2799, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 142])
Epoch 0, Step 176: train/loss = -41.892433166503906, train/raw-loss = 0.6432857513427734, train/logprobs = tensor([[-87.0531, -87.4928],
        [-87.1554, -87.3599]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.07144165039062
RAW KL tensor(-85.5059, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 177: train/loss = -42.3519172668457, train/raw-loss = 0.5557130575180054, train/logprobs = tensor([[-86.9037, -88.1469],
        [-87.1659, -87.7870]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.81526184082031
RAW KL tensor(-85.7235, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 259])
Epoch 0, Step 178: train/loss = -42.25818634033203, train/raw-loss = 0.6008694767951965, train/logprobs = tensor([[-87.0185, -87.9437],
        [-87.2679, -87.7739]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.71810913085938
RAW KL tensor(-86.3035, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 171])
Epoch 0, Step 179: train/loss = -42.34236526489258, train/raw-loss = 0.5826358199119568, train/logprobs = tensor([[-87.1867, -87.7857],
        [-87.2997, -87.3563]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.8499984741211
RAW KL tensor(-86.9833, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 180: train/loss = -42.262550354003906, train/raw-loss = 0.649459958076477, train/logprobs = tensor([[-87.0252, -87.4401],
        [-87.2439, -87.4702]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.82402038574219
RAW KL tensor(-85.5288, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 326])
Epoch 0, Step 181: train/loss = -42.267982482910156, train/raw-loss = 0.6146938800811768, train/logprobs = tensor([[-87.0715, -87.6866],
        [-87.4743, -87.7330]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.76534271240234
RAW KL tensor(-85.9596, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 182: train/loss = -42.40333557128906, train/raw-loss = 0.6641210317611694, train/logprobs = tensor([[-87.6275, -88.0293],
        [-87.8235, -88.1003]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.13491821289062
RAW KL tensor(-85.3577, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 183: train/loss = -42.3079719543457, train/raw-loss = 0.543799638748169, train/logprobs = tensor([[-86.9532, -88.3658],
        [-87.1549, -87.8252]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.70354461669922
RAW KL tensor(-85.8529, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 184: train/loss = -42.6038932800293, train/raw-loss = 0.5778745412826538, train/logprobs = tensor([[-87.5424, -89.0362],
        [-87.7625, -88.6902]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.363525390625
RAW KL tensor(-86.6342, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 185: train/loss = -42.271854400634766, train/raw-loss = 0.6294395923614502, train/logprobs = tensor([[-87.8233, -88.3029],
        [-88.0641, -88.2614]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.8025894165039
RAW KL tensor(-85.0713, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 186: train/loss = -42.165924072265625, train/raw-loss = 0.519279956817627, train/logprobs = tensor([[-87.3471, -88.6937],
        [-87.7110, -88.2543]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.37040710449219
RAW KL tensor(-85.2456, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 187: train/loss = -41.82944869995117, train/raw-loss = 0.6635011434555054, train/logprobs = tensor([[-87.6612, -87.8321],
        [-87.6927, -87.7393]], device='cuda:0', grad_fn=<DivBackward0>), KL = -84.98590087890625
RAW KL tensor(-85.7690, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 223])
Epoch 0, Step 188: train/loss = -42.241268157958984, train/raw-loss = 0.6279479265213013, train/logprobs = tensor([[-87.7707, -87.7189],
        [-87.9472, -87.5988]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.73844146728516
RAW KL tensor(-86.6624, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 189: train/loss = -42.04650115966797, train/raw-loss = 0.6070761680603027, train/logprobs = tensor([[-87.0442, -86.9295],
        [-87.5184, -87.0125]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.30715942382812
RAW KL tensor(-85.7421, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 190: train/loss = -42.42534637451172, train/raw-loss = 0.6100236177444458, train/logprobs = tensor([[-87.6933, -87.9413],
        [-87.8261, -87.6403]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.07073974609375
RAW KL tensor(-86.1131, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 191: train/loss = -42.637184143066406, train/raw-loss = 0.6025000810623169, train/logprobs = tensor([[-87.9898, -88.0594],
        [-88.0753, -87.6826]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.47936248779297
RAW KL tensor(-85.4390, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 192: train/loss = -42.110206604003906, train/raw-loss = 0.647816002368927, train/logprobs = tensor([[-87.6549, -88.2310],
        [-87.6481, -88.0328]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.51603698730469
RAW KL tensor(-85.2217, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 193: train/loss = -42.12962341308594, train/raw-loss = 0.5920675992965698, train/logprobs = tensor([[-87.6575, -88.0691],
        [-87.6893, -87.6497]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.44337463378906
RAW KL tensor(-85.8546, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 282])
Epoch 0, Step 194: train/loss = -42.485267639160156, train/raw-loss = 0.653445839881897, train/logprobs = tensor([[-88.2339, -88.0209],
        [-88.2372, -87.8556]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.27742767333984
RAW KL tensor(-86.1060, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 271])
Epoch 0, Step 195: train/loss = -42.54231643676758, train/raw-loss = 0.5901553630828857, train/logprobs = tensor([[-87.3794, -88.3492],
        [-87.4992, -87.9957]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.26493835449219
RAW KL tensor(-88.0406, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 124])
Epoch 0, Step 196: train/loss = -42.68597412109375, train/raw-loss = 0.6065477132797241, train/logprobs = tensor([[-88.5217, -88.6802],
        [-88.6127, -88.3836]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.58505249023438
RAW KL tensor(-86.6299, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 197: train/loss = -42.380863189697266, train/raw-loss = 0.6070232391357422, train/logprobs = tensor([[-87.4453, -87.9165],
        [-87.6365, -87.7167]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.97576904296875
RAW KL tensor(-86.8397, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 198: train/loss = -42.563941955566406, train/raw-loss = 0.6528284549713135, train/logprobs = tensor([[-87.5046, -87.8284],
        [-87.6620, -87.8182]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.43354034423828
RAW KL tensor(-86.7340, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 199: train/loss = -42.610076904296875, train/raw-loss = 0.6725733280181885, train/logprobs = tensor([[-87.5813, -87.6833],
        [-87.7046, -87.7213]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.5653076171875
RAW KL tensor(-86.4131, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 200: train/loss = -42.503150939941406, train/raw-loss = 0.6238011121749878, train/logprobs = tensor([[-88.1916, -88.6806],
        [-88.2006, -88.3790]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.25390625
RAW KL tensor(-86.2722, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 167])
Epoch 0, Step 201: train/loss = -42.198875427246094, train/raw-loss = 0.5482028722763062, train/logprobs = tensor([[-87.6724, -88.9662],
        [-87.6311, -88.2687]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.49415588378906
RAW KL tensor(-86.4098, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 202: train/loss = -42.39255142211914, train/raw-loss = 0.5557641386985779, train/logprobs = tensor([[-87.9575, -88.4544],
        [-87.9982, -87.8617]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.8966293334961
RAW KL tensor(-86.5109, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 203: train/loss = -42.67399597167969, train/raw-loss = 0.6011121869087219, train/logprobs = tensor([[-88.2942, -88.2826],
        [-88.3445, -87.8937]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.55021667480469
RAW KL tensor(-85.6209, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 204: train/loss = -42.509178161621094, train/raw-loss = 0.6255063414573669, train/logprobs = tensor([[-88.2208, -88.3442],
        [-88.2316, -88.0389]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.26937866210938
RAW KL tensor(-85.0896, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 328])
Epoch 0, Step 205: train/loss = -42.49872589111328, train/raw-loss = 0.5393399596214294, train/logprobs = tensor([[-87.4848, -88.5416],
        [-87.5767, -87.8781]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.07613372802734
RAW KL tensor(-86.7143, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 206: train/loss = -42.60266876220703, train/raw-loss = 0.5733914375305176, train/logprobs = tensor([[-87.4960, -87.9834],
        [-87.5846, -87.4926]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.35211944580078
RAW KL tensor(-86.2504, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 207: train/loss = -42.36544418334961, train/raw-loss = 0.6227061152458191, train/logprobs = tensor([[-87.7767, -88.0458],
        [-87.8486, -87.8027]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.97630310058594
RAW KL tensor(-87.6774, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 208: train/loss = -42.99818420410156, train/raw-loss = 0.5673660039901733, train/logprobs = tensor([[-88.3267, -89.3746],
        [-88.3919, -88.8435]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.13108825683594
RAW KL tensor(-86.0335, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 209: train/loss = -42.590904235839844, train/raw-loss = 0.5154396891593933, train/logprobs = tensor([[-88.0278, -89.1030],
        [-88.0987, -88.0703]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.21269226074219
RAW KL tensor(-87.5024, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 210: train/loss = -42.8591194152832, train/raw-loss = 0.6121402978897095, train/logprobs = tensor([[-88.8173, -89.0621],
        [-88.8357, -88.6503]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.94252014160156
RAW KL tensor(-86.8838, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 211: train/loss = -42.52222442626953, train/raw-loss = 0.5726111531257629, train/logprobs = tensor([[-88.1340, -89.1273],
        [-88.2170, -88.6516]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.18966674804688
RAW KL tensor(-85.9831, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 212: train/loss = -42.54090118408203, train/raw-loss = 0.5866497159004211, train/logprobs = tensor([[-88.0276, -88.5525],
        [-88.1584, -88.1658]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.2551040649414
RAW KL tensor(-86.5438, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 213: train/loss = -42.711788177490234, train/raw-loss = 0.6094098687171936, train/logprobs = tensor([[-88.3260, -88.5201],
        [-88.4128, -88.2247]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.64240264892578
RAW KL tensor(-86.0646, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 214: train/loss = -42.67155075073242, train/raw-loss = 0.5905487537384033, train/logprobs = tensor([[-87.7248, -88.3401],
        [-87.9954, -88.0879]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.5241928100586
RAW KL tensor(-86.6539, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 185])
Epoch 0, Step 215: train/loss = -42.828853607177734, train/raw-loss = 0.5269132256507874, train/logprobs = tensor([[-87.9210, -89.1449],
        [-88.0885, -88.4336]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.71153259277344
RAW KL tensor(-86.5760, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 243])
Epoch 0, Step 216: train/loss = -42.85133743286133, train/raw-loss = 0.6871019601821899, train/logprobs = tensor([[-88.8744, -88.6624],
        [-88.7969, -88.5585]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.07687377929688
RAW KL tensor(-88.1722, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 217: train/loss = -42.81203079223633, train/raw-loss = 0.6092650294303894, train/logprobs = tensor([[-88.1701, -88.8886],
        [-88.3108, -88.6260]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.84259033203125
RAW KL tensor(-85.0618, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 218: train/loss = -42.9078483581543, train/raw-loss = 0.4682162404060364, train/logprobs = tensor([[-88.3797, -89.9449],
        [-88.5811, -88.7974]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.75213623046875
RAW KL tensor(-87.0018, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 496])
Epoch 0, Step 219: train/loss = -42.55445098876953, train/raw-loss = 0.6361997127532959, train/logprobs = tensor([[-88.3066, -88.2225],
        [-88.2845, -87.8858]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.38130187988281
RAW KL tensor(-86.4117, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 144])
Epoch 0, Step 220: train/loss = -42.75965118408203, train/raw-loss = 0.5123760104179382, train/logprobs = tensor([[-88.6223, -89.4130],
        [-88.7061, -88.5671]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.5440444946289
RAW KL tensor(-87.2003, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 221: train/loss = -42.7032470703125, train/raw-loss = 0.6768345236778259, train/logprobs = tensor([[-88.7583, -88.2349],
        [-88.8187, -88.2223]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.76016235351562
RAW KL tensor(-86.4515, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 222: train/loss = -42.99506378173828, train/raw-loss = 0.5876468420028687, train/logprobs = tensor([[-88.6019, -89.0109],
        [-88.6389, -88.5045]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.16543579101562
RAW KL tensor(-86.5112, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 207])
Epoch 0, Step 223: train/loss = -42.953556060791016, train/raw-loss = 0.6101124882698059, train/logprobs = tensor([[-88.9619, -89.1112],
        [-89.0815, -88.8580]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.12733459472656
RAW KL tensor(-82.6410, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 224: train/loss = -42.29656219482422, train/raw-loss = 0.5548545718193054, train/logprobs = tensor([[-88.5685, -89.7168],
        [-88.6267, -89.1201]], device='cuda:0', grad_fn=<DivBackward0>), KL = -85.70281982421875
RAW KL tensor(-87.1251, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 360])
Epoch 0, Step 225: train/loss = -43.294830322265625, train/raw-loss = 0.42783722281455994, train/logprobs = tensor([[-88.2547, -90.4124],
        [-88.3234, -89.1803]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.4453353881836
RAW KL tensor(-86.7718, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 200])
Epoch 0, Step 226: train/loss = -42.878448486328125, train/raw-loss = 0.4564736485481262, train/logprobs = tensor([[-88.1993, -89.5004],
        [-88.5277, -88.6204]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.66984558105469
RAW KL tensor(-86.3667, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 227: train/loss = -42.72797775268555, train/raw-loss = 0.6024881601333618, train/logprobs = tensor([[-87.9659, -88.5035],
        [-88.1928, -88.3129]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.66093444824219
RAW KL tensor(-86.7207, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 228: train/loss = -42.52509689331055, train/raw-loss = 0.5420798659324646, train/logprobs = tensor([[-88.6114, -89.4085],
        [-88.8370, -88.8870]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.13435363769531
RAW KL tensor(-87.0125, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 283])
Epoch 0, Step 229: train/loss = -42.90815734863281, train/raw-loss = 0.5641952753067017, train/logprobs = tensor([[-88.2759, -88.8092],
        [-88.5690, -88.3639]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.9447021484375
RAW KL tensor(-87.7964, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 230: train/loss = -43.30390167236328, train/raw-loss = 0.38832807540893555, train/logprobs = tensor([[-88.4919, -90.5859],
        [-88.8516, -89.2592]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.38446044921875
RAW KL tensor(-86.7026, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 231: train/loss = -43.102684020996094, train/raw-loss = 0.6027038097381592, train/logprobs = tensor([[-88.9099, -89.3599],
        [-89.0770, -89.1362]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.41078186035156
RAW KL tensor(-88.3824, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 232: train/loss = -42.91487503051758, train/raw-loss = 0.5260202884674072, train/logprobs = tensor([[-89.1602, -89.6987],
        [-89.3676, -88.9735]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.88179779052734
RAW KL tensor(-87.0164, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 233: train/loss = -43.02931213378906, train/raw-loss = 0.5987844467163086, train/logprobs = tensor([[-90.2155, -89.7142],
        [-90.2844, -89.2103]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.25619506835938
RAW KL tensor(-82.6746, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 234: train/loss = -42.456947326660156, train/raw-loss = 0.6225558519363403, train/logprobs = tensor([[-88.8666, -88.8630],
        [-89.3566, -89.0541]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.15901184082031
RAW KL tensor(-86.2450, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 235: train/loss = -42.55476379394531, train/raw-loss = 0.5636934041976929, train/logprobs = tensor([[-88.2477, -88.6235],
        [-88.7169, -88.3942]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.23692321777344
RAW KL tensor(-87.1743, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 236: train/loss = -42.924644470214844, train/raw-loss = 0.42518240213394165, train/logprobs = tensor([[-87.4140, -89.0135],
        [-88.3143, -88.3747]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.69966125488281
RAW KL tensor(-87.3571, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 237: train/loss = -42.708961486816406, train/raw-loss = 0.6146111488342285, train/logprobs = tensor([[-87.5182, -87.8226],
        [-88.0042, -87.9397]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.64714813232422
RAW KL tensor(-87.1298, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 238: train/loss = -42.79090881347656, train/raw-loss = 0.5346119403839111, train/logprobs = tensor([[-88.1064, -88.6413],
        [-88.6108, -88.3656]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.65103912353516
RAW KL tensor(-88.0240, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 239: train/loss = -42.858848571777344, train/raw-loss = 0.66068434715271, train/logprobs = tensor([[-88.5822, -88.5919],
        [-88.7702, -88.6472]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.0390625
RAW KL tensor(-85.9779, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 177])
Epoch 0, Step 240: train/loss = -42.9388427734375, train/raw-loss = 0.6003491878509521, train/logprobs = tensor([[-89.1426, -89.2607],
        [-89.3475, -88.9787]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.07838439941406
RAW KL tensor(-87.1222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 131])
Epoch 0, Step 241: train/loss = -42.97599792480469, train/raw-loss = 0.5270241498947144, train/logprobs = tensor([[-89.2796, -89.7568],
        [-89.4046, -89.0344]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.00605010986328
RAW KL tensor(-87.6081, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 242: train/loss = -43.23368453979492, train/raw-loss = 0.6585797071456909, train/logprobs = tensor([[-89.8625, -89.9255],
        [-89.9437, -89.8622]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.7845230102539
RAW KL tensor(-87.4357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 243: train/loss = -42.796695709228516, train/raw-loss = 0.6584006547927856, train/logprobs = tensor([[-89.6062, -89.0579],
        [-89.6775, -88.9874]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.91020202636719
RAW KL tensor(-84.6736, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 244: train/loss = -42.77013397216797, train/raw-loss = 0.5082387924194336, train/logprobs = tensor([[-88.7406, -89.6011],
        [-88.9444, -88.8933]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.55675506591797
RAW KL tensor(-87.4314, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 245: train/loss = -42.99812316894531, train/raw-loss = 0.6213791370391846, train/logprobs = tensor([[-88.7064, -88.9958],
        [-88.9986, -88.9668]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.239013671875
RAW KL tensor(-86.0970, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 140])
Epoch 0, Step 246: train/loss = -42.82722473144531, train/raw-loss = 0.6592466235160828, train/logprobs = tensor([[-88.3037, -88.8151],
        [-88.4245, -88.7975]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.97294616699219
RAW KL tensor(-87.6491, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 247: train/loss = -43.2818603515625, train/raw-loss = 0.41240939497947693, train/logprobs = tensor([[-88.4245, -89.9944],
        [-88.6026, -88.6700]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.38853454589844
RAW KL tensor(-86.8549, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 248: train/loss = -42.98926544189453, train/raw-loss = 0.5043020844459534, train/logprobs = tensor([[-88.7596, -89.6979],
        [-88.8749, -88.8020]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.98713684082031
RAW KL tensor(-88.0446, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 249: train/loss = -42.851829528808594, train/raw-loss = 0.6557941436767578, train/logprobs = tensor([[-89.5299, -89.0244],
        [-89.6498, -88.9881]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.01524353027344
RAW KL tensor(-87.3951, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 250: train/loss = -43.077701568603516, train/raw-loss = 0.4535811245441437, train/logprobs = tensor([[-88.7850, -90.2903],
        [-89.0576, -89.3446]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.06256866455078
RAW KL tensor(-87.7705, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 229])
Epoch 0, Step 251: train/loss = -43.10141372680664, train/raw-loss = 0.5114967226982117, train/logprobs = tensor([[-89.2499, -90.2672],
        [-89.4597, -89.5086]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.22581481933594
RAW KL tensor(-87.9398, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 158])
Epoch 0, Step 252: train/loss = -42.97504425048828, train/raw-loss = 0.6616840958595276, train/logprobs = tensor([[-89.8358, -89.5065],
        [-89.7693, -89.3102]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.2734603881836
RAW KL tensor(-87.1893, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 253: train/loss = -43.25938415527344, train/raw-loss = 0.6894751787185669, train/logprobs = tensor([[-89.8324, -89.7615],
        [-89.9605, -89.8746]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.89772033691406
RAW KL tensor(-86.9059, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 254: train/loss = -43.247650146484375, train/raw-loss = 0.5096744298934937, train/logprobs = tensor([[-88.9380, -89.9505],
        [-89.3275, -89.3932]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.51465606689453
RAW KL tensor(-86.8110, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 255: train/loss = -43.46238708496094, train/raw-loss = 0.43849241733551025, train/logprobs = tensor([[-88.8991, -90.5185],
        [-89.4757, -89.8017]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.80176544189453
RAW KL tensor(-87.6320, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 256: train/loss = -43.48719787597656, train/raw-loss = 0.3698631525039673, train/logprobs = tensor([[-88.7967, -90.6541],
        [-89.4641, -89.4213]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.71411895751953
RAW KL tensor(-87.9758, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 257: train/loss = -43.251434326171875, train/raw-loss = 0.401691198348999, train/logprobs = tensor([[-89.3585, -91.1954],
        [-89.6757, -89.9237]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.30625915527344
RAW KL tensor(-88.1375, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 116])
Epoch 0, Step 258: train/loss = -43.26328659057617, train/raw-loss = 0.6465352773666382, train/logprobs = tensor([[-90.2737, -89.4709],
        [-90.3649, -89.3706]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.81964111328125
RAW KL tensor(-88.0394, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 259: train/loss = -43.26118087768555, train/raw-loss = 0.65981125831604, train/logprobs = tensor([[-90.1598, -89.6610],
        [-90.2362, -89.5953]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.84197998046875
RAW KL tensor(-86.6346, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 260: train/loss = -43.04478454589844, train/raw-loss = 0.5716707110404968, train/logprobs = tensor([[-89.5183, -90.4156],
        [-89.7636, -90.0505]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.23291015625
RAW KL tensor(-88.9607, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 261: train/loss = -43.367576599121094, train/raw-loss = 0.44913381338119507, train/logprobs = tensor([[-89.3158, -90.7915],
        [-89.6067, -89.6786]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.6334228515625
RAW KL tensor(-87.7119, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 262: train/loss = -43.22545623779297, train/raw-loss = 0.5399737358093262, train/logprobs = tensor([[-89.2988, -90.0440],
        [-89.2578, -89.1384]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.5308609008789
RAW KL tensor(-87.1737, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 263: train/loss = -43.472755432128906, train/raw-loss = 0.576901376247406, train/logprobs = tensor([[-90.1851, -90.7263],
        [-90.2754, -90.2007]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.09930419921875
RAW KL tensor(-87.5346, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 231])
Epoch 0, Step 264: train/loss = -43.501441955566406, train/raw-loss = 0.45832329988479614, train/logprobs = tensor([[-89.5184, -90.4939],
        [-89.7222, -89.3420]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.9195327758789
RAW KL tensor(-87.5802, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 265: train/loss = -43.43020248413086, train/raw-loss = 0.6237553358078003, train/logprobs = tensor([[-89.6414, -90.3131],
        [-89.9694, -90.3481]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.10791015625
RAW KL tensor(-87.8147, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 120])
Epoch 0, Step 266: train/loss = -43.342628479003906, train/raw-loss = 0.6457099914550781, train/logprobs = tensor([[-90.0110, -89.5329],
        [-90.3934, -89.6883]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.97666931152344
RAW KL tensor(-84.9333, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 161])
Epoch 0, Step 267: train/loss = -42.843841552734375, train/raw-loss = 0.6451832056045532, train/logprobs = tensor([[-89.7823, -89.6513],
        [-90.0012, -89.6651]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.97805786132812
RAW KL tensor(-87.1669, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 170])
Epoch 0, Step 268: train/loss = -43.063011169433594, train/raw-loss = 0.5723162889480591, train/logprobs = tensor([[-90.0237, -90.1669],
        [-90.6001, -90.1070]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.27066040039062
RAW KL tensor(-87.3878, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 269: train/loss = -43.274444580078125, train/raw-loss = 0.5916990041732788, train/logprobs = tensor([[-89.6640, -90.5890],
        [-90.1204, -90.5218]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.73228454589844
RAW KL tensor(-87.6370, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 192])
Epoch 0, Step 270: train/loss = -42.89550018310547, train/raw-loss = 0.5875919461250305, train/logprobs = tensor([[-89.0913, -89.5331],
        [-89.3604, -89.2809]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.9661865234375
RAW KL tensor(-87.7708, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 245])
Epoch 0, Step 271: train/loss = -43.70602798461914, train/raw-loss = 0.31102508306503296, train/logprobs = tensor([[-88.7461, -91.6776],
        [-89.3808, -90.2537]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.03410339355469
RAW KL tensor(-87.3760, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 272: train/loss = -43.25006103515625, train/raw-loss = 0.5373356342315674, train/logprobs = tensor([[-88.7524, -89.8319],
        [-89.1105, -89.3942]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.57479858398438
RAW KL tensor(-87.5423, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 273: train/loss = -43.170108795166016, train/raw-loss = 0.5146142840385437, train/logprobs = tensor([[-88.6756, -89.7707],
        [-88.9506, -89.1887]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.36943817138672
RAW KL tensor(-87.9990, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 274: train/loss = -43.25943374633789, train/raw-loss = 0.6395649909973145, train/logprobs = tensor([[-89.2202, -89.4178],
        [-89.4897, -89.4628]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.79800415039062
RAW KL tensor(-87.9377, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 275: train/loss = -43.56460952758789, train/raw-loss = 0.4994767904281616, train/logprobs = tensor([[-89.4320, -90.4023],
        [-89.9371, -89.8851]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.128173828125
RAW KL tensor(-88.4893, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 186])
Epoch 0, Step 276: train/loss = -43.82843017578125, train/raw-loss = 0.3401287794113159, train/logprobs = tensor([[-89.1159, -91.2712],
        [-89.7309, -89.8277]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.33712005615234
RAW KL tensor(-88.0253, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 277: train/loss = -43.67945861816406, train/raw-loss = 0.5749309062957764, train/logprobs = tensor([[-90.2805, -90.8733],
        [-90.4270, -90.4995]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.50877380371094
RAW KL tensor(-88.2057, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 278: train/loss = -43.34410858154297, train/raw-loss = 0.5064882040023804, train/logprobs = tensor([[-90.2009, -90.7360],
        [-90.4996, -90.0781]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.7011947631836
RAW KL tensor(-86.2259, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 182])
Epoch 0, Step 279: train/loss = -42.98345947265625, train/raw-loss = 0.5570681691169739, train/logprobs = tensor([[-89.5472, -90.2647],
        [-89.6447, -89.6274]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.0810546875
RAW KL tensor(-88.6222, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 222])
Epoch 0, Step 280: train/loss = -43.44181442260742, train/raw-loss = 0.45370638370513916, train/logprobs = tensor([[-89.8516, -91.2527],
        [-90.1186, -90.2372]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.7910385131836
RAW KL tensor(-86.8816, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 172])
Epoch 0, Step 281: train/loss = -43.275794982910156, train/raw-loss = 0.6705222129821777, train/logprobs = tensor([[-90.5673, -89.7956],
        [-90.5448, -89.6771]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.89263916015625
RAW KL tensor(-88.3326, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 282: train/loss = -42.876731872558594, train/raw-loss = 0.5361884832382202, train/logprobs = tensor([[-89.3521, -90.2047],
        [-89.7817, -89.7034]], device='cuda:0', grad_fn=<DivBackward0>), KL = -86.82583618164062
RAW KL tensor(-87.8345, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 283: train/loss = -43.65840530395508, train/raw-loss = 0.34837985038757324, train/logprobs = tensor([[-89.4328, -91.3160],
        [-90.0333, -90.0202]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.01358032226562
RAW KL tensor(-88.3971, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 284: train/loss = -43.67072296142578, train/raw-loss = 0.5092024207115173, train/logprobs = tensor([[-89.7071, -90.5533],
        [-90.2720, -89.9784]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.35984802246094
RAW KL tensor(-88.6136, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 285: train/loss = -43.57648468017578, train/raw-loss = 0.47006040811538696, train/logprobs = tensor([[-90.0002, -90.4603],
        [-90.5601, -89.8568]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.09308624267578
RAW KL tensor(-87.0089, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 355])
Epoch 0, Step 286: train/loss = -43.43144989013672, train/raw-loss = 0.4452759921550751, train/logprobs = tensor([[-89.2586, -91.0114],
        [-89.7866, -90.2343]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.75344848632812
RAW KL tensor(-88.1998, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 287: train/loss = -43.647743225097656, train/raw-loss = 0.5133360624313354, train/logprobs = tensor([[-90.0398, -90.4656],
        [-90.6357, -90.0602]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.32215881347656
RAW KL tensor(-88.6407, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 288: train/loss = -43.68019485473633, train/raw-loss = 0.3964480757713318, train/logprobs = tensor([[-90.4433, -91.0995],
        [-90.8226, -89.8409]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.15328979492188
RAW KL tensor(-88.2486, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 289: train/loss = -43.61423873901367, train/raw-loss = 0.5938284397125244, train/logprobs = tensor([[-90.2188, -90.3854],
        [-90.6179, -90.3561]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.4161376953125
RAW KL tensor(-87.4106, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 113])
Epoch 0, Step 290: train/loss = -43.33808135986328, train/raw-loss = 0.6100261211395264, train/logprobs = tensor([[-89.5178, -89.7498],
        [-89.6818, -89.5255]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.89620971679688
RAW KL tensor(-88.0355, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 242])
Epoch 0, Step 291: train/loss = -43.69038009643555, train/raw-loss = 0.5311260223388672, train/logprobs = tensor([[-89.7905, -90.4221],
        [-90.0464, -89.9172]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.44300842285156
RAW KL tensor(-88.3615, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 292: train/loss = -43.643428802490234, train/raw-loss = 0.41684433817863464, train/logprobs = tensor([[-90.2075, -91.2189],
        [-90.5339, -89.9954]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.12054443359375
RAW KL tensor(-88.4728, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 294])
Epoch 0, Step 293: train/loss = -43.38597106933594, train/raw-loss = 0.3660796880722046, train/logprobs = tensor([[-90.0163, -91.4224],
        [-90.3962, -89.9811]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.50409698486328
RAW KL tensor(-87.7200, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 232])
Epoch 0, Step 294: train/loss = -43.569297790527344, train/raw-loss = 0.5229867696762085, train/logprobs = tensor([[-90.0696, -90.9008],
        [-90.3661, -90.2904]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.18457794189453
RAW KL tensor(-90.0055, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 295: train/loss = -44.192352294921875, train/raw-loss = 0.4961339831352234, train/logprobs = tensor([[-90.9188, -91.6348],
        [-91.3172, -90.9798]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.37696838378906
RAW KL tensor(-89.4103, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 296: train/loss = -44.22534942626953, train/raw-loss = 0.2827838659286499, train/logprobs = tensor([[-90.3154, -91.7719],
        [-90.9169, -90.0198]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.01627349853516
RAW KL tensor(-87.4959, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 297: train/loss = -43.908756256103516, train/raw-loss = 0.4495293200016022, train/logprobs = tensor([[-90.5485, -91.8337],
        [-91.1123, -90.9255]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.7165756225586
RAW KL tensor(-88.3527, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 298: train/loss = -43.96694564819336, train/raw-loss = 0.4312560558319092, train/logprobs = tensor([[-90.1723, -91.1613],
        [-90.8380, -90.1882]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.79640197753906
RAW KL tensor(-88.9777, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 299: train/loss = -43.82862854003906, train/raw-loss = 0.5323138236999512, train/logprobs = tensor([[-90.5092, -90.6223],
        [-90.7149, -90.0631]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.72187805175781
RAW KL tensor(-88.3374, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 134])
Epoch 0, Step 300: train/loss = -43.91417694091797, train/raw-loss = 0.6378684639930725, train/logprobs = tensor([[-90.9953, -91.5719],
        [-91.2785, -91.6236]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.10408020019531
RAW KL tensor(-88.8075, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 301: train/loss = -43.815738677978516, train/raw-loss = 0.5220591425895691, train/logprobs = tensor([[-90.0572, -90.8163],
        [-90.3719, -90.1321]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.67559051513672
RAW KL tensor(-89.0998, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 302: train/loss = -44.08525466918945, train/raw-loss = 0.4302467107772827, train/logprobs = tensor([[-90.5484, -92.2721],
        [-90.8251, -91.0091]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.03099822998047
RAW KL tensor(-89.2438, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 160])
Epoch 0, Step 303: train/loss = -43.53572082519531, train/raw-loss = 0.5112593173980713, train/logprobs = tensor([[-89.7584, -90.5606],
        [-89.9295, -89.8003]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.09396362304688
RAW KL tensor(-88.5910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 304: train/loss = -44.07414627075195, train/raw-loss = 0.42672574520111084, train/logprobs = tensor([[-91.0379, -91.9401],
        [-91.4340, -90.7928]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.00175476074219
RAW KL tensor(-87.1360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 125])
Epoch 0, Step 305: train/loss = -43.444087982177734, train/raw-loss = 0.5855302214622498, train/logprobs = tensor([[-90.3040, -89.6794],
        [-90.6630, -89.5583]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.05924224853516
RAW KL tensor(-88.5231, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 306: train/loss = -43.82126998901367, train/raw-loss = 0.4193691313266754, train/logprobs = tensor([[-89.9794, -91.5309],
        [-90.6229, -90.6838]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.48127746582031
RAW KL tensor(-87.8293, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 159])
Epoch 0, Step 307: train/loss = -43.3181266784668, train/raw-loss = 0.48429054021835327, train/logprobs = tensor([[-90.0122, -90.5281],
        [-90.4364, -89.8836]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.60482788085938
RAW KL tensor(-88.9025, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 213])
Epoch 0, Step 308: train/loss = -44.03289794921875, train/raw-loss = 0.395596444606781, train/logprobs = tensor([[-90.5657, -91.7948],
        [-91.0436, -90.7306]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.85697937011719
RAW KL tensor(-89.2370, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 309: train/loss = -44.17437744140625, train/raw-loss = 0.4031701385974884, train/logprobs = tensor([[-90.0775, -91.5059],
        [-90.7591, -90.5709]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.15509796142578
RAW KL tensor(-88.4086, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 198])
Epoch 0, Step 310: train/loss = -43.2840576171875, train/raw-loss = 0.41056132316589355, train/logprobs = tensor([[-90.2428, -91.3197],
        [-90.6448, -90.0375]], device='cuda:0', grad_fn=<DivBackward0>), KL = -87.38924407958984
RAW KL tensor(-87.5637, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 224])
Epoch 0, Step 311: train/loss = -43.617713928222656, train/raw-loss = 0.4408504366874695, train/logprobs = tensor([[-89.9697, -91.3049],
        [-90.6983, -90.5413]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.11712646484375
RAW KL tensor(-88.6468, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 312: train/loss = -43.862037658691406, train/raw-loss = 0.45431339740753174, train/logprobs = tensor([[-90.3942, -91.3333],
        [-91.0349, -90.7421]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.63269805908203
RAW KL tensor(-88.4855, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 313: train/loss = -43.9404411315918, train/raw-loss = 0.26169532537460327, train/logprobs = tensor([[-90.1403, -92.6280],
        [-90.8797, -90.7120]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.40428161621094
RAW KL tensor(-87.9613, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 447])
Epoch 0, Step 314: train/loss = -43.958282470703125, train/raw-loss = 0.28512415289878845, train/logprobs = tensor([[-90.0980, -92.4459],
        [-90.8784, -90.7178]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.48681640625
RAW KL tensor(-88.7702, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 202])
Epoch 0, Step 315: train/loss = -43.90576934814453, train/raw-loss = 0.31198763847351074, train/logprobs = tensor([[-89.7805, -91.8861],
        [-90.7868, -90.5939]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.43551635742188
RAW KL tensor(-88.7603, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 239])
Epoch 0, Step 316: train/loss = -44.16822052001953, train/raw-loss = 0.37508752942085266, train/logprobs = tensor([[-90.2730, -92.1202],
        [-90.9592, -90.7303]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.08660888671875
RAW KL tensor(-88.3357, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 189])
Epoch 0, Step 317: train/loss = -44.01595687866211, train/raw-loss = 0.49940571188926697, train/logprobs = tensor([[-90.7543, -91.4634],
        [-91.8531, -91.6196]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.03071594238281
RAW KL tensor(-88.8593, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 218])
Epoch 0, Step 318: train/loss = -44.15869140625, train/raw-loss = 0.4480198919773102, train/logprobs = tensor([[-90.5854, -92.1913],
        [-91.4275, -91.4861]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.21341705322266
RAW KL tensor(-90.1118, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 319: train/loss = -43.769432067871094, train/raw-loss = 0.40412962436676025, train/logprobs = tensor([[-90.3606, -91.6708],
        [-91.1243, -90.9896]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.34712219238281
RAW KL tensor(-89.4293, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 320: train/loss = -44.257957458496094, train/raw-loss = 0.35461127758026123, train/logprobs = tensor([[-90.0010, -91.9957],
        [-90.7474, -90.5642]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.22512817382812
RAW KL tensor(-90.6083, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 151])
Epoch 0, Step 321: train/loss = -44.01262283325195, train/raw-loss = 0.36047857999801636, train/logprobs = tensor([[-90.5764, -92.3537],
        [-91.2472, -91.0473]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.74620056152344
RAW KL tensor(-87.3316, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 183])
Epoch 0, Step 322: train/loss = -43.678104400634766, train/raw-loss = 0.44061389565467834, train/logprobs = tensor([[-90.6353, -91.0880],
        [-91.0821, -90.3416]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.23743438720703
RAW KL tensor(-88.4124, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 225])
Epoch 0, Step 323: train/loss = -43.76299285888672, train/raw-loss = 0.6552791595458984, train/logprobs = tensor([[-91.1048, -90.7141],
        [-91.2013, -90.6308]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.8365478515625
RAW KL tensor(-85.6951, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 99])
Epoch 0, Step 324: train/loss = -43.77031707763672, train/raw-loss = 0.3648366332054138, train/logprobs = tensor([[-90.3080, -92.5889],
        [-90.7946, -90.9889]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.27029418945312
RAW KL tensor(-89.2279, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 128])
Epoch 0, Step 325: train/loss = -43.62018585205078, train/raw-loss = 0.6297800540924072, train/logprobs = tensor([[-92.3365, -91.4287],
        [-92.1337, -90.8445]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.49992370605469
RAW KL tensor(-90.0679, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 153])
Epoch 0, Step 326: train/loss = -44.21222686767578, train/raw-loss = 0.404435396194458, train/logprobs = tensor([[-91.7332, -91.9380],
        [-92.3037, -91.0459]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.23332214355469
RAW KL tensor(-89.5260, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 327: train/loss = -43.856632232666016, train/raw-loss = 0.45385849475860596, train/logprobs = tensor([[-90.7749, -91.6902],
        [-91.3822, -90.9942]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.62097930908203
RAW KL tensor(-89.0583, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 328: train/loss = -44.12468719482422, train/raw-loss = 0.5127352476119995, train/logprobs = tensor([[-91.0256, -91.0773],
        [-91.5557, -90.7189]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.27484893798828
RAW KL tensor(-88.7313, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 166])
Epoch 0, Step 329: train/loss = -44.28397750854492, train/raw-loss = 0.37520527839660645, train/logprobs = tensor([[-90.7559, -91.8150],
        [-91.6488, -90.9926]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.318359375
RAW KL tensor(-89.5973, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 165])
Epoch 0, Step 330: train/loss = -44.242828369140625, train/raw-loss = 0.4423596262931824, train/logprobs = tensor([[-91.0445, -91.4499],
        [-91.9074, -91.0604]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.37037658691406
RAW KL tensor(-89.6957, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 147])
Epoch 0, Step 331: train/loss = -44.380836486816406, train/raw-loss = 0.4042355716228485, train/logprobs = tensor([[-91.4194, -91.6736],
        [-92.0826, -90.8163]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.57014465332031
RAW KL tensor(-89.7463, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 332: train/loss = -44.182861328125, train/raw-loss = 0.48373720049858093, train/logprobs = tensor([[-91.2702, -91.2670],
        [-91.9499, -90.9581]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.33320617675781
RAW KL tensor(-88.2087, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 126])
Epoch 0, Step 333: train/loss = -43.84838104248047, train/raw-loss = 0.557198166847229, train/logprobs = tensor([[-91.5480, -91.7561],
        [-91.4336, -91.0056]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.8111572265625
RAW KL tensor(-88.7404, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 141])
Epoch 0, Step 334: train/loss = -43.779273986816406, train/raw-loss = 0.5162690877914429, train/logprobs = tensor([[-91.3226, -92.4382],
        [-91.7997, -92.0318]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.59107971191406
RAW KL tensor(-89.5256, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 209])
Epoch 0, Step 335: train/loss = -44.41999053955078, train/raw-loss = 0.42063668370246887, train/logprobs = tensor([[-91.3815, -91.9648],
        [-91.9637, -91.1631]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.68125915527344
RAW KL tensor(-88.7671, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 336: train/loss = -44.07080078125, train/raw-loss = 0.5060057640075684, train/logprobs = tensor([[-91.4749, -91.4939],
        [-92.0830, -91.1796]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.15361785888672
RAW KL tensor(-89.1136, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 194])
Epoch 0, Step 337: train/loss = -44.12282180786133, train/raw-loss = 0.4227526783943176, train/logprobs = tensor([[-91.2867, -91.7765],
        [-91.7960, -90.9844]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.09114837646484
RAW KL tensor(-89.2053, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 208])
Epoch 0, Step 338: train/loss = -44.28801727294922, train/raw-loss = 0.47264018654823303, train/logprobs = tensor([[-91.5546, -91.8215],
        [-92.1831, -91.3667]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.52132415771484
RAW KL tensor(-89.4895, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 339: train/loss = -44.120094299316406, train/raw-loss = 0.43788057565689087, train/logprobs = tensor([[-91.3378, -91.1696],
        [-92.0935, -90.6321]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.11595153808594
RAW KL tensor(-91.4765, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 150])
Epoch 0, Step 340: train/loss = -44.393035888671875, train/raw-loss = 0.4013015627861023, train/logprobs = tensor([[-91.5808, -92.0640],
        [-92.3752, -91.3330]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.58867645263672
RAW KL tensor(-88.7819, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 148])
Epoch 0, Step 341: train/loss = -44.25054931640625, train/raw-loss = 0.39834561944007874, train/logprobs = tensor([[-91.3510, -91.3664],
        [-92.1178, -90.5741]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.29778289794922
RAW KL tensor(-90.1359, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 135])
Epoch 0, Step 342: train/loss = -43.94743728637695, train/raw-loss = 0.4861484169960022, train/logprobs = tensor([[-91.3052, -91.5026],
        [-91.8626, -90.9866]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.86717224121094
RAW KL tensor(-89.1272, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 211])
Epoch 0, Step 343: train/loss = -44.37194061279297, train/raw-loss = 0.413102388381958, train/logprobs = tensor([[-90.4910, -92.2062],
        [-91.2742, -91.5006]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.57008361816406
RAW KL tensor(-90.1911, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 244])
Epoch 0, Step 344: train/loss = -44.663753509521484, train/raw-loss = 0.4868241846561432, train/logprobs = tensor([[-92.0880, -92.2874],
        [-93.0521, -92.2082]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.30115509033203
RAW KL tensor(-88.7768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 195])
Epoch 0, Step 345: train/loss = -44.12456512451172, train/raw-loss = 0.4617405831813812, train/logprobs = tensor([[-91.2396, -91.0729],
        [-91.8340, -90.5500]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.17261505126953
RAW KL tensor(-91.6085, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 346: train/loss = -44.36711502075195, train/raw-loss = 0.3692999482154846, train/logprobs = tensor([[-90.9191, -92.8518],
        [-91.8607, -92.0024]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.47283172607422
RAW KL tensor(-83.4633, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 176])
Epoch 0, Step 347: train/loss = -43.98756408691406, train/raw-loss = 0.41214948892593384, train/logprobs = tensor([[-91.7128, -92.4473],
        [-92.6131, -91.8809]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.79942321777344
RAW KL tensor(-88.7541, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 348: train/loss = -44.11212158203125, train/raw-loss = 0.4912026524543762, train/logprobs = tensor([[-90.6433, -92.0756],
        [-91.0029, -91.2045]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.20664978027344
RAW KL tensor(-88.8811, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 181])
Epoch 0, Step 349: train/loss = -44.20399475097656, train/raw-loss = 0.38570624589920044, train/logprobs = tensor([[-90.7068, -91.7711],
        [-91.7282, -90.9319]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.17939758300781
RAW KL tensor(-88.5846, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 241])
Epoch 0, Step 350: train/loss = -43.947635650634766, train/raw-loss = 0.2184489369392395, train/logprobs = tensor([[-90.6569, -93.4152],
        [-91.9096, -91.6478]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.33216857910156
RAW KL tensor(-90.3757, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 302])
Epoch 0, Step 351: train/loss = -44.46477127075195, train/raw-loss = 0.30827000737190247, train/logprobs = tensor([[-91.2496, -92.2871],
        [-92.1790, -91.1378]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.54608154296875
RAW KL tensor(-89.2392, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 352: train/loss = -44.32299041748047, train/raw-loss = 0.2718590497970581, train/logprobs = tensor([[-90.7479, -92.3023],
        [-92.0390, -91.1124]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.189697265625
RAW KL tensor(-90.1703, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 353: train/loss = -44.36470031738281, train/raw-loss = 0.3049251437187195, train/logprobs = tensor([[-91.0273, -91.8309],
        [-92.3357, -90.8844]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.33924865722656
RAW KL tensor(-88.5517, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 457])
Epoch 0, Step 354: train/loss = -44.64923858642578, train/raw-loss = 0.25227096676826477, train/logprobs = tensor([[-91.1508, -93.7361],
        [-92.0430, -91.8628]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.80301666259766
RAW KL tensor(-90.3083, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 210])
Epoch 0, Step 355: train/loss = -44.55026626586914, train/raw-loss = 0.48452621698379517, train/logprobs = tensor([[-91.5952, -93.0147],
        [-92.8112, -92.6292]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.069580078125
RAW KL tensor(-89.4613, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 356: train/loss = -44.505069732666016, train/raw-loss = 0.5268139839172363, train/logprobs = tensor([[-91.5024, -92.7067],
        [-92.9140, -93.1309]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.06376647949219
RAW KL tensor(-89.6685, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 345])
Epoch 0, Step 357: train/loss = -44.11756134033203, train/raw-loss = 0.26249071955680847, train/logprobs = tensor([[-90.8406, -93.5065],
        [-91.8392, -91.7840]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.76010131835938
RAW KL tensor(-89.1608, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 358: train/loss = -44.667110443115234, train/raw-loss = 0.28920283913612366, train/logprobs = tensor([[-92.1811, -93.1568],
        [-93.1603, -91.5993]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.91262817382812
RAW KL tensor(-91.2909, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 359: train/loss = -44.553619384765625, train/raw-loss = 0.30163291096687317, train/logprobs = tensor([[-91.4034, -94.1257],
        [-92.4514, -92.8364]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.71049499511719
RAW KL tensor(-89.4438, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 249])
Epoch 0, Step 360: train/loss = -44.26637268066406, train/raw-loss = 0.3065488636493683, train/logprobs = tensor([[-90.8898, -93.1990],
        [-91.6835, -91.6690]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.14584350585938
RAW KL tensor(-89.9360, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 184])
Epoch 0, Step 361: train/loss = -44.431983947753906, train/raw-loss = 0.24947135150432587, train/logprobs = tensor([[-90.6573, -93.4467],
        [-91.6214, -91.7245]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.3629150390625
RAW KL tensor(-90.2560, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 190])
Epoch 0, Step 362: train/loss = -44.5605354309082, train/raw-loss = 0.42995887994766235, train/logprobs = tensor([[-91.4271, -93.0493],
        [-92.4148, -92.5259]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.98098754882812
RAW KL tensor(-90.0620, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 174])
Epoch 0, Step 363: train/loss = -44.708587646484375, train/raw-loss = 0.3538260757923126, train/logprobs = tensor([[-91.6881, -92.7112],
        [-92.8910, -92.1178]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.12482452392578
RAW KL tensor(-90.7411, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 364: train/loss = -44.75993347167969, train/raw-loss = 0.341531366109848, train/logprobs = tensor([[-91.7251, -92.7028],
        [-92.7921, -91.7164]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.20293426513672
RAW KL tensor(-88.9051, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 130])
Epoch 0, Step 365: train/loss = -44.364715576171875, train/raw-loss = 0.4217780530452728, train/logprobs = tensor([[-92.4674, -92.2659],
        [-92.8647, -91.3045]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.57299041748047
RAW KL tensor(-90.6114, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 456])
Epoch 0, Step 366: train/loss = -43.684844970703125, train/raw-loss = 0.3596023917198181, train/logprobs = tensor([[-92.1380, -93.9977],
        [-93.1909, -93.0485]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.08889770507812
RAW KL tensor(-89.2162, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 168])
Epoch 0, Step 367: train/loss = -44.670711517333984, train/raw-loss = 0.30582737922668457, train/logprobs = tensor([[-91.4841, -93.5411],
        [-92.2385, -92.1387]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.95307159423828
RAW KL tensor(-87.9902, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 155])
Epoch 0, Step 368: train/loss = -44.38517761230469, train/raw-loss = 0.2548809051513672, train/logprobs = tensor([[-92.1842, -93.6696],
        [-93.1225, -91.9823]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.28012084960938
RAW KL tensor(-89.5280, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 205])
Epoch 0, Step 369: train/loss = -44.44892883300781, train/raw-loss = 0.2959159016609192, train/logprobs = tensor([[-91.6707, -92.5490],
        [-93.0116, -91.4225]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.48969268798828
RAW KL tensor(-90.1122, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 233])
Epoch 0, Step 370: train/loss = -44.863555908203125, train/raw-loss = 0.2607685923576355, train/logprobs = tensor([[-91.5316, -94.4583],
        [-92.4149, -92.4626]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.24864959716797
RAW KL tensor(-90.3466, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 461])
Epoch 0, Step 371: train/loss = -44.48552703857422, train/raw-loss = 0.46575772762298584, train/logprobs = tensor([[-90.9437, -91.8514],
        [-91.8550, -91.4505]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.9025650024414
RAW KL tensor(-89.4175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 372: train/loss = -44.19500732421875, train/raw-loss = 0.47503942251205444, train/logprobs = tensor([[-91.3211, -92.2444],
        [-92.7004, -92.4413]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.340087890625
RAW KL tensor(-89.0456, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 173])
Epoch 0, Step 373: train/loss = -44.63081359863281, train/raw-loss = 0.22936683893203735, train/logprobs = tensor([[-91.4115, -93.3154],
        [-92.9298, -91.6200]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.72036743164062
RAW KL tensor(-90.0899, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 374: train/loss = -44.21836853027344, train/raw-loss = 0.3733043670654297, train/logprobs = tensor([[-90.8124, -91.6214],
        [-92.4672, -91.4269]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.183349609375
RAW KL tensor(-90.5823, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 375: train/loss = -44.87508773803711, train/raw-loss = 0.34791815280914307, train/logprobs = tensor([[-91.8220, -92.5740],
        [-93.1479, -91.7606]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.44600677490234
RAW KL tensor(-91.4048, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 152])
Epoch 0, Step 376: train/loss = -44.518245697021484, train/raw-loss = 0.39896583557128906, train/logprobs = tensor([[-92.3172, -93.0433],
        [-93.0504, -91.8938]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.83441925048828
RAW KL tensor(-90.4371, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 188])
Epoch 0, Step 377: train/loss = -44.033084869384766, train/raw-loss = 0.4945203959941864, train/logprobs = tensor([[-91.3080, -92.0830],
        [-92.1978, -91.7603]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.05520629882812
RAW KL tensor(-89.9026, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 378: train/loss = -44.9008674621582, train/raw-loss = 0.38855111598968506, train/logprobs = tensor([[-93.9004, -92.8723],
        [-94.5874, -92.0122]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.57882690429688
RAW KL tensor(-89.0897, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 156])
Epoch 0, Step 379: train/loss = -44.719024658203125, train/raw-loss = 0.4225752651691437, train/logprobs = tensor([[-93.3299, -93.8231],
        [-93.6452, -92.6429]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.28318786621094
RAW KL tensor(-90.3054, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 178])
Epoch 0, Step 380: train/loss = -44.052520751953125, train/raw-loss = 0.4033687114715576, train/logprobs = tensor([[-92.2946, -92.6780],
        [-93.0416, -91.7935]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.91177368164062
RAW KL tensor(-90.2690, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 143])
Epoch 0, Step 381: train/loss = -44.03612518310547, train/raw-loss = 0.37766313552856445, train/logprobs = tensor([[-91.8868, -92.8979],
        [-92.2450, -91.4773]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.82757568359375
RAW KL tensor(-89.1762, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 382: train/loss = -44.62680435180664, train/raw-loss = 0.401181697845459, train/logprobs = tensor([[-92.3880, -92.8520],
        [-92.9533, -91.9048]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.05596923828125
RAW KL tensor(-89.9720, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 138])
Epoch 0, Step 383: train/loss = -44.967708587646484, train/raw-loss = 0.6283756494522095, train/logprobs = tensor([[-93.2076, -93.8251],
        [-93.7926, -94.1272]], device='cuda:0', grad_fn=<DivBackward0>), KL = -91.19216918945312
RAW KL tensor(-90.5707, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 206])
Epoch 0, Step 384: train/loss = -44.69919967651367, train/raw-loss = 0.37645575404167175, train/logprobs = tensor([[-91.5840, -93.3805],
        [-92.2162, -92.0772]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.15130615234375
RAW KL tensor(-88.5519, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 145])
Epoch 0, Step 385: train/loss = -44.42726135253906, train/raw-loss = 0.5074844360351562, train/logprobs = tensor([[-92.2862, -92.2754],
        [-92.8138, -91.9483]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.86949157714844
RAW KL tensor(-89.3724, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 386: train/loss = -44.92436218261719, train/raw-loss = 0.4601251184940338, train/logprobs = tensor([[-92.5910, -93.0666],
        [-93.4764, -92.6673]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.76897430419922
RAW KL tensor(-91.4165, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 123])
Epoch 0, Step 387: train/loss = -44.95258331298828, train/raw-loss = 0.5035074949264526, train/logprobs = tensor([[-92.3958, -93.9275],
        [-93.1367, -93.5831]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.91218566894531
RAW KL tensor(-89.9114, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 454])
Epoch 0, Step 388: train/loss = -44.720516204833984, train/raw-loss = 0.5852283835411072, train/logprobs = tensor([[-91.7334, -92.1946],
        [-92.1970, -92.1690]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.61148834228516
RAW KL tensor(-90.1975, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 389: train/loss = -44.203678131103516, train/raw-loss = 0.4870540499687195, train/logprobs = tensor([[-91.4760, -92.4907],
        [-92.1423, -92.0981]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.38146209716797
RAW KL tensor(-92.6457, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 129])
Epoch 0, Step 390: train/loss = -44.38576126098633, train/raw-loss = 0.44739648699760437, train/logprobs = tensor([[-92.7998, -95.5838],
        [-93.4609, -94.9201]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.66632843017578
RAW KL tensor(-90.4425, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 247])
Epoch 0, Step 391: train/loss = -43.956886291503906, train/raw-loss = 0.5139299631118774, train/logprobs = tensor([[-91.8969, -92.0796],
        [-92.2451, -91.5727]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.94163513183594
RAW KL tensor(-89.6852, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 162])
Epoch 0, Step 392: train/loss = -44.57635498046875, train/raw-loss = 0.3726194202899933, train/logprobs = tensor([[-92.2637, -93.0470],
        [-93.4210, -92.3545]], device='cuda:0', grad_fn=<DivBackward0>), KL = -89.89794158935547
RAW KL tensor(-90.4004, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 139])
Epoch 0, Step 393: train/loss = -44.95927429199219, train/raw-loss = 0.2998530864715576, train/logprobs = tensor([[-92.4486, -93.5853],
        [-93.5923, -92.4989]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.51825714111328
RAW KL tensor(-91.3976, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 115])
Epoch 0, Step 394: train/loss = -44.79677963256836, train/raw-loss = 0.30201637744903564, train/logprobs = tensor([[-92.0027, -93.3946],
        [-92.7930, -91.9675]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.19759368896484
RAW KL tensor(-90.2174, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 395: train/loss = -43.99423599243164, train/raw-loss = 0.3327403664588928, train/logprobs = tensor([[-91.2783, -92.7491],
        [-92.2879, -91.4020]], device='cuda:0', grad_fn=<DivBackward0>), KL = -88.65394592285156
RAW KL tensor(-90.4156, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 149])
Epoch 0, Step 396: train/loss = -44.90392303466797, train/raw-loss = 0.2817293107509613, train/logprobs = tensor([[-91.8117, -93.3147],
        [-92.8509, -91.7876]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.37130737304688
RAW KL tensor(-90.8513, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 146])
Epoch 0, Step 397: train/loss = -44.92927551269531, train/raw-loss = 0.5012761354446411, train/logprobs = tensor([[-92.7024, -93.1307],
        [-93.6720, -92.9927]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.86109924316406
RAW KL tensor(-90.2236, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 215])
Epoch 0, Step 398: train/loss = -45.04266357421875, train/raw-loss = 0.406277596950531, train/logprobs = tensor([[-92.4671, -93.7459],
        [-93.3879, -93.1483]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.89788055419922
RAW KL tensor(-90.6768, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 230])
Epoch 0, Step 399: train/loss = -44.757591247558594, train/raw-loss = 0.37239930033683777, train/logprobs = tensor([[-92.0276, -93.4720],
        [-93.0441, -92.6697]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.25997924804688
RAW KL tensor(-91.6431, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 187])
Epoch 0, Step 400: train/loss = -44.86503982543945, train/raw-loss = 0.2772126793861389, train/logprobs = tensor([[-93.1321, -93.9982],
        [-94.0932, -92.6120]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.28450012207031
RAW KL tensor(-90.8735, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 154])
Epoch 0, Step 401: train/loss = -45.07828903198242, train/raw-loss = 0.20971247553825378, train/logprobs = tensor([[-92.1718, -93.8391],
        [-93.7201, -92.2074]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.57599639892578
RAW KL tensor(-89.8699, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 164])
Epoch 0, Step 402: train/loss = -44.7978515625, train/raw-loss = 0.3243514597415924, train/logprobs = tensor([[-91.6878, -92.7725],
        [-93.0815, -92.0640]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.24441528320312
RAW KL tensor(-91.4861, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 157])
Epoch 0, Step 403: train/loss = -45.171756744384766, train/raw-loss = 0.22811678051948547, train/logprobs = tensor([[-92.5508, -93.9287],
        [-94.1309, -92.6811]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.79974365234375
RAW KL tensor(-89.9028, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 133])
Epoch 0, Step 404: train/loss = -45.13185119628906, train/raw-loss = 0.22451727092266083, train/logprobs = tensor([[-91.7554, -94.3647],
        [-93.2904, -92.9611]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.71273803710938
RAW KL tensor(-90.5422, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 212])
Epoch 0, Step 405: train/loss = -44.90962219238281, train/raw-loss = 0.369601845741272, train/logprobs = tensor([[-92.1332, -93.0803],
        [-93.0850, -92.1227]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.5584487915039
RAW KL tensor(-90.7400, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 137])
Epoch 0, Step 406: train/loss = -44.81696701049805, train/raw-loss = 0.4010264575481415, train/logprobs = tensor([[-93.5672, -93.4834],
        [-94.1086, -92.2946]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.43597412109375
RAW KL tensor(-89.2910, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 179])
Epoch 0, Step 407: train/loss = -44.68083190917969, train/raw-loss = 0.3811566233634949, train/logprobs = tensor([[-92.9832, -93.5763],
        [-93.6512, -92.4353]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.12397766113281
RAW KL tensor(-91.1894, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 169])
Epoch 0, Step 408: train/loss = -45.098243713378906, train/raw-loss = 0.3676411211490631, train/logprobs = tensor([[-92.8337, -93.3734],
        [-93.8450, -92.4882]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.93177032470703
RAW KL tensor(-91.0175, device='cuda:0', grad_fn=<ToCopyBackward0>)
SHAPES: torch.Size([4, 175])
Epoch 0, Step 409: train/loss = -45.0716438293457, train/raw-loss = 0.3331056833267212, train/logprobs = tensor([[-92.0908, -93.8393],
        [-93.6417, -93.1905]], device='cuda:0', grad_fn=<DivBackward0>), KL = -90.80950164794922
