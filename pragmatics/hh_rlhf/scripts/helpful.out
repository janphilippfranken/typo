INFO 03-01 12:14:27 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 03-01 12:14:34 llm_engine.py:275] # GPU blocks: 27161, # CPU blocks: 2048
INFO 03-01 12:14:35 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-01 12:14:35 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 03-01 12:14:38 model_runner.py:547] Graph capturing finished in 3 secs.
Skipping example 4
Skipping example 18
Skipping example 25
Skipping example 28
Skipping example 32
Skipping example 39
Skipping example 47
Skipping example 55
Skipping example 95
Skipping example 110
Skipping example 170
Skipping example 171
Skipping example 195
Skipping example 216
Skipping example 224
Skipping example 251
Skipping example 261
Skipping example 270
Skipping example 272
Skipping example 297
Skipping example 318
Skipping example 329
