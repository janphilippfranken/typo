INFO 03-02 17:10:40 llm_engine.py:70] Initializing an LLM engine with config: model='/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/merged-no-icl/ppo-beta-0.1-iteration-1-0-1k/epoch-0/', tokenizer='/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/merged-no-icl/ppo-beta-0.1-iteration-1-0-1k/epoch-0/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/trained_models/Mistral-7B-v0.1/merged-no-icl/ppo-beta-0.1-iteration-1-0-1k/epoch-0/', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 03-02 17:10:50 llm_engine.py:275] # GPU blocks: 27161, # CPU blocks: 2048
INFO 03-02 17:10:51 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-02 17:10:51 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 03-02 17:10:54 model_runner.py:547] Graph capturing finished in 3 secs.
Skipping example 3009
Skipping example 3017
Skipping example 3018
Skipping example 3172
Skipping example 3186
Skipping example 3192
Skipping example 3253
Skipping example 3287
Skipping example 3403
Skipping example 3496
Skipping example 3503
Skipping example 3508
Skipping example 3609
Skipping example 3671
Skipping example 3723
Skipping example 3747
Skipping example 3826
Skipping example 3965
Skipping example 4212
Skipping example 4216
Skipping example 4230
