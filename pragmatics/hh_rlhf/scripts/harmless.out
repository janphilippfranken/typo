INFO 03-01 12:34:52 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/scr/jphilipp/scai/pretrained_models/Mistral-7B-v0.1', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 03-01 12:34:59 llm_engine.py:275] # GPU blocks: 27161, # CPU blocks: 2048
INFO 03-01 12:35:00 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 03-01 12:35:00 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 03-01 12:35:03 model_runner.py:547] Graph capturing finished in 3 secs.
Skipping example 1
Skipping example 6
Skipping example 8
Skipping example 11
Skipping example 12
Skipping example 15
Skipping example 18
Skipping example 22
Skipping example 23
Skipping example 26
Skipping example 34
Skipping example 35
Skipping example 38
Skipping example 41
Skipping example 42
Skipping example 43
Skipping example 45
Skipping example 46
Skipping example 48
Skipping example 53
Skipping example 56
Skipping example 58
Skipping example 59
Skipping example 63
Skipping example 67
Skipping example 69
Skipping example 75
Skipping example 79
Skipping example 80
Skipping example 83
Skipping example 84
Skipping example 85
Skipping example 90
Skipping example 94
Skipping example 97
Skipping example 101
