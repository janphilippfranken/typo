wandb_version: 1

model:
  desc: null
  value:
    model_type: huggingface
    name: mistral_tiny_base
    model_config:
      pretrained_model_name_or_path: openaccess-ai-collective/tiny-mistral
      cache_dir: /Users/iphilipp/Documents/research/scai-tuning/pragmatics/conf/model/local_cache
    tokenizer_config:
      pretrained_model_name_or_path: openaccess-ai-collective/tiny-mistral
      cache_dir: /Users/iphilipp/Documents/research/scai-tuning/pragmatics/conf/model/local_cache
      model_max_length: 2048
data:
  desc: null
  value:
    data_path: data/train.json
    n_responses: 2
wandb:
  desc: null
  value:
    project: scai-tuning
    log_model: checkpoint
    name: sft_hh_cdpo
training:
  desc: null
  value:
    n_epochs: 1000
    lr: 1.0e-06
    train_batch_size: 1
    eval_batch_size: 1
    train_split: 0.9
    checkpoint_dir: conf/model/local_cache/checkpoints
    loss: constitutional_dpo
    eval_steps: 100
    save_every: 100
    max_grad_norm: 10.0
    num_warmup_steps: 150
_wandb:
  desc: null
  value:
    python_version: 3.10.0
    cli_version: 0.16.0
    framework: huggingface
    huggingface_version: 4.35.2
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1707514392.724719
    t:
      1:
      - 1
      - 11
      - 49
      - 50
      - 51
      - 55
      - 71
      2:
      - 1
      - 11
      - 49
      - 50
      - 51
      - 55
      - 71
      3:
      - 13
      - 16
      - 23
      4: 3.10.0
      5: 0.16.0
      6: 4.35.2
      8:
      - 4
      - 5
      13: darwin-arm64
